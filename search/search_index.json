{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Hepapi Knowledge Hub","text":""},{"location":"#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This repository was created with the singular goal of fostering collaboration, knowledge exchange, and continuous learning among our team members at Hepapi.</p> <p>We recognize that knowledge is power, and in our constantly evolving field, it's crucial to keep up with the latest technologies, methodologies, and best practices.</p> <p>This repository is a live document, and we encourage everyone to contribute. If you have something valuable to share, don't hesitate to make a contribution. Remember, what may be obvious to you could be new to someone else.</p>"},{"location":"#help-us-improve","title":"Help us improve","text":"<p>We are always looking for ways to improve our documentation. If you have any suggestions, please feel free to open an issue or a pull request.</p> <p>Follow the docs on: How to contribute</p>"},{"location":"repo-credit/","title":"Credits","text":"File Contributors .github/workflows/deploy-mkdocs-website.yml Oguzhan Yilmaz,ersinsari13 .github/workflows/git-contributors-per-file.sh ersinsari13 .gitignore Oguzhan Yilmaz .vscode/settings.json Oguzhan Yilmaz,deniz-icin LICENSE Oguzhan Yilmaz README.md Oguzhan Yilmaz ansible/ansible-installations.md ersinsari13 ansible/ansible-roles.md ersinsari13 ansible/ansible.md ersinsari13 ansible/config-file.md ersinsari13 ansible/inventory-file.md ersinsari13 ansible/playbook.md ersinsari13 ansible/what-is-ansible.md ersinsari13 aws/cli.md Oguzhan Yilmaz azure/agent-installation.md Utku Toraman falcon-logscale/agent-installation.md Erdem Do\u011fanay falcon-logscale/falcon-logscale-installation-docker.md Erdem Do\u011fanay falcon-logscale/humioinstallation.md Erdem Do\u011fanay falcon-logscale/humiosetup.md Erdem Do\u011fanay falcon-logscale/javainstallation.md Erdem Do\u011fanay falcon-logscale/kafkainstallation.md Erdem Do\u011fanay falcon-logscale/zookeeperinstallation.md Erdem Do\u011fanay git/Commands.md Onur Ozcelik git/Description.md Onur Ozcelik git/installation.md Onur Ozcelik how-to-contribute/about-markdown.md Oguzhan Yilmaz how-to-contribute/about-mkdocs.md Oguzhan Yilmaz images/hepapi-logo.png Oguzhan Yilmaz index.md Oguzhan Yilmaz jenkins/README.md Oguzhan Yilmaz jenkins/jenkins-installation.md Erdem Do\u011fanay jenkins/shared-library.md ersinsari13 k8s-engine/k3s-installation.md Erdem Do\u011fanay k8s-engine/rke2-ha/rke2-ha-etcd-restore.md Erdem Do\u011fanay,Oguzhan Yilmaz k8s-engine/rke2-ha/rke2-highly-available-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz k8s-engine/rke2-installation-ansible.md Erdem Do\u011fanay,Utku Toraman k8s-engine/rke2-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman k8s-storage/longhorn.md Erdem Do\u011fanay k8s-storage/nfs-install.md Erdem Do\u011fanay kubernetes/eks/pod-security-group.yaml ersinsari13 kubernetes/keda/keda.md ersinsari13 linux/shell/ampersand-nohup.md can linux/shell/cat.md Oguzhan Yilmaz linux/shell/chtsh.md Erdem Do\u011fanay linux/shell/jobs-bg-fg.md can linux/shell/netstat.md Erdem Do\u011fanay linux/shell/nmap.md Erdem Do\u011fanay linux/shell/nslookup.md Erdem Do\u011fanay linux/shell/scp.md Erdem Do\u011fanay linux/shell/script.md Oguzhan Yilmaz linux/tips.md Oguzhan Yilmaz linux/tooling.md Oguzhan Yilmaz nexus/docker-hosted-repo.md deniz-icin nexus/docker-proxy-repo.md deniz-icin nexus/nexus-installation.md deniz-icin nexus/nexus-user-and-roles.md deniz-icin nexus/pull-to-kubernetes.md deniz-icin nexus/registry-configuration.md deniz-icin postgres/backup-restore.md Oguzhan Yilmaz postgres/configuration.md Oguzhan Yilmaz postgres/poc-backup-restore.md Oguzhan Yilmaz postgres/psql.md Oguzhan Yilmaz rancher/rancher-installation.md Utku Toraman,deniz-icin repo-credit.md ersinsari13 sealed-secrets/sealed-secrets.md sametustaoglu sumo/sumo-linux-collector.md can sumo/sumo-local-file-management.md can sumo/sumo-windows-collector.md can vagrant/vagrant-quickstart.md Utku Toraman git.md Onur Ozcelik mkdocs.yml Erdem Do\u011fanay,Oguzhan Yilmaz,Onur Ozcelik,Utku Toraman,can,deniz-icin,ersinsari13,sametustaoglu"},{"location":"ansible/ansible-installations/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core==2.12.3\n</code></pre>"},{"location":"ansible/ansible-installations/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"ansible/ansible-installations/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre> <p>Lastly you can install the Ansible software with:</p> <pre><code>sudo apt install ansible -y\n</code></pre>"},{"location":"ansible/ansible-installations/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"ansible/ansible-roles/","title":"Roles","text":""},{"location":"ansible/ansible-roles/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"ansible/ansible-roles/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"ansible/ansible-roles/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic</p> <p>my_role/ |-- defaults/    |   |-- main.yml |-- files/       |-- handlers/    |   |-- main.yml |-- meta/         |   |-- main.yml |-- tasks/       |   |-- main.yml |-- templates/   |-- tests/       |-- vars/        |   |-- main.yml |-- README.md     </p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"ansible/ansible-roles/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"ansible/ansible/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core\n</code></pre>"},{"location":"ansible/ansible/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"ansible/ansible/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre>"},{"location":"ansible/ansible/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"ansible/ansible/#what-is-ansible","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <p>1-Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators. 2-Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems. 3-Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times. 4-Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality. 5-Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures. 6-Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows. 7-Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</p>"},{"location":"ansible/ansible/#inventory-file-and-building-an-inventory","title":"Inventory file and Building an inventory","text":"<p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"ansible/ansible/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"ansible/ansible/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role. </p>"},{"location":"ansible/ansible/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"ansible/ansible/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"ansible/ansible/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"ansible/ansible/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"ansible/ansible/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"ansible/ansible/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"ansible/ansible/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic my_role/ |-- defaults/ |   |-- main.yml |-- files/ |-- handlers/ |   |-- main.yml |-- meta/ |   |-- main.yml |-- tasks/ |   |-- main.yml |-- templates/ |-- tests/ |-- vars/ |   |-- main.yml |-- README.md</p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"ansible/ansible/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"ansible/config-file/","title":"Configuration File","text":""},{"location":"ansible/config-file/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"ansible/inventory-file/","title":"Inventory File","text":"<p>Inventory file and Building an inventory</p> <p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"ansible/inventory-file/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"ansible/inventory-file/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements</p> <p>The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml</p> <p>NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: </p> <p>my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role.</p>"},{"location":"ansible/playbook/","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"ansible/playbook/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"ansible/playbook/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"ansible/what-is-ansible/","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <ul> <li>Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators.</li> <li>Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems.</li> <li>Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times.</li> <li>Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality.</li> <li>Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures.</li> <li>Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows.</li> <li>Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</li> </ul>"},{"location":"aws/cli/","title":"AWS CLI","text":"<p>The AWS Command Line Interface (CLI) is a unified tool for managing AWS services from the command line. With just one tool, you can control multiple AWS services, including Amazon S3, Amazon EC2, and Amazon CloudFront.</p>"},{"location":"aws/cli/#installation","title":"Installation","text":"<p>Install the dependencies:</p> <pre><code>apt install glibc groff less -y\n</code></pre> <p>Follow the official guide as the <code>curl</code>ed .zip link there updates frequently.</p> <p>AWS CLIv2 Official Installation Documentation</p>"},{"location":"azure/agent-installation/","title":"Azure Self-Hosted Agent Installation","text":""},{"location":"azure/agent-installation/#creating-a-personal-access-token","title":"Creating a Personal Access Token","text":"<ol> <li>Log into Azure DevOps.</li> <li>Under User settings select <code>Personal access tokens</code></li> <li>Click <code>New Token</code></li> <li>Fill the <code>Name</code> and <code>Expiration</code> fields.</li> <li>In scope select <code>Custom defined</code>, then click <code>Show all scopes</code> and tick <code>Read &amp; manage</code>under <code>Agent Pools</code></li> <li>Click <code>Create</code> and make sure to securely store the token because it will not be accessible later.</li> </ol>"},{"location":"azure/agent-installation/#installing-and-configuring-the-agent","title":"Installing and configuring the agent","text":"<ol> <li> <ul> <li>If you want the agent to be usable by different projects in your organization go to <code>Organization Settings</code> on your Azure Devops Organization page.</li> <li>If you want the agent to be exclusive to a specific project, go to <code>Project Settings</code> on the Azure Devops Project page.</li> </ul> </li> <li> <p>Select <code>Agent pools</code> under the <code>Pipelines</code> section on the left</p> </li> <li>Select an existing <code>Agent Pool</code> or create a new one.</li> <li>After you've selected an <code>Agent Pool</code>, click <code>New Agent</code><ul> <li>Linux: <ul> <li>Select <code>Linux</code></li> <li>Press the Copy button next to the Download button to copy the URL.</li> </ul> </li> </ul> </li> </ol>"},{"location":"azure/agent-installation/#linux","title":"Linux","text":"<p>In the Linux machine:</p> <ol> <li>Create a user for the Azure Agent:     <pre><code>sudo adduser azureagent\n</code></pre></li> <li>Add the user to sudoers:     <pre><code>sudo usermod -aG sudo azureagent\n</code></pre></li> <li>If needed add the user to other necessary groups like <code>docker</code>:     <pre><code>sudo usermod -aG docker azureagent\n</code></pre></li> <li>Create any necessary working directories and grant ownership to the user:     <pre><code>sudo chown -R azureagent:azureagent /home/app/foo\n</code></pre></li> <li>Switch to the azureagent user and navigate to the <code>/home/azureagent</code> directory:     <pre><code>su - azureagent\ncd /home/azureagent\n</code></pre></li> <li>Download the agent using the URL we copied earlier:     <pre><code>wget https://vstsagentpackage.azureedge.net/agent/3.220.2/vsts-agent-linux-x64-3.220.2.tar.gz\n</code></pre></li> <li>Create a new directory for the agent and extract the tar.gz inside and confirm with ls:     <pre><code>mkdir agent\ncd agent\ntar zxf ../vsts-agent-linux-x64-3.220.2.tar.gz\nls\n</code></pre></li> <li>Run the config script to start the agent configuration:     <pre><code>./config.sh\n</code></pre></li> <li> <p>Provide the info requested by the script</p> <ul> <li>Enter your Azure Devops server URL:     <pre><code>https://dev.azure.com/orgname\n</code></pre></li> <li>Press Enter to continue using PAT then paste the PAT we created earlier.</li> <li>Enter the agent pool name and a name for the agent we're creating.</li> <li>Press enter to use the default work folder (<code>_work</code>)</li> </ul> </li> <li> <p>Configure the agent to run as a service:     <pre><code>sudo ./svc.sh install azureagent\nsudo ./svc.sh start\n</code></pre></p> </li> <li> <p>Navigate to the <code>Agent pools</code> page on Azure Devops and select the relevant <code>Agent Pool</code>, then click on the <code>Agents</code> tab to verify that our new <code>Agent</code> is added as a self-hosted agent.</p> </li> </ol>"},{"location":"falcon-logscale/agent-installation/","title":"Falcon LogScale Agent(Log Collector) Setup","text":"<p>First of all, you need to create a new repo from <code>Repositories and View</code>.After creating the repo, you will go into the repo.You need to enter <code>Settings</code> from the upper menus.You need to enter <code>Ingest Tokens</code> under the <code>Ingest</code> category in the left menu.Then you will create a new token with <code>Add New Token</code>.</p> <p>Remember that this token will be useful later.</p>"},{"location":"falcon-logscale/agent-installation/#download-collector","title":"Download Collector","text":"<p>You should follow the steps below to reach the download page, select the package suitable for your system and download it.</p> <p>Main Page \u27a1\ufe0f Fleet Management \u27a1\ufe0f LogScale Collector Download</p> <p>Important Installations Notes</p> <p>You may get an error while downloading.This is because the download URL is wrong or incorrect.</p> <p>For example: <code>http://10.40.140.2:8080/None/api/v1/log-collector/download/humio-log-collector_1.4.1_linux_amd64.deb</code>. </p> <p>The <code>NONE</code> here may cause you to download an incorrect file.Delete it !.After downloading the file, check its size with the <code>ls -alh</code> command.  </p> <p>Follow these steps after downloading the file.</p> <pre><code>dpkg -i humio-log-collector_x.x.x_linux_amd64.deb\n</code></pre> <p>By default, the humio-log-collector process will run as the humio-log-collector user, which is installed by the package and won't have access to logs in <code>/var/log</code>.</p> <p>This can be granted by adding the user to the adm group. <pre><code>sudo usermod -a -G adm humio-log-collector\n</code></pre></p>"},{"location":"falcon-logscale/agent-installation/#you-can-run-the-logscale-collector-as-a-standalone-process-and-ignore-the-service-file-etc","title":"You can run the LogScale Collector as a standalone process and ignore the service file etc.","text":"<pre><code>/etc/humio-log-collector/config.yaml\n</code></pre> <p>Open the source field and enter the token and ip address you created in the relevant fields.</p> <p>Remember the source option specifies where you want to get the logs from</p> <pre><code>sources:\n  var_log:\n    type: file\n    include: /var/log/*\n    exclude: /var/log/*.gz\n    sink: humio\nsinks:\n  humio:\n    type: humio\n    token: &lt;Ingest Token&gt; # 210f4309-0d71-4d3d-b4e3-d503d46b93b9\n    url: &lt;host ip-address of the humio&gt; # http://52.91.72.78:8080/\n</code></pre> <p>Now you need to start and enable the services to apply the changes</p> <p><pre><code>sudo systemctl start humio-log-collector.service\n</code></pre> <pre><code>sudo systemctl enable humio-log-collector.service\n</code></pre></p> <p>Check the status of the service</p> <pre><code>sudo systemctl status humio-log-collector.service\n</code></pre> <p>Success</p> <p>If Active:active (running), you can go to the repository from the interface and check your logs</p> <p>Important Installations Notes</p> <p>If you get your logs as <code>Docker Container</code>, you can get your logs without installing an agent by using the following command with <code>Driver:Splunk</code></p> <pre><code>export YOUR_LOGSCALE_URL=\"&lt;10.40.140.2:8080&gt;\"\nexport INGEST_TOKEN=\"&lt;210f4309-0d71-4d3d-b4e3-d503d46b93b9&gt;\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\n</code></pre>"},{"location":"falcon-logscale/agent-installation/#to-generate-logs-with-docker-for-testing-you-can-generate-test-logs-using-the-following-docker-compose-commands","title":"To generate logs with <code>docker</code> for testing, you can generate test logs using the following <code>docker-compose</code> commands.","text":"<pre><code>export YOUR_LOGSCALE_URL=\"\"\nexport INGEST_TOKEN=\"\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  chentex-random-logger:\n    image: chentex/random-logger:latest\n    command: [\"100\", \"300\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-apache-common:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"apache_common\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-rfc5424:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"rfc5424\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-json:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"json\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"]\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\ndocker-compose up -d\ndocker-compose logs\n</code></pre>"},{"location":"falcon-logscale/falcon-logscale-installation-docker/","title":"Falcon LogScale Setup With Docker","text":""},{"location":"falcon-logscale/falcon-logscale-installation-docker/#falcon-logscale-setup-requirements","title":"Falcon LogScale Setup Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>30GB</code>      STORAGE <p> The first step to install LogScale using <code>Docker</code> is to <code>install Docker</code> on the machine where you want to run <code>Docker</code> with LogScale. You can Download Docker from their site or by using a package installation program like yum or apt-get.</p> <p>Or You Can Use These Command For Ubuntu:</p> <pre><code>sudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt install docker.io\nsystemctl start docker\nsystemctl enable docker\ndocker --version\n</code></pre> <p>Now let's run a container on port 8080 using the following commands and watch it with <code>docker ps</code></p> <p>Important Installations Notes</p> <p>If your machine is not open to port 8080, make it open</p> <pre><code>export HOST_DATA_DIR=/home/ubuntu/mounts/data\nexport HOST_KAFKA_DATA_DIR=/home/ubuntu/mounts/kafka-data\nexport PATH_TO_READONLY_FILES=/home/ubuntu/mounts/readonly\nexport HOST_ENV_FILE=/home/ubuntu/mounts/\n\nmkdir -p $PATH_TO_READONLY_FILES\nmkdir -p $HOST_KAFKA_DATA_DIR\nmkdir -p $HOST_DATA_DIR\ntouch $HOST_ENV_FILE.env\n\ndocker run -v $HOST_DATA_DIR:/data  \\\n   -v $HOST_KAFKA_DATA_DIR:/data/kafka-data  \\\n   -v $PATH_TO_READONLY_FILES:/etc/humio:ro  \\\n   -e AUTHENTICATION_METHOD=single-user \\\n   -e SINGLE_USER_USERNAME=hepapi \\\n   -e SINGLE_USER_PASSWORD=123456 \\\n   --net=host \\\n   --name=humio \\\n   --ulimit=\"nofile=8192:8192\"  \\\n   --stop-timeout 300 \\\n   --env-file=$HOST_ENV_FILE  \\\n   -d \\\n   -p 8080:8080 \\\n   humio/humio\n</code></pre> 'HOST_DATA_DIR, HOST_KAFKA_DATA_DIR, HOST_ENV_FILE, PATH_TO_READONLY_FILES'<pre><code>We bind these exported variables to the downloaded HUMIO container.\n</code></pre> <p>Info</p> <p>SINGLE_USER_USERNAME= Your Username for login </p> <p>SINGLE_USER_PASSWORD= Your Password for login</p> <p>Success</p> <p>Let's go your http://ip-address:8080 and enter activation key and enter your <code>SINGLE_USER_USERNAME</code> and <code>SINGLE_USER_PASSWORD</code></p>"},{"location":"falcon-logscale/humioinstallation/","title":"LogScale Installation","text":"<p>First, you'll need to create a non-administrative user named, <code>humio</code> to run LogScale software in the background. You can do this by executing the following from the command-line:  <pre><code>adduser humio --shell=/bin/false --no-create-home --system --group\n</code></pre></p> <p>You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install Kafka.</p> <p>Next, create the LogScale system directories and give the <code>humio</code> user ownership of them: <pre><code>mkdir -p /opt/humio /etc/humio/filebeat /var/log/humio /var/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /var/log/humio /var/humio/data\n</code></pre></p>"},{"location":"falcon-logscale/humioinstallation/#installation","title":"Installation","text":"<p>You're now ready to download and install LogScale's software. You should go to the LogScale directory and use wget to download the LogScale Java Archive. You can do this from the command-line like so:  <pre><code>cd /opt/humio/\n\nwget https://repo.humio.com/repository/maven-releases/com/humio/server/1.112.0/server-1.112.0.tar.gz\n\ntar xzf /opt/humio/server-1.112.0.tar.gz\n</code></pre> The wget here is used to download the latest release from Download Humio Server. You'll have to adjust the lines for the correct directory and file name, based on the version at the time. After you've downloaded it, enter the last line here to create a symbolic link to it. </p>"},{"location":"falcon-logscale/humioinstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, create the LogScale configuration file, server.conf in the <code>/etc/humio</code> directory. There are a few environment variables you will need to enter in this configuration file in order to run LogScale on a single server or instance. Below are those basic settings: <pre><code>AUTHENTICATION_METHOD=single-user\nSINGLE_USER_PASSWORD=&lt;Your-Password-Here&gt;\nSINGLE_USER_USERNAME=&lt;Your-Username-Here&gt;\nBOOTSTRAP_HOST_ID=1\nDIRECTORY=/var/humio/data\nHUMIO_AUDITLOG_DIR=/var/log/humio\nHUMIO_DEBUGLOG_DIR=/var/log/humio\nJVM_LOG_DIR=/var/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\nZOOKEEPER_URL=127.0.0.1:2181\nKAFKA_SERVERS=127.0.0.1:9092\nEXTERNAL_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;:8080\nPUBLIC_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;\nHUMIO_SOCKET_BIND=0.0.0.0\nHUMIO_HTTP_BIND=0.0.0.0\n</code></pre> Next you should set up a service file. Using a simple text editor, create a file named, humio.service in the <code>/etc/systemd/system/</code> sub-directory. Add these lines to that file: <pre><code>[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/var/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\n\n[Install]\nWantedBy=default.target\n</code></pre>  You will need to change the ownership of the LogScale files and start the LogScale service. To change the ownership, execute the following two lines from the command-line: <pre><code>chown -R humio:humio /opt/humio /etc/humio/filebeat\nchown -R humio:humio /var/log/humio /var/humio/data\n</code></pre> You're ready to start LogScale. <pre><code>systemctl start humio\n</code></pre> Just to be sure LogScale is running and everything is fine, check it with the journalctl tool. You can do this by entering the following from the command-line <pre><code>journalctl -fu humio\n</code></pre> If there are no errors, open a web browser and enter the domain name or IP address with port 8080. For example, you would enter something like http://example.com:8080 in the browser's address field.</p>"},{"location":"falcon-logscale/humiosetup/","title":"Humio Single Node Installation Guide","text":""},{"location":"falcon-logscale/humiosetup/#logscale-installers-are-available-for-several-linux-distributions","title":"LogScale installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Red Hat</li> </ul>"},{"location":"falcon-logscale/humiosetup/#prerequisites","title":"Prerequisites ;","text":""},{"location":"falcon-logscale/humiosetup/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>16GB</code>  MEMORY <code>8CPU</code>  CPU <code>100GB</code>      STORAGE <p>Access Permissions</p> <p>The machine to be installed must have access to the following addresses.</p> <p>http://humio.com</p> <p>https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz </p> <p>https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz [downloads.apache.org]</p> <p>hkp://keyserver.ubuntu.com:80</p> <p>http://repos.azulsystems.com/ubuntu [repos.azulsystems.com]</p> <p>https://repo.humio.com/repository/maven-releases/com/humio/server/1.117.0/server-1.117.0.tar.gz</p> <p>The following ports must be open; <pre><code>80,443,8080,1514\n</code></pre></p> <p>You must follow the sequence below to install Falcon LogScale.</p> <ul> <li>Java Installation Page</li> <li>Zookeeper Installation Page</li> <li>Kafka Installation Page</li> <li>LogScale Installation Page</li> </ul>"},{"location":"falcon-logscale/javainstallation/","title":"Java Installation","text":"<p>Install at least java version 17 in the following order</p> <p>Download java package <pre><code>https://cdn.azul.com/zulu/bin/zulu17.48.15-ca-jdk17.0.10-linux_amd64.deb\n</code></pre></p> <p>Import Azul\u2019s public key: <pre><code>sudo apt install gnupg ca-certificates curl\n</code></pre> <pre><code>curl -s https://repos.azul.com/azul-repo.key | sudo gpg --dearmor -o /usr/share/keyrings/azul.gpg\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/azul.gpg] https://repos.azul.com/zulu/deb stable main\" | sudo tee /etc/apt/sources.list.d/zulu.list\n</code></pre></p> <p>Install the required Azul Zulu package: <pre><code>sudo apt install zulu17-jdk\n</code></pre> Check java version <pre><code>java -version\n</code></pre></p>"},{"location":"falcon-logscale/kafkainstallation/","title":"Kafka Installation","text":"<p>LogScale recommend that the latest Kafka version be used with your LogScale deployment. The latest version of Kafka is available at Kafka Downloads</p> <p><pre><code>apt-get update\napt-get upgrade\n</code></pre>  Next, create a non-administrative user named, <code>kafka</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser kafka --shell=/bin/false --no-create-home --system --group\n</code></pre></p>"},{"location":"falcon-logscale/kafkainstallation/#installation","title":"Installation","text":"<p>To install Kafka, you'll need to go to the /opt directory and download the latest release. You can do that like so with wget.  <pre><code>cd /opt\nwget https://www-us.apache.org/dist/kafka/x.x.x/kafka_x.x.x.x.tgz\n</code></pre> You would adjust this last line, change the Xs to the latest version number. Once it downloads, untar the file and then create the directories it needs like this:  <pre><code>tar zxf kafka_x.x.x.x.tgz\n\nmkdir /var/log/kafka\nmkdir /var/kafka/data\nchown kafka:kafka /var/log/kafka\nchown kafka:kafka /var/kafka/data\n\nln -s /opt/kafka_x.x.x.x /opt/kafka\n</code></pre></p> <p>The four lines in the middle here create the directories for Kafka's logs and data, and changes the ownership of those directories to the kafka user. The last line creates a symbolic to /opt/kafka. You would adjust that, though, replacing the Xs with the version number.</p>"},{"location":"falcon-logscale/kafkainstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, open the Kafka properties file, server.properties, located in the kafka/config sub-directory. You'll need to set a few options \u2014 the lines below are not necessarily the order in which they'll be found in the configuration file: <pre><code>broker.id=1\nlog.dirs=/var/kafka/data\ndelete.topic.enable = true\n</code></pre> The first line sets the broker.id value to match the server number <code>(myid)</code> you set when configuring ZooKeeper. The second sets the data directory. The third line should be added to the end of the configuration file. When you're finished, save the file and change the owner to the kafka user:  <pre><code>chown -R kafka:kafka /opt/kafka_x.x.x.x\n</code></pre></p> <p>You'll have to adjust this to the version you installed. Note, changing the ownership of the link <code>/opt/kafka</code> doesn't change the ownership of the files in the directory.</p> <p>Now you'll need to create a service file for starting Kafka. Use a simple text editor to create a file named, kafka.service in the <code>/etc/systemd/system/</code> sub-directory. Then add the following lines to the service file: <pre><code>[Unit]\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/var/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Now you're ready to start the Kafka service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start kafka\nsystemctl status kafka\nsystemctl enable kafka\n</code></pre></p>"},{"location":"falcon-logscale/zookeeperinstallation/","title":"Zookeeper Installation","text":"<p><pre><code>apt-get update\napt-get upgrade\n</code></pre> Next, create a non-administrative user named, <code>zookeeper</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser zookeeper --shell=/bin/false --no-create-home --system --group\n</code></pre> You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install ZooKeeper.</p>"},{"location":"falcon-logscale/zookeeperinstallation/#installation","title":"Installation","text":"<p>Navigate to opt directory and download a of ZooKeeper. The official release site is Apache Zookeeper Release <pre><code>cd /opt\nwget https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz\n</code></pre> After the file downloads, untar the ZooKeeper file and create a symbolic to <code>/opt/zookeeper</code> like so:  <pre><code>tar -zxf apache-zookeeper-x.x.x-bin.tar.gz\nln -s /opt/apache-zookeeper-x.x.x-bin /opt/zookeeper\n</code></pre> Navigate to zookeeper sub-directory and create a data directory for ZooKeeper:  <pre><code>cd /opt/zookeeper\nmkdir -p /var/zookeeper/data\n</code></pre></p>"},{"location":"falcon-logscale/zookeeperinstallation/#configuration","title":"Configuration","text":"<p>Using a text editor, create the ZooKeeper configuration file in the conf sub-directory. Name the file, zoo.cfg. For example, <code>/opt/zookeeper/conf/zoo.cfg</code>. Copy the lines below into that file: <pre><code>tickTime = 2000\ndataDir = /var/zookeeper/data\nclientPort = 2181\ninitLimit = 5\nsyncLimit = 2\nmaxClientCnxns=60\nautopurge.purgeInterval=1\nadmin.enableServer=false\n4lw.commands.whitelist=*\nserver.1=127.0.0.1:2888:3888\nadmin.enableServer=false\n</code></pre> Create a myid file in the data sub-directory with just the number 1 as its contents. They you can start ZooKeeper to verify that the configuration is working:</p> <p><pre><code>bash -c 'echo 1 &gt; /var/zookeeper/data/myid'\n\n./bin/zkServer.sh start\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-x.x.x/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n</code></pre> Stop ZooKeeper and change the ownership of the zookeeper directory like so, adjusting for the version number you installed:  <pre><code>./bin/zkServer.sh stop\n\nchown -R zookeeper:zookeeper /opt/apache-zookeeper-x.x.x\nchown -R zookeeper:zookeeper /var/zookeeper/data\n</code></pre></p> <p>So that ZooKeeper will start when the server is rebooted, you'll need to create a ZooKeeper service file named zookeeper.service in the <code>/etc/systemd/system/</code> sub-directory. Use a text editor to create the file and copy the following lines into it. <pre><code>[Unit]\nDescription=ZooKeeper Daemon\nDocumentation=http://zookeeper.apache.org\nRequires=network.target\nAfter=network.target\n\n[Service]\nType=forking\nWorkingDirectory=/opt/zookeeper\nUser=zookeeper\nGroup=zookeeper\nExecStart=/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg\nExecStop=/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg\nExecReload=/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg\nTimeoutSec=30\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre> Start the ZooKeeper service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start zookeeper\nsystemctl status zookeeper\nsystemctl enable zookeeper\n</code></pre></p>"},{"location":"git/Commands/","title":"Commands","text":"Command Explanation Usage push Pushing is the process of sending local commits to a remote repository. <code>git push origin branch_name</code> pull Pulling is the process of fetching and merging remote changes into the local repository. <code>git pull origin branch_name</code> add . Adds all modified and new files to the staging area. <code>git add .</code> add  Adds a specific file to the staging area. <code>git add file_name</code> commit A commit is a snapshot of changes made to the repository. <code>git commit -m \"Commit message\"</code> init Initializes a new Git repository in the current directory. <code>git init</code> log Log displays the commit history of the repository. <code>git log</code> status Status shows the current state of the repository. <code>git status</code> branch A branch is a separate line of development within a repository. <code>git branch branch_name</code> merge Merging combines changes from different branches into a single branch. <code>git merge branch_name</code> revert Revert creates a new commit that undoes changes from a previous commit. <code>git revert commit_hash</code> reset Reset moves the current branch pointer to a specific commit, potentially discarding commits. <code>git reset commit_hash</code> rm / remove Removes a file from the repository and the working directory. <code>git rm file_name</code> mv / move Renames or moves a file or directory within the repository. <code>git mv old_file_name new_file_name</code> clone Cloning creates a local copy of a remote repository. <code>git clone repository_url</code> echo Echo prints a message or value to the terminal or a file. <code>echo \"Hello, World!\"</code> touch Touch creates an empty file or updates the timestamp of an existing file. <code>touch file_name</code> ls / list List displays the files and directories in the current directory. <code>ls</code> or <code>list</code> cat Cat displays the contents of a file. <code>cat file_name</code> diff Diff shows the differences between different versions of files. <code>git diff</code> checkout Checkout allows you to switch between branches or restore files from previous commits. <code>git checkout branch_name</code> .gitignore A .gitignore file specifies files and patterns to be ignored by Git. Create a .gitignore file and list files/patterns to ignore repository A repository is a location where Git stores all the files, history, and changes for a project. <code>git init</code> fork Forking creates a copy of a repository under your GitHub account. Click on the \"Fork\" button in the GitHub UI pull request A pull request proposes changes from a forked repository to the original repository. Create a pull request through the GitHub UI PR (Pull Request) PR is an abbreviation for pull request. Use the term \"PR\" interchangeably with \"pull request\" master Master is the default branch in Git. The initial branch created in a repository (commonly used) config Config sets configuration options for Git. <code>git config --global user.name \"Your Name\"</code> remote Remote refers to a remote repository, typically on a server. <code>git remote add origin repository_url</code> stash Stash temporarily saves local modifications for later use. <code>git stash save \"Stash message\"</code> pop Pop applies the most recent stash and removes it from the stash list. <code>git stash pop</code> reflog Reflog shows a log of all reference updates in the repository. <code>git reflog</code> Action Explanation Usage Create and Switch to New Branch Creates a new branch and switches to it. <code>git checkout -b &lt;branch&gt;</code> Merge Branch into Main Branch Merges changes from a feature branch into the main branch. <code>git checkout &lt;main_branch&gt;</code><code>git merge &lt;feature_branch&gt;</code> Rebase Branch onto Main Branch Updates the feature branch with the latest changes from the main branch. <code>git checkout &lt;feature_branch&gt;</code><code>git rebase &lt;main_branch&gt;</code> Interactive Rebase Allows interactive modification, reordering, or squashing of commits. <code>git rebase -i HEAD~&lt;number_of_commits&gt;</code> Revert a Commit Creates a new commit that undoes the changes from a specific commit. <code>git revert &lt;commit_hash&gt;</code> Undo Last Commit (Keep Changes) Moves the branch pointer back one commit, keeping changes in the staging area. <code>git reset --soft HEAD~1</code> Discard Last Commit (Lose Changes) Moves the branch pointer back one commit, discarding changes in the working directory and staging area. <code>git reset --hard HEAD~1</code>"},{"location":"git/Description/","title":"Description","text":""},{"location":"git/Description/#version-control-system-vcs","title":"Version Control System (VCS)","text":"<p>A Version Control System (VCS) is a software tool that helps track changes made to files and directories over time. It allows multiple people to collaborate on a project, keeping a history of changes, and providing mechanisms to manage different versions of files. VCS enables teams to work concurrently, facilitating efficient collaboration and providing features like branching, merging, and conflict resolution.</p>"},{"location":"git/Description/#git","title":"Git","text":"<p>Git is a widely used distributed version control system designed for speed, flexibility, and data integrity. It is free and open-source, offering powerful features that make it popular among individuals and large organizations. Git provides a decentralized approach, allowing users to have a local copy of the entire repository, including its history, branches, and tags.</p>"},{"location":"git/installation/","title":"Installation","text":""},{"location":"git/installation/#downloading-git","title":"Downloading Git","text":"<p>To download Git for different operating systems, follow these instructions:</p>"},{"location":"git/installation/#windows","title":"Windows","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the latest Git version for Windows.</li> <li>Run the installer and follow the prompts.</li> <li>Choose the desired installation options and complete the installation process.</li> </ul>"},{"location":"git/installation/#macos","title":"macOS","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the macOS version of Git.</li> <li>Run the installer package and follow the prompts.</li> <li>Complete the installation process.</li> </ul>"},{"location":"git/installation/#linux-ubuntu","title":"Linux (Ubuntu)","text":"<ul> <li>Open the terminal.</li> <li>Install Git using the package manager:</li> <li>For Debian/Ubuntu-based systems: <code>sudo apt-get install git</code></li> <li>For Fedora: <code>sudo dnf install git</code></li> <li>For CentOS/RHEL: <code>sudo yum install git</code></li> </ul>"},{"location":"git/installation/#linux-generic","title":"Linux (Generic)","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the source code archive for Linux.</li> <li>Extract the archive to a desired location.</li> <li>In the terminal, navigate to the extracted directory.</li> <li>Run the following commands:</li> <li><code>make prefix=/usr/local all</code></li> <li><code>sudo make prefix=/usr/local install</code></li> </ul>"},{"location":"git/installation/#verification","title":"Verification","text":"<p>Once Git is installed on your system, you can verify the installation by opening a terminal and running <code>git --version</code> to display the installed version.</p>"},{"location":"git/installation/#interfaces","title":"Interfaces","text":"<p>Git provides a command-line interface (CLI) and various graphical user interface (GUI) tools. You can choose the interface that suits your preference and start using Git for version control in your projects.</p>"},{"location":"how-to-contribute/about-markdown/","title":"Markdown Syntax","text":""},{"location":"how-to-contribute/about-markdown/#links","title":"Links","text":"<pre><code>[Link Text](https://www.example.com)\n</code></pre>"},{"location":"how-to-contribute/about-markdown/#images","title":"Images","text":"<p>Put a <code>!</code> in front of the link syntax.</p> <pre><code>![Alt Text](https://www.example.com/image.png)\n</code></pre>"},{"location":"how-to-contribute/about-markdown/#syntax-highlighting","title":"Syntax Highlighting","text":"<p>Insert the language name after the first set of backticks.</p> <pre><code># ```python\nimport os\nos.system(\"echo 'Hello World'\")\n</code></pre> <pre><code># ```bash\nexport HELLO=\"world\"\ncat some-file | grep \"hello\"\n</code></pre> <pre><code># ```yaml\nsome: key\nanother: key\n</code></pre>"},{"location":"how-to-contribute/about-markdown/#tables","title":"Tables","text":"<pre><code>| name | value |\n| ---- | ----- |\n| a    | b     |\n</code></pre> <p>You can align headers to the left, center, or right by adding colons to the header syntax.</p> <pre><code>| --name-- | --value-- | description |\n| :------- | :-------: | ----------: |\n| a        |     b     |           c |\n</code></pre>"},{"location":"how-to-contribute/about-markdown/#headers","title":"Headers","text":"<pre><code># H1 Header (biggest)\n\n## H2 Header\n\n### H3 Header\n\n#### H4 Header\n\n##### H5 Header\n\n###### H6 Header (smallest)\n</code></pre>"},{"location":"how-to-contribute/about-mkdocs/","title":"creating new mkdocs pages","text":""},{"location":"how-to-contribute/about-mkdocs/#how-to-add-a-new-page","title":"How to add a new Page","text":"<ol> <li>Create a new <code>.md</code> file under in the <code>docs/</code> folder</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section</li> <li>Do not put <code>docs/</code> prefix on the filepath</li> <li>Commit and push your changes to the <code>main</code> branch</li> <li>GitHub Action will automatically build and deploy the changes to the website.</li> </ol>"},{"location":"how-to-contribute/about-mkdocs/#about-this-website","title":"About this website","text":"<p>Stack</p> Name Description mkDocs Docs static site generator Material for MkDocs Material theme for mkDocs"},{"location":"how-to-contribute/about-mkdocs/#steps-to-run-it-locally","title":"Steps to run it locally","text":"<ol> <li>Make sure <code>python3</code> is installed</li> <li>Install python requirements    <pre><code>pip install mkdocstrings[python]\npip install mkdocs-material\n</code></pre></li> <li>Clone the repository &amp; <code>cd</code> into it</li> <li>Run the local server    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"jenkins/","title":"Jenkins","text":""},{"location":"jenkins/jenkins-installation/","title":"Jenkins Install","text":""},{"location":"jenkins/jenkins-installation/#jenkins-installers-are-available-for-several-linux-distributions","title":"Jenkins installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Fedora</li> <li>Red Hat/Alma/Rocky</li> </ul>"},{"location":"jenkins/jenkins-installation/#prerequisites","title":"Prerequisites ;","text":""},{"location":"jenkins/jenkins-installation/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>256MB</code>  MEMORY <code>512MB</code>  CPU <code>1GB</code>      STORAGE <p>1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container)</p>"},{"location":"jenkins/jenkins-installation/#recommended-hardware-configuration-for-a-small-team","title":"Recommended hardware configuration for a small team:","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>50GB</code>      STORAGE"},{"location":"jenkins/jenkins-installation/#installation-of-java","title":"Installation of Java","text":"<p><code>Jenkins requires Java in order to run, yet certain distributions don\u2019t include this by default and some Java versions are incompatible with Jenkins. There are multiple Java implementations which you can use. OpenJDK is the most popular one at the moment, we will use it in this guide. Update the Debian apt repositories, install OpenJDK 11, and check the installation with the commands:</code></p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jre\njava -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment (build 11.0.12+7-post-Debian-2)\nOpenJDK 64-Bit Server VM (build 11.0.12+7-post-Debian-2, mixed mode, sharing)\n</code></pre> <p>After installing Java without any problems, we will install jenkins</p>"},{"location":"jenkins/jenkins-installation/#jenkins-install_1","title":"Jenkins Install","text":"<pre><code>curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install jenkins\n</code></pre> <p>Quote</p> <p>If Jenkins fails to start because a port is in use, run -- systemctl edit jenkins -- and add the following ; <pre><code>[Service]\nEnvironment=\"JENKINS_PORT=8081\"\n</code></pre></p>"},{"location":"jenkins/jenkins-installation/#start-jenkins","title":"Start Jenkins","text":"<p>You can enable the Jenkins service to start at boot with the command: <pre><code>sudo systemctl enable jenkins\n</code></pre></p> <p>You can start the Jenkins service with the command: <pre><code>sudo systemctl start jenkins\n</code></pre></p> <p>You can check the status of the Jenkins service using the command: <pre><code>sudo systemctl status jenkins\n</code></pre></p> <p>If everything has been set up correctly, you should see an output like this: <pre><code>Loaded: loaded (/lib/systemd/system/jenkins.service; enabled; vendor preset: enabled)\nActive: active (running) since Tue 2018-11-13 16:19:01 +03; 4min 57s ago\n</code></pre></p> <p>If you have a firewall installed, you must add Jenkins as an exception. You must change YOURPORT in the script below to the port you want to use. Port 8080 is the most common. <pre><code>YOURPORT=8080\nPERM=\"--permanent\"\nSERV=\"$PERM --service=jenkins\"\n\nfirewall-cmd $PERM --new-service=jenkins\nfirewall-cmd $SERV --set-short=\"Jenkins ports\"\nfirewall-cmd $SERV --set-description=\"Jenkins port exceptions\"\nfirewall-cmd $SERV --add-port=$YOURPORT/tcp\nfirewall-cmd $PERM --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n</code></pre></p> <ul> <li> After installing Jenkins, we go to the instance ip address e.g : <code>&lt; ip-address &gt;:8080</code> </li> <li> <p> On the page that opens, it asks us to enter a token. We take this token from the server with the <code>cat /var/lib/jenkins/secrets/initialAdminPassword</code> code and paste it into the input and log in.</p> </li> <li> <p> Jenkins gives you two options.</p> <ul> <li> <code>Install Suggested Plugins</code></li> <li> <code>Select Plugins To Install</code></li> </ul> </li> </ul> <p>If you don't see jenkins when you go to your server's ip address, allow port 8080</p> <p> You can login to Jenkins by choosing the one you want and creating Username and Password.</p> <p>For more installation details</p>"},{"location":"jenkins/jenkins-installation/#happy-jenkins","title":"Happy Jenkins","text":""},{"location":"jenkins/shared-library/","title":"Jenkins Shared Library","text":"<p>Hello everybody from Hepapi.We will talk about Jenkins Shared Library.Lets start. We are in a period where modern applications are generally divided into small components and run on a microservice architecture. Compared to a monolithic application, there are many extra pipelines in an application with microservice architecture. Therefore, it is very important to ensure modularity and reusability of the created pipelines. Thanks to Jenkins Shared Library, we can get rid of the code complexity in the pipeline and comply with the DRY (Don't Repeat Yourself) principle.</p> <p>Example: You can perform the docker build process that you perform jointly for more than one service by simply sending the docker image information to a single function for all services.</p> <pre><code>// var/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String REPOSITORY) {\n  String REGISTRY = \"ersinsari\"\n  sh \"docker build -t ${REGISTRY}/${REPOSITORY}:latest .\"\n}\n// Jenkinsfile\n@Library('mylibrary') _\npipeline {\n  stages {\n    stage('Docker Build') {\n      dockerBuild \"nodejs-helloworld\"    \n    }\n  }\n}\n</code></pre>"},{"location":"jenkins/shared-library/#folder-structure-of-shared-library","title":"Folder Structure of Shared Library","text":"<pre><code>\u2514\u2500\u2500 jenkins-shared\n    \u251c\u2500\u2500 src\n    \u2502   \u251c\u2500\u2500 main\n    \u2502       \u2514\u2500\u2500 groovy\n    |           \u2514\u2500\u2500 *.groovy\n    \u251c\u2500\u2500 vars           \n    |   \u251c\u2500\u2500 *.groovy\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 config.properties\n        \u2514\u2500\u2500 template.xml\n</code></pre> <ul> <li> <p>The src directory is structured like a standard Java project. This means that you can use the import statement to import classes from other directories in the src directory.</p> </li> <li> <p>The vars directory is a special directory that contains global variables that are defined in the shared library. These variables can be accessed from any Jenkins job that imports the shared library.</p> </li> <li> <p>The resources directory is a regular directory that can contain any type of file. However, it is typically used to store static resources that are used by the shared library.</p> </li> </ul>"},{"location":"jenkins/shared-library/#global-shared-libraries","title":"Global Shared Libraries","text":"<p>There are several places where Shared Libraries can be defined, depending on the use-case</p> <ul> <li>Manage Jenkins \u00bb System \u00bb Global Pipeline Libraries as many libraries as necessary can be configured</li> </ul> <p>These libraries are considered \"trusted:\" they can run any methods in Java, Groovy, Jenkins internal APIs, Jenkins plugins, or third-party libraries. This allows you to define libraries which encapsulate individually unsafe APIs in a higher-level wrapper safe for use from any Pipeline. Beware that anyone able to push commits to this SCM repository could obtain unlimited access to Jenkins. You need the Overall/RunScripts permission to configure these libraries (normally this will be granted to Jenkins administrators).</p>"},{"location":"jenkins/shared-library/#folder-level-shared-libraries","title":"Folder-level Shared Libraries","text":"<p>Any Folder created can have Shared Libraries associated with it. This mechanism allows scoping of specific libraries to all the Pipelines inside of the folder or subfolder.Folder-based libraries are not considered \"trusted:\" they run in the Groovy sandbox just like typical Pipelines.</p>"},{"location":"jenkins/shared-library/#automatic-shared-libraries","title":"Automatic Shared Libraries","text":"<p>Other plugins may add ways of defining libraries on the fly. For example, the Pipeline: GitHub Groovy Libraries plugin allows a script to use an untrusted library named like github.com/someorg/somerepo without any additional configuration. In this case, the specified GitHub repository would be loaded, from the master branch, using an anonymous checkout.</p>"},{"location":"jenkins/shared-library/#using-libraries","title":"Using Libraries","text":"<p>Shared Libraries marked Load implicitly allows Pipelines to immediately use classes or global variables defined by any such libraries. To access other shared libraries, the Jenkinsfile needs to use the @Library annotation, specifying the library\u2019s name:</p> <pre><code>@Library('my-shared-library') _\n\n/* Using a version specifier, such as branch, tag, etc */\n@Library('my-shared-library@1.0') _\n\n/* Accessing multiple libraries with one statement */\n@Library(['my-shared-library', 'otherlib@abc1234']) _\n</code></pre>"},{"location":"jenkins/shared-library/#loading-libraries-dynamically","title":"Loading libraries dynamically","text":"<p>As of version 2.7 of the Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time during the build.</p> <p>If you are only interested in using global variables/functions (from the vars/ directory), the syntax is quite simple:</p> <pre><code>library 'my-shared-library'\n</code></pre> <p>Thereafter, any global variables from that library will be accessible to the script.</p> <p>Using classes from the src/ directory is also possible, but trickier. Whereas the @Library annotation prepares the \u201cclasspath\u201d of the script prior to compilation, by the time a library step is encountered the script has already been compiled. Therefore you cannot import or otherwise \u201cstatically\u201d refer to types from the library.</p>"},{"location":"jenkins/shared-library/#demo","title":"DEMO","text":"<p>First of all we need jenkins server for this demo.There are some options to deploy jenkins-server</p> <pre><code>https://www.jenkins.io/doc/book/installing/\n</code></pre>"},{"location":"jenkins/shared-library/#step-1-creating-shared-library","title":"Step-1 Creating Shared library","text":"<p>Let's first create the groovy scripts of the Jenkins shared library and push them to git. We have two simple scripts for this example. We will perform docker build and docker push operations using these scripts.</p> <pre><code>// vars/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n    dir(\"${WORKSPACE}\") {\n        sh \"docker build -t ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER) .\"\n    }\n}\n</code></pre> <pre><code>// vars/dockerPush.groovy\n#!/usr/bin/env groovy\n\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n\n    dir(\"${WORKSPACE}\") {\n        sh \"echo $DOCKERHUB_CRED_PSW | docker login -u $DOCKERHUB_CRED_USR --password-stdin\"\n        sh \"docker push ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER)\"\n    }\n}\n</code></pre> <pre><code>// vars/sayHello.groovy\n#!/usr/bin/env groovy\n\ndef call(String name = 'human') {\n  echo \"Hello, ${name}.\"\n}\n</code></pre> <p>Go to the Global Pipeline Library to Jenkins. After accessing the Jenkins dashboard, navigate to Manage Jenkins &gt;  System to find the Global Pipeline Libraries section and click on the add button to add a new library. Manage Jenkins ---&gt; System ---&gt; Global Pipeline Libraries ---&gt; Click Add button</p> <ul> <li>Library Name: my-shared-library</li> <li>Default version: main</li> <li>Retrivial method: Modern SCM</li> <li>Source Code Management</li> <li> <p>Project Repository: https://github.com/ersinsari13/jenkins-shared-library.git  #enter your repo name</p> </li> <li> <p>We can leave other parameters as default and save the configuration.</p> </li> </ul>"},{"location":"jenkins/shared-library/#step-2-creating-jenkinsfile","title":"Step-2 Creating jenkinsfile","text":"<p>Let's first create github repo named docker-build-push then create the Jenkinsfile and push them to git.</p> <pre><code>@Library('my-shared-library') _\npipeline {\n    agent any\n\n    environment {\n        DOCKERHUB_CRED=credentials('docker-hub-credential')\n    }\n    stages {\n        stage('Set Environment') {\n            steps {\n                script {\n                    REGISTRY = \"ersinsari\"\n                    REPOSITORY = \"docker-build-push\"\n                }\n            }\n        }\n        stage('Hello') {\n            steps {\n                sayHello \"hepapi\"\n            }\n        }\n        stage('Build') {\n            steps {\n                dockerBuild \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n        stage('Push') {\n            steps {\n                dockerPush \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins/shared-library/#step-3-creating-pipeline","title":"Step-3 Creating Pipeline","text":"<ul> <li>Go to Jenkins Server Dashboard</li> <li>Click New Item</li> <li>Enter \"docker-build-push\" as a name and select pipeline and click Ok</li> <li> <p>Under Pipeline Section:</p> </li> <li> <p>Definition: Pipeline script from SCM</p> </li> <li>SCM: Git</li> <li>Repository URL: https://github.com/ersinsari13/docker-build-push.git  #enter your repository name</li> <li>Branch: main</li> </ul> <p>Since the Jenkinsfile in the repo is not under any folder, it will be sufficient to just specify its name. For example, if my Jenkinsfile file was located under a folder named build; I should have written build/Jenkinsfile in the Script Path section.</p> <ul> <li>Save the pipeline configuration</li> </ul>"},{"location":"jenkins/shared-library/#step-4-credential-for-dockerhub","title":"Step-4 Credential for Dockerhub","text":"<p>In order for the images we created in Pipeline to be pushed to the registry, we need to provide registry credential information to the Jenkins server.</p> <ul> <li>Go to Jenkins Dashboard</li> <li>Select Manage Jenkins</li> <li>Click Credentials</li> <li>Select Global</li> <li> <p>Click Add Credentials button:</p> </li> <li> <p>Kind: Username with password</p> </li> <li>username: enter your dockerhub registry username</li> <li>password: enter your dockerhub registry password</li> <li>id: dockerhub-registry-credentials</li> <li> <p>description: dockerhub-registry-credentials</p> </li> <li> <p>Save credentials configuration</p> </li> </ul>"},{"location":"jenkins/shared-library/#step-5-build-pipeline","title":"Step-5 Build Pipeline","text":"<p>After all the steps, the pipeline we created is now ready to be tested.</p> <ul> <li>Go to Jenkins Server Dashboard</li> <li>Select Pipeline was created</li> <li>Click Build Now</li> <li>You should see the pipeline result success</li> <li>You can check your new image on the Dockerhub registry</li> </ul>"},{"location":"k8s-engine/k3s-installation/","title":"K3S Setup","text":""},{"location":"k8s-engine/k3s-installation/#k3s-requirements","title":"K3S Requirements :","text":"Resources Limits <code>512MB</code>  MEMORY (we recommend at least 1GB) <code>1CPU</code>  CPU (we recommend at least 2CPU) <code>20GB</code>      STORAGE"},{"location":"k8s-engine/k3s-installation/#cpu-and-memory","title":"CPU and Memory","text":"<p>The following are the minimum CPU and memory requirements for nodes in a high-availability K3S server</p> Deployment Size Nodes VCPUS RAM Small Up to 10 2 4 GB Medium Up to 100 4 8 GB Large Up to 250 8 16 GB X-Large Up to 500 16 32 GB XX-Large 500+ 32 64 GB"},{"location":"k8s-engine/k3s-installation/#disks","title":"Disks","text":"<p> The cluster performance depends on database performance. To ensure optimal speed, we recommend always using SSD disks to back your K3S cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS.</p>"},{"location":"k8s-engine/k3s-installation/#database","title":"Database","text":"<p>K3S supports different databases including MySQL, PostgreSQL, MariaDB, and etcd, the following is a sizing guide for the database resources you need to run large clusters:</p> Deployment Size Nodes VCPUS RAM Small Up to 10 1 2 GB Medium Up to 100 2 8 GB Large Up to 250 4 16 GB X-Large Up to 500 8 32 GB XX-Large 500+ 16 64 GB"},{"location":"k8s-engine/k3s-installation/#single-master-node-k3s-install","title":"Single Master Node K3S Install","text":"<p>A single-node server installation is a fully-functional Kubernetes cluster, including all the datastore, control-plane, kubelet, and container runtime components necessary to host workload pods. It is not necessary to add additional server or agents nodes, but you may want to do so to add additional capacity or redundancy to your cluster.</p> <p><code>Run this command :</code> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p>"},{"location":"k8s-engine/k3s-installation/#high-available-k3s-install","title":"High Available K3S Install","text":"<p>To run K3S in this mode, you must have an odd number of server nodes. We recommend starting with three nodes.</p> <p>To get started, first launch a server node with the <code>cluster-init</code> flag to enable clustering and a token that will be used as a shared secret to join additional servers to the cluster.</p> <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s.service</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 13:54:17 UTC; 37min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>After launching the first server, join the second and third servers to the cluster using the shared secret: <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://&lt;ip or hostname of server1&gt;:6443\n</code></pre></p>"},{"location":"k8s-engine/k3s-installation/#cluster-access","title":"Cluster Access","text":"<p>The kubeconfig file stored at <code>/etc/rancher/k3s/k3s.yaml</code> is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path.This can be done by either exporting the <code>KUBECONFIG</code> environment variable or by invoking the <code>--kubeconfig</code> command line flag. Refer to the examples below for details.</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Check to see that the second and third servers are now part of the cluster: <pre><code>$ kubectl get nodes\n\nNAME        STATUS   ROLES                       AGE   VERSION\nserver1     Ready    control-plane,etcd,master   28m   vX.Y.Z\nserver2     Ready    control-plane,etcd,master   13m   vX.Y.Z\nserver3     Ready    control-plane,etcd,master   10m   vX.Y.Z\n</code></pre></p>"},{"location":"k8s-engine/k3s-installation/#k3s-agent-install","title":"K3S Agent Install","text":"<p>To install additional agent nodes and add them to the cluster, run the installation script with the <code>K3S_URL</code> and <code>K3S_TOKEN</code> environment variables. Here is an example showing how to join an agent: <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -\n</code></pre></p> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s-agent</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s-agent.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 14:24:53 UTC; 7min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>Setting the <code>K3S_URL</code> parameter causes the installer to configure K3S as an agent, instead of a server. The K3S agent will register with the K3S server listening at the supplied URL. if you have not set a stable token the value to use for <code>K3S_TOKEN</code> is stored at <code>/var/lib/rancher/k3s/server/node-token</code> on your server node.</p> <p>Example</p> Leverage the KUBECONFIG environment variable:Or specify the location of the kubeconfig file in the command: <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n</code></pre> <pre><code>kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces\nhelm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces\n</code></pre>"},{"location":"k8s-engine/k3s-installation/#uninstalling-k3s","title":"Uninstalling K3S","text":"<p>Example</p> Uninstalling Servers:Uninstalling Agents: <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre> <pre><code>/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre> <p>Make sure all required port numbers are open</p> <p>For More Details</p>"},{"location":"k8s-engine/rke2-installation-ansible/","title":"RKE2 Cluster Installation With Ansible","text":"<p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> </ol>"},{"location":"k8s-engine/rke2-installation-ansible/#set-up-passwordless-ssh","title":"Set up passwordless SSH","text":"<ol> <li> <p>Generate ssh key</p> <pre><code>ssh-keygen\n</code></pre> <p>Simply press enter when prompted for default name and no password, this will create <code>~/.ssh/id_rsa</code></p> </li> <li> <p>Copy ssh keys to master and worker nodes</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa root@10.40.140.4\n</code></pre> <p>Enter the ssh password when prompted and repeat for all master and worker nodes.</p> </li> <li> <p>Add the ssh key to the ssh-agent</p> <pre><code>ssh-agent bash\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>If you get an error run the following and try again</p> <pre><code>eval `ssh-agent`\n</code></pre> </li> </ol>"},{"location":"k8s-engine/rke2-installation-ansible/#install-ansible-and-rke2-deployment-role","title":"Install Ansible and RKE2 deployment role","text":"<ol> <li> <p>Install ansible</p> <pre><code>sudo apt update\nsudo apt install ansible -y\n</code></pre> </li> <li> <p>Install the lablabs/rke2 ansible role (https://galaxy.ansible.com/lablabs/rke2)</p> <pre><code>ansible-galaxy install lablabs.rke2\n</code></pre> <p>This will install the role under the <code>~/.ansible</code> directory</p> </li> <li> <p>Navigate to the <code>~/.ansible</code> directory</p> <pre><code>cd ~/.ansible\n</code></pre> </li> <li> <p>Create inventory file in <code>~/.ansible</code> (can be copied from local with scp)</p> <pre><code>vi inventory\n</code></pre> <p>Go into insert mode by pressing i</p> <p>Copy the following and paste with ctrl+shift+v</p> <pre><code>[masters]\nmaster-01 ansible_host=10.40.140.4 rke2_type=server\nmaster-02 ansible_host=10.40.140.5 rke2_type=server\nmaster-03 ansible_host=10.40.140.6 rke2_type=server\n\n[workers]\nworker-01 ansible_host=10.40.140.7 rke2_type=agent\nworker-02 ansible_host=10.40.140.8 rke2_type=agent\nworker-03 ansible_host=10.40.140.9 rke2_type=agent\n\n[k8s_cluster:children]\nmasters\nworkers\n</code></pre> <p>Host names (ex: master-01) can be changed, however they must be lowercase.</p> <p>Save and quit by pressing Esc, then :wq!</p> </li> <li> <p>Create playbook.yaml file in <code>~/.ansible</code> (check previous step for help with vi)</p> <pre><code>- name: Deploy RKE2\n  hosts: all\n  become: yes\n  vars:\n    rke2_ha_mode: true\n    rke2_api_ip : 10.40.140.4\n    rke2_download_kubeconf: true\n    rke2_ha_mode_keepalived: false\n    rke2_server_node_taints:\n      - 'CriticalAddonsOnly=true:NoExecute'\n  roles:\n    - role: lablabs.rke2\n</code></pre> <p><code>rke2_api_ip</code> should point to your load balancer for master nodes, this should be a TCP load balancer on port 6443. If you don't have one it can point to one of the master nodes.</p> <p>Alternatively you can use keepalived by removing the <code>rke2_ha_mode_keepalived</code> line, and pointing the <code>rke2_api_ip</code> to an unused static IP such as <code>10.40.140.100</code></p> </li> <li> <p>Confirm inventory is working</p> <pre><code>ansible all -i inventory -m ping\n</code></pre> <p>There should be no errors.</p> </li> </ol>"},{"location":"k8s-engine/rke2-installation-ansible/#run-the-playbook-and-confirm-the-rke2-cluster-is-up","title":"Run the playbook and confirm the RKE2 cluster is up","text":"<ol> <li> <p>Run the ansible playbook</p> <pre><code>ansible-playbook -i inventory playbook.yaml\n</code></pre> <p>If you're ssh'ing to the other machines as a non-root user, run the following instead:</p> <pre><code>ansible-playbook -i inventory playbook.yaml -K\n</code></pre> <p>Provide the password for the user you're logging in as when prompted.</p> <p>If it hangs at 'Wait for remaining nodes to be ready', check if rke2 is installed on all machines, if not the <code>rke2.sh</code> script may not be running. To fix this, edit the <code>~/.ansible/roles/lablabs.rke2/tasks/rke2.yml</code> by removing the <code>Check RKE2 version</code> task and replacing it with:</p> <pre><code>- name: Check rke2 bin exists\n  ansible.builtin.stat:\n    path: \"{{ rke2_bin_path }}\"\n  register: rke2_exists\n\n- name: Check RKE2 version\n  ansible.builtin.shell: |\n    set -o pipefail\n    {{ rke2_bin_path }} --version | grep -E \"rke2 version\" | awk '{print $3}'\n  args:\n    executable: /bin/bash\n  changed_when: false\n  register: installed_rke2_version\n  when: rke2_exists.stat.exists\n</code></pre> </li> <li> <p>Install kubectl</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Copy kubeconfig file to a better directory and export kubeconfig</p> <pre><code>cp /tmp/rke2.yaml ~/rke2.yaml\n</code></pre> <p>Now we can manage our cluster with <code>kubectl --kubeconfig ~/rke2.yaml</code>, or we can do the following to shorten our commands:</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm our cluster is running and with correct internal IP addresses</p> <pre><code>kubectl get nodes -o wide\n</code></pre> </li> <li> <p>Check the health of our pods</p> <pre><code>kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"k8s-engine/rke2-installation-ansible/#cleanup","title":"Cleanup","text":""},{"location":"k8s-engine/rke2-installation-ansible/#jump","title":"Jump","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Kubectl and Ansible</p> <pre><code>sudo snap remove kubectl\nsudo apt remove ansible\n</code></pre> </li> <li> <p>Remove remaining artifacts</p> <pre><code>rm -rf /root/.ansible\nrm /root/.ssh/id_rsa /root/.ssh/id_rsa.pub\nrm ~/rke2.yaml\nrm -rf /root/.kube\napt autoremove -y\n</code></pre> <p>If you followed the installation guide as a non-root user the directories might be different.</p> </li> </ol>"},{"location":"k8s-engine/rke2-installation-ansible/#master-and-worker-nodes","title":"Master and Worker nodes","text":"<p>Repeat the following on all master and worker nodes</p> <ol> <li> <p>ssh into the node</p> <pre><code>ssh root@10.40.140.4\n</code></pre> </li> <li> <p>Run the RKE2 uninstall script</p> <pre><code>sudo /usr/local/bin/rke2-uninstall.sh\n</code></pre> </li> </ol>"},{"location":"k8s-engine/rke2-installation/","title":"RKE2 Setup","text":""},{"location":"k8s-engine/rke2-installation/#rke2-requirements","title":"RKE2 Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY (we recommend at least 8GB) <code>2</code>  CPU (we recommend at least 4CPU) <code>60GB</code>      STORAGE <p> We turn off the firewall to avoid problems in the future. We update the packages and clean up any files left over from previous installations.</p> <p>Ubuntu:</p> <pre><code># Ubuntu instructions \n# stop the software firewall\nsystemctl disable --now ufw\n# get updates, install nfs, and apply\napt update\napt install nfs-common -y  \napt upgrade -y\n# clean up\napt autoremove -y\n</code></pre> <p>Now that we have all the nodes up to date, let's focus on <code>rancher1</code>. While this might seem controversial, <code>curl | bash</code> does work nicely. The install script will use the tarball install for Ubuntu and the RPM install for Rocky/Centos. Please be patient, the start command can take a minute. Here are the rke2 docs and install options for reference.</p>"},{"location":"k8s-engine/rke2-installation/#rke2-server-install","title":"RKE2 Server Install","text":"<pre><code>#rancher1\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=server sh -\n\nmkdir -p /etc/rancher/rke2/\ncat &lt;&lt; EOF &gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\nEOF\n\n# enable and start\nsystemctl enable --now rke2-server.service\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status rke2-server</code> and make sure it is <code>active</code>.</p> <p>Quote</p> <p>If you want to install a specific version use the following command ;</p> <pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.24 INSTALL_RKE2_TYPE=server sh -\n</code></pre> <p>Perfect!  Now we can start talking Kubernetes. We need to symlink the <code>kubectl</code> cli on <code>rancher1</code> that gets installed from RKE2.</p> <pre><code># add kubectl conf\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# check node status\nkubectl get node\n</code></pre> <p>Quote</p> <p>In addition these commands can be used for KUBECONFIG</p> <pre><code># simlink all the things - kubectl\nln -s $(find /var/lib/rancher/rke2/data/ -name kubectl) /usr/local/bin/kubectl\n\n# add kubectl conf\nexport KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n</code></pre> <p>We will also need to get the token from <code>rancher1</code>.</p> <pre><code># save this for rancher2 and rancher3\ncat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"k8s-engine/rke2-installation/#rke2-agent-install","title":"RKE2 Agent Install","text":"<p>The agent install is VERY similar to the server install. Except that we need an agent config file before starting. We will start with <code>rancher2</code>. We need to install the agent and setup the configuration file.</p> <pre><code># we add INSTALL_RKE2_TYPE=agent\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=agent sh -\n\n# create config file\nmkdir -p /etc/rancher/rke2/ \n\n# change the ip to reflect your rancher1 ip\necho \"server: https://$RANCHER1_IP:9345\" &gt; /etc/rancher/rke2/config.yaml\n\n# change the Token to the one from rancher1 /var/lib/rancher/rke2/server/node-token \necho \"token: $TOKEN\" &gt;&gt; /etc/rancher/rke2/config.yaml\n\n# enable and start\nsystemctl enable --now rke2-agent.service\n</code></pre> <p>If you want to add your Node as control plane, etcd replace here with below code</p> <p>Example</p> Orjinal CodeChange Code <pre><code>systemctl enable --now rke2-agent.service\n</code></pre> <pre><code>systemctl enable --now rke2-server.service\n</code></pre> <p>Rinse and repeat. Run the same install commands on <code>rancher2</code>, <code>rancher3</code>. Next we can validate all the nodes are playing nice by running kubectl get node -o wide on rancher1. </p>"},{"location":"k8s-engine/rke2-installation/#run-this-code","title":"Run this code !","text":"'To check that Kubernetes is running'<pre><code>kubectl get node\n</code></pre> <p>Important Installations Notes</p> <p>If you are having problems with installations, make sure there are no problems with instances' accessing each other <code>(For Example --Ssh connection--: Permission Denied)</code></p> <p>Check below steps if RKE2-Server or RKE2-Agent is not working </p> <ul> <li> Rancher1 instances Node Token is correct ? </li> <li> Instances IP address is Correct ? </li> <li> Instance Ports is open (9345, 6443) ?</li> </ul>"},{"location":"k8s-engine/rke2-ha/rke2-ha-etcd-restore/","title":"Restoring RKE2 Clusters","text":"<p>During this process, the cluster will be unavailable.</p> <ul> <li>ETCD should be restored on the first master node that was created in the cluster.</li> <li>ETCD restoration process starts by disabling all rke2 services on all nodes.</li> <li>Don't forget to remove the <code>etcd database directory</code> on secondary master nodes before starting the <code>rke2-server</code>.</li> <li>ETCD restoration process starts with limitin etcd to single node. </li> </ul>"},{"location":"k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-existing-nodes","title":"Restoring RKE2 Cluster ETCD on Existing Nodes","text":"<p>This process will restore the ETCD on the existing nodes. </p> <p>If you're adding new nodes to the cluster, you should follow the Restoring RKE2 Cluster ETCD on New Nodes section.</p> <p>1) On ALL nodes, disable the <code>rke2-server</code> service: <pre><code>systemctl disable rke2-server -now\n</code></pre></p> <p>2) On ALL nodes, kill all remaining processes: <pre><code>rke2-killall.sh\n</code></pre> 3 On FIRST NODE, restore etcd: <pre><code>rke2 server --cluster-reset \\\n    --cluster-reset-restore-path=$PATH_TO_SNAPSHOT_FILE\n</code></pre> 4) On FIRST NODE, start the rke2: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p> <p>5) On SECONDARY MASTER NODES, delete the etcd data directory: <pre><code># delete the old etcd data directory\n# make sure the directory is correct for your installation\nrm -rf /var/lib/rancher/rke2/server/db\n</code></pre> 6) On SECONDARY MASTER NODES, start the rke2 server: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p>"},{"location":"k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-new-nodes","title":"Restoring RKE2 Cluster ETCD on New Nodes","text":"<p>TODO: add docs</p>"},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/","title":"Deploy RKE2 Highly Available Cluster","text":""},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#prerequisites","title":"Prerequisites","text":"a b c 3 Linux Hosts RKE2 System Requirements"},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#prepare-the-hosts","title":"Prepare the hosts","text":"<p>[ATTENTION]: Do this on all of the master nodes</p> <p>1) Become root</p> <pre><code>sudo su\n</code></pre> <p>2) Install the OS dependencies</p> <pre><code>apt update -y &amp;&amp; apt upgrade -y\n\nsystemctl disable --now ufw\napt install nfs-common jq libselinux1 curl apparmor-profiles -y\napt autoremove -y\n\napparmor_status # check if apparmor is running\n</code></pre>"},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#master-nodes","title":"MASTER NODES","text":""},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-1","title":"MASTER NODE 1","text":"<p>Notes</p> <p>[ATTENTION] Run this steps on the first master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation.</p> <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> <p>2) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\n\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n#etcd-s3: true\n#etcd-s3-endpoint: \"s3.amazonaws.com\"\n#etcd-s3-access-key: \"AKIAAAAAAAAAAAAAAAA\"\n#etcd-s3-secret-key: \"BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\"\n#etcd-s3-region: \"eu-central-1\"\n#etcd-s3-bucket: \"---my-s3-bucket-where-i-store-etcd-backups\"\n#etcd-s3-folder: \"---myclustername-etcd-backups-folder/\"\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>3) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>4) Start the <code>rke2-server</code></p> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> <p>5) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre> <p>6) [ATTENTION] Copy <code>node-token</code> to a safe place for joining other master nodes later on</p> <pre><code>cat /var/lib/rancher/rke2/server/node-token\n# output should look like this:\n# K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\n</code></pre>"},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-2-3","title":"MASTER NODE 2 &amp; 3","text":"<p>Prerequisites</p> Required Name Description \u2705 <code>MASTER_IP</code> Static endpoint for the first master node \u2705 <code>MASTER_NODE_TOKEN</code> Should get this from first master node installation \u2705 <code>INSTALL_RKE2_VERSION</code> The RKE2 release version that you want to upgrade to <p>Notes</p> <p>[ATTENTION] Run this steps on the second and third master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Prerequisites. <pre><code>export MASTER_IP=\"172.31.15.113\" # TODO: change this to your master-1 IP\nexport MASTER_NODE_TOKEN=\"K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\"\n</code></pre> 2) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation. <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> 3) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\n# IMPORTANT\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nserver: https://$MASTER_IP:9345\ntoken: $MASTER_NODE_TOKEN\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>4) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>5) Start the <code>rke2-server</code> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> 6) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre>"},{"location":"k8s-engine/rke2-ha/rke2-highly-available-installation/#adding-worker-nodes","title":"Adding Worker Nodes","text":"<p>TODO: add docs</p>"},{"location":"k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/","title":"System Upgrade Controller","text":"<p>rancher/system-upgrade-controller</p> <p>RKE2 Docs: Automatic Upgrades</p>"},{"location":"k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Highly Available RKE2 Cluster</li> <li>Install the <code>system-upgrade-controller</code> on the cluster</li> </ul> <p>Visit for the latest installation instructions: RKE2 Docs: Install the system-upgrade-controller</p> <p>For now, the command is: (the version might change, better to follow the docs) <pre><code>kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml\n</code></pre></p>"},{"location":"k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#upgrading-master-nodes-1-by-1","title":"Upgrading Master Nodes 1-by-1","text":"<p>1) [ATTENTION] Decide on which version of RKE2 to upgrade to. <pre><code># Kubernetes Label: remove '+' and replace with '-' or it doesnt work\n\n# TODO: SET THIS TO A RELEASE NUMBER or `latest` or `stable` works as well\nexport RKE2_UPGRADE_VERSION=\"v1.21.14+rke2r1\" \nexport RKE2_UPGRADE_VERSION_SAFE_STRING=$(echo -n $RKE2_UPGRADE_VERSION | tr '+' '-')\n# \"v1.21.14+rke2r1\" --becomes--&gt; \"v1.21.14-rke2r1\"\n</code></pre></p> <p>2) [ATTENTION] Create Master Node Upgrade Plan yaml file.</p> <p>This command will create a file named <code>rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml</code></p> <p>This plan can only be applied to master nodes that have the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p>You can also add the label <code>rke2-upgrade: disabled</code> to a master node to prevent it from being upgraded. <pre><code># create the upgrade-plan for it \ncat &lt;&lt; EOF &gt; rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: server-plan-for-$RKE2_UPGRADE_VERSION_SAFE_STRING\n  namespace: system-upgrade\n  labels:\n    rke2-upgrade: server\nspec:\n  concurrency: 1\n  nodeSelector:\n    matchExpressions:\n      - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]}\n      - {key: rke2-upgrade, operator: Exists}\n      - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]}\n      - {key: rke2-upgrade-to, operator: In, values: [\"$RKE2_UPGRADE_VERSION_SAFE_STRING\"]}\n  serviceAccountName: system-upgrade\n  cordon: true\n  drain:\n    force: true\n  upgrade:\n    image: rancher/rke2-upgrade\n  version: \"$RKE2_UPGRADE_VERSION\"\nEOF\n</code></pre></p> <p>3) Apply the <code>Plan</code> to the cluster</p> <p>This will not upgrade the nodes. That'll happen when we label the nodes correctly.</p> <p><code>Plan</code> will create <code>Job</code>s for each master node that has the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p><pre><code># apply the plan and check it\nkubectl apply -f rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL.yaml\n\n# check the plan and jobs\nkubectl -n system-upgrade get plans,jobs\n</code></pre> 4) [OPTIONAL] Disable a node from upgrading <pre><code>kubectl label nodes &lt;node-name&gt; rke2-upgrade=disabled --overwrite\n</code></pre></p> <p>5) [ATTENTION] Labeling &amp; Upgrading the master nodes 1-by-1</p> <p>This step should be done for each master node one after other.</p> <p>Labeling the master nodes will trigger an automatic upgrade.</p> <p>Do this step for all master nodes one after other.</p> <pre><code>echo \"RKE2_UPGRADE_VERSION=$RKE2_UPGRADE_VERSION\"\necho \"RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL=$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL\"\n\n# label the a master node to upgrade\nkubectl label nodes &lt;node-name&gt; rke2-upgrade=enabled --overwrite\nkubectl label nodes &lt;node-name&gt; rke2-upgrade-to=$RKE2_UPGRADE_VERSION_SAFE_STRING --overwrite\n\n# check if a job is created for the upgrade of the node\nkubectl -n system-upgrade get jobs,plans\n\n# watch as the node is upgraded\nkubectl get nodes --watch\n</code></pre>"},{"location":"k8s-storage/longhorn/","title":"Longhorn Block Storage","text":"<p>Hello everybody! \ud83e\udee1</p> <p>Today I'm going to tell you about Longhorn Block Storage. First of all, what is Longhorn? What is it for? Why is it used? Let's find out... \ud83d\udcaa</p> <p>Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes.</p> <p>Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation.</p> <p>With Longhorn, you can:</p> <ul> <li>Use Longhorn volumes as persistent storage for the distributed stateful applications in your Kubernetes cluster</li> <li>Partition your block storage into Longhorn volumes so that you can use Kubernetes volumes with or without a cloud provider</li> <li>Replicate block storage across multiple nodes and data centers to increase availability</li> <li>Store backup data in external storage such as NFS or AWS S3</li> <li>Create cross-cluster disaster recovery volumes so that data from a primary Kubernetes cluster can be quickly recovered from backup in a second Kubernetes cluster</li> <li>Schedule recurring snapshots of a volume, and schedule recurring backups to NFS or S3-compatible secondary storage</li> <li>Restore volumes from backup</li> <li>Upgrade Longhorn without disrupting persistent volumes</li> </ul> <p>So what have we learned about Longhorn so far? Now let's see how we install K8s on it.</p> <p>\ud83d\udccc First of all, we need to set up the <code>helm</code> in our cluster.</p> <p>Okay, let's go on now.</p> <p>Now let's add <code>Longhorn</code> to the helm repository and update our repom. <pre><code>helm repo add longhorn https://charts.longhorn.io \nHelm repo update\n</code></pre></p> <p>If we get here smoothly, we can go on.</p> <p>Next, we have to set up the Longhorn. Let's create a new namespace and install Longhorn. \ud83d\udc47</p> <pre><code>kubectl create namespace longhorn-system helm install longhorn longhorn/longhorn --namespace longhorn-system\n</code></pre> <p>Installation finished \u2705</p> <p>How easy it was, wasn't it? It would be nice to see Longhorn as the UI at once,is it? Let's do that.</p> <pre><code>kubectl get service -n longhorn-system\n</code></pre> <p>Let's list <code>longhorn-frontend</code> with the command above. And with the command below, let's get this service <code>port-forward</code>.</p> <pre><code>kubectl port-forward -n longhorn-system service/longhorn-frontend 8080:80\n</code></pre> <p>Now visit <code>localhost:8080</code> and enjoy <code>longhorn</code>.</p> <p>We need to test the last <code>Longhorn</code> that we set up, right?</p> <pre><code>helm install my-release oci://registry-1.docker.io/bitnamicharts/mysql\n</code></pre> <p>Let's use the following command to display bitnamicharts/mysql <code>PersistentVolume, PersistentVolumeClaim</code> and the <code>Longhorn StorageClass</code> that we have installed.</p> <pre><code>kubectl get pv,pvc -n default\nkubectl get storageclass -n default\n</code></pre> <p>In fact, the <code>Longhorn</code> installation is so simple. See you on the next blog. \ud83c\udf88</p>"},{"location":"k8s-storage/nfs-install/","title":"NFS Setup Requirements","text":"<ul> <li>1 Master Node</li> <li>1 Worker Node (NFS)</li> </ul> <p>First of all, we save the ip address of our NFS server in the <code>/etc/hosts</code> file on all our nodes.(<code>master - nfs</code>)</p> <pre><code>/etc/hosts\n\n172.31.18.194 master\n172.31.20.138 k8s-ankara-nfs01\n</code></pre> <p>Then we install the necessary applications for our NFS structure.(<code>nfs</code>)</p> <pre><code>sudo apt-get update\nsudo apt-get install -y nfs-kernel-server\n</code></pre> <p>Let's create a directory on our server to store files.(<code>nfs</code>) <pre><code>sudo mkdir /k8s-data &amp;&amp; sudo mkdir /k8s-data/ankara-data\nsudo chmod 1777 /k8s-data/ankara-data\ntouch /k8s-data/ankara-data/ankara-cluster.txt\n</code></pre> We need to edit the NFS server file for the directory we just created. At this stage, we will share the directory with all our nodes.(<code>nfs</code>) <pre><code>sudo nano /etc/exports\n</code></pre></p> <p>After opening, add the following values at the end.(<code>nfs</code>) <pre><code># /etc/exports: the access control list for filesystems which may be exported\n#               to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n\n/k8s-data/ankara-data *(rw,sync,no_root_squash,subtree_check)\n</code></pre></p> <p>Then run the following command to re-read exportfs and confirm the changes.(<code>nfs</code>) <pre><code>sudo exportfs -ra\n</code></pre></p> <p>Now we will do the following operations on all our Kubernetes nodes.(<code>master - nfs</code>) <pre><code>sudo apt-get -y install nfs-common\n</code></pre></p> <p>After the installation is finished, we check whether there is an access problem by running the following command on all our nodes.(<code>master</code>) <pre><code>showmount -e k8s-ankara-nfs01\n</code></pre></p> <p>There does not seem to be any problem. Now we can mount the folder we opened on our NFS server to our nodes.(<code>master</code>) <pre><code>sudo mount k8s-ankara-nfs01:/k8s-data/ankara-data /mnt\nls -l /mnt\n</code></pre></p> <p>We have made all the setup and adjustments for NFS. We can store the data of our pods without any problems.</p> <p>To make our NFS available to pods, the sample Persistent Volume, Persistent Volume Claim and Pod yaml file should be as follows. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ankara-cluster-test-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /k8s-data/ankara-data\n    server: k8s-ankara-nfs01\n    readOnly: false\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ankara-pv-claim\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: data-generator-pod\nspec:\n  containers:\n  - name: data-generator-container\n    image: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      while true; do\n        echo \"$(date) - Data content: $(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 10)\" &gt; /data/data-$(date +%s).txt\n        sleep 1\n      done\n    volumeMounts:\n    - name: data-volume\n      mountPath: /data\n  volumes:\n  - name: data-volume\n    persistentVolumeClaim:\n      claimName: ankara-pv-claim\n</code></pre></p>"},{"location":"kubernetes/keda/keda/","title":"KEDA (Kubernetes Event-driven Autoscaling)","text":"<p>What is KEDA ?</p> <p>KEDA is a lightweight, open-source Kubernetes event-driven autoscaler used by DevOps, SRE, and Ops teams to horizontally scale pods based on external events or triggers. KEDA helps to extend the capability of native Kubernetes autoscaling solutions, which rely on standard resource metrics such as CPU or memory. You can deploy KEDA into a Kubernetes cluster and manage the scaling of pods using custom resource definitions (CRDs). Built on top of Kubernetes HPA, KEDA scales pods based on information from event sources such as AWS SQS, Kafka, RabbitMQ, etc. These event sources are monitored using scalers, which activate or deactivate deployments based on the rules set for them. KEDA scalers can also feed custom metrics for a specific event source, helping DevOps teams observe metrics relevant to them</p> <p> KEDA scales down the number of pods to zero in case there are no events to process. This is harder to do using the standard HPA, and it helps ensure effective resource utilization and cost optimization, ultimately bringing down the cloud bills..</p> <p>Quote</p> <p>KEDA supports a lot of built-in scalers and external scalers. External scalers include Redis, MYSQL,Prometheus,Rabbit MQ etc. Using external events as triggers aids efficient autoscaling, especially for message-driven microservices like payment gateways or order systems. Since KEDA can be extended by developing integrations with any data source, it can easily fit into any DevOps toolchain. </p>"},{"location":"kubernetes/keda/keda/#keda-components","title":"KEDA Components","text":"<p>Event Sources</p> <p>These are the external event/trigger sources by which KEDA changes the number of pods. Prometheus, RabbitMQ, and Apache Pulsar are some examples of event sources.</p> <p>Metric Adapter</p> <p>Metrics adapter takes metrics from scalers and translates or adapts them into a form that HPA/controller component can understand.</p> <p>Controller</p> <p>The controller/operator acts upon the metrics provided by the adapter and brings about the desired deployment state specified in the ScaledObject (refer below).</p> <p>ScaledObject and ScaledJob:</p> <p>ScaledObject represents the mapping between event sources and objects, and specifies the scaling rules for a Deployment, StatefulSet, Jobs or any Custom Resource in a K8s cluster. Similarly, ScaledJob is used to specify scaling rules for Kubernetes Jobs.</p> <p>Below is an example of a ScaledObject which configures KEDA autoscaling based on Prometheus metrics. Here, the deployment object \u2018keda-test\u2019 is scaled based on the trigger threshold (50) from Prometheus metrics. KEDA will scale the number of replicas between a minimum of 1 and a maximum of 10, and scale down to 0 replicas if the metric value drops below the threshold.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: demo3\nspec:\n  scaleTargetRef:\n    apiVersion: argoproj.io/v1alpha1\n    kind: Rollout\n    name: keda-test-demo3\n  triggers:\n    - type: prometheus\n      metadata:\n      serverAddress:  http://&lt;prometheus-host&gt;:9090\n      metricName: http_request_total\n      query: envoy_cluster_upstream_rq{appId=\"300\", cluster_name=\"300-0\", container=\"envoy\", namespace=\"test\", response_code=\"200\" }\n      threshold: \"50\"\n  idleReplicaCount: 0                       \n  minReplicaCount: 1\n  maxReplicaCount: 10\n</code></pre> <p>Deploying KEDA on any Kubernetes cluster is easy, as it doesn\u2019t need overwriting or duplication of existing functionalities. Once deployed and the components are ready, the event-based scaling starts with the external event source. The scaler will continuously monitor for events based on the source set in ScaledObject and pass the metrics to the metrics adapter in case of any trigger events. The metrics adapter then adapts the metrics and provides them to the controller component, which then scales up or down the deployment based on the scaling rules set in ScaledObject.</p> <p>Note that KEDA activates or deactivates a deployment by scaling the number of replicas to zero or one. It then triggers HPA to scale the number of workloads from one to n based on the cluster resources</p>"},{"location":"kubernetes/keda/keda/#keda-deployment","title":"Keda Deployment","text":"<p>KEDA can be deployed in a Kubernetes cluster through Helm charts, operator hub, or YAML declarations</p> <p>Helm Chart</p> <ul> <li>Add Helm repo</li> </ul> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\n</code></pre> <ul> <li>Update Helm repo</li> </ul> <pre><code>helm repo update\n</code></pre> <ul> <li>Install keda Helm chart</li> </ul> <pre><code>helm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre> <p>Deploying with operator Hub</p> <p>On Operator Hub Marketplace locate and install KEDA operator to namespace keda Create KedaController resource named keda in namespace keda</p> <p>NOTE: Further information on Operator Hub installation method can be found in the following repository.</p> <p>https://github.com/kedacore/keda-olm-operator</p> <p>Deploying using the deployment YAML files</p> <p>Run the following command (if needed, replace the version, in this case 2.13.0, with the one you are using):</p> <pre><code># Including admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0.yaml\n# Without admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0-core.yaml\n</code></pre>"},{"location":"linux/tips/","title":"Linux Tips","text":""},{"location":"linux/tooling/","title":"Linux Tooling","text":""},{"location":"linux/tooling/#fzf-fuzzy-finder","title":"FZF - Fuzzy Finder ().","text":"<ul> <li>Github: https://github.com/junegunn/fzf</li> <li>Description: A command-line fuzzy finder. Use <code>&lt;C-r&gt;</code> to search through your command history, <code>&lt;C-t&gt;</code> to search through your files.</li> </ul>"},{"location":"linux/tooling/#tldr","title":"tldr ()","text":"<ul> <li>Website: https://tldr.sh/</li> <li>Description: Too Long Didn't Read. Use it to learn about a command and its most useful options. <code>tldr &lt;any command&gt;</code> (e.g. <code>tldr curl</code>).   </li> </ul>"},{"location":"linux/shell/ampersand-nohup/","title":"nohup, &","text":""},{"location":"linux/shell/ampersand-nohup/#nohup-and","title":"nohup and &amp;","text":"<p>Ampersand (&amp;)</p> <p>The ampersand (<code>&amp;</code>) is an operator that is used to send the execution of a command to the background.</p> <p>Syntax:</p> <p><code>[command] &amp;</code></p> <p>For example, the command <code>sleep 5 &amp;</code> will start the sleep command in the background. You can then run other commands without having to wait for the sleep command to finish.</p> <p>The nohup command</p> <p>The <code>nohup</code> command is used to run a command in the background without any interruption, even when the terminal session is closed.</p> <p>Syntax:</p> <p><code>nohup [command]</code></p> <p>For example, the command <code>nohup sleep 5 &amp;</code> will start the sleep command in the background and will continue to run even if you close the terminal session.</p>"},{"location":"linux/shell/ampersand-nohup/#difference-between-nohup-and","title":"Difference between <code>nohup</code> and <code>&amp;</code>","text":"<p>The <code>nohup</code> and <code>&amp;</code> commands are both used to run commands in the background. However, there are some key differences between the two commands.</p> <ul> <li><code>nohup</code> prevents the command from being interrupted by the <code>HUP</code> signal, even if the terminal session is closed. The <code>HUP</code> signal is typically sent to a process when the terminal session is closed. This can cause the process to stop running. However, the <code>nohup</code> command catches the <code>HUP</code> signal and ignores it, so that the command continues to run even after the terminal session is closed.</li> <li><code>&amp;</code> does not prevent the command from being interrupted by the <code>HUP</code> signal. If you run a command with the <code>&amp;</code> operator and then close the terminal session, the command will stop running.</li> </ul> <p>Differences between the <code>nohup</code> and <code>&amp;</code> commands:</p> <code>nohup</code> <code>&amp;</code> Prevents <code>HUP</code> signal Yes No Redirects output to file Yes No Suitable for Running commands that need to continue running even after the terminal session is closed Running commands that don't need to continue running after the terminal session is closed"},{"location":"linux/shell/cat/","title":"cat","text":"<ul> <li>Description: Concatenate files and print on the standard output. Can be used to create a file from the standard input.</li> </ul> <p>This would display the contents of the file in the terminal window.</p> <p>Options</p> <p>The <code>cat</code> command provides several options that can be used to modify its behavior. Here are some of the most useful options:</p> <ul> <li><code>-n</code>: Shows line numbers in the output.</li> <li><code>-e</code>: Shows end-of-line characters as \"$\".</li> <li><code>-t</code>: Shows tab characters as \"^I\".</li> <li><code>-v</code>: Shows non-printable characters as \"^M\".</li> <li><code>-s</code>: Squeezes consecutive empty lines into a single line.</li> <li><code>-E</code>: Prints a \"$\" character at the end of each line.</li> </ul> <p>Heredoc (Here Document)</p> <p>The <code>heredoc</code> (Here Document) is a type of redirection that allows you to pass multiple lines of input to a command.</p> <p>The basic syntax for <code>heredoc</code> looks like this:</p> <pre><code>cat &lt;&lt; LimitString\n  text...\nLimitString\n</code></pre> <p>Here, LimitString is any string you choose, and text... is the text you want to pass to the command.</p> <p>Inline file creation with redirection</p> <p>Same thing above applies here and it saves you from creating a file and then editing it.</p> <pre><code>cat &lt;&lt; EOF &gt; newfile.txt\nThis is line 1.\nThis is line 2.\nEOF\n</code></pre> <p>This command will create <code>newfile.txt</code> file with the two lines of text.</p>"},{"location":"linux/shell/chtsh/","title":"cht.sh Command Tool","text":"<p>In Linux it can be hard to remember some commands or options and there is a great tool for that. <code>cht.sh</code></p> <p>For example :</p> <pre><code>curl cht.sh/cat\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/cat\n cheat.sheets:cat\n# POSIX way in which to cat(1); see cat(1posix).\ncat -u [FILE_1 [FILE_2] ...]\n\n# Output a file, expanding any escape sequences (default). Using this short\n# one-liner let's you view the boot log how it was show at boot-time.\ncat /var/log/boot.log\n\n# This is an ever-popular useless use of cat.\ncat /etc/passwd | grep '^root'\n# The sane way:\ngrep '^root' /etc/passwd\n\n# If in bash(1), this is often (but not always) a useless use of cat(1).\nBuffer=`cat /etc/passwd`\n# The sane way:\nBuffer=`&lt; /etc/passwd`\n\n cheat:cat\n# To display the contents of a file:\ncat &lt;file&gt;\n\n# To display file contents with line numbers\ncat -n &lt;file&gt;\n\n# To display file contents with line numbers (blank lines excluded)\ncat -b &lt;file&gt;\n\n tldr:cat\n# cat\n# Print and concatenate files.\n# More information: &lt;https://www.gnu.org/software/coreutils/cat&gt;.\n\n# Print the contents of a file to the standard output:\ncat path/to/file\n\n# Concatenate several files into an output file:\ncat path/to/file1 path/to/file2 ... &gt; path/to/output_file\n\n# Append several files to an output file:\ncat path/to/file1 path/to/file2 ... &gt;&gt; path/to/output_file\n\n# Copy the contents of a file into an output file without buffering:\ncat -u /dev/tty12 &gt; /dev/tty13\n\n# Write `stdin` to a file:\ncat - &gt; path/to/file\n</code></pre> <p>You may say, why do you need this when you already have the <code>man</code> command in Linux? The most important feature that makes <code>cht.sh</code> different is that it explains each option in the simplest and most simple way. It does not require installation.</p> <p>Here are a few more examples :</p> <pre><code>curl cht.sh/tail\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/tail\n cheat:tail\n# To show the last 10 lines of &lt;file&gt;:\ntail &lt;file&gt;\n\n# To show the last &lt;number&gt; lines of &lt;file&gt;:\ntail -n &lt;number&gt; &lt;file&gt;\n\n# To show the last lines of &lt;file&gt; starting with &lt;number&gt;:\ntail -n +&lt;number&gt; &lt;file&gt;\n\n# To show the last &lt;number&gt; bytes of &lt;file&gt;:\ntail -c &lt;number&gt; &lt;file&gt;\n\n# To show the last 10 lines of &lt;file&gt; and to wait for &lt;file&gt; to grow:\ntail -f &lt;file&gt;\n\n tldr:tail\n# tail\n# Display the last part of a file.\n# See also: `head`.\n# More information: &lt;https://www.gnu.org/software/coreutils/tail&gt;.\n\n# Show last 'count' lines in file:\ntail --lines count path/to/file\n\n# Print a file from a specific line number:\ntail --lines +count path/to/file\n\n# Print a specific count of bytes from the end of a given file:\ntail --bytes count path/to/file\n\n# Print the last lines of a given file and keep reading file until `Ctrl + C`:\ntail --follow path/to/file\n\n# Keep reading file until `Ctrl + C`, even if the file is inaccessible:\ntail --retry --follow path/to/file\n\n# Show last 'num' lines in 'file' and refresh every 'n' seconds:\ntail --lines count --sleep-interval seconds --follow path/to/file\n</code></pre> <pre><code>curl cht.sh/touch\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/touch\n cheat:touch\n# To change a file's modification time:\ntouch -d &lt;time&gt; &lt;file&gt;\ntouch -d 12am &lt;file&gt;\ntouch -d \"yesterday 6am\" &lt;file&gt;\ntouch -d \"2 days ago 10:00\" &lt;file&gt;\ntouch -d \"tomorrow 04:00\" &lt;file&gt;\n\n# To put the timestamp of a file on another:\ntouch -r &lt;refrence-file&gt; &lt;target-file&gt;\n\n tldr:touch\n# touch\n# Create files and set access/modification times.\n# More information: &lt;https://manned.org/man/freebsd-13.1/touch&gt;.\n\n# Create specific files:\ntouch path/to/file1 path/to/file2 ...\n\n# Set the file [a]ccess or [m]odification times to the current one and don't [c]reate file if it doesn't exist:\ntouch -c -a|m path/to/file1 path/to/file2 ...\n\n# Set the file [t]ime to a specific value and don't [c]reate file if it doesn't exist:\ntouch -c -t YYYYMMDDHHMM.SS path/to/file1 path/to/file2 ...\n\n# Set the file time of a specific file to the time of anothe[r] file and don't [c]reate file if it doesn't exist:\ntouch -c -r ~/.emacs path/to/file1 path/to/file2 ...\n</code></pre>"},{"location":"linux/shell/jobs-bg-fg/","title":"jobs, bg, fg","text":""},{"location":"linux/shell/jobs-bg-fg/#jobs-bg-and-fg","title":"jobs, bg, and fg","text":""},{"location":"linux/shell/jobs-bg-fg/#jobs","title":"jobs","text":"<p>The <code>jobs</code> command will list all jobs on the system; active, stopped, or otherwise.</p>"},{"location":"linux/shell/jobs-bg-fg/#example-usage","title":"Example usage:","text":"<p>1.Create a job with using</p> <p><code>sleep 500 &amp;</code></p> <p>and stop it with <code>ctrl + z</code>. </p> <p>2.List all the jobs with the command : <code>jobs</code></p> <p>You will see that you have a single stopped job identified by the job number [1].</p> <p>Other options to know for this command include:</p> <ul> <li><code>-l</code> - list PIDs in addition to default info</li> <li><code>-n</code> - list only processes that have changed since the last notification</li> <li><code>-p</code> - list PIDs only</li> <li><code>-r</code> - show only running jobs</li> <li><code>-s</code> - show only stopped jobs</li> </ul>"},{"location":"linux/shell/jobs-bg-fg/#background","title":"Background","text":"<p>The <code>bg</code> command restarts a suspended job, and runs it in the background.</p> <p><code>bg [JOB_SPEC]</code></p> <p>Where JOB_SPEC can be one of the following:</p> <ul> <li><code>%n</code>: where <code>n</code> is the job number.</li> <li><code>%abc</code>: refers to a job started by a command beginning with <code>abc</code>.</li> <li><code>%?abc</code>: refers to a job started by a command containing <code>abc</code>.</li> <li><code>%-</code>: specifies the previous job.</li> </ul>"},{"location":"linux/shell/jobs-bg-fg/#foreground","title":"Foreground","text":"<p>The <code>fg</code> command switches a job running in the background into the foreground.</p> <p><code>fg [JOB_SPEC]</code></p> <p>NOTE: If no <code>JOB_SPEC</code> is provided, <code>bg</code> and <code>fg</code> operate on the current job.</p> <p>For example, if you have two jobs running in the background, and you run the command <code>bg</code>, the job that was most recently started will be brought to the foreground.</p> <p>You can also use the <code>%</code> character to specify a job by its job number, or by a partial command name.</p>"},{"location":"linux/shell/netstat/","title":"Netstat &amp; SS Command","text":"<p>You can check the listening ports and applications with netstat as follows.</p> <p>Prerequisite By default, netstat command may not be installed on your system. Hence, use the apk command on Alpine Linux, dnf command/yum command on RHEL &amp; co, apt command/apt-get command on Debian, Ubuntu &amp; co, zypper command on SUSE/OpenSUSE, pacman command on Arch Linux to install the netstat.</p> <pre><code>sudo apt update\nsudo apt install net-tools\n</code></pre> <p>Run the netstat command along with grep command to filter out port in LISTEN state:</p> <pre><code>netstat -tulpn | grep LISTEN\nnetstat -tulpn | more\n</code></pre> <p>Where netstat command options are:</p> <p><code>-t</code> : Select all TCP ports</p> <p><code>-u</code> : Select all UDP ports</p> <p><code>-l</code> : Show listening server sockets (open TCP and UDP ports in listing state)</p> <p><code>-p</code> : Display PID/Program name for sockets. In other words, this option tells who opened the TCP or UDP port. For example, on my system, Nginx opened TCP port 80/443, so I will /usr/sbin/nginx or its PID.</p> <p><code>-n</code> : Don\u2019t resolve name (avoid dns lookup, this speed up the netstat on busy Linux/Unix servers)</p> <p>The netstat command <code>deprecated</code> for some time on Linux. Therefore, you need to use the ss command as follows:</p> <pre><code>sudo ss -tulw\nsudo ss -tulwn\nsudo ss -tulwn | grep LISTEN\n</code></pre> <p><code>-t</code> : Show only TCP sockets on Linux</p> <p><code>-u</code> : Display only UDP sockets on Linux</p> <p><code>-l</code> : Show listening sockets. For example, TCP port 22 is opened by SSHD server.</p> <p><code>-p</code> : List process name that opened sockets</p> <p><code>-n</code> : Don\u2019t resolve service names i.e. don\u2019t use DNS</p>"},{"location":"linux/shell/netstat/#ps-command","title":"PS Command","text":"<p>The ps command without any options displays information about processes that are bound by the controlling terminal. <pre><code>ps\n</code></pre> The command returns a similar output: <pre><code>PID TTY      TIME     CMD\n285 pts/2    00:00:00 zsh\n334 pts/2    00:00:00 ps\n</code></pre></p> <p>The default output of the ps command contains four columns that provide the following information:</p> <p><code>PID</code>: The process ID is your system\u2019s tracking number for the process. The PID is useful when you need to use a command like kill or nice, which take a PID as their input.</p> <p><code>TTY</code>: The controlling terminal associated with the process. Processes that do not originate from a controlling terminal and were initiated by the system at boot are displayed with a question mark.</p> <p><code>TIME</code>: The CPU usage of the process. Displays the amount of CPU time used by the process. This value is not the run time of the process.</p> <p><code>CMD</code>: The name of the command or executable that is running. The output only includes the name of the command or executable and does not display any options that were passed in.</p>"},{"location":"linux/shell/netstat/#the-aux-shortcut","title":"The <code>aux</code> shortcut","text":"<p>Now that you understand the basics of the <code>ps</code> command, this section covers the benefits to the <code>ps</code> <code>aux</code> command. The <code>ps</code> <code>aux</code> displays the most amount of information a user usually needs to understand the current state of their system\u2019s running processes. Take a look at the following example: <pre><code>ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0    892   572 ?        Sl   Nov28   0:00 /init\nroot       227  0.0  0.0    900    80 ?        Ss   Nov28   0:00 /init\nroot       228  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nzaphod     229  0.0  0.1 749596 31000 pts/0    Ssl+ Nov28   0:15 docker\nroot       240  0.0  0.0      0     0 ?        Z    Nov28   0:00 [init] &lt;defunct&gt;\nroot       247  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nroot       248  0.0  0.1 1758276 31408 pts/1   Ssl+ Nov28   0:10 /mnt/wsl/docker-desktop/docker-desktop-proxy\nroot       283  0.0  0.0    892    80 ?        Ss   Dec01   0:00 /init\nroot       284  0.0  0.0    892    80 ?        R    Dec01   0:00 /init\nzaphod     285  0.0  0.0  11964  5764 pts/2    Ss   Dec01   0:00 -zsh\nzaphod     343  0.0  0.0  23764  9836 pts/2    T    17:44   0:00 vi foo\nroot       349  0.0  0.0    892    80 ?        Ss   17:45   0:00 /init\nroot       350  0.0  0.0    892    80 ?        S    17:45   0:00 /init\nzaphod     351  0.0  0.0  11964  5764 pts/3    Ss+  17:45   0:00 -zsh\nzaphod     601  0.0  0.0  10612  3236 pts/2    R+   18:24   0:00 ps aux\n</code></pre></p> <p>The <code>ps aux</code> command displays more useful information than other similar options. For example, the <code>UID</code> column is replaced with a human-readable <code>username</code> column. <code>ps aux</code> also displays statistics about your Linux system, like the percent of CPU and memory that the process is using. The <code>VSZ</code> column displays amount of virtual memory being consumed by the process. <code>RSS</code> is the actual physical wired-in memory that is being used. The <code>START</code> column shows the date or time for when the process was started. This is different from the CPU time reported by the <code>TIME</code> column.</p>"},{"location":"linux/shell/nmap/","title":"NMap Command","text":"<p>Nmap (an acronym of Network Mapper) is an open-source command-line utility to securely manage the network. Nmap command has an extensive list of options to deal with security auditing and network exploration.</p> <p>Prerequisites To use the Nmap utility, the Nmap must be installed on your Ubuntu 22.04. Nmap is available on the official repository of Ubuntu 22.04. Before installation, it is a better practice to update the core libraries of Ubuntu 22.04 as follows:</p> <p><pre><code>sudo apt update\nsudo apt install nmap\n</code></pre> or <pre><code>sudo apt update\nsnap install nmap\n</code></pre></p>"},{"location":"linux/shell/nmap/#syntax-of-nmap-command","title":"Syntax of Nmap command","text":"<p>The syntax of the Nmap command is given below: <pre><code>nmap [options] [IP-adress or web-address]\n</code></pre> The Nmap command can be used to scan through the open ports of the host. For instance, the following command will scan the \u201cxxx.xxx.xxx\u201d for open ports..</p>"},{"location":"linux/shell/nmap/#how-to-use-the-nmap-command-to-scan-specific-ports","title":"How to use the Nmap command to scan specific port(s)","text":"<p>By default, the Nmap scans through only 1000 most used ports (these are not consecutive but important). However, there are a total of 65535 ports. The Nmap command can be used to scan a specific port or all the ports.</p> <p>To scan all ports: The -p- flag of the Nmap command helps to scan through all 65535 ports: <pre><code>nmap -p- 192.168.214.138\n</code></pre></p> <p>To scan a <code>specific port</code>: One can specify the port number as well. For instance, the following command will scan for port 88 only: <pre><code>nmap -p 88 88 192.168.214.138\n</code></pre></p>"},{"location":"linux/shell/nmap/#how-to-use-the-nmap-command-to-get-the-os-information","title":"How to use the Nmap command to get the OS information","text":"<p>The Nmap command can be used to get the <code>Operating System\u2019s information</code>. For instance, the following command will get the information of the OS associated with the IP address.</p> <pre><code>sudo nmap -O 192.168.214.138\n</code></pre>"},{"location":"linux/shell/nslookup/","title":"NSLOOKUP","text":""},{"location":"linux/shell/nslookup/#what-is-the-nslookup","title":"What is the 'nslookup'","text":""},{"location":"linux/shell/nslookup/#nslookup-stands-for-name-server-lookup-is-a-useful-command-for-getting-information-from-the-dns-server-it-is-a-network-administration-tool-for-querying-the-domain-name-system-dns-to-obtain-domain-name-or-ip-address-mapping-or-any-other-specific-dns-record-it-is-also-used-to-troubleshoot-dns-related-problems","title":"Nslookup (stands for \u201cName Server Lookup\u201d) is a useful command for getting information from the DNS server. It is a network administration tool for querying the Domain Name System (DNS) to obtain domain name or IP address mapping or any other specific DNS record. It is also used to troubleshoot DNS-related problems.","text":"<p>Syntax of the -<code>nslookup</code>- command in Linux System <pre><code>nslookup [option] [hosts]\n</code></pre></p>"},{"location":"linux/shell/nslookup/#options-of-nslookup-command","title":"Options of nslookup command:","text":"Options Description -domain=[domain-name] <code>allows you to change the default DNS name.</code> -debug <code>enables the display of debugging information.</code> -port=[port-number] <code>Use the -port option to specify the port number for queries. By default, nslookup uses port 53 for DNS queries</code> -timeout=[seconds] <code>you can specify the time allowed for the DNS server to respond. By default, the timeout is set to a few seconds</code> -type=a <code>Lookup for a record We can also view all the available DNS records for a particular record using the -type=a option</code> -type=any <code>Lookup for any record We can also view all the available DNS records using the -type=any option.</code> -type=hinfo <code>displays hardware-related information about the host. It provides details about the operating system and hardware platform</code> -type=mx <code>Lookup for an mx record MX (Mail Exchange) maps a domain name to a list of mail exchange servers for that domain. The MX record says that all the mails sent to \u201cgoogle.com\u201d should be routed to the Mail server in that domain.</code> -type=ns <code>Lookup for an ns record NS (Name Server) record maps a domain name to a list of DNS servers authoritative for that domain. It will output the name serves which are associated with the given domain.</code> -type=ptr <code>used in reverse DNS lookups. It retrieves the Pointer (PTR) records, which map IP addresses to domain names.</code> -type=soa <code>Lookup for a soa record SOA record (start of authority), provides the authoritative information about the domain, the e-mail address of the domain admin, the domain serial number, etc\u2026</code>"},{"location":"linux/shell/nslookup/#examples-for-k8s-service","title":"Examples For K8S Service","text":"<p><pre><code>kubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre> <code>kubectl exec busybox -- nslookup nginx-svc</code> <pre><code>Name:   nginx-svc.default.svc.cluster.local\nAddress: 10.100.245.19\n\nnslookup: can't resolve 'kubernetes.default'\n</code></pre></p>"},{"location":"linux/shell/nslookup/#examples-for-k8s-pod","title":"Examples For K8S Pod","text":"<p><pre><code>pod-ip-address.my-namespace.pod.cluster-domain.example\n</code></pre> <code>kubectl exec busybox -- nslookup 10-244-1-2.default.pod.cluster.local</code> <pre><code>172-17-0-3.default.pod.cluster.local\n</code></pre></p>"},{"location":"linux/shell/scp/","title":"SCP Command","text":"<p>SCP (secure copy) is a command-line utility that allows you to securely copy files and directories between two locations.</p> <p>From your local system to a remote system. <pre><code>scp -i \"pam.pem\" /home/kullanici/dizin/local_file.txt ubuntu@18.204.206.157:/home/ubuntu/\n</code></pre> From a remote system to your local system. <pre><code>scp -i \"pam.pem\" -r ubuntu@3.86.225.192:/home/ubuntu/  .\n</code></pre></p> <p>Attention  pam.pem is the password file of Ec2 instance</p> <p><code>scp</code> provides a number of options that control every aspect of its behavior. The most widely used options are:</p> <p><code>-P</code> - Specifies the remote host ssh port.</p> <p><code>-p</code> - Preserves files modification and access times.</p> <p><code>-q</code> - Use this option if you want to suppress the progress meter and non-error messages.</p> <p><code>-C</code> - This option forces scp to compresses the data as it is sent to the destination machine.</p> <p><code>-r</code> - This option tells scp to copy directories recursively.</p>"},{"location":"linux/shell/script/","title":"<code>script</code> command","text":"<pre><code>man script\n</code></pre>"},{"location":"linux/shell/script/#save-your-terminal-session","title":"Save your terminal session","text":"<pre><code>script -a -t 5 sav-my-session-name.log\n\n# do stuff\n\nexit\n# run cat sav-my-session-name.log to see the output\n</code></pre>"},{"location":"nexus/docker-hosted-repo/","title":"Docker Hosted Repository","text":""},{"location":"nexus/docker-hosted-repo/#docker-hosted-repository","title":"Docker Hosted Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(hosted)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8082.</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul>"},{"location":"nexus/docker-hosted-repo/#push-an-image-to-nexus","title":"Push an image to Nexus","text":"<p>Tag the image to repository url with HTTP connector port:</p> <pre><code>docker tag &lt;IMAGE&gt;:&lt;VERSION&gt; &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8082\" # hosted repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>If access to a repository requires the user to be authenticated, Docker will check for authentication access in the <code>.docker/config.json</code>file on your local machine. If authentication is not found, you will need to perform a <code>docker login</code> command.</p> <pre><code>docker login -u &lt;username&gt; &lt;EC2_PUBLIC_IP&gt;:8082\n</code></pre> <p>Then, push the image:</p> <pre><code>docker push &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre>"},{"location":"nexus/docker-proxy-repo/","title":"Docker Proxy Repository","text":""},{"location":"nexus/docker-proxy-repo/#docker-proxy-repository","title":"Docker Proxy Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(proxy)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8083.</li> <li>Enable Docker V1 API support if required by the remote repository.</li> <li>Add remote storage URL being proxied (e.g. https://registry-1.docker.io, https://gcr.io)</li> <li>If your remote repository is docker hub, select docker index as \"Use Docker Hub\". Otherwise, select \"Use proxy registry (specified above)\"</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8083\"  # proxy repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>From docker cli, pull an image but don't pull it from docker hub or gcr, pull it through the HTTP endpoint of your docker proxy repo that you have created above like so:</p> <pre><code>docker pull &lt;EC2_PUBLIC_IP&gt;:8083/example-image\n</code></pre> <p>This will create a pull request to your Nexus OSS, which will proxy the request to remote repository you specified before. The image from remote repository will be cached in your Nexus and will be delivered to you.</p>"},{"location":"nexus/nexus-installation/","title":"Nexus Installation - Docker Private Registry","text":"<p>This guide shows how to install Nexus to an EC2 instance and run as a container.</p>"},{"location":"nexus/nexus-installation/#requirements","title":"Requirements","text":"<ul> <li>EC2 Instance</li> <li>Docker</li> </ul>"},{"location":"nexus/nexus-installation/#ec2-configurations","title":"EC2 Configurations","text":"<ul> <li>Min. 4 GB memory</li> <li>Edit security group rules to allow port range from 8080 to 8082.</li> <li>Enable Auto-assign public IP</li> </ul>"},{"location":"nexus/nexus-installation/#installation","title":"Installation","text":"<p>Since docker volumes are persistent, a volume can be created specifically for this purpose. This is the recommended approach.</p> <pre><code>docker volume create --name nexus-data\n</code></pre> <p>The next step is to mount the volume with docker run command. We will use port 8081 to connect Nexus. Other port(s) are used for repository connections. Repository ports must be unique.</p> <p>In this guide, we open port 8082 as docker hosted repository connection and port 8083 as docker proxy connection.</p> <pre><code>docker run -d -p 8081:8081 -p 8082:8082 -p 8083:8083 --restart always --name nexus -v nexus-data:/nexus-data sonatype/nexus3\n</code></pre> <p>Browse following URL:</p> <pre><code>http://&lt;EC2_INSTANCE_PUBLIC_IP&gt;:8081\n</code></pre> <p>Default username is admin. To see the password, run the command:</p> <pre><code>sudo cat /var/lib/docker/volumes/nexus-data/_data/admin.password\n</code></pre> <p> After login, it is mandatory to set a new password.</p>"},{"location":"nexus/nexus-user-and-roles/","title":"User And Roles","text":""},{"location":"nexus/nexus-user-and-roles/#principle-of-least-privilege","title":"Principle of least privilege","text":"<p>For security purposes, we should use roles and users to grant permissions for specific tasks.</p>"},{"location":"nexus/nexus-user-and-roles/#create-role-and-user","title":"Create Role and User","text":"<ul> <li>Type : Select Nexus role</li> <li>Privileges: Add <code>nx-repository-admin-*-*-*</code> This permission will allow all actions for all artifact and repository types. <ul> <li>First and second \"*\" represent recipe and repository type (docker hosted, docker proxy, apt hosted, apt proxy etc.) </li> <li>Last one represents actions (add,browse,read,edit,delete)</li> </ul> </li> <li>Create a new user using the role just created.</li> </ul> <p> In Nexus Repository, the <code>Docker Bearer Token Realm</code> is required in order to allow anonymous pulls from Docker repositories</p> <p>To allow anonymous pull:</p> <ul> <li>Go to <code>Realms</code> in Secutiry, add <code>Docker Bearer Token Realm</code> to active category.</li> <li>Edit the repo and click <code>Allow anonymous docker pull</code></li> </ul>"},{"location":"nexus/pull-to-kubernetes/","title":"Pull Images to Kubernetes","text":""},{"location":"nexus/pull-to-kubernetes/#copy-nexus-credentials-into-kubernetes","title":"Copy Nexus Credentials into Kubernetes","text":"<p>As we mentioned before, the login process creates or updates a config.json file that holds an authorization token.</p> <p>View the config.json file:</p> <pre><code>cat ~/.docker/config.json\n</code></pre> <p>The output contains a section similar to this:</p> <p><pre><code>{\n    \"auths\": {\n        \"&lt;EC2_PUBLIC_IP&gt;:8082\": {\n            \"auth\": \"c3R...zE2\"\n        }\n    }\n}\n</code></pre> A Kubernetes cluster uses the secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p> <p>If you already ran docker login, you can copy that credential into Kubernetes:</p> <pre><code>kubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=~/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then, add the secret to default service account.</p> <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"secret-name\"}]}'\n</code></pre> <p>Here is a manifest for an example Pod that needs access to your Docker credentials:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-private-pod\nspec:\n  containers:\n    - name: private\n      image: yourusername/privateimage:version\n  imagePullSecrets:\n    - name: secret-name\n</code></pre>"},{"location":"nexus/registry-configuration/","title":"RKE2 Registry Configuration","text":"<p>Upon startup, RKE2 will check to see if a <code>registries.yaml</code> file exists at <code>/etc/rancher/rke2/</code> and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.</p> <p>Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the <code>registries.yaml</code> file and give different examples of using private registry configuration in RKE2.</p>"},{"location":"nexus/registry-configuration/#configuration-file","title":"Configuration File","text":"<p>The file consists of two main sections:</p> <ul> <li>mirrors</li> <li>configs</li> </ul> <p>Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\n</code></pre> <p>When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one.</p> <p>The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define <code>auth</code> and/or <code>tls</code>. The credentials consist of either username/password or authentication token.</p>"},{"location":"nexus/registry-configuration/#with-tls","title":"With TLS","text":"<p>Below are examples showing how you may configure <code>/etc/rancher/rke2/registries.yaml</code> on each node when using TLS.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: username # this is the registry username\n      password: password # this is the registry password\n    tls:\n      cert_file:            # path to the cert file used to authenticate to the registry\n      key_file:             # path to the key file for the certificate used to authenticate to the registry\n      ca_file:              # path to the ca file used to verify the registry's certificate\n      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate\n</code></pre> <p> If using a registry using plaintext HTTP without TLS, you need to specify <code>http://</code> as the endpoint URI scheme.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"http://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n</code></pre>"},{"location":"postgres/backup-restore/","title":"Backup","text":"<p>https://www.postgresql.org/docs/current/app-pgdump.html</p>"},{"location":"postgres/backup-restore/#opsec","title":"OPSEC","text":"<p>About the versions</p> <p>Make sure to use the same version on all postgres tooling. Do this OPSEC every time!</p> <ul> <li>Check the postgres config, including the version number on the last line:   <pre><code>pg_config\n</code></pre></li> <li>Check the <code>pg_dump</code> version:   <pre><code>pg_dump --version\npg_dumpall --version\n</code></pre></li> <li>Check the <code>psql</code> version:   <pre><code>psql --version\n</code></pre></li> <li>Check the <code>pg_restore</code> version:   <pre><code>pg_restore --version\n</code></pre></li> </ul> <p>Make sure all the versions match. Otherwise you might run into issues when restoring the backup.</p>"},{"location":"postgres/backup-restore/#choosing-the-right-backup-tool","title":"Choosing the right backup tool","text":"tool right time to use <code>pg_basebackup</code> [Physical Copy] Typically used for disaster recovery scenarios or when you need a complete copy of the database cluster. It allows for easy and efficient restoration of the entire database cluster to a specific point in time. <code>pg_dumpall</code> [All DBs in Server] Routine backups, database migration, or replicating the database structure and data to another server. <code>pg_dump</code> [Single DB in Server] Commonly used for routine backups of individual databases, database migration, and selective restoration of specific databases or objects."},{"location":"postgres/backup-restore/#pg_dump","title":"pg_dump","text":"<ul> <li><code>pg_dump</code> only dumps a single database.</li> </ul>"},{"location":"postgres/backup-restore/#pg_dumpall","title":"pg_dumpall","text":"<ul> <li> <p>Use <code>pg_dumpall</code> to back up an entire cluster, or to back up global objects that are common to all databases in a cluster (such as roles and tablespaces).</p> </li> <li> <p>Used for logical backups, creating a script that contains SQL commands to recreate the database objects and data.</p> </li> </ul>"},{"location":"postgres/backup-restore/#pg_basebackup","title":"pg_basebackup","text":"<ul> <li>Primarily used for creating physical backups of the entire PostgreSQL database cluster, including all databases, tablespaces, and configuration files.</li> </ul>"},{"location":"postgres/backup-restore/#pg_restore","title":"pg_restore","text":"<p>TODO: add pg_restore docs</p>"},{"location":"postgres/configuration/","title":"Postgres Configuration","text":""},{"location":"postgres/configuration/#helpers","title":"Helpers","text":"<ul> <li>postgresqlco.nf/ Documentation for explanations of all the configuration options.<ul> <li>You can upload your <code>postgresql.conf</code> file and it will give you recommendations on how to improve it.</li> </ul> </li> <li>pgtune.leopard.in.ua/#/ for generating a <code>postgresql.conf</code> file based on your hardware and database usage.</li> </ul>"},{"location":"postgres/configuration/#notes","title":"Notes","text":"<ul> <li> <p>Always set the <code>PGDATA</code> environment variable to the path of the data directory. As the default directory used by the postgres image could change in the future.</p> </li> <li> <p>Always set the <code>POSTGRES_PASSWORD</code> environment variable. If not set, a random password will be generated and printed in the logs. </p> </li> <li>Always manage the password externally, for example by using a Kubernetes Secret.</li> </ul> <p>Status of postgres process</p> <ul> <li>Check the status     <pre><code>sudo systemctl restart postgresql.service\n</code></pre></li> <li>[If Applicable] Enable the service to start on boot      <pre><code>sudo systemctl enable postgresql.service\n</code></pre></li> </ul>"},{"location":"postgres/configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li> <p>Located at: <code>/etc/postgresql/&lt;version&gt;/main/*</code></p> </li> <li> <p>After changing configuration, you must apply it with      <pre><code>sudo systemctl restart postgresql.service\n</code></pre></p> </li> </ul>"},{"location":"postgres/configuration/#pg_identconf-os-user-identification","title":"pg_ident.conf (OS User Identification)","text":"<ul> <li>This file is used for Operation System User -&gt; Database User mapping in PostgreSQL. </li> <li>It allows you to define mappings between a local operating system user and a PostgreSQL database user. </li> </ul>"},{"location":"postgres/configuration/#postgresqlconf-configuration","title":"postgresql.conf (Configuration)","text":"<ul> <li>This is the primary configuration file for PostgreSQL. </li> <li>It contains a wide range of settings that control the behavior of the database server. </li> <li>These settings include parameters such as: <ul> <li>database connection settings, </li> <li>memory allocation, </li> <li>logging options, </li> <li>security configurations,</li> <li>and performance-related settings.</li> </ul> </li> </ul>"},{"location":"postgres/configuration/#pg_hbaconf-host-based-authentication","title":"pg_hba.conf (Host Based Authentication)","text":"<ul> <li>This file controls client authentication in PostgreSQL. </li> <li>It specifies the rules for allowing or denying client connections based on various authentication methods such as: <ul> <li>password authentication, </li> <li>certificate-based authentication, </li> <li>or IP address-based authentication. </li> </ul> </li> <li>It is responsible for defining who can connect to the database and how they are authenticated.</li> </ul>"},{"location":"postgres/configuration/#pg_statconf-statistics","title":"pg_stat.conf (Statistics)","text":"<ul> <li>This file is related to the statistics collection in PostgreSQL. </li> <li>It defines which statistics are collected and how they are stored. </li> <li>By configuring this file, you can control the collection of various statistics such as: <ul> <li>the number of tuples read or written, </li> <li>index usage, </li> <li>query execution time, and more.  These statistics help in monitoring and performance tuning of the database.</li> </ul> </li> </ul>"},{"location":"postgres/poc-backup-restore/","title":"PoC","text":""},{"location":"postgres/poc-backup-restore/#setup","title":"Setup","text":"<ul> <li>create an Ubuntu VM</li> <li>install postgres</li> </ul>"},{"location":"postgres/poc-backup-restore/#backup","title":"Backup","text":"<ul> <li>Assumes you have a shell access running postgres instance.<ul> <li>If you're trying to do this over a network, you'll need to figure out other options for the commands run (e.g. host, port, etc.)</li> </ul> </li> </ul>"},{"location":"postgres/poc-backup-restore/#steps","title":"Steps","text":""},{"location":"postgres/poc-backup-restore/#1-go-to-a-directory-where-the-postgres-user-has-write-access","title":"1. Go to a directory where the <code>postgres</code> user has write access","text":"<pre><code>cd /tmp\n</code></pre>"},{"location":"postgres/poc-backup-restore/#2-create-a-backup-file","title":"2. Create a Backup file","text":"<p>!!! Note If you're not certain on which tool to use, try to use the <code>pg_dumpall</code>.</p> <p>2.A. Using <code>pg_dump</code></p> <p>Only dumps a single database named <code>postgres</code>. </p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dump -U postgres -d postgres -f db_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dump -Fc -U postgres postgres &gt; db_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul> <p>2.B. Using <code>pg_dumpall</code></p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul>"},{"location":"postgres/poc-backup-restore/#upload-the-backup-file-to-a-remote-storage","title":"Upload the backup file to a remote storage","text":"<ul> <li>Create a script that will upload the backup file to a remote storage (e.g. AWS S3, Azure Blob Storage, etc.)</li> </ul>"},{"location":"postgres/poc-backup-restore/#restore","title":"Restore","text":""},{"location":"postgres/poc-backup-restore/#restore-a-single-db","title":"Restore a Single DB","text":"<p>We are now working on the host machine that'll be backed up to.</p> <p>[IA] Delete the existing DB from the server <pre><code>sudo -u postgres -- dropdb existing-db-name\n</code></pre></p> <p>[IA] Create a new DB to restore to</p> <p>Create a new DB named <code>new-db-name</code> from the <code>template0</code> template. <pre><code>sudo -u postgres -- createdb -T template0 new-db-name\n</code></pre></p> <p>Restore a single DB from .sql file</p> <p>SQL files are restored with <code>psql</code>.</p> <pre><code>sudo -u postgres -- psql -U postgres -f db_backup.sql new-db-name \n</code></pre>"},{"location":"postgres/poc-backup-restore/#restore-a-entire-db-server","title":"Restore a Entire DB Server","text":""},{"location":"postgres/poc-backup-restore/#check-the-restoration","title":"Check the Restoration","text":"<p>Enter the psql shell to verify the restore was successful. <pre><code>sudo -u postgres -- psql -U postgres\n</code></pre> Run psql commands: <pre><code>-- list databases \n\\l\n-- connect to the new db\n\\c new-db-name\n-- list tables\n\\dt\n-- list rows in a table\nSELECT * FROM users;\n-- list users\n\\du\n-- list groups\n\\dg\n</code></pre></p>"},{"location":"postgres/psql/","title":"psql CLI","text":""},{"location":"postgres/psql/#installation","title":"Installation","text":"<p>Make sure to use the appropriate version of the client for the server</p> <p><code>bash     sudo apt install postgresql-client-&lt;version-number&gt;</code></p> <p>Go to official Download Page</p>"},{"location":"postgres/psql/#running-commands","title":"Running commands","text":"<p>Accessing with the OS user</p> <p>Run your commands as the <code>postgres</code> user. This is the default user created by the postgres image.</p> <pre><code># sudo -u postgres:: will run the command as the postgres user\nsudo -u postgres psql &lt;database-name&gt;\n</code></pre> <p>The <code>postgres</code> user is the only user that can connect to the database without a password, create other users and databases.</p> <p>Using the credentials</p> <pre><code>psql --host &lt;your-servers-dns-or-ip&gt; \\\n    --username postgres \\\n    --password \\\n    --dbname template1\n</code></pre>"},{"location":"postgres/psql/#psql-commands","title":"psql commands","text":"Command Description <code>\\conninfo</code> Details about the current db connection <code>\\l</code> List all databases <code>\\c &lt;db-name&gt;</code> Connect/Select a database <code>\\dt+</code> Show tables in the current database <code>\\dg+</code> Show users in the current database"},{"location":"rancher/rancher-installation/","title":"Rancher Installation","text":"<p>This is a follow up to \"RKE2 Ansible Installation\" and assumes you're working on an RKE2 cluster similar to the one set up in that document.</p> <p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> <li> <p>Install kubectl if it's not already installed</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm that our nodes and pods are correct and health</p> <pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"rancher/rancher-installation/#install-rancher-on-the-rke2-cluster","title":"Install Rancher on the RKE2 cluster","text":"<ol> <li> <p>Install Helm</p> <pre><code>sudo snap install helm --classic\n</code></pre> </li> <li> <p>Add Helm chart repository (used latest here, can be latest, stable or alpha)</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> </li> <li> <p>Create a namespace for Rancher</p> <pre><code>kubectl create namespace cattle-system\n</code></pre> </li> </ol>"},{"location":"rancher/rancher-installation/#using-rancher-generated-tls-cert","title":"Using Rancher-Generated TLS Cert","text":"<ol> <li> <p>Install cert-manager (needed if using Rancher-generated TLS cert or Let\u2019s Encrypt)</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.11.0\n</code></pre> </li> <li> <p>Verify that cert-manager is deployed correctly</p> <pre><code>kubectl get pods --namespace cert-manager\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --set hostname=10.40.140.8.nip.io \\\n  --set bootstrapPassword=admin \\\n  --set global.cattle.psp.enabled=false\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> <li> <p>Wait for Rancher to be rolled out</p> <pre><code>watch kubectl -n cattle-system get pods\n</code></pre> </li> <li> <p>In a web browser navigate to the DNS name that points to your load balancer (ex: <code>10.40.140.8:nip.io</code>), you should see the login page</p> </li> </ol>"},{"location":"rancher/rancher-installation/#using-your-own-certs","title":"Using your own Certs","text":""},{"location":"rancher/rancher-installation/#formatting-certs","title":"Formatting Certs","text":"<p>It can cause complications while editing files on a machine running some form of Windows and uploading them to a Linux server. Windows-based text editors put special characters at the end of lines to denote a line return or newline. There is a simple way to correct this problem.</p> <ul> <li> <p>Install <code>dos2unix</code> and execute the command to convert line endings from DOS to Unix</p> <pre><code>sudo apt install -y dos2unix\n\ndos2unix /path/to/file/&lt;file-name&gt;\n</code></pre> </li> <li> <p>Windows servers use .pfx files which contain the public and private key. However, this can also be converted to .pem files to be used on Linux server</p> <pre><code>openssl pkcs12 -in cert.pfx -nocerts -out tls.key -nodes\nopenssl pkcs12 -in cert.pfx -nokeys -out tls.crt\n</code></pre> </li> </ul>"},{"location":"rancher/rancher-installation/#validating-certs","title":"Validating Certs","text":"<p>Before you set up your certificates, it's a good idea to test them to ensure that they are correct and will work together.</p> <ol> <li> <p>Check to see if the private key and main certificate are in PEM format. <code>openssl</code> must be installed</p> <pre><code>sudo apt install openssl -y\n\nopenssl rsa -inform PEM -in /path/to/key/tls.key\nopenssl x509 -inform PEM -in /path/to/cert/tls.crt\n</code></pre> </li> <li> <p>Verify that the private key and main certificate match</p> <pre><code>openssl x509 -noout -modulus -in tls.crt | openssl md5\nopenssl rsa -noout -modulus -in tls.key | openssl md5\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Verify that the public keys contained in the private key file and the main certificate are the same</p> <pre><code>openssl x509 -in tls.crt -noout -pubkey\nopenssl rsa -in tls.key -pubout\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Check the validty of certificate chain</p> <pre><code>openssl verify -CAfile cacerts.pem tls.crt\n\n# Response must be OK.\n</code></pre> </li> <li> <p>Check if <code>Subject Alternative Names</code> contains <code>Common Name</code></p> </li> </ol> <p>Subject Alternative Name must contains the same value of the CN. If it does not, the certificate is not valid because the industry moves away from CN</p> <pre><code>openssl x509 -noout -subject -in tls.crt\n# subject= /CN=&lt;example.domain.com&gt;\nopenssl x509 -noout -in tls.crt -text | grep DNS\n# DNS:&lt;example.domain.com&gt;\n</code></pre>"},{"location":"rancher/rancher-installation/#create-secrets-and-install","title":"Create Secrets and Install","text":"<ol> <li> <p>Create tls-ca secret with your private CA's root certificate</p> <pre><code>kubectl -n cattle-system create secret generic tls-ca \\\n    --from-file=cacerts.pem=./cacerts.pem\n</code></pre> </li> <li> <p>Create cert and key secrets</p> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n    --cert=tls.crt \\\n    --key=tls.key\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n    --namespace cattle-system \\\n    --set hostname=10.40.140.8.nip.io \\\n    --set bootstrapPassword=admin \\\n    --set global.cattle.psp.enabled=false \\\n    --set ingress.tls.source=secret \\\n    --set privateCA=true\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> </ol>"},{"location":"rancher/rancher-installation/#cleanup","title":"Cleanup","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Rancher using helm</p> <pre><code>helm uninstall rancher -n cattle-system\n</code></pre> </li> <li> <p>Remove Helm repositories</p> <pre><code>helm repo remove jetstack\nhelm repo remove rancher-latest\n</code></pre> <p>Change <code>rancher-latest</code> with the version you used while installing (ex: <code>stable</code>, <code>alpha</code>)</p> </li> <li> <p>Remove Helm</p> <pre><code>sudo snap remove helm\n</code></pre> </li> </ol>"},{"location":"sealed-secrets/sealed-secrets/","title":"Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets","text":"<p>Let's start by defining Sealed Secrets and addressing the problem it aims to solve.</p> <p>When working with Kubernetes manifests, we often store them in Git version control systems alongside our application code. However, a challenge arises when dealing with Kubernetes Secret manifests, as they contain data that needs to be hidden but is stored in base64 format, which can be easily decrypted.</p> <p>This is where Sealed Secrets comes to the rescue. It allows us to encrypt our Secrets into SealedSecrets, making them safe to store, even in public repositories. The decryption of SealedSecrets is only possible by the controller running in the target cluster, ensuring that not even the original author can obtain the original Secret from the SealedSecret.</p> <p>To achieve this, two components are required: kubeseal, which we install locally, and the controller running in Kubernetes. With kubeseal, we encrypt our Secret manifests before pushing them to the Git repository. The Kubernetes controller is responsible for decrypting these encrypted Secret objects.</p>"},{"location":"sealed-secrets/sealed-secrets/#sealed-secret-example","title":"Sealed Secret Example","text":"<p>These encrypted secrets are encoded in a SealedSecret resource, which you can see as a recipe for creating a secret. Here is how it looks:</p> <p><pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\nspec:\n  encryptedData:\n    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n</code></pre> Once unsealed, this produces a Secret equivalent to the following: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> Having explored how a Secret is securely stored in a Git repository, let's proceed to the usage.</p>"},{"location":"sealed-secrets/sealed-secrets/#installation","title":"Installation","text":"<p>Firstly, let's set up the Sealed Secrets controller in Kubernetes. I'll perform this task using Helm.</p>"},{"location":"sealed-secrets/sealed-secrets/#helm-chart","title":"Helm Chart","text":"<p>The Sealed Secrets helm chart is now officially supported and hosted in this GitHub repo.</p> <p><pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\n</code></pre> <pre><code>helm install sealed-secrets -n kube-system sealed-secrets/sealed-secrets\n</code></pre></p> <p>When the controller in the cluster up-and-running, it will generate a key. We will perform the encryption process with kubeseal, which we will install locally. You can find the kubeseal installation below for Mac and linux.</p>"},{"location":"sealed-secrets/sealed-secrets/#kubeseal","title":"Kubeseal","text":""},{"location":"sealed-secrets/sealed-secrets/#for-mac-homebrew","title":"For Mac (Homebrew)","text":"<p>The <code>kubeseal</code> client is also available on homebrew:</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"sealed-secrets/sealed-secrets/#for-linux","title":"For Linux","text":"<p>The <code>kubeseal</code> client can be installed on Linux, using the below commands:</p> <p><pre><code>KUBESEAL_VERSION='' # Set this to, for example, KUBESEAL_VERSION='0.23.0'\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION:?}/kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> If you have <code>curl</code> and <code>jq</code> installed on your machine, you can get the version dynamically this way. This can be useful for environments used in automation and such.</p> <p><pre><code># Fetch the latest sealed-secrets version using GitHub API\nKUBESEAL_VERSION=$(curl -s https://api.github.com/repos/bitnami-labs/sealed-secrets/tags | jq -r '.[0].name' | cut -c 2-)\n\n# Check if the version was fetched successfully\nif [ -z \"$KUBESEAL_VERSION\" ]; then\n    echo \"Failed to fetch the latest KUBESEAL_VERSION\"\n    exit 1\nfi\n\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION}/kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> With the tools in place, both the cluster-side controller/operator and the client-side utility kubeseal are ready.</p>"},{"location":"sealed-secrets/sealed-secrets/#usage","title":"Usage","text":"<p>Assuming we have a Kubernetes Secret manifest file named <code>mysecret.yaml</code> as follows: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> We can encrypt the Kubernetes Secret file using the following command: <pre><code>kubeseal --controller-name sealed-secrets -o yaml -n kube-system &lt; mysecrets.yaml &gt; encrypted-mysecret.yaml\n</code></pre> Subsequently, we can apply the <code>encrypted-mysecret.yaml</code> file: <pre><code>kubectl apply -f encrypted-mysecret.yaml\n</code></pre> To summarize the process:</p> <ul> <li>The Sealed Secrets controller, installed with Helm, generated public and private keys.</li> <li>We encrypted the decrypted mysecret.yaml file locally using the kubeseal command.</li> <li>When deploying the encrypted-mysecret.yaml file into Kubernetes, the controller decrypted it with the private key, converting it into a Kubernetes Secret.</li> </ul> <p>Now, with peace of mind, we can store our Secret manifests in Git repositories, especially if you are using GitOps, where you can automate your work by planning the file directory containing your encrypted manifests as an ArgoCD application.</p>"},{"location":"sumo/sumo-linux-collector/","title":"Install a Sumo Logic Collector on Linux","text":""},{"location":"sumo/sumo-linux-collector/#download-a-sumo-logic-collector-from-a-static-url","title":"Download a Sumo Logic Collector from a Static URL","text":"<p>Invoke a web request utility such as wget. For Linux 64-bit host, you can wget the Collector from the command line:</p> <pre><code>wget \"https://collectors.sumologic.com/rest/download/linux/64\" -O SumoCollector.sh &amp;&amp; chmod +x SumoCollector.sh\n</code></pre> <p>For other hosts choose the related one above:</p> Platform Download URL Linux 64 https://collectors.au.sumologic.com/rest/download/linux/64 Linux Debian https://collectors.au.sumologic.com/rest/download/deb/64 Linux RPM https://collectors.au.sumologic.com/rest/download/rpm/64 Tarball https://collectors.au.sumologic.com/rest/download/tar Windows 32 https://collectors.au.sumologic.com/rest/download/windows Windows 64 https://collectors.au.sumologic.com/rest/download/win64 <p>Important Note: The latest release of the Sumo Collector targets the Java 8 runtime. Java 6 and Java 7 are no longer supported as the Collector runtime, and Solaris is no longer supported. When you upgrade Collectors, JRE 8 or later is required. The Sumo Collector with a bundled JRE now ships with JRE 8.</p>"},{"location":"sumo/sumo-linux-collector/#system-requirements","title":"System Requirements","text":"<p>The Sumo Logic Collector has the following system requirements:</p> <ul> <li>Operating System: Linux, major distributions 64-bit, or any generic Unix capable of running Java 1.8</li> <li>CPU: Single core</li> <li>RAM: 512MB</li> <li>Disk Space: 8GB</li> <li>TLS: Package installers require TLS 1.2 or higher</li> </ul>"},{"location":"sumo/sumo-linux-collector/#install-using-the-command-line-installer","title":"Install using the command line installer","text":"<ol> <li>Add execution permissions to the downloaded Collector file (.sh):</li> </ol> <p><pre><code>chmod +x SumoCollector.sh\n</code></pre> 2. Run the script with the parameters that you want to configure </p>"},{"location":"sumo/sumo-linux-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<p><pre><code>sudo ./SumoCollector.sh -q -Vsumo.token_and_url=&lt;installationToken&gt; -Vsources=&lt;absolute_filepath&gt;\n</code></pre> By default, the Collector will be installed in either <code>/opt/SumoCollector</code> or <code>/usr/local/SumoCollector</code>.</p>"},{"location":"sumo/sumo-linux-collector/#other-parameters-for-the-command-line-installer","title":"Other parameters for the command line installer","text":"<p>The command line installer also supports a number of other parameters, including:</p> <ul> <li>-dir [directory] : Sets a different installation directory than the default.</li> <li>-Vsumo.accessid=[accessId] : The access ID is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.accesskey=[accessKey] : The access key is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.token_and_url=[token] : The token can be either an Installation Token or Setup Wizard Token.</li> </ul> <p>The command line installer can also use all of the parameters available in the user.properties file. To use parameters from user.properties just add a <code>-V</code> to the beginning of the parameter without a space character.</p> <p>The following parameters have a different format in the command line installer:</p> user.properties cli name <code>-Vcollector.name</code> url <code>-Vcollector.url</code> proxyHost <code>-Vproxy.host</code> proxyPort <code>-Vproxy.port</code> proxyUser <code>-Vproxy.user</code> proxyPassword <code>-Vproxy.password</code>"},{"location":"sumo/sumo-linux-collector/#other-userproperties-parameters","title":"Other user.properties parameters","text":"<p>The user.properties file can also be used to configure the following parameters:</p> <ul> <li>-Vdescription : Description of the collector</li> <li>-VhostName : Name of the host machine that the collector is installed</li> <li>-Vsources : The contents of the file or files are read upon Collector registration only, it is not synchronized with the Collector's configuration on an on-going basis.</li> <li>-VsyncSources : The Source definitions will be continuously monitored and synchronized with the Collector's configuration.</li> </ul>"},{"location":"sumo/sumo-linux-collector/#start-or-stop-a-collector-using-scripts","title":"Start or Stop a Collector using Scripts","text":"<p>To start, stop, check the status of the Collector or restart it, run one of the following commands from the Collector installation directory: <pre><code>sudo ./collector start\n</code></pre> <pre><code>sudo ./collector stop\n</code></pre> <pre><code>sudo ./collector status\n</code></pre> <pre><code>sudo ./collector restart\n</code></pre></p>"},{"location":"sumo/sumo-linux-collector/#uninstall-using-the-command-line","title":"Uninstall using the command line","text":"<ol> <li> <p>In a terminal prompt, change the directory to the collector installation directory:</p> </li> <li> <p>Run the uninstall binary with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts:</p> <pre><code>sudo ./uninstall -q\n</code></pre> </li> </ol>"},{"location":"sumo/sumo-local-file-management/","title":"Local Configuration File Management","text":""},{"location":"sumo/sumo-local-file-management/#local-configuration-file-management","title":"Local Configuration File Management","text":"<p>With local configuration file management, you can configure Sources for an Installed Collector in one or more UTF-8 encoded JSON files.</p> <p>IMPORTANT NOTE: After you switch over to local configuration file management, you can no longer manage Sources through the Sumo web application or the Collector Management API.</p> <p>Local configuration file management is available on Collector version v19.108 and later.</p> <p>Benefits of local configuration file management</p> <ul> <li>You don't need to log in to the Sumo web app or use API calls. Instead, you edit the JSON configuration file(s), and they are read almost immediately by the Collector.</li> <li>If you have a large scale deployment, it can be impractical to add or edit Sources one at a time. Using local configuration management allows you to manage Sources more easily.</li> <li>You can use deployment tools so that established policies for deployments are not interrupted.</li> </ul>"},{"location":"sumo/sumo-local-file-management/#options-for-specifying-sources-in-local-configuration-files","title":"Options for specifying Sources in local configuration file(s)","text":"<p>There are two ways to implement local configuration file management:</p> <ol> <li>Specify all Sources in a single UTF-8 encoded JSON file.</li> <li>Use multiple UTF-8 encoded JSON files to specify your Sources, and put all of those files in a single folder.</li> </ol> <p>Note: Each JSON file must have a <code>.json</code> extension.</p>"},{"location":"sumo/sumo-local-file-management/#define-multiple-sources-in-a-json-file","title":"Define multiple Sources in a JSON file","text":"<p>When you define multiple Sources in a JSON file, you can define each Source in a <code>sources</code> JSON array. For example:</p> <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"LocalFile\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    },\n    {\n      \"sourceType\": \"RemoteFile\",\n      \"name\": \"Example2\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre> <p>To define a single source in a JSON file, you just have one source definition under the <code>sources</code> array. <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"DockerLogs\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"sumo/sumo-local-file-management/#configure-the-location-of-json-file-or-folder","title":"Configure the location of JSON file or folder","text":"<p>When using local file configuration management, you specify the location of the JSON file or the folder that contains multiple JSON files in the Collector's <code>config/user.properties</code> file. You need to use the <code>syncSources</code> parameter to point to your configuration file or folder.</p> <ul> <li> <p>To point to a JSON file that defines Sources for a Collector:</p> <p><code>syncSources=/path/to/sources.json</code></p> </li> <li> <p>To point to a folder that contains JSON files that define Sources for a Collector: <code>syncSources=/path/to/sources-folder</code></p> </li> <li> <p>On Windows (note the escaped backslashes): <code>syncSources=C:\\path\\to\\sources-folder\\</code></p> </li> </ul>"},{"location":"sumo/sumo-local-file-management/#type-of-sources","title":"Type of Sources","text":"<p>In our case, we are using the <code>\"LocalFile\"</code> type value for Local File Type.</p> <p>Note: You should add the parameter <code>\"sourceType\":\"LocalFile\"</code> in the source file.</p> <p>JSON Parameters for Installed Sources</p> Parameter Description sourceType The type of the data that the collector will collect. description Type a description of the Source. category Type a category of the source. cutoffTimestamp If you have a file that contains logs with timestamps spanning an entire week and set the cutoffTimestamp to two days ago, all of the logs from the entire week will be ingested since the file itself was modified more recent than the cutoffTimestamp pathExpression A valid path expression (full path) of the file to collect denylist Comma-separated list of valid path expressions from which logs will not be collected. encoding Defines the encoding form. Default is \"UTF-8\"; options include \"UTF-16\"; \"UTF-16BE\"; \"UTF-16LE\"."},{"location":"sumo/sumo-local-file-management/#important-tip","title":"Important Tip:","text":"<p>While giving the <code>pathExpression</code> use a single asterisk wildcard <code>[*]</code> for file or folder names. For example:</p> <p><code>pathExpression: \"/var/foo/*.log\"</code></p> <p>Use two asterisks <code>[**]</code> to recurse within directories and subdirectories. For example:</p> <p><code>pathExpression: \"var/**/*.log\"</code></p>"},{"location":"sumo/sumo-local-file-management/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>You can edit the JSON configuration file at any time to edit Source attributes or add new Sources. When you delete Sources from the file, they are deleted from the Collector.</p> <p>To make the changes take effect, you need to restart the Collector.</p> <p>To restart the Collector, use these commands:</p> <ul> <li> <p>Linux: <code>sudo ./collector restart</code></p> </li> <li> <p>Windows: <code>net restart sumo-collector</code></p> </li> </ul>"},{"location":"sumo/sumo-local-file-management/#full-example-of-local-source-file","title":"Full example of Local Source file :","text":"<pre><code>{\n   \"api.version\":\"v1\",\n   \"sources\":[\n      {\n         \"name\":\"Hepapi-Test-Logs\",\n         \"category\":\"DockerLogs\",\n         \"automaticDateParsing\":true,\n         \"multilineProcessingEnabled\":false,\n         \"useAutolineMatching\":false,\n         \"forceTimeZone\":false,\n         \"timeZone\":\"UTC\",\n         \"cutoffTimestamp\":0,\n         \"encoding\":\"UTF-8\",\n         \"pathExpression\":\"/var/lib/docker/containers/*/*-json.log\",\n         \"sourceType\":\"LocalFile\"\n      } \n   ]\n}\n</code></pre>"},{"location":"sumo/sumo-windows-collector/","title":"Install a Sumo Logic Collector on Windows","text":""},{"location":"sumo/sumo-windows-collector/#system-requirements-for-windows","title":"System Requirements for Windows","text":"<ul> <li>Windows 7, 32 or 64 bit</li> <li>Windows 8, 32 or 64 bit</li> <li>Windows 8.1, 32 or 64 bit</li> <li>Windows 10, 32 or 64 bit</li> <li>Windows 11, 32 or 64 bit</li> <li>Windows Server 2012</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> <li>Single core, 512MB RAM</li> <li>8GB disk space</li> <li>Package installers require TLS 1.2 or higher.</li> </ul>"},{"location":"sumo/sumo-windows-collector/#download-a-collector-from-a-static-url","title":"Download a Collector from a Static URL","text":"<ol> <li>Open a terminal window or command prompt.</li> <li>If you're using PowerShell on a 64-bit Windows host, you can use Invoke-WebRequest:</li> </ol>"},{"location":"sumo/sumo-windows-collector/#configure-usage-of-tls","title":"Configure usage of TLS","text":"<pre><code>[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Tls,Tls11,Tls12'\n</code></pre>"},{"location":"sumo/sumo-windows-collector/#download-the-installer","title":"Download the installer","text":"<pre><code>Invoke-WebRequest 'https://collectors.sumologic.com/rest/download/win64' -outfile '&lt;download_path&gt;\\SumoCollector.exe'\n</code></pre> <p>Replace the  with the location where you want to download the Collector. For example, <code>C:\\user\\example\\path\\to\\SumoCollector.exe</code>"},{"location":"sumo/sumo-windows-collector/#install-the-sumo-logic-collector-using-the-command-line-installer","title":"Install the Sumo Logic Collector using the command line installer","text":"<p>From the command prompt, run the downloaded EXE file with the parameters that you want to configure. For example:</p>"},{"location":"sumo/sumo-windows-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<pre><code>./SumoCollector.exe -console -q \"-Vsumo.token_and_url=&lt;installationToken&gt;\" \"-Vsources=&lt;filepath&gt;\"\n</code></pre> <p>Reminder: You can pass other <code>user.properties</code> parameters as well inside <code>\"\"</code>.</p> <p>When you see the <code>Finishing installation...</code> message, you can close the command prompt window. The installation is complete.</p> Parameter Description <code>-console</code> Only has an effect when used with <code>-q</code>. Causes the installer to send progress messages to the console. On Windows, for this option to take effect, you must run the installer with start /wait. For example: <code>start /wait installer.exe -q -console</code> <code>-q</code> Causes the installer to run silently, which means you won't be prompted to supply installation parameters. For any installation parameter that you do not define at the command line, Sumo will use a default value. No output is sent to the console during installation, unless you also use the <code>-console</code> parameter."},{"location":"sumo/sumo-windows-collector/#configuring-sources-for-collectors","title":"Configuring Sources for Collectors","text":"<p>After installing Collectors, you can configure Sources directly in Sumo Logic or by providing the Source settings in a JSON file.</p>"},{"location":"sumo/sumo-windows-collector/#using-a-json-file","title":"Using a JSON file","text":"<p>If you're using a UTF-8 encoded JSON file, you must provide the file before starting the Collector. The JSON file needs to be UTF-8 encoded.</p> <p>Important Note:</p> <p>In Windows Host by using double backslashes, the JSON parser will interpret each backslash as a literal character rather than an escape character. For example:</p> <ul> <li>Backslashes are NOT treated correctly:  <code>\"-Vsources=&lt;C:\\some\\path\\to\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\path\\to\\source.json&gt;\"</code></li> <li>Backslashes ARE  treated correctly:    <code>\"-Vsources=&lt;C:\\\\some\\\\path\\\\to\\\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\\\path\\\\to\\\\source.json&gt;\"</code></li> </ul>"},{"location":"sumo/sumo-windows-collector/#uninstalling-from-the-command-line","title":"Uninstalling from the command line","text":"<p>From the command prompt, run the <code>uninstall.exe</code> file with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts.</p> <pre><code>./uninstall.exe -q -console\n</code></pre>"},{"location":"vagrant/vagrant-quickstart/","title":"Vagrant","text":""},{"location":"vagrant/vagrant-quickstart/#what-is-vagrant","title":"What is Vagrant?","text":"<p>CLI tool for managing the life-cycle of VMs</p> <ul> <li>Reproducible local dev environments  </li> <li>Vagrantfile, akin to Dockerfile for VMs</li> </ul>"},{"location":"vagrant/vagrant-quickstart/#installation","title":"Installation","text":"<p>See the official Vagrant downloads page Ubuntu/Debian: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vagrant\n</code></pre></p> <p>You also need VirtualBox, VMware or Hyper-V on your machine.</p>"},{"location":"vagrant/vagrant-quickstart/#important-commands","title":"Important Commands","text":"<p>Initialize a new Vagrant environment by creating a Vagrantfile: <code>vagrant init</code> Starts and provisions the Vagrant environment: <code>vagrant up</code> Connects to machine via ssh: <code>vagrant ssh</code> Outputs status of the machine: <code>vagrant status</code> Suspends the machine: <code>vagrant suspend</code> Stops the machine: <code>vagrant halt</code> Stops and deletes all traces of the machine: <code>vagrant destroy</code> </p>"},{"location":"vagrant/vagrant-quickstart/#boxes","title":"Boxes","text":"<p>Vagrant base images are called boxes. See the official Vagrant boxes page</p>"},{"location":"vagrant/vagrant-quickstart/#synced-folders","title":"Synced Folders","text":"<p>By default, Vagrant will share your project directory (the directory with the Vagrantfile) to <code>/vagrant</code>.</p>"},{"location":"vagrant/vagrant-quickstart/#vagrantfile","title":"Vagrantfile","text":"<p>Example Vagrantfiles:</p> Vagrantfile for Jenkins   This Vagrantfile installs Jenkins, AWS CLI, unzip and zip tools.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/focal64\"\n\n  # Port forwarding\n  config.vm.network \"forwarded_port\", guest: 8080, host: 8080\n\n  config.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n    # Update repositories\n    sudo apt-get update\n\n    # Install CA certificates (optional but recommended)\n    sudo apt-get install -y ca-certificates\n\n    # Install Java (a requirement for Jenkins)\n    sudo apt-get install -y openjdk-11-jdk\n\n    # Add Jenkins repository\n    wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n    sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'\n\n    sudo apt-get update\n\n    # Install Jenkins\n    sudo apt-get install -y jenkins\n\n    # Start Jenkins\n    sudo systemctl start jenkins\n\n    # Install necessary utilities\n    sudo apt-get install -y unzip\n    sudo apt-get install -y zip\n\n    # Install AWS CLI version 2\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    unzip awscliv2.zip\n    sudo ./aws/install\n  SHELL\nend\n</code></pre> Vagrantfile using Ansible for provisioning   This Vagrantfile uses an Ansible playbook for provisioning.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.box = \"ubuntu/bionic64\"\n  config.vm.network :forwarded_port, guest: 80, host: 8080\n  config.vm.network :forwarded_port, guest: 443, host: 8081\n  config.vm.network :forwarded_port, guest: 8080, host: 8082\n  config.vm.provision \"ansible\" do |ansible|\n    ansible.playbook = \"main.yml\"\n  end\n\nend\n</code></pre> Vagrantfile with advanced networking   This Vagrantfile brings up 2 VMs, assigns them static hostnames and IPs, allows root login and password authentication, and installs Ansible on one of the VMs.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  # Define VMs\n  (1..2).each do |i|\n    config.vm.define \"vm#{i}\" do |vmconfig|\n\n      # Use CentOS 8\n      vmconfig.vm.box = \"generic/centos8\"\n\n      # Set hostname\n      vmconfig.vm.hostname = \"vm#{i}\"\n\n      # Set private network\n      vmconfig.vm.network \"private_network\", ip: \"192.168.56.1#{i}\"\n\n      # Sync project directory to /vagrant\n      vmconfig.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\"\n\n      # Enable provisioning with a shell script\n      vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n        echo 'vagrant:vagrant' | chpasswd\n        sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config\n        sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config\n        systemctl restart sshd\n      SHELL\n\n      if i == 1\n        vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n          sudo yum update -y\n          sudo yum install -y epel-release\n          sudo yum install -y python3-pip gcc openssl-devel libffi-devel python3-devel\n          sudo pip3 install --upgrade pip\n          sudo pip3 install setuptools_rust\n          pip3 install ansible\n        SHELL\n      end\n    end\n  end\n\n  # Enable ssh agent forwarding\n  config.ssh.forward_agent = true\n\nend\n</code></pre>"}]}