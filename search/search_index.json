{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Hepapi Knowledge Hub","text":""},{"location":"#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This repository was created with the singular goal of fostering collaboration, knowledge exchange, and continuous learning among our team members at Hepapi.</p> <p>We recognize that knowledge is power, and in our constantly evolving field, it's crucial to keep up with the latest technologies, methodologies, and best practices. </p> <p>This repository is a live document, and we encourage everyone to contribute. If you have something valuable to share, don't hesitate to make a contribution. Remember, what may be obvious to you could be new to someone else.</p> <p> Follow us on Linkedin   hepapi.com </p>"},{"location":"#help-us-improve","title":"Help us improve","text":"<p>We are always looking for ways to improve our documentation. If you have any suggestions, please feel free to open an issue or a pull request.</p> <p>Follow the docs on: How to contribute</p>"},{"location":"#compendium","title":"Compendium","text":"<p>opentelemetry</p> <ul> <li>OpenTelemetry Instrumentation: Manual vs. Automatic</li> </ul> <p>sealed-secrets</p> <ul> <li>Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets</li> </ul> <p>sumo</p> <ul> <li>Install a Sumo Logic Collector on Windows</li> <li>Install a Sumo Logic Collector on Linux</li> <li>Local Configuration File Management</li> </ul> <p>falcon-logscale</p> <ul> <li>falcon-logscale/javainstallation.md</li> <li>Apt package update ####</li> <li>Apt package update ####</li> <li>Installation</li> <li>Kafka Installation (kafka.sh)</li> <li>Falcon LogScale Agent(Log Collector) Setup </li> <li>Installation</li> <li>Humio Single Node Installation Guide </li> <li>Falcon LogScale Setup With Docker </li> <li>Installation</li> </ul> <p>aws</p> <ul> <li>AWS CLI</li> <li>AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?</li> </ul> <p>nexus</p> <ul> <li>Docker Hosted Repository</li> <li>Copy Nexus Credentials into Kubernetes</li> <li>Principle of least privilege</li> <li>RKE2 Registry Configuration</li> <li>Nexus Installation - Docker Private Registry</li> <li>Docker Proxy Repository</li> </ul> <p>gitlab</p> <ul> <li>GitLab Runner Installation Guide  </li> <li>GitLab Self-Hosted on Kubernetes - Installation Guide</li> </ul> <p>rancher</p> <ul> <li>Rancher Installation</li> </ul> <p>monitoring</p> <ul> <li>\ud83d\udd10 Monitoring TLS Certificate Expiration in Kubernetes with Alerting</li> </ul> <p>terragrunt</p> <ul> <li>What is Terragrunt? </li> </ul> <p>service-mesh</p> <ul> <li>What is Istio?</li> </ul> <p>docker</p> <ul> <li>Docker Management with Portainer</li> </ul> <p>kubernetes</p> <ul> <li>Upgrading an EKS Hybrid Cluster Using nodeadm</li> <li>KEDA (Kubernetes Event-driven Autoscaling) </li> <li>KEDA (Kubernetes Event-driven Autoscaling) WITH CRON</li> <li>Simplify Cluster Backups with Velero</li> <li>Velero Cheat Sheet</li> </ul> <p>postgres</p> <ul> <li>Postgres Configuration</li> <li>psql CLI</li> <li>Backup</li> <li>PoC</li> </ul> <p>k8s-storage</p> <ul> <li>k8s-storage/longhorn.md</li> <li>Prerequisites &amp; Key Considerations</li> <li>NFS Setup Requirements</li> <li>Ceph Installation Guide</li> </ul> <p>sre</p> <ul> <li>k8sgpt</li> </ul> <p>ansible</p> <ul> <li>Ansible Roles</li> <li>Install Ansible with pipx</li> <li>What is Ansible ?</li> <li>Ansible Configuration File</li> <li>Install Ansible with pipx</li> <li>Inventory and Variables</li> <li>Ansible Playbooks</li> </ul> <p>azure</p> <ul> <li>Azure Self-Hosted Agent Installation</li> </ul> <p>git</p> <ul> <li>git/Commands.md</li> <li>Downloading Git</li> <li>Version Control System (VCS)</li> </ul> <p>linux</p> <ul> <li>Linux Tooling</li> <li>Linux Tips</li> <li>NMap Command</li> <li><code>script</code> command</li> <li>nohup and &amp; </li> <li>NSLOOKUP</li> <li>Netstat &amp; SS Command</li> <li>SCP Command</li> <li>cht.sh Command Tool</li> <li>cat</li> <li>jobs, bg, and fg</li> </ul> <p>networking</p> <ul> <li>Export Required Environment Variables</li> </ul> <p>devsecops</p> <ul> <li>External Secret Operator</li> <li>Deep Dive into Security Monitoring in Kubernetes Environments: An Introduction to Falco </li> <li>DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy</li> <li>Vault Installation - Using Helm and running on Kubernetes Clsuter</li> </ul> <p>sonarqube</p> <ul> <li>How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide</li> </ul> <p>vagrant</p> <ul> <li>Vagrant</li> </ul> <p>logging</p> <ul> <li>ELK Stack with FileBeat</li> <li>Elasticsearch + Kibana Upgrade (Rolling, 3 Master Nodes)</li> <li>Elasticsearch Index Lifecycle Management</li> <li>Elasticsearch-Snapshot</li> <li>Install Loki,Promtail,Grafana</li> <li>Grafana Loki: Distributed Log Management and Collection</li> <li>Elasticsearch-Exporter</li> <li>elastalert2</li> <li>EFK Stack (Elasticsearch, Fluentbit, Kibana) via Minikube</li> </ul> <p>k8s-engine</p> <ul> <li>K3S Setup </li> <li>RKE2 Setup </li> <li>RKE2 Cluster Installation With Ansible</li> <li>Deploy RKE2 Highly Available Cluster</li> <li>Restoring RKE2 Clusters</li> <li>System Upgrade Controller</li> </ul> <p>jenkins</p> <ul> <li>Jenkins Install </li> <li>Jenkins Shared Library </li> <li>Jenkins</li> </ul>"},{"location":"repo-credit/","title":"Credits","text":"File Contributors .DS_Store deniz-icin,ersinsari13,necipulusoy,oaltinoluk,turan.topcuoglu,turantopcuoglu .github/workflows/auto-index-generator.sh Oguzhan Yilmaz .github/workflows/deploy-mkdocs-website.yml Oguzhan Yilmaz,ersinsari13 .github/workflows/git-contributors-per-file.sh ersinsari13 .gitignore Oguzhan Yilmaz .idea/.gitignore turan.topcuoglu .idea/knowledge-hub.iml turan.topcuoglu .idea/misc.xml turan.topcuoglu .idea/modules.xml turan.topcuoglu .idea/vcs.xml turan.topcuoglu LICENSE Oguzhan Yilmaz README.md Oguzhan Yilmaz .DS_Store deniz-icin,ersinsari13,necipulusoy,turan.topcuoglu devops/.DS_Store deniz-icin,ersinsari13,oaltinoluk devops/ansible/ansible-installations.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible-roles.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible.md Oguzhan Yilmaz,ersinsari13 devops/ansible/config-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/inventory-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/playbook.md Oguzhan Yilmaz,ersinsari13 devops/ansible/what-is-ansible.md Oguzhan Yilmaz,ersinsari13 devops/aws/cli.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/aws/images/SSO-Architecture.png gokhanwell devops/aws/images/SSO-Attach-Account.png gokhanwell devops/aws/images/SSO-Enable.png gokhanwell devops/aws/images/SSO-Important.png gokhanwell devops/aws/images/SSO-Linked.png gokhanwell devops/aws/images/SSO-MFA.png gokhanwell devops/aws/images/SSO-Mail-Verify.png gokhanwell devops/aws/images/SSO-Permission-Set.png gokhanwell devops/aws/sso.md gokhanwell devops/azure/agent-installation.md Oguzhan Yilmaz,Utku Toraman devops/devsecops/.DS_Store deniz-icin,ersinsari13 devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy.md ersinsari13 devops/devsecops/What is Falco.md Necip Ulusoy devops/devsecops/external-secret-operator.md ersinsari13 devops/devsecops/image-eso/external-secret-1.png ersinsari13 devops/devsecops/image-eso/external-secret-operator-resources.png ersinsari13 devops/devsecops/image-eso/external-secret-operator.png ersinsari13 devops/devsecops/image-eso/iam-1.png ersinsari13 devops/devsecops/image-eso/iam-10.png ersinsari13 devops/devsecops/image-eso/iam-11.png ersinsari13 devops/devsecops/image-eso/iam-2.png ersinsari13 devops/devsecops/image-eso/iam-3.png ersinsari13 devops/devsecops/image-eso/iam-4.png ersinsari13 devops/devsecops/image-eso/iam-5.png ersinsari13 devops/devsecops/image-eso/iam-6.png ersinsari13 devops/devsecops/image-eso/iam-7.png ersinsari13 devops/devsecops/image-eso/iam-8.png ersinsari13 devops/devsecops/image-eso/iam-9.png ersinsari13 devops/devsecops/image-eso/iam.png ersinsari13 devops/devsecops/image-eso/kube-secret-1.png ersinsari13 devops/devsecops/image-eso/pod-shell.png ersinsari13 devops/devsecops/image-eso/secret-manager-1.png ersinsari13 devops/devsecops/image-eso/secret-manager-2.png ersinsari13 devops/devsecops/image-eso/secret-manager-3.png ersinsari13 devops/devsecops/image-eso/secret-manager-4.png ersinsari13 devops/devsecops/image-eso/secret-manager-5.png ersinsari13 devops/devsecops/image-eso/secret-manager-6.png ersinsari13 devops/devsecops/image-eso/secret-store-1.png ersinsari13 devops/devsecops/image-eso/secret-store.png ersinsari13 devops/devsecops/image/check-1.png ersinsari13 devops/devsecops/image/check-2.png ersinsari13 devops/devsecops/image/check-3.png ersinsari13 devops/devsecops/image/conftest-1.png ersinsari13 devops/devsecops/image/conftest-2.png ersinsari13 devops/devsecops/image/gate-3.png ersinsari13 devops/devsecops/image/gate-4.png ersinsari13 devops/devsecops/image/gates-1.png ersinsari13 devops/devsecops/image/gates-2.png ersinsari13 devops/devsecops/image/jdk.png ersinsari13 devops/devsecops/image/jenkins-passwd.png ersinsari13 devops/devsecops/image/jenkins-plug.png ersinsari13 devops/devsecops/image/jenkins-plugin.png ersinsari13 devops/devsecops/image/jenkins-user.png ersinsari13 devops/devsecops/image/maven-tool.png ersinsari13 devops/devsecops/image/pipe-1.png ersinsari13 devops/devsecops/image/pipeline-result.png ersinsari13 devops/devsecops/image/pipeline-script-2.png ersinsari13 devops/devsecops/image/pipeline-script.png ersinsari13 devops/devsecops/image/qality-1.png ersinsari13 devops/devsecops/image/qality-3.png ersinsari13 devops/devsecops/image/quality-2.png ersinsari13 devops/devsecops/image/sonar-dash.png ersinsari13 devops/devsecops/image/sonar-login.png ersinsari13 devops/devsecops/image/sonar-server.png ersinsari13 devops/devsecops/image/sonar-token-1.png ersinsari13 devops/devsecops/image/sonar-token-2.png ersinsari13 devops/devsecops/image/sonar-token-3.png ersinsari13 devops/devsecops/image/sonarqube-1.png ersinsari13 devops/devsecops/image/sonarqube-2.png ersinsari13 devops/devsecops/image/token-jenkins.png ersinsari13 devops/devsecops/image/trivy-1.png ersinsari13 devops/devsecops/image/trivy-2.png ersinsari13 devops/devsecops/image/trivy-3.png ersinsari13 devops/devsecops/image/webhook.png ersinsari13 devops/devsecops/images/image-1.png Necip Ulusoy devops/devsecops/images/image.png Necip Ulusoy devops/devsecops/vault.md oaltinoluk devops/docker/portainer/README.md MehmetG171 devops/docker/portainer/docker-compose.yml MehmetG171 devops/docker/portainer/pics/dashboard.png MehmetG171 devops/docker/portainer/pics/env.png MehmetG171 devops/docker/portainer/pics/inspect.png MehmetG171 devops/docker/portainer/pics/login.png MehmetG171 devops/docker/portainer/pics/logs.png MehmetG171 devops/docker/portainer/pics/shell.png MehmetG171 devops/docker/portainer/pics/stats.png MehmetG171 devops/docker/portainer/pics/templates.png MehmetG171 devops/docker/portainer/user-data.sh MehmetG171 devops/falcon-logscale/agent-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/falcon-logscale-installation-docker.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humio.md FIRST_NAME LAST_NAME devops/falcon-logscale/humioinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humiosetup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/javainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/kafka.md FIRST_NAME LAST_NAME devops/falcon-logscale/kafkainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/readme.md FIRST_NAME LAST_NAME devops/falcon-logscale/zookeeperinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/git/Commands.md Oguzhan Yilmaz,Onur Ozcelik devops/git/Description.md Oguzhan Yilmaz,Onur Ozcelik devops/git/installation.md Oguzhan Yilmaz,Onur Ozcelik devops/gitlab/gitlab-runner-installation.md BoraKostem devops/gitlab/gitlab-self-hosted-installation.md BoraKostem devops/gitlab/images/image.png BoraKostem devops/gitlab/images/image1.png BoraKostem devops/gitlab/images/image2.png BoraKostem devops/gitlab/images/image3.png BoraKostem devops/index.md Oguzhan Yilmaz devops/jenkins/README.md Oguzhan Yilmaz devops/jenkins/jenkins-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/jenkins/shared-library.md Oguzhan Yilmaz,ersinsari13 devops/k8s-engine/k3s-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-highly-available-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-installation-ansible.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-engine/rke2-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-storage/images/ceph-status.png oaltinoluk devops/k8s-storage/images/rook-ceph-general.png oaltinoluk devops/k8s-storage/longhorn.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-storage/nfs-install.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-storage/nfs-storageclass-k8s.md deniz-icin devops/k8s-storage/rook-ceph.md Necip Ulusoy,oaltinoluk devops/kubernetes/Velero/Cheat-Sheet.md MehmetG171 devops/kubernetes/Velero/README.md MehmetG171 devops/kubernetes/Velero/credentials.txt MehmetG171 devops/kubernetes/Velero/pics/bucket-structure.png MehmetG171 devops/kubernetes/Velero/pics/s3-backup.png MehmetG171 devops/kubernetes/Velero/pics/s3-restore.png MehmetG171 devops/kubernetes/Velero/pics/velero-backup.png MehmetG171 devops/kubernetes/Velero/pics/velero-schedule.png MehmetG171 devops/kubernetes/Velero/pics/velero-status.png MehmetG171 devops/kubernetes/eks/hybrid-upgrade.md deniz-icin devops/kubernetes/eks/pod-security-group.yaml Oguzhan Yilmaz,ersinsari13 devops/kubernetes/keda/keda-with-cron.md BERAT UYANIK,Berat Uyan\u0131k devops/kubernetes/keda/keda.md Oguzhan Yilmaz,ersinsari13 devops/linux/shell/ampersand-nohup.md Oguzhan Yilmaz,can devops/linux/shell/cat.md Oguzhan Yilmaz devops/linux/shell/chtsh.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/jobs-bg-fg.md Oguzhan Yilmaz,can devops/linux/shell/netstat.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nmap.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nslookup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/scp.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/script.md Oguzhan Yilmaz devops/linux/tips.md Oguzhan Yilmaz devops/linux/tooling.md Oguzhan Yilmaz devops/logging/.DS_Store deniz-icin,ersinsari13 devops/logging/ELK-stack-with-FileBeat.md ersinsari13 devops/logging/Index-Lifecycle-Management.md ersinsari13,ozihan devops/logging/efk-image/efk-0.png ersinsari13 devops/logging/efk-image/efk-1.png ersinsari13 devops/logging/efk.md ersinsari13 devops/logging/elastalert2.md oaltinoluk devops/logging/elasticsearch-exporter.md ersinsari13 devops/logging/elk-snapshot.md ersinsari13 devops/logging/elk-upgrade.md necipulusoy devops/logging/image/image-1.png ersinsari13 devops/logging/image/image-10.png ersinsari13 devops/logging/image/image-11.png ersinsari13 devops/logging/image/image-12.png ersinsari13 devops/logging/image/image-13.png ersinsari13 devops/logging/image/image-2.png ersinsari13 devops/logging/image/image-3.png ersinsari13 devops/logging/image/image-4.png ersinsari13 devops/logging/image/image-5.png ersinsari13 devops/logging/image/image-6.png ersinsari13 devops/logging/image/image-7.png ersinsari13 devops/logging/image/image-8.png ersinsari13 devops/logging/image/image-9.png ersinsari13 devops/logging/images/1.png ersinsari13,ozihan devops/logging/images/2.png ersinsari13,ozihan devops/logging/images/3.png ersinsari13,ozihan devops/logging/images/4.png ersinsari13,ozihan devops/logging/images/5.png ersinsari13,ozihan devops/logging/images/6.png ersinsari13,ozihan devops/logging/images/7.png ersinsari13,ozihan devops/logging/images/dash-1.png ersinsari13 devops/logging/images/dash-2.png ersinsari13 devops/logging/images/dash-3.png ersinsari13 devops/logging/images/dash-4.png ersinsari13 devops/logging/images/dash-5.png ersinsari13 devops/logging/images/dash-6.png ersinsari13 devops/logging/images/elastalert1.png oaltinoluk devops/logging/images/elk-snap-1.png ersinsari13 devops/logging/images/elk-snap-2.png ersinsari13 devops/logging/images/elk-snap-3.png ersinsari13 devops/logging/loki-distributed-images/image1.png gokhanwell devops/logging/loki-distributed-images/image10.png gokhanwell devops/logging/loki-distributed-images/image11.png gokhanwell devops/logging/loki-distributed-images/image12.png gokhanwell devops/logging/loki-distributed-images/image13.png gokhanwell devops/logging/loki-distributed-images/image14.png gokhanwell devops/logging/loki-distributed-images/image15.png gokhanwell devops/logging/loki-distributed-images/image16.png gokhanwell devops/logging/loki-distributed-images/image17.png gokhanwell devops/logging/loki-distributed-images/image2.png gokhanwell devops/logging/loki-distributed-images/image3.png gokhanwell devops/logging/loki-distributed-images/image4.png gokhanwell devops/logging/loki-distributed-images/image5.png gokhanwell devops/logging/loki-distributed-images/image6.png gokhanwell devops/logging/loki-distributed-images/image7.png gokhanwell devops/logging/loki-distributed-images/image8.png gokhanwell devops/logging/loki-distributed-images/image9.png gokhanwell devops/logging/loki-distributed.md gokhanwell devops/logging/loki-single-binary.md ersinsari13,gokhanwell devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring.md oaltinoluk devops/networking/aws-onprem-network-connectivity.md deniz-icin devops/nexus/docker-hosted-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/docker-proxy-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-installation.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-user-and-roles.md Oguzhan Yilmaz,deniz-icin devops/nexus/pull-to-kubernetes.md Oguzhan Yilmaz,deniz-icin devops/nexus/registry-configuration.md Oguzhan Yilmaz,deniz-icin devops/opentelemetry/auto-vs-manuel-inject.md ersinsari13 devops/postgres/backup-restore.md Oguzhan Yilmaz devops/postgres/configuration.md Oguzhan Yilmaz devops/postgres/poc-backup-restore.md Oguzhan Yilmaz devops/postgres/psql.md Oguzhan Yilmaz devops/rancher/rancher-installation.md Oguzhan Yilmaz,Utku Toraman,deniz-icin devops/sealed-secrets/sealed-secrets.md Oguzhan Yilmaz,sametustaoglu devops/service-mesh/istio.md Barkin Atici devops/sonarqube/.DS_Store deniz-icin devops/sonarqube/advanced-installation.md deniz-icin devops/sonarqube/images/after_login.png deniz-icin devops/sonarqube/images/login_screen.png deniz-icin devops/sre/.DS_Store deniz-icin,ersinsari13 devops/sre/k8sgpt.md ersinsari13 devops/sre/k8sgpt/image-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-1.png ersinsari13 devops/sre/k8sgpt/k8sgpt-2.png ersinsari13 devops/sre/k8sgpt/k8sgpt-3.png ersinsari13 devops/sre/k8sgpt/k8sgpt-4.png ersinsari13 devops/sre/k8sgpt/k8sgpt-5.png ersinsari13 devops/sre/k8sgpt/k8sgpt-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-7.png ersinsari13 devops/sre/k8sgpt/k8sgpt-8.png ersinsari13 devops/sre/k8sgpt/k8sgpt-9.png ersinsari13 devops/sumo/sumo-linux-collector.md Oguzhan Yilmaz,can devops/sumo/sumo-local-file-management.md Oguzhan Yilmaz,can devops/sumo/sumo-windows-collector.md Oguzhan Yilmaz,can devops/terragrunt/Readme.md MehmetG171 devops/terragrunt/empty.txt MehmetG171 devops/terragrunt/environments/dev/env.hcl MehmetG171 devops/terragrunt/environments/dev/us-east-1/region.hcl MehmetG171 devops/terragrunt/environments/dev/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/dev/us-west-2/region.hcl MehmetG171 devops/terragrunt/environments/dev/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/prod/env.hcl MehmetG171 devops/terragrunt/environments/prod/us-east-1/region.hcl MehmetG171 devops/terragrunt/environments/prod/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/prod/us-west-2/region.hcl MehmetG171 devops/terragrunt/environments/prod/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/terragrunt.hcl MehmetG171 devops/terragrunt/initial_configs/AWSTerraformInitialConfigs_Environment.yaml MehmetG171 devops/terragrunt/initial_configs/AWSTerraformInitialConfigs_Management.yaml MehmetG171 devops/terragrunt/modules/vpc/main.tf MehmetG171 devops/terragrunt/modules/vpc/outputs.tf MehmetG171 devops/terragrunt/modules/vpc/variables.tf MehmetG171 devops/terragrunt/modules/vpc/versions.tf MehmetG171 devops/vagrant/vagrant-quickstart.md Oguzhan Yilmaz,Utku Toraman images/hepapi-logo.png Oguzhan Yilmaz index.md Oguzhan Yilmaz,vfarukhepapi misc/how-to-contribute/about-markdown.md Oguzhan Yilmaz misc/how-to-contribute/about-mkdocs.md Oguzhan Yilmaz misc/how-to-contribute/mkdocs-features.md Oguzhan Yilmaz qa/Fundamentals_of_QA.md turan.topcuoglu qa/QA_Bug_Reporting.md turan.topcuoglu,turantopcuoglu qa/QA_Testing_Process.md turan.topcuoglu,turantopcuoglu qa/SSH_Config.md turan.topcuoglu qa/Software_Testing_Tools.md turan.topcuoglu,turantopcuoglu qa/accessibility_tools_overview.md gizemakcay,yelizakdag qa/accessibility_wcag_2_1_standards.md gizemakcay,yelizakdag qa/accessibility_what_is.md gizemakcay,yelizakdag qa/api_karate_assertions_validations.md yelizakdag qa/api_karate_cicd_integration.md yelizakdag qa/api_karate_data_driven_testing.md yelizakdag qa/api_karate_feature_files_gherkin.md yelizakdag qa/api_karate_project_structure.md yelizakdag qa/api_karate_what_is.md yelizakdag qa/api_postman_collections_environments.md yelizakdag qa/api_postman_newman_cli.md yelizakdag qa/api_postman_test_scripts.md yelizakdag qa/api_soapui_basic_usage.md yelizakdag qa/api_soapui_soap_vs_rest.md yelizakdag qa/approaches_black_white_grey_box.md gizemakcay,yelizakdag qa/approaches_exploratory.md gizemakcay,yelizakdag qa/approaches_risk_based.md gizemakcay,yelizakdag qa/approaches_shift_left.md gizemakcay,yelizakdag qa/automation_testing_and_best_practices.md turan.topcuoglu,turantopcuoglu qa/best_continuous_testing.md yelizakdag qa/best_cross_browser_testing.md yelizakdag qa/best_flaky_test_management.md yelizakdag qa/best_parallel_test_execution.md yelizakdag qa/best_test_automation_pyramid.md yelizakdag qa/best_test_code_review.md yelizakdag qa/best_test_maintenance_strategies.md yelizakdag qa/bug_jira_usage.md yelizakdag qa/bug_life_cycle.md yelizakdag qa/bug_reporting_best_practices.md yelizakdag qa/bug_sprint_planning_qa.md yelizakdag qa/cicd_artifacts_reporting.md yelizakdag qa/cicd_azure_devops_pipelines.md yelizakdag qa/cicd_github_actions.md yelizakdag qa/cicd_jenkins.md yelizakdag qa/cicd_test_automation_integration.md yelizakdag qa/cicd_yaml_pipeline_examples.md yelizakdag qa/fundamentals_agile_scrum_qa_role.md gizemakcay,yelizakdag qa/fundamentals_sdlc_stlc.md gizemakcay,yelizakdag qa/fundamentals_test_levels.md gizemakcay,yelizakdag qa/fundamentals_test_types.md gizemakcay,yelizakdag qa/fundamentals_what_is_qa_qa_vs_qc.md gizemakcay,yelizakdag qa/index.md Oguzhan Yilmaz,turantopcuoglu qa/perf_gatling_assertions_reporting.md yelizakdag qa/perf_gatling_load_models.md yelizakdag qa/perf_gatling_test_structure.md yelizakdag qa/perf_gatling_what_is.md yelizakdag qa/perf_jmeter_listeners_reporting.md yelizakdag qa/perf_jmeter_test_plan_structure.md yelizakdag qa/perf_jmeter_thread_groups_samplers.md yelizakdag qa/perf_jmeter_what_is.md yelizakdag qa/perf_k6_cloud_grafana_integration.md yelizakdag qa/perf_k6_metrics_reporting.md yelizakdag qa/perf_k6_scenarios_executors.md yelizakdag qa/perf_k6_script_writing.md yelizakdag qa/perf_k6_studio.md yelizakdag qa/perf_k6_thresholds_checks.md yelizakdag qa/perf_k6_what_is.md yelizakdag qa/rca_example_and_qa_perspective.md gizemakcay,yelizakdag qa/rca_what_is_rca.md gizemakcay,yelizakdag qa/rca_why_and_when.md gizemakcay,yelizakdag qa/reporting_allure.md gizemakcay,yelizakdag qa/reporting_custom_reporting_approaches.md gizemakcay,yelizakdag qa/reporting_extent.md gizemakcay,yelizakdag qa/reporting_jira_integrations.md gizemakcay,yelizakdag qa/reporting_tms_automation_integrations.md gizemakcay,yelizakdag qa/test_approaches_in_quality_assurance.md turan.topcuoglu,turantopcuoglu qa/test_processes_and_qa_practices.md turan.topcuoglu,turantopcuoglu qa/tms_testrail_test_case_management.md gizemakcay,yelizakdag qa/tms_testrail_test_runs_reporting.md gizemakcay,yelizakdag qa/types_of_tests.md turan.topcuoglu,turantopcuoglu qa/ui_appium_android_ios_setup.md yelizakdag qa/ui_appium_best_practices.md yelizakdag qa/ui_appium_desired_capabilities.md yelizakdag qa/ui_appium_device_farm.md yelizakdag qa/ui_appium_mobile_locator_strategies.md yelizakdag qa/ui_appium_what_is.md yelizakdag qa/ui_playwright_agents_mcp.md yelizakdag qa/ui_playwright_cicd_integration.md yelizakdag qa/ui_playwright_installation.md yelizakdag qa/ui_playwright_locator_strategies.md yelizakdag qa/ui_playwright_pom.md yelizakdag qa/ui_playwright_project_structure_best_practices.md yelizakdag qa/ui_playwright_test_scenarios_examples.md yelizakdag qa/ui_playwright_what_is.md yelizakdag qa/ui_selenium_framework_junit_testng.md yelizakdag qa/ui_selenium_vs_playwright_when_to_use.md yelizakdag qa/ui_selenium_webdriver_grid.md yelizakdag qa/ui_selenium_what_is.md yelizakdag repo-credit.md ersinsari13 mkdocs.yml Barkin Atici,Berat Uyan\u0131k,BoraKostem,Erdem Do\u011fanay,FIRST_NAME LAST_NAME,MehmetG171,Necip Ulusoy,Oguzhan Yilmaz,Onur Ozcelik,Utku Toraman,can,deniz-icin,ersinsari13,gokhanwell,necipulusoy,oaltinoluk,ozihan,sametustaoglu,turan.topcuoglu,turantopcuoglu,yelizakdag"},{"location":"devops/","title":"devops index","text":""},{"location":"devops/ansible/ansible-installations/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core==2.12.3\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre> <p>Lastly you can install the Ansible software with:</p> <pre><code>sudo apt install ansible -y\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible-roles/","title":"Roles","text":""},{"location":"devops/ansible/ansible-roles/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible-roles/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible-roles/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic</p> <p>my_role/ |-- defaults/    |   |-- main.yml |-- files/       |-- handlers/    |   |-- main.yml |-- meta/         |   |-- main.yml |-- tasks/       |   |-- main.yml |-- templates/   |-- tests/       |-- vars/        |   |-- main.yml |-- README.md     </p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible-roles/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/ansible/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre>"},{"location":"devops/ansible/ansible/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible/#what-is-ansible","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <p>1-Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators. 2-Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems. 3-Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times. 4-Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality. 5-Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures. 6-Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows. 7-Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</p>"},{"location":"devops/ansible/ansible/#inventory-file-and-building-an-inventory","title":"Inventory file and Building an inventory","text":"<p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/ansible/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role. </p>"},{"location":"devops/ansible/ansible/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/ansible/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/ansible/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/ansible/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic my_role/ |-- defaults/ |   |-- main.yml |-- files/ |-- handlers/ |   |-- main.yml |-- meta/ |   |-- main.yml |-- tasks/ |   |-- main.yml |-- templates/ |-- tests/ |-- vars/ |   |-- main.yml |-- README.md</p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/config-file/","title":"Configuration File","text":""},{"location":"devops/ansible/config-file/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/inventory-file/","title":"Inventory File","text":"<p>Inventory file and Building an inventory</p> <p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/inventory-file/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/inventory-file/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements</p> <p>The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml</p> <p>NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: </p> <p>my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role.</p>"},{"location":"devops/ansible/playbook/","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/playbook/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/playbook/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/what-is-ansible/","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <ul> <li>Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators.</li> <li>Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems.</li> <li>Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times.</li> <li>Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality.</li> <li>Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures.</li> <li>Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows.</li> <li>Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</li> </ul>"},{"location":"devops/aws/cli/","title":"AWS CLI","text":"<p>The AWS Command Line Interface (CLI) is a unified tool for managing AWS services from the command line. With just one tool, you can control multiple AWS services, including Amazon S3, Amazon EC2, and Amazon CloudFront.</p>"},{"location":"devops/aws/cli/#installation","title":"Installation","text":"<p>Install the dependencies:</p> <pre><code>apt install glibc groff less -y\n</code></pre> <p>Install the dependencies:</p> <pre><code>   apt install glibc groff less -y\n</code></pre> <p>Follow the official guide as the <code>curl</code>ed .zip link there updates frequently.</p> <p>AWS CLIv2 Official Installation Documentation</p>"},{"location":"devops/aws/sso/","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":""},{"location":"devops/aws/sso/#aws-iam-identity-center-successor-to-aws-single-sign-on-what-is-sso","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":"<p>IAM Identity Center (successor to AWS Single Sign-On)</p> <p>Single sign-on (SSO) is an authentication solution that allows users to log in to multiple applications and websites with one-time user authentication. Given that users today frequently access applications directly from their browsers, organizations are prioritizing access management strategies that improve both security and the user experience. SSO delivers both aspects, as users can access all password-protected resources without repeated logins once their identity is validated.</p>"},{"location":"devops/aws/sso/#why-is-sso-important","title":"Why is SSO important?","text":"<p>Using SSO to streamline user logins benefits users and organizations in several ways.</p> <p></p> <ol> <li> <p>Strengthen password security </p> <p>When people don\u2019t use SSO, they must remember multiple passwords for different websites. This might lead to non-recommended security practices, such as using simple or repetitive passwords for different accounts. Besides, users might forget or mistype their credentials when logging in to a service. SSO prevents password fatigue and encourages users to create a strong password that can be used for multiple websites.</p> </li> <li> <p>Improve productivity </p> <p>Employees often use more than one enterprise application that requires separate authentication. Manually entering the username and password for every application is time-consuming and unproductive. SSO streamlines the user validation process for enterprise applications and makes it easier to access protected resources.</p> </li> <li> <p>Reduce costs </p> <p>In their attempt to remember numerous passwords, enterprise users may forget their login credentials. This results in frequent requests to retrieve or reset their passwords, which increases workload for the in-house IT teams. Implementing SSO reduces occurrences of forgotten passwords and thus minimizes the support resources in handling requests for password resets.</p> </li> <li> <p>Improve security posture</p> <p>By minimizing the number of passwords per user, SSO facilitates user access auditing and provides robust access control to all types of data. This reduces the risk of security events that target passwords, while helping organizations comply with data security regulations.</p> </li> <li> <p>Provide a better customer experience </p> <p>Cloud application vendors use SSO to provide end-users with a seamless login experience and credential management. Users manage fewer passwords and can still securely access the information and apps they need to complete their day-to-day jobs.</p> </li> </ol>"},{"location":"devops/aws/sso/#is-sso-secure","title":"Is SSO secure?","text":"<p>Yes, SSO is an advanced and desirable identity access management solution. When deployed, a single sign-on solution helps organizations with user access management for enterprise applications and resources. An SSO solution makes setting and remembering strong passwords easier for application users. In addition, the IT team can use the SSO tool to monitor user behavior, improve system resilience, and reduce security risks. </p>"},{"location":"devops/aws/sso/#how-can-aws-help-with-sso","title":"How can AWS help with SSO?","text":"<p>AWS IAM Identity Center is a cloud authentication solution that allows organizations to securely create or connect their workforce identities and manage their access centrally across AWS accounts and applications. You can create user identities or import them from external identity providers such as Okta Universal Directory or Azure. Some benefits of AWS IAM Identity Center include:</p> <ol> <li> <p>A central dashboard to manage identities for your AWS account or business applications.</p> </li> <li> <p>Multi-factor authentication support to provide a highly secure authentication experience for users. </p> </li> <li> <p>Integration support with other AWS applications for zero-configuration authentication and authorization.</p> </li> </ol>"},{"location":"devops/aws/sso/#sso-setup-aws-console","title":"SSO Setup AWS Console","text":"<ol> <li> <p>In order to use SSO, you must first enable the AWS Single Sign-On service. AWS Organizations supports IAM Identity Center (Single Sign-On) in only one AWS Region at a time. AWS Single Sign-On service works as a single service only in one region. Once activated, accounts in AWS Organization appear in the SSO service and each account in the organization creates an SSO role. Every account that is included or leave in the organization automatically update on SSO.</p> <p> </p> </li> <li> <p>By creating users and groups within the SSO service, access can be given to all accounts within the organization or to a specific account. An e-mail is sent to the people whose users have been created, and the users are verified via e-mail and directed to the relevant link.</p> <p> </p> </li> <li> <p>After completing the password process, MFA authenticator must be installed and logged in with SSO.</p> <p></p> </li> <li> <p>Then, permission is set and the user or group is given authority on the account. For example, AdministratorAccess, Billing etc. Additionally, this permission can be limited to session duration, such as 1 hour, 2 hours.</p> <p></p> </li> <li> <p>Access permission for the desired account is attached to the user or group with the permission set by going to the AWS accounts section within the SSO service. Now, the desired authority has been given to the desired user or group to access the desired account.</p> <p></p> </li> <li> <p>Access is provided via the link on SSO or the invitation email sent to users. The authorized account within the organization is entered. While performing the redirection process, it allows both through the AWS manenmagent console and provides transaction permissions through the AWS CLI. It presents the relevant credentials for AWS CLI to the user during access.</p> <p></p> </li> <li> <p>For accounts to which access is not desired, no user or permission assignment should be made.</p> </li> </ol>"},{"location":"devops/aws/sso/#references","title":"References","text":"<p>What is SSO (Single-Sign-On)?</p>"},{"location":"devops/azure/agent-installation/","title":"Azure Self-Hosted Agent Installation","text":""},{"location":"devops/azure/agent-installation/#creating-a-personal-access-token","title":"Creating a Personal Access Token","text":"<ol> <li>Log into Azure DevOps.</li> <li>Under User settings select <code>Personal access tokens</code></li> <li>Click <code>New Token</code></li> <li>Fill the <code>Name</code> and <code>Expiration</code> fields.</li> <li>In scope select <code>Custom defined</code>, then click <code>Show all scopes</code> and tick <code>Read &amp; manage</code>under <code>Agent Pools</code></li> <li>Click <code>Create</code> and make sure to securely store the token because it will not be accessible later.</li> </ol>"},{"location":"devops/azure/agent-installation/#installing-and-configuring-the-agent","title":"Installing and configuring the agent","text":"<ol> <li> <ul> <li>If you want the agent to be usable by different projects in your organization go to <code>Organization Settings</code> on your Azure Devops Organization page.</li> <li>If you want the agent to be exclusive to a specific project, go to <code>Project Settings</code> on the Azure Devops Project page.</li> </ul> </li> <li> <p>Select <code>Agent pools</code> under the <code>Pipelines</code> section on the left</p> </li> <li>Select an existing <code>Agent Pool</code> or create a new one.</li> <li>After you've selected an <code>Agent Pool</code>, click <code>New Agent</code><ul> <li>Linux: <ul> <li>Select <code>Linux</code></li> <li>Press the Copy button next to the Download button to copy the URL.</li> </ul> </li> </ul> </li> </ol>"},{"location":"devops/azure/agent-installation/#linux","title":"Linux","text":"<p>In the Linux machine:</p> <ol> <li>Create a user for the Azure Agent:     <pre><code>sudo adduser azureagent\n</code></pre></li> <li>Add the user to sudoers:     <pre><code>sudo usermod -aG sudo azureagent\n</code></pre></li> <li>If needed add the user to other necessary groups like <code>docker</code>:     <pre><code>sudo usermod -aG docker azureagent\n</code></pre></li> <li>Create any necessary working directories and grant ownership to the user:     <pre><code>sudo chown -R azureagent:azureagent /home/app/foo\n</code></pre></li> <li>Switch to the azureagent user and navigate to the <code>/home/azureagent</code> directory:     <pre><code>su - azureagent\ncd /home/azureagent\n</code></pre></li> <li>Download the agent using the URL we copied earlier:     <pre><code>wget https://vstsagentpackage.azureedge.net/agent/3.220.2/vsts-agent-linux-x64-3.220.2.tar.gz\n</code></pre></li> <li>Create a new directory for the agent and extract the tar.gz inside and confirm with ls:     <pre><code>mkdir agent\ncd agent\ntar zxf ../vsts-agent-linux-x64-3.220.2.tar.gz\nls\n</code></pre></li> <li>Run the config script to start the agent configuration:     <pre><code>./config.sh\n</code></pre></li> <li> <p>Provide the info requested by the script</p> <ul> <li>Enter your Azure Devops server URL:     <pre><code>https://dev.azure.com/orgname\n</code></pre></li> <li>Press Enter to continue using PAT then paste the PAT we created earlier.</li> <li>Enter the agent pool name and a name for the agent we're creating.</li> <li>Press enter to use the default work folder (<code>_work</code>)</li> </ul> </li> <li> <p>Configure the agent to run as a service:     <pre><code>sudo ./svc.sh install azureagent\nsudo ./svc.sh start\n</code></pre></p> </li> <li> <p>Navigate to the <code>Agent pools</code> page on Azure Devops and select the relevant <code>Agent Pool</code>, then click on the <code>Agents</code> tab to verify that our new <code>Agent</code> is added as a self-hosted agent.</p> </li> </ol>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/","title":"DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy","text":"<p>Since DevOps entered our lives, it has been loved and widely adopted, and it seems it will continue to spread rapidly. While automating and speeding up delivery processes with DevOps, we cannot overlook the security aspect, which brings the DevSecOps methodology into focus. In this article, I discussed examples of how to fully implement DevSecOps in CI by checking code quality with SonarQube, scanning code dependencies with OWASP Dependency-Check, validating your Kubernetes, Terraform, and Dockerfile files with Conftest, and scanning Docker images with Trivy. Enjoy your learning!</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#sonarqube","title":"SonarQube","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-vulnerability","title":"What is Vulnerability ?","text":"<p>Vulnerabilities are basically the security weaknesses that one might use to undermine the availability, integrity, or security of information systems. Software, hardware, networks, or even human activities are among the several parts of a system that could have these flaws. A vulnerability could be as basic as a setting gone wrong or as sophisticated as a zero-day attack.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-sonarqube","title":"What is Sonarqube ?","text":"<p>SonarQube, previously named Sonar, is an open-source platform created by SonarSource. Its purpose is to consistently examine and evaluate the quality of code, identify security vulnerabilities, and assess technical debt across different programming languages. SonarQube delivers a single dashboard that provides real-time information into the health and security of software projects.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#how-is-sonarqube-used","title":"How is SonarQube Used?","text":"<p>SonarQube functions by looking at source code and finding possible problems and vulnerabilities. It uses static analysis, code smell recognition, and security vulnerability scanning all together to give complete results. SonarQube can be added to developers' work processes to help them find problems early in the development process. SonarQube works with many computer languages, such as Python, JavaScript, TypeScript, C#, and more. It has add-ons and plugins for well-known Integrated Development Environments (IDEs) like Eclipse, IntelliJ IDEA, and Visual Studio. This lets writers get feedback and suggestions while they're writing code.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#key-features-of-sonarqube","title":"Key Features of SonarQube","text":"<ul> <li>Code Quality Analysis</li> <li>Security Vulnerability Detection</li> <li>Technical Debt Management</li> <li>Continuous Integration/Continuous Deployment (CI/CD) Integration</li> <li>Customizable Rules and Quality Profiles</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#owasp-dependency-check","title":"OWASP Dependency-Check","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-owasp-dependency-check","title":"What is OWASP Dependency-Check?","text":"<p>Dependency-Check is a Software Composition Analysis (SCA) tool that attempts to detect publicly disclosed vulnerabilities contained within a project\u2019s dependencies. It does this by determining if there is a Common Platform Enumeration (CPE) identifier for a given dependency. If found, it will generate a report linking to the associated CVE entries. Dependency-check has a command line interface, a Maven plugin, an Ant task, and a Jenkins plugin. The core engine contains a series of analyzers that inspect the project dependencies, collect pieces of information about the dependencies (referred to as evidence within the tool). The evidence is then used to identify the Common Platform Enumeration (CPE) for the given dependency. If a CPE is identified, a listing of associated Common Vulnerability and Exposure (CVE) entries are listed in a report. Other 3rd party services and data sources such as the NPM Audit API, the OSS Index, RetireJS, and Bundler Audit are utilized for specific technologies.Dependency-check automatically updates itself using the NVD Data Feeds hosted by NIST.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#conftest","title":"Conftest","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-conftest","title":"What is Conftest?","text":"<p>Conftest leverages the Open Policy Agent (OPA) to evaluate policies written in Rego language against configuration files. It's commonly used for: - Kubernetes configurations: Ensuring that Kubernetes manifests meet security and compliance requirements. - Terraform files: Validating that Terraform plans and configurations adhere to organizational policies. - Dockerfiles: Checking that Docker images are built securely and according to best practices.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#trivy","title":"Trivy","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-trivy","title":"What is Trivy?","text":"<p>Trivy is a vulnerability scanner that is open-source and has been specifically developed for containers. This program is efficient and user-friendly, helping in the detection of vulnerabilities in container images and filesystems. Trivy's primary objective is to conduct scans on container images to identify any known vulnerabilities present in the installed packages and libraries.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#some-key-features-of-trivy-include","title":"Some key features of Trivy include:","text":"<ul> <li>Comprehensive vulnerability database</li> <li>Fast and efficient scanning</li> <li>Easy integration</li> <li>Multiple output formats</li> <li>Continuous updates</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#hands-on","title":"Hands-On","text":"<p>Let's include the devsecops tools we briefly mentioned above into the pipeline and do some hands-on. Let's get started.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-1-launch-ec2-instance","title":"Step-1 Launch EC2 Instance","text":"<p>Launch an AWS t2-large Instance. Use the image as Amazon Linux. You can create a new key pair or use an existing one.  - Enable 80, 443, 8080 and 9000 port settings in the Security Group. - You can add the userdata below for jenkins,docker,trivy installation.</p> <pre><code>#! /bin/bash\n# update os\ndnf update -y\n# set server hostname as jenkins-server\nhostnamectl set-hostname jenkins-server\n# install git\ndnf install git -y\n# install java 17\ndnf install java-17-amazon-corretto-devel -y\n# install jenkins\nwget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nrpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\ndnf upgrade\ndnf install jenkins -y\nsystemctl enable jenkins\nsystemctl start jenkins\n# install docker\ndnf install docker -y\nsystemctl start docker\nsystemctl enable docker\nusermod -a -G docker ec2-user\nusermod -a -G docker jenkins\n# configure docker as cloud agent for jenkins\ncp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.bak\nsed -i 's/^ExecStart=.*/ExecStart=\\/usr\\/bin\\/dockerd -H tcp:\\/\\/127.0.0.1:2376 -H unix:\\/\\/\\/var\\/run\\/docker.sock/g' /lib/systemd/system/docker.service\nsystemctl daemon-reload\nsystemctl restart jenkins\n# install trivy\nrpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.31.3/trivy_0.31.3_Linux-64bit.rpm\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-2-configure-jenkins-server","title":"Step-2 Configure Jenkins-Server","text":"<ul> <li>After instance state running, we can configure the jenkins server.Now, grab your Public IP Address</li> </ul> <pre><code>&lt;EC2 Public IP Address:8080&gt;\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <ul> <li>Unlock Jenkins using an administrative password and install the required plugins.</li> </ul> <ul> <li>Jenkins will now get installed and install all the libraries.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-3-install-sonarqube-as-a-docker-container","title":"Step-3 Install Sonarqube as a docker container","text":"<ul> <li>Go to Instance terminal and enter below code to install sonarqube</li> </ul> <pre><code>docker run -d --name sonar -p 9000:9000 sonarqube:lts-community\n</code></pre> <pre><code>&lt;EC2 Public IP Address:9000&gt;\nusername: admin\npassword: admin\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-4-install-plugins","title":"Step-4 Install Plugins","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Plugins --&gt; Available Plugins Install below plugins</li> </ul> <p>1-Eclipse Temurin Installer:  It allows you to automatically download and install different versions of the Temurin JDK on your Jenkins agents. This is useful for ensuring that your builds run with the correct version of Java without needing to manually manage JDK installations.</p> <p>2-SonarQube Scanner: You can configure Jenkins jobs to run SonarQube scans as part of your build process. The plugin sends the code analysis results to a SonarQube server, where you can view detailed reports and track quality metrics over time.</p> <p>3-OWASP Dependency-Check: You can use this plugin to scan your project dependencies for vulnerabilities as part of your Jenkins build process. The results include detailed reports on any vulnerabilities found, helping you to mitigate security risks by updating or replacing affected dependencies.</p> <p>4-Blue Ocean: With Blue Ocean, you can create, edit, and visualize pipelines using a graphical interface. It also provides enhanced visualization of pipeline stages and steps, making it easier to track the progress and status of builds and deployments.</p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-java-maven-in-global-tool-configuration","title":"Step-5 Configure Java, Maven in Global Tool Configuration","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Tools --&gt; Install JDK, Maven and SonarQube Scanner --&gt;Click on Apply and Save</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-sonarqube-in-manage-jenkins","title":"Step-5 Configure Sonarqube in Manage Jenkins","text":"<p><pre><code>&lt;EC2 Public IP Address:9000&gt;\n</code></pre> - Go to your Sonarqube Server. Click on Administration \u2192 Security \u2192 Users \u2192 Click on Tokens and Update Token \u2192 Give it a name \u2192 and click on Generate Token</p> <p></p> <p></p> <p></p> <ul> <li>Copy this Token</li> <li>Go to Jenkins WebUI --&gt; Manage Jenkins \u2192 Credentials \u2192 Add Secret Text.</li> </ul> <p></p> <ul> <li>Go to Jenkins Dashboard \u2192 Manage Jenkins \u2192 Configure System</li> <li>Give a name whatever you want</li> <li>Add Sonarqube url</li> <li>Select sonarqube credential token</li> </ul> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-6-webhook-configuration-on-sonarqube","title":"Step-6 WebHook Configuration on Sonarqube","text":"<ul> <li>Go to SonarQube WebUI --&gt; Administration \u2013&gt; Configuration \u2013&gt; webhooks</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-7-create-a-pipeline","title":"Step-7 Create a pipeline","text":"<ul> <li>Go to Jenkins WebUI --&gt;New item--&gt;Pipeline</li> </ul> <ul> <li>Add below jenkins code to pipeline section</li> </ul> <p><pre><code>pipeline {\n    agent any\n    tools {\n        jdk 'jdk'\n        maven 'maven'\n    }\n    stages {\n        stage(\"Git Checkout\") {\n            steps {\n                git branch: 'main', changelog: false, poll: false, url: 'https://github.com/ersinsari13/devsecops.git'\n            }\n        }\n        stage(\"Compile\") {\n            steps {\n                sh \"mvn clean compile\"\n            }\n        }\n        stage(\"Test Cases\") {\n            steps {\n                sh \"mvn test\"\n            }\n        }\n        stage(\"Sonarqube Analysis\") {\n            steps {\n                withSonarQubeEnv('sonar-server') {\n                    sh ''' \n                        mvn clean verify sonar:sonar \\\n                        -Dsonar.projectKey=Petclinic\n                    '''\n                }\n            }\n        }\n        stage(\"Quality Gate\") {\n            steps {\n                timeout(time: 2, unit: 'MINUTES') {\n                    script {\n                        waitForQualityGate abortPipeline: true\n                    }\n                }\n            }\n        }\n        stage(\"Build\") {\n            steps {\n                sh \"mvn clean install\"\n            }\n        }\n        stage('OWASP-Dependency-Check') {\n            steps {\n                sh \"mvn dependency-check:check\"\n            }\n            post {\n                always {\n                    dependencyCheckPublisher pattern: 'target/dependency-check-report.xml'\n                }\n            }\n        }\n\n        stage('Scan Dockerfile with conftest') {\n            steps {\n                echo 'Scanning Dockerfile'\n                sh \"docker run --rm -v $(pwd):/project openpolicyagent/conftest test --policy dockerfile-conftest.rego Dockerfile\"\n            }\n        }\n\n        stage('Prepare Tags for Docker Images') {\n            steps {\n                echo 'Preparing Tags for Docker Images'\n                script {\n                    MVN_VERSION=sh(script:'. ${WORKSPACE}/target/maven-archiver/pom.properties &amp;&amp; echo $version', returnStdout:true).trim()\n                    env.IMAGE_TAG_DEVSECOPS=\"ersinsari/devsecops:${MVN_VERSION}-b${BUILD_NUMBER}\"\n                }\n            }\n        }\n        stage('Build App Docker Images') {\n            steps {\n                echo 'Building App Dev Images'\n                sh \"docker build --force-rm -t ${IMAGE_TAG_DEVSECOPS} .\"\n                sh 'docker image ls'\n            }\n        }\n        stage('Scan Image with Trivy') {\n            steps {\n                script {\n                    def scanResult = sh(script: \"trivy image --severity CRITICAL --exit-code 1 ${IMAGE_TAG_DEVSECOPS}\", returnStatus: true)\n                    if (scanResult != 0) {\n                        error \"Critical vulnerabilities found in Docker image. Failing the pipeline.\"\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> - Tools: Specifies the tools needed for the pipeline, in this case, JDK and Maven.</p> <ul> <li> <p>Git Checkout: Check out the code from the specified Git repository.</p> </li> <li> <p>Compile: Runs Maven commands to clean the workspace and compile the code.</p> </li> <li> <p>Test Cases: Executes the Maven test phase to run the unit tests.</p> </li> <li> <p>SonarQube Analysis: Analyze the code quality using SonarQube.</p> </li> <li> <p>Quality Gate: Check the SonarQube quality gate status and abort the pipeline if it fails.</p> </li> <li> <p>Build: Runs Maven commands to clean the workspace and install the build artifacts.</p> </li> <li> <p>OWASP-Dependency-Check: Perform a security vulnerability check on project dependencies.</p> </li> <li> <p>Scan Dockerfile with conftest: Runs Conftest in a Docker container to test the Dockerfile against the specified policy.</p> </li> <li> <p>Prepare Tags for Docker Images: Extracts the Maven version from the build and sets the environment variable IMAGE_TAG_DEVSECOPS with the image tag.</p> </li> <li> <p>Build App Docker Images: Build the Docker image for the application.</p> </li> <li> <p>Scan Image with Trivy: Scans the Docker image for critical vulnerabilities and fails the pipeline if any are found.</p> </li> <li> <p>Click Build Now and Open Blue Ocean</p> </li> </ul> <p></p> <ul> <li>After the pipeline runs, you should receive a failure at the \"Scan Dockerfile with conftest\" step; this is a normal occurrence.</li> </ul> <p></p> <ul> <li>The reason for this is that if you check the GitHub repository we included in the pipeline, you will see a file named dockerfile-conftest.rego. Conftest performs the Dockerfile scan based on the conditions in this file. We received a failure because the Dockerfile we want to use does not meet the necessary requirements specified. We will correct this.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-8-sonarqube-inspection-and-add-custom-quality-gate","title":"Step-8 Sonarqube inspection and add Custom Quality Gate","text":"<ul> <li> <p>But first, let's discuss the pipeline output and then talk a bit about the SonarQube interface and quality gates.</p> </li> <li> <p>You can inspect your source code qality by clicking SonarQube section</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>You can add custom Quality-Gates depends on your company rules</p> </li> <li> <p>SonarQube UI click Qualiyy Gates --&gt; Create --&gt; give name and save --&gt; Unlock editing --&gt; Add Condition --&gt; On Overall Code</p> </li> </ul> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-9-dependency-check-inspection","title":"Step-9 Dependency-Check inspection","text":"<ul> <li>You can inspect your source code dependency-check score by clicking Dependency-Check section</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-10-improving-dockerfile-security","title":"Step-10 Improving Dockerfile security","text":"<p>Now it's time to improve the Dockerfile security based on the Conftest results.</p> <p></p> <ul> <li>Change your Dockerfile as below</li> </ul> <p><pre><code>FROM openjdk:8\nEXPOSE 8082\nRUN addgroup -S devops-security &amp;&amp; adduser -u 999 -S devsecops -G devops-security\nCOPY target/petclinic.war petclinic.war\nUSER 999\nENTRYPOINT [\"java\",\"-jar\",\"/home/devsecops/petclinic.war\"]\n</code></pre> After this change, you should be able to successfully pass the Dockerfile scanning stage with Conftest.</p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-11-docker-image-scan-via-trivy","title":"Step-11 Docker Image Scan via Trivy","text":"<p>Lastly, the pipeline will fail at the image scanning stage with Trivy. If we look at the Jenkinsfile, it is designed to fail if a critical vulnerability is found during the image scan with Trivy. At this stage, the critical vulnerabilities in the image need to be resolved before proceeding. The pipeline output includes recommendations on how to resolve the vulnerabilities.</p> <p></p> <p></p> <p>Once the image scan is successfully completed according to your requirements, the next step is to push the Docker image to the registry and then deploy your application. The key point here is to ensure maximum security before deploying the application, which is what we have aimed to achieve. Have a nice day.</p>"},{"location":"devops/devsecops/What%20is%20Falco/","title":"Deep Dive into Security Monitoring in Kubernetes Environments: An Introduction to Falco","text":""},{"location":"devops/devsecops/What%20is%20Falco/#introduction","title":"Introduction","text":"<p>With the rise of container-based and cloud-native applications, security risks have become increasingly complex. Falco, an open-source tool developed by Sysdig and supported by CNCF (Cloud Native Computing Foundation), offers real-time security monitoring for Kubernetes and container environments. Falco monitors system calls to detect suspicious activities and notifies users immediately about these events. By using Falco, you can respond quickly to security breaches.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#what-is-falco","title":"What is Falco?","text":"<p>Falco monitors security in container environments by tracking system calls at the kernel level. It analyzes these system calls against defined rules to detect anomalies. For example, if an unauthorized bash session is opened within a container or an unexpected process is executed, Falco logs this and alerts the user.</p> <p></p>"},{"location":"devops/devsecops/What%20is%20Falco/#key-use-cases","title":"Key Use Cases","text":"<ul> <li> <p>Real-time security monitoring</p> </li> <li> <p>Preventing potential security breaches in containers</p> </li> <li> <p>Monitoring network activities and detecting suspicious connections</p> </li> <li> <p>Logging configuration changes made inside containers</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#how-falco-works","title":"How Falco Works","text":"<p>Falco is a security monitoring solution that observes and analyzes system calls at the kernel level. Specifically, it tracks process activities in the system and compares them against predefined rules. These rules determine what is considered normal or abnormal based on security policies. Suspicious operations are then reported to users through specified channels.</p> <p></p> <p>Falco uses sysdig libraries to analyze system calls in the user space. These events are filtered through Falco's policy engine based on predefined rules. If an event is deemed suspicious, it is reported through various output channels, such as:</p> <ul> <li> <p>Log files</p> </li> <li> <p>Standard output</p> </li> <li> <p>Alert mechanisms like Slack or email notifications</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#core-features-of-falco","title":"Core Features of Falco","text":"<ol> <li> <p>Real-Time Monitoring: Falco detects security events in real-time that require immediate attention.</p> </li> <li> <p>Customizable Rule Sets: Falco offers default security rules, but users can add their own in the falco_rules.local.yaml file.</p> </li> <li> <p>Hierarchical Severity Levels: Falco classifies detected events into different levels of importance. Below is a brief explanation of the log levels:</p> </li> <li> <p>emergency: System is unusable. Used for very serious system errors.</p> </li> <li> <p>alert: Immediate action required. Suitable for critical system failures or security breaches.</p> </li> <li> <p>critical: Critical conditions. Used for errors that could cause system or application crashes.</p> </li> <li> <p>error: Error conditions. Logs system or application errors.</p> </li> <li> <p>warning: Warning conditions. Indicates potential issues.</p> </li> <li> <p>notice: Normal but significant conditions. Highlights situations that aren't errors or warnings but still noteworthy.</p> </li> <li> <p>info: Informational messages. Provides general information about system and application status (e.g., successful starts, configuration loads).</p> </li> <li> <p>debug: Debug-level messages. Includes detailed information for troubleshooting and debugging (e.g., variable values, workflow steps).</p> <p>These log levels allow Falco users to track events based on their severity.</p> </li> <li> <p>Kernel-Level System Call Monitoring: Tracks events at the kernel level, offering deep insights into container activities.</p> </li> </ol>"},{"location":"devops/devsecops/What%20is%20Falco/#installing-falco","title":"Installing Falco","text":""},{"location":"devops/devsecops/What%20is%20Falco/#running-falco-in-a-kubernetes-environment","title":"Running Falco in a Kubernetes Environment","text":"<p>There are two main ways to deploy Falco in a Kubernetes environment:</p> <ol> <li>As a Standalone Service: This method installs Falco as a system service independent of Kubernetes.</li> </ol> <p>\ud83d\udcc4 Standalone Falco Installation Guide</p> <ol> <li>As a DaemonSet: This option deploys Falco as a pod on each Kubernetes node.</li> </ol> <p>\ud83d\udcc4 Installing Falco on Kubernetes</p> <p>After installation, Falco-related files can be found in the /etc/falco directory.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#managing-rules-in-falco","title":"Managing Rules in Falco","text":"<p>One of Falco's strongest features is its configurable rule system, allowing users to define custom rules. Default rules are stored in the falco_rules.yaml file, which is updated during upgrades. User-defined rules go into the falco_rules.local.yaml file, which is not affected by updates. This setup enables users to tailor Falco to their specific security needs.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#example-rule-unauthorized-bash-access","title":"Example Rule: Unauthorized Bash Access","text":"<p>In this example, only <code>bash</code> processes running within a container environment are tracked. If a bash shell is opened, Falco detects this activity and generates an alert:</p> <pre><code>- rule: Unauthorized Shell in Container\n  desc: Detect when a bash shell is opened in a container\n  condition: container.id != host and proc.name = bash\n  output: \"Unauthorized shell detected in container (command=%proc.cmdline container_name=%container.name)\"\n  priority: warning\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#explanation-of-the-rule","title":"Explanation of the Rule","text":"<ul> <li> <p>condition: The <code>container.id != host</code> condition ensures that the rule applies only in the container environment. This means the rule will not trigger when <code>bash</code> is executed on the host. The <code>proc.name = bash</code> part specifies that the process being monitored must be <code>bash</code>.</p> </li> <li> <p>output: This defines the alert message that will be triggered when the event occurs. In this example, the alert includes the container name and the executed command.</p> </li> <li> <p>priority: Defines the severity of the event, which is set to <code>warning</code> in this case.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#using-references-when-creating-rules","title":"Using References When Creating Rules","text":"<p>This rule was created using Falco's extensive rule-writing reference. To see all the fields supported by Falco and how to use them, you can consult the official documentation:</p> <p>\ud83d\udcc4 Falco Supported Fields for Conditions and Outputs</p> <p>This page lists all the fields available for conditions and outputs. For instance, <code>proc.name</code> tracks the process name, while <code>container.id</code> identifies the container ID. When writing your own rules or customizing existing ones, this reference document helps you choose the correct fields and formats.</p> <p>This approach makes rule writing both more accurate and comprehensive, allowing you to easily detect any anomalies in your system.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#using-lists-in-falco","title":"Using Lists in Falco","text":"<p>If you want to monitor other shell types besides <code>bash</code>, you can specify them in a list. For instance, to track <code>bash</code>, <code>sh</code>, and <code>zsh</code>, you can write a rule like this:</p> <pre><code>- list: linux_shells\n  items: [bash, sh, zsh]\n\n- rule: Detect Shell inside a Container\n  desc: Alert if a shell such as bash is open inside the container\n  condition: container.id != host and proc.name in (linux_shells)\n  output: \"Shell Opened (user=%user.name container=%container.id)\"\n  priority: WARNING\n</code></pre> <p>By doing so, different types of shell processes in the container environment can be monitored, addressing potential security risks more comprehensively.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#advantages-of-using-lists","title":"Advantages of Using Lists","text":"<ul> <li> <p>Readability: Instead of writing long conditions to check multiple values, a list provides a cleaner structure.</p> </li> <li> <p>Reusability: The same list can be reused across multiple rules, simplifying management.</p> </li> <li> <p>Ease of Updates: Adding or modifying items only requires changes to the list.</p> </li> </ul> <p>This structure is particularly useful for monitoring different types of shell processes or other similar events.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#defining-rules-with-macros","title":"Defining Rules with Macros","text":"<p>In Falco, you can use macros to define frequently used conditions. Macros can be reused within rules, making the rules more readable and easier to manage.</p> <pre><code>- macro: unauthorized_shells\n  condition: proc.name in (bash, sh, zsh)\n\n- rule: Unauthorized Shell in Container\n  desc: Detect unauthorized shell in a container\n  condition: container.id != host and unauthorized_shells\n  output: \"Unauthorized shell detected in container (command=%proc.cmdline container_name=%container.name)\"\n  priority: warning\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#advantages-of-using-macros","title":"Advantages of Using Macros","text":"<ul> <li> <p>Reduced Code Duplication: Macros allow you to define a condition once and reuse it across multiple rules, reducing repetition.</p> </li> <li> <p>Improved Readability: Using macros makes rules more understandable and easier to manage.</p> </li> </ul> <p>These features make Falco\u2019s security monitoring rules more flexible and manageable in container environments.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#log-management-in-falco","title":"Log Management in Falco","text":"<p>Falco offers flexibility in storing and managing logs in various formats and destinations. Below, you will find several configuration options and how to use them.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#1-saving-falco-logs-to-a-file","title":"1. Saving Falco Logs to a File","text":"<p>To save Falco logs to a specific file, update the /etc/falco/falco.yaml file with the following settings:</p> <pre><code>file_output:\n  enabled: true\n  keep_alive: true\n  filename: ./falco_events.txt\n</code></pre> <ul> <li> <p>enabled: When set to <code>true</code>, file output is enabled, and logs are written to the specified file.</p> </li> <li> <p>keep_alive:</p> </li> <li><code>true</code>: Keeps the file open for continuous writing, which can improve performance since the file doesn\u2019t reopen for each event.</li> <li> <p><code>false</code>: Opens, writes, and closes the file for each event.</p> </li> <li> <p>filename: Specifies the path where logs will be saved. In this example, logs will be saved to <code>/var/log/falco_events.txt</code>.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#2-saving-falco-logs-in-json-format","title":"2. Saving Falco Logs in JSON Format","text":"<p>To save Falco events in JSON format, use the following configuration:</p> <pre><code>json_output:\n  enabled: true\n</code></pre> <p>By default, this setting is <code>enabled:false</code>, which saves logs in text format. To enable JSON logging, set the value to <code>enabled:true</code>.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#3-program-output-for-notifications","title":"3. Program Output for Notifications","text":"<p>To process logs through a program or send notifications, use the following configuration:</p> <pre><code>program_output:\n  enabled: true\n  program: mail -s \"Falco Alert\" admin@example.com\n</code></pre> <ul> <li>program: Falco sends detected events through this program. In this example, events are sent as an email.</li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#4-setting-the-log-level","title":"4. Setting the Log Level","text":"<p>You can define the minimum log level that Falco will record using <code>log_level</code>:</p> <pre><code>log_level: info\n</code></pre> <p>Events with a severity level of <code>info</code> and above will be recorded.</p> <ul> <li> <p>Log level descriptions:</p> </li> <li> <p><code>debug</code>: Records all log levels.</p> </li> <li> <p><code>info</code>: Provides general information about system status.</p> </li> <li> <p><code>warning</code>, <code>error</code>, <code>critical</code>, <code>alert</code>, and <code>emergency</code>: Indicate progressively higher levels of severity.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#5-rule-file-loading-order","title":"5. Rule File Loading Order","text":"<p>Falco loads rule files in the order specified in the configuration. Order matters, because:</p> <ul> <li> <p>If a rule defined in the falco_rules.yaml file is redefined in the falco_rules.local.yaml file, the rule in falco_rules.local.yaml will take precedence.</p> </li> <li> <p>This approach allows users to customize and override default rules.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#example-falcoyaml-configuration","title":"Example falco.yaml Configuration","text":"<pre><code>rules_file:\n  - /etc/falco/falco_rules.yaml\n  - /etc/falco/falco_rules.local.yaml\n</code></pre> <p>This configuration indicates that the default rules will be loaded first, followed by user-customized rules.</p> <p>In summary, the rule defined in the last file will always take precedence (overwrite).</p>"},{"location":"devops/devsecops/What%20is%20Falco/#important-note","title":"Important Note:","text":"<p>After modifying Falco's configuration file, you need to restart the Falco service for the changes to take effect:</p> <pre><code>systemctl restart falco\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#conclusion","title":"Conclusion","text":"<p>Falco is a powerful and flexible tool for security monitoring in Kubernetes and container environments. With its easily customizable rule system and real-time monitoring capabilities, it helps you identify security vulnerabilities efficiently. If you\u2019re looking to enhance the security of your Kubernetes infrastructure, Falco is definitely worth considering.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#references","title":"References","text":"<p>This document was prepared using the following resources:</p> <ul> <li>Official Falco Documentation</li> <li>Certified Kubernetes Security Specialist (CKS) Course - KodeKloud</li> </ul>"},{"location":"devops/devsecops/external-secret-operator/","title":"External Secret Operator","text":"<p>In the dynamic landscape of modern application development, managing secrets securely is crucial, especially within Kubernetes environments. Secrets such as API keys, passwords, and certificates are vital for the functionality and security of applications, but they also pose significant risks if not handled correctly. Kubernetes provides built-in mechanisms for secret management, but as applications grow in complexity, so does the challenge of managing these secrets effectively. This is where the External Secret Operator comes into play. By integrating with external secret management systems, the External Secret Operator enhances Kubernetes' native capabilities, allowing for seamless and secure management of sensitive data. In this blog, we'll explore why secrets are so critical in Kubernetes and how the External Secret Operator can streamline and fortify your secret management strategy.</p> <p></p> <p>External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, IBM Cloud Secrets Manager, CyberArk Conjur and many more. The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret.</p>"},{"location":"devops/devsecops/external-secret-operator/#secretstore","title":"SecretStore","text":"<p>The SecretStore is namespaced and specifies how to access the external API. The SecretStore maps to exactly one instance of an external API. By design, SecretStores are bound to a namespace and can not reference resources across namespaces. If you want to design cross-namespace SecretStores you must use ClusterSecretStores which do not have this limitation.</p> <p></p> <p>For a full list of supported fields see spec </p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: example\n  namespace: example-ns\nspec:\n\n  # Used to select the correct ESO controller (think: ingress.ingressClassName)\n  # The ESO controller is instantiated with a specific controller name\n  # and filters ES based on this property\n  # Optional\n  controller: dev\n\n  # You can specify retry settings for the http connection\n  # these fields allow you to set a maxRetries before failure, and\n  # an interval between the retries.\n  # Current supported providers: AWS, Hashicorp Vault, IBM\n  retrySettings:\n    maxRetries: 5\n    retryInterval: \"10s\"\n\n  # provider field contains the configuration to access the provider\n  # which contains the secret exactly one provider must be configured.\n  provider:\n\n    # (1): AWS Secrets Manager\n    # aws configures this store to sync secrets using AWS Secret Manager provider\n    aws:\n      service: SecretsManager\n      # Role is a Role ARN which the SecretManager provider will assume\n      role: iam-role\n      # AWS Region to be used for the provider\n      region: eu-central-1\n      # Auth defines the information necessary to authenticate against AWS by\n      # getting the accessKeyID and secretAccessKey from an already created Kubernetes Secret\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n\n    # (2) Hashicorp Vault\n    vault:\n      server: \"https://vault.acme.org\"\n      # Path is the mount path of the Vault KV backend endpoint\n      # Used as a path prefix for the external secret key\n      path: \"secret\"\n      # Version is the Vault KV secret engine version.\n      # This can be either \"v1\" or \"v2\", defaults to \"v2\"\n      version: \"v2\"\n      # vault enterprise namespace: https://www.vaultproject.io/docs/enterprise/namespaces\n      namespace: \"a-team\"\n      # base64 encoded string of certificate\n      caBundle: \"...\"\n      # Instead of caBundle you can also specify a caProvider\n      # this will retrieve the cert from a Secret or ConfigMap\n      caProvider:\n        # Can be Secret or ConfigMap\n        type: \"Secret\"\n        name: \"my-cert-secret\"\n        key: \"cert-key\"\n      # client side related TLS communication, when the Vault server requires mutual authentication\n      tls:\n        clientCert:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.crt\"\n        secretRef:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.key\"\n\n      auth:\n        # static token: https://www.vaultproject.io/docs/auth/token\n        tokenSecretRef:\n          name: \"my-secret\"\n          key: \"vault-token\"\n\n        # AppRole auth: https://www.vaultproject.io/docs/auth/approle\n        appRole:\n          path: \"approle\"\n          roleId: \"db02de05-fa39-4855-059b-67221c5c2f63\"\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault-token\"\n\n        # Kubernetes auth: https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"demo\"\n          # Optional service account reference\n          serviceAccountRef:\n            name: \"my-sa\"\n          # Optional secret field containing a Kubernetes ServiceAccount JWT\n          # used for authenticating with Vault\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault\"\n\n        # TLS certificates auth method: https://developer.hashicorp.com/vault/docs/auth/cert\n        cert:\n          clientCert:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.crt\"\n          secretRef:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.key\"\n\n    # (3): GCP Secret Manager\n    gcpsm:\n      # Auth defines the information necessary to authenticate against GCP by getting\n      # the credentials from an already created Kubernetes Secret.\n      auth:\n        secretRef:\n          secretAccessKeySecretRef:\n            name: gcpsm-secret\n            key: secret-access-credentials\n      projectID: myproject\n    # (TODO): add more provider examples here\n\nstatus:\n  # Standard condition schema\n  conditions:\n  # SecretStore ready condition indicates the given store is in ready\n  # state and able to referenced by ExternalSecrets\n  # If the `status` of this condition is `False`, ExternalSecret controllers\n  # should prevent attempts to fetch secrets\n  - type: Ready\n    status: \"False\"\n    reason: \"ConfigError\"\n    message: \"SecretStore validation failed\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#external-secret","title":"External Secret","text":"<p>The ExternalSecret describes what data should be fetched, how the data should be transformed and saved as a Kind=Secret:</p> <ul> <li>tells the operator what secrets should be synced by using spec.data to explicitly sync individual keys or use spec.dataFrom to get all values from the external API.</li> <li>you can specify how the secret should look like by specifying a spec.target.template</li> </ul> <p>Take a look at an annotated example to understand the design behind the ExternalSecret</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: \"hello-world\"\n\n  # labels and annotations are copied over to the\n  # secret that will be created\n  labels:\n    acme.org/owned-by: \"q-team\"\n  annotations:\n    acme.org/sha: 1234\n\nspec:\n\n  # Optional, SecretStoreRef defines the default SecretStore to use when fetching the secret data.\n  secretStoreRef:\n    name: aws-store\n    kind: SecretStore  # or ClusterSecretStore\n\n  # RefreshInterval is the amount of time before the values reading again from the SecretStore provider\n  # Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration)\n  # May be set to zero to fetch and create it once\n  refreshInterval: \"1h\"\n\n  # the target describes the secret that shall be created\n  # there can only be one target per ExternalSecret\n  target:\n\n    # The secret name of the resource\n    # Defaults to .metadata.name of the ExternalSecret\n    # It is immutable\n    name: application-config\n\n    # Specifies the ExternalSecret ownership details in the created Secret. Options:\n    # - Owner: (default) Creates the Secret and sets .metadata.ownerReferences. If the ExternalSecret is deleted, the Secret will also be deleted.\n    # - Merge: Does not create the Secret but merges data fields into the existing Secret (expects the Secret to already exist).\n    # - Orphan: Creates the Secret but does not set .metadata.ownerReferences. If the Secret already exists, it will be updated.\n    # - None: Does not create or update the Secret (reserved for future use with injector).\n    creationPolicy: Merge\n\n    # Specifies what happens to the Secret when data fields are deleted from the provider (e.g., Vault, AWS Parameter Store). Options:\n    # - Retain: (default) Retains the Secret if all Secret data fields have been deleted from the provider.\n    # - Delete: Removes the Secret if all Secret data fields from the provider are deleted.\n    # - Merge: Removes keys from the Secret but not the Secret itself.\n    deletionPolicy: Retain\n\n    # Specify a blueprint for the resulting Kind=Secret\n    template:\n      type: kubernetes.io/dockerconfigjson # or TLS...\n\n      metadata:\n        annotations: {}\n        labels: {}\n\n      # Use inline templates to construct your desired config file that contains your secret\n      data:\n        config.yml: |\n          database:\n            connection: postgres://{{ .username }}:{{ .password }}@{{ .database_host }}:5432/payments\n\n      # Uses an existing template from configmap\n      # Secret is fetched, merged and templated within the referenced configMap data\n      # It does not update the configmap, it creates a secret with: data[\"alertmanager.yml\"] = ...result...\n      templateFrom:\n      - configMap:\n          name: application-config-tmpl\n          items:\n          - key: config.yml\n\n  # Data defines the connection between the Kubernetes Secret keys and the Provider data\n  data:\n    - secretKey: username\n      remoteRef:\n        key: database-credentials\n        version: v1\n        property: username\n        decodingStrategy: None # can be None, Base64, Base64URL or Auto\n\n      # define the source of the secret. Can be a SecretStore or a Generator kind\n      sourceRef:\n        # point to a SecretStore that should be used to fetch a secret.\n        # must be defined if no spec.secretStoreRef is defined.\n        storeRef:\n          name: aws-secretstore\n          kind: ClusterSecretStore\n\n  # Used to fetch all properties from the Provider key\n  # If multiple dataFrom are specified, secrets are merged in the specified order\n  # Can be defined using sourceRef.generatorRef or extract / find\n  # Both use cases are exemplified below\n  dataFrom:\n  - sourceRef:\n      generatorRef:\n        apiVersion: generators.external-secrets.io/v1alpha1\n        kind: ECRAuthorizationToken\n        name: \"my-ecr\"\n  #Or\n  dataFrom:\n  - extract:\n      key: database-credentials\n      version: v1\n      property: data\n      conversionStrategy: Default\n      decodingStrategy: Auto\n    rewrite:\n    - regexp:\n        source: \"exp-(.*?)-ression\"\n        target: \"rewriting-${1}-with-groups\"\n  - find:\n      path: path-to-filter\n      name:\n        regexp: \".*foobar.*\"\n      tags:\n        foo: bar\n      conversionStrategy: Unicode\n      decodingStrategy: Base64\n    rewrite:\n    - regexp:\n        source: \"foo\"\n        target: \"bar\"\n\nstatus:\n  # refreshTime is the time and date the external secret was fetched and\n  # the target secret updated\n  refreshTime: \"2019-08-12T12:33:02Z\"\n  # Standard condition schema\n  conditions:\n  # ExternalSecret ready condition indicates the secret is ready for use.\n  # This is defined as:\n  # - The target secret exists\n  # - The target secret has been refreshed within the last refreshInterval\n  # - The target secret content is up-to-date based on any target templates\n  - type: Ready\n    status: \"True\" # False if last refresh was not successful\n    reason: \"SecretSynced\"\n    message: \"Secret was synced\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#hands-on","title":"Hands-On","text":"<p>Here\u2019s the step-by-step guide to using an external secret manager in a local Minikube cluster with AWS Secret Manager</p>"},{"location":"devops/devsecops/external-secret-operator/#set-up-minikube","title":"Set Up Minikube","text":"<pre><code>minikube start\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#install-external-secrets-operator","title":"Install External Secrets Operator","text":"<ul> <li>Add the Helm chart repository for the external secrets operator</li> </ul> <pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm repo update\n</code></pre> <ul> <li>Install the External Secrets Operator using Helm</li> </ul> <pre><code>helm install external-secrets \\\n   external-secrets/external-secrets \\\n    -n external-secrets \\\n    --create-namespace\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-in-aws-secrets-manager","title":"Create secret in AWS Secrets Manager","text":"<ul> <li>Go to the AWS Management Console</li> <li>In the AWS Management Console, search for and select Secrets Manager</li> <li>On the Secrets Manager dashboard, click the \"Store a new secret\" button.</li> <li>You can store a variety of secrets, such as database credentials, API keys, or custom key-value pairs.</li> <li>Select Other type of secret.</li> <li>Input the keys and values you want to store. DB_PASSWORD=mypassword DB_USER=ersin</li> <li>Secret name=DB-CREDENTIAL</li> <li>Leave all settings as default and save.</li> </ul>"},{"location":"devops/devsecops/external-secret-operator/#create-a-user-in-aws","title":"Create a user in AWS","text":"<ul> <li>Navigate to IAM (Identity and Access Management)</li> <li>In the IAM dashboard, click on Users in the left-hand menu</li> <li>Click the Add user button.</li> <li>Enter a unique username for the new user</li> <li>Select Programmatic access.</li> <li>Set User Permissions via below policy.</li> </ul> <p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\"\n            ],\n            \"Resource\": \"&lt;ENTER-SECRET-MANAGER-ARN&gt;\"\n        }\n    ]\n}\n</code></pre> - Review the user details and permissions - Click Create user to finish. - On the confirmation page, you\u2019ll see the user's access key ID and secret access key. -  Download the credentials or save them securely, as you won\u2019t be able to retrieve the secret access key again.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#create-a-secret-store-in-kubernetes","title":"Create a Secret Store in Kubernetes","text":"<p>Create a YAML file for the Secret Store configuration. This will define how the External Secrets Operator interacts with AWS Secrets Manager.</p> <p>secret-store.yaml <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: secretstore-sample\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: us-east-1\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n</code></pre></p> <pre><code>kubectl apply -f secret-store.yaml\n</code></pre> <p></p> <p>spec: Defines the specification for the SecretStore. provider: Specifies the external secrets provider and its configuration. aws: Indicates that AWS Secrets Manager is the provider. service: Should be SecretsManager to specify the AWS Secrets Manager service. region: AWS region where the secrets are stored. auth: Authentication configuration for accessing AWS Secrets Manager. secretRef: Refers to the Kubernetes secret containing AWS credentials. accessKeyIDSecretRef: Refers to the Kubernetes secret and key storing the AWS access key ID. secretAccessKeySecretRef: Refers to the Kubernetes secret and key storing the AWS secret access key.</p>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-for-access-key-and-secret-key","title":"Create Secret for access key and secret key.","text":"<p>The External Secrets Operator needs to authenticate with AWS Secrets Manager to fetch secrets. The access key and secret key provide the credentials for this authentication.</p> <pre><code>kubectl create secret generic awssm-secret --from-literal=access-key=&lt;ACCESS-KEY&gt; --from-literal=secret-access-key=&lt;SECRET_KEY&gt; -n external-secrets\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-the-externalsecret","title":"Create the ExternalSecret","text":"<p>ExternalSecret retrieves secrets from AWS Secrets Manager and makes them available in your Kubernetes cluster.</p> <p>external-secret.yaml</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: external-secret-example\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore-sample\n    kind: SecretStore\n  target:\n    name: kube-secret\n    creationPolicy: Owner\n  dataFrom:\n  - extract:\n      key: DB-CREDENTIAL\n</code></pre> <p><pre><code>kubectl apply -f external-secret.yaml\n</code></pre> You should see Status as SecretSynced.</p> <p></p> <p>spec: Defines the specification for the ExternalSecret. refreshInterval: 1h means the secret will be refreshed every hour from AWS Secrets Manager. secretStoreRef: References the SecretStore to use for accessing AWS Secrets Manager. name: The name of the SecretStore resource (e.g., secretstore-sample). kind: Specifies the type of reference, which should be SecretStore. target: Defines the Kubernetes secret that will be created or updated. name: The name of the Kubernetes secret (e.g., kube-secret). creationPolicy: Determines when the Kubernetes secret is created, with Owner meaning the External Secrets Operator will manage its lifecycle. dataFrom: Specifies that the ExternalSecret should pull data from AWS Secrets Manager. extract: Defines the secret to extract. key: The name of the secret in AWS Secrets Manager (e.g., DB-CREDENTIAL).</p> <ul> <li>After you've created the ExternalSecret resource, you'll be able to see the new Kubernetes Secret that has been synchronized with the Secrets Manager store. Execute the following command:</li> </ul> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#consuming-secret-in-pod","title":"Consuming Secret in Pod","text":"<p>By syncing your AWS Secrets Manager secret to a Kubernetes Secret, External Secrets allows you to use and consume the secret in your Pod specification.</p> <p>We will deploy a simple busybox pod in the external-secrets namespace and use the secret via pod environment variables.</p> <p>Create a manifest file external-secrets-demo-pod.yaml with the following specifications.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: external-secrets\nspec:\n  containers:\n  - image-eso: busybox:1.35.0\n    command:\n      - sleep\n      - \"3600\"\n    image-esoPullPolicy: IfNotPresent\n    name: busybox\n    env:\n      - name: password\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_PASSWORD\n      - name: username\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_USER\n  restartPolicy: Always\n</code></pre> <ul> <li>To deploy the pod, run the following command:</li> </ul> <pre><code>kubectl apply -f external-secrets-demo-pod.yaml\n</code></pre> <ul> <li>Once the pod is in a running state, run the following commands to get the container\u2019s shell.</li> </ul> <pre><code>kubectl exec -it busybox -- sh\n</code></pre> <p>From the container\u2019s shell, you can use echo to print the environment variables to view the secrets.</p> <p></p>"},{"location":"devops/devsecops/vault/","title":"Vault in Kubernetes/VM installation","text":""},{"location":"devops/devsecops/vault/#vault-installation-using-helm-and-running-on-kubernetes-clsuter","title":"Vault Installation - Using Helm and running on Kubernetes Clsuter","text":"<ul> <li>\ufffd\ufffd Vault Helm Installation repo</li> </ul>"},{"location":"devops/devsecops/vault/#add-helm-repository","title":"Add Helm Repository","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\n</code></pre>"},{"location":"devops/devsecops/vault/#create-namespace","title":"Create Namespace","text":"<ul> <li>Create the argocd namespace if it doesn't exist:</li> </ul> <pre><code>kubectl create namespace vault\n</code></pre>"},{"location":"devops/devsecops/vault/#install-using-helm","title":"Install (Using Helm)","text":"<ul> <li> <p>Install Vault with your custom ext-vault-values.yaml:</p> </li> <li> <p>Example <code>ext-vault-values.yaml</code> </p> </li> </ul> <pre><code>vi ext-vault-values.yaml\n</code></pre> <pre><code>global:\n  enabled: true\n  namespace: \"\"\ninjector:\n  enabled: true\n  replicas: 1\n  port: 8080\n  resources: {}\n  # resources:\n  #   requests:\n  #     memory: 256Mi\n  #     cpu: 250m\n  #   limits:\n  #     memory: 256Mi\n  #     cpu: 250m\n  nodeSelector: {}\nserver:\n  ingress:\n    enabled: false\n    annotations: {}\n    ingressClassName: \"\"\n    pathType: Prefix\n    activeService: true\n    hosts:\n      - host: chart-example.local\n        paths: []\n    tls: []\n  dataStorage:\n    enabled: true\n    # Size of the PVC created\n    size: 1Gi\n    # Location where the PVC will be mounted.\n    mountPath: \"/vault/data\"\n    # Name of the storage class to use.  If null it will use the\n    # configured default Storage Class.\n    storageClass: \n    # Access Mode of the storage device being used for the PVC\n    accessMode: ReadWriteOnce\n    # Annotations to apply to the PVC\n    annotations: {}\n    # Labels to apply to the PVC\n    labels: {}\n  affinity: {}\n  ha:\n    enabled: true\n    replicas: 3\n    raft: \n      enabled: true\n    config: |\n      ui = true\n\n      listener \"tcp\" {\n        tls_disable = 1\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n      }\n      storage \"raft\" {\n          path = \"/vault/data\"\n\n          retry_join {\n            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n          }\n          retry_join {\n            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n          }\n          retry_join {\n            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n          }\n        }\n\n      service_registration \"kubernetes\" {}\nui:\n  enabled: true\n  publishNotReadyAddresses: true\n  # The service should only contain selectors for active Vault pod\n  activeVaultPodOnly: false\n  serviceType: \"ClusterIP\"\n  serviceNodePort: null\n  externalPort: 8200\n  targetPort: 8200\ncsi:\n  enabled: false\n  image:\n    repository: \"hashicorp/vault-csi-provider\"\n    tag: \"1.5.0\"\n    pullPolicy: IfNotPresent\nserverTelemetry:\n  serviceMonitor:\n    enabled: false\n  prometheusRules:\n    enabled: false\n</code></pre> <pre><code>helm upgrade --install vault hashicorp/vault --version 0.30.0 -f ext-vault-values.yaml ## change or remove version if need\n</code></pre> <ul> <li>Verify Installation</li> </ul> <pre><code>kubectl exec -it vault-0 -n vault -- vault status\nkubectl exec -ti vault-0 -n vault -- vault operator init -key-shares=3 -key-threshold=2 ## default  -key-shares=5 -key-threshold=3\n\nkubectl exec -it vault-0 -n vault -- /bin/bash \n## show output like this\n\nUnseal Key 1: &lt;unsealkey1&gt;\nUnseal Key 2: &lt;unsealkey2&gt;\nUnseal Key 3: &lt;unsealkey3&gt;\n\nInitial Root Token: hvs.xxxxxxxx\n\n## for unseal vault \n\nvault operator unseal &lt;unsealkey1&gt;\nvault operator unseal &lt;unsealkey2&gt;\n\n## verify status vault and see \"Sealed status is false\"\nvault status\nKey                     Value\n---                     -----\nSeal Type               shamir\nInitialized             true\nSealed                  false\nTotal Shares            3\nThreshold               2\nVersion                 1.19.3\nBuild Date              2025-04-29T10:34:52Z\nStorage Type            raft\nCluster Name            vault-cluster-ebfc0d3a\nCluster ID              7ec57d9a-8086-e22c-e1c3-79fcba04eee8\nRemoved From Cluster    false\nHA Enabled              true\nHA Cluster              n/a\nHA Mode                 standby\nActive Node Address     &lt;none&gt;\nRaft Committed Index    32\nRaft Applied Index      32\n</code></pre>"},{"location":"devops/devsecops/vault/#join-vault-1-and-vault-2-node-to-vault-cluster","title":"Join vault-1 and vault-2 node to Vault cluster","text":"<pre><code>kubectl exec -it vault-1 -- vault operator raft join http://vault-0.vault-internal:8200\nkubectl exec -it vault-2 -- vault operator raft join http://vault-0.vault-internal:8200\n\n## and unseal nodes\nkubectl exec -it vault-1 -- vault operator unseal &lt;unsealkey1&gt;\nkubectl exec -it vault-1 -- vault operator unseal &lt;unsealkey2&gt;\n\nkubectl exec -it vault-2 -- vault operator unseal &lt;unsealkey1&gt;\nkubectl exec -it vault-2 -- vault operator unseal &lt;unsealkey2&gt;\n</code></pre> <ul> <li>check leader node <pre><code>curl -s --header \"X-Vault-Token: &lt;root_token&gt;\" http://127.0.0.1:8200/v1/sys/leader | jq\n\n# view leader_cluster_address: xxx\nor you can check on vault ui raft storage section on left handside\n</code></pre></li> </ul>"},{"location":"devops/devsecops/vault/#vault-installation-install-as-systemd-service","title":"Vault Installation - install as systemd service","text":""},{"location":"devops/devsecops/vault/#single-node-deployment","title":"Single-Node Deployment","text":"<ul> <li>Using package manager</li> </ul> <pre><code>wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vault\n</code></pre> <pre><code>Not: # binary path is /usr/bin/vault check that \"which vault\"\n\nls /usr/bin/vault\n---\nsudo tee /lib/systemd/system/vault.service &lt;&lt;EOF\n[Unit]\nDescription=\"HashiCorp Vault\"\nDocumentation=\"https://developer.hashicorp.com/vault/docs\"\nConditionFileNotEmpty=\"/etc/vault.d/vault.hcl\"\n\n[Service]\nUser=vault\nGroup=vault\nSecureBits=keep-caps\nAmbientCapabilities=CAP_IPC_LOCK\nCapabilityBoundingSet=CAP_SYSLOG CAP_IPC_LOCK\nNoNewPrivileges=yes\nExecStart=/usr/bin/vault server -config=/etc/vault.d/vault.hcl\nExecReload=/bin/kill --signal HUP\nKillMode=process\nKillSignal=SIGINT\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <pre><code>## change &lt;vault-vm-ip&gt;\n## auto-unseal is active on AWS KMS for detail check auto-unseal section. if dont need delete seal \"awskms\" lines\n\nsudo tee /etc/vault.d/vault.hcl &lt;&lt;EOF\nstorage \"raft\" {\n  path    = \"/opt/vault/data\"\n  node_id = \"vault-node\"\n}\n\nlistener \"tcp\" {\n address = \"0.0.0.0:8200\" \n cluster_address = \"0.0.0.0:8201\"\n tls_disable = 1\n\napi_addr = \"http://&lt;vault-vm-ip&gt;:8200\"\ncluster_addr = \"http://&lt;vault-vm-ip&gt;:8201\"\nui = true\nlog_level = \"INFO\"\n\ndisable_mlock=true\n\nseal \"awskms\" {\n  region     = \"eu-central-1\"\n  access_key = \"xxx\"\n  secret_key = \"xxx\"\n  kms_key_id = \"xxx\"\n}\nEOF\n</code></pre> <pre><code># Create required directories\n\nsudo mkdir -p /opt/vault/data\nsudo mkdir -p /var/log/vault\n\n\nsudo chown -R vault:vault /opt/vault\nsudo chown -R vault:vault /var/log/vault\nsudo chown vault:vault /etc/vault.d/vault.hcl\nsudo chmod 640 /etc/vault.d/vault.hcl\nsudo chmod 750  /var/log/vault\n\n#restart vault service\n\nsudo systemctl daemon-reload\nsudo systemctl stop vault\nsudo systemctl start vault\nsudo systemctl status vault\nsudo systemctl enable vault \n\n\nexport VAULT_ADDR=http://&lt;vault-vm-ip&gt;:8200 \n\n## check status vault before initialize\nvault status\n\nvault operator init -key-shares=3 -key-threshold=2 # for 3 unseal key and 2 of them is required\nvault status\n\n## check vault cluster is healty\ncurl http://&lt;vault-vm-ip&gt;:8200/v1/sys/health\n</code></pre>"},{"location":"devops/devsecops/vault/#high-availability-cluster","title":"High Availability Cluster","text":""},{"location":"devops/devsecops/vault/#run-on-node-1","title":"Run on NODE-1","text":"<ul> <li>Using package manager</li> </ul> <pre><code>wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vault\n</code></pre> <pre><code>Not: # binary path is /usr/bin/vault check that \"which vault\"\n\nsudo tee /lib/systemd/system/vault.service &lt;&lt;EOF\n[Unit]\nDescription=\"HashiCorp Vault - A tool for managing secrets\"\nDocumentation=https://www.vaultproject.io/docs/\nRequires=network-online.target\nAfter=network-online.target\nConditionFileNotEmpty=/etc/vault.d/vault.hcl\nStartLimitIntervalSec=60\nStartLimitBurst=3\n\n[Service]\nUser=vault\nGroup=vault\nProtectSystem=full\nProtectHome=read-only\nPrivateTmp=yes\nPrivateDevices=yes\nSecureBits=keep-caps\nAmbientCapabilities=CAP_IPC_LOCK\nCapabilityBoundingSet=CAP_SYSLOG CAP_IPC_LOCK\nNoNewPrivileges=yes\nExecStart=/usr/bin/vault server -config=/etc/vault.d/vault.hcl\nExecReload=/bin/kill --signal HUP \\$MAINPID\nKillMode=process\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=5\nTimeoutStopSec=30\nLimitNOFILE=65536\nLimitMEMLOCK=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <pre><code>## change &lt;node-x-ip&gt;\n## auto-unseal is active on AWS KMS for detail check auto-unseal section. if dont need delete seal \"awskms\" lines\n\nsudo tee /etc/vault.d/vault.hcl &lt;&lt;EOF\nstorage \"raft\" {\n  path    = \"/opt/vault/data\"\n  node_id = \"vault-node-1\"\n\n  retry_join {\n    leader_api_addr = \"http://&lt;node-1-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-2-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-3-ip&gt;:8200\"\n  }\n}\n\nlistener \"tcp\" {\naddress = \"&lt;node-1-ip&gt;:8200\" \ncluster_address = \"&lt;node-1-ip&gt;:8201\"\ntls_disable = 1\n}\n\napi_addr = \"http://&lt;node-1-ip&gt;:8200\"\ncluster_addr = \"http://&lt;node-1-ip&gt;:8201\"\nui = true\nlog_level = \"INFO\"\n\ndisable_mlock=true\n\nseal \"awskms\" {\n  region     = \"eu-central-1\"\n  access_key = \"xxx\"\n  secret_key = \"xxx\"\n  kms_key_id = \"xxx\"\n}\n\naudit \"file\" {\n  path = \"/var/log/vault/audit.log\"\n  log_requests = true\n  log_responses = true\n}\nEOF\n</code></pre> <p><pre><code># Create required directories\nsudo mkdir -p /opt/vault/data\nsudo mkdir -p /var/log/vault\n\n# Set ownership and permissions\nsudo chown -R vault:vault /opt/vault\nsudo chown -R vault:vault /var/log/vault\nsudo chown vault:vault /etc/vault.d/vault.hcl\nsudo chmod 640 /etc/vault.d/vault.hcl\nsudo chmod 750  /var/log/vault\n\n# Enable and start vault service\nsudo systemctl daemon-reload\nsudo systemctl stop vault\nsudo systemctl start vault\nsudo systemctl status vault\nsudo systemctl enable vault \n\nexport VAULT_ADDR=http://&lt;node-1-ip&gt;:8200\nvault status\n</code></pre> - initialize vault</p> <p><pre><code>vault operator init -key-shares=3 -key-threshold=2\nnot: aws kms i\u00e7in 5-3 olmal\u0131 \n\nvault status\n</code></pre> - check raft peer list</p> <pre><code>vault login &lt;root token&gt;\n\nvault operator raft list-peers\n\nroot@GLMGVSM01:~# vault operator raft list-peers\nNode            Address             State       Voter\n----            -------             -----       -----\nvault-node-1    &lt;node-ip&gt;:8201    leader      true\n</code></pre>"},{"location":"devops/devsecops/vault/#run-on-node-2","title":"Run on NODE-2","text":"<ul> <li>Using package manager</li> </ul> <pre><code>wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vault\n</code></pre> <pre><code>Not: # binary path is /usr/bin/vault check that \"which vault\"\n\nsudo tee /lib/systemd/system/vault.service &lt;&lt;EOF\n[Unit]\nDescription=\"HashiCorp Vault - A tool for managing secrets\"\nDocumentation=https://www.vaultproject.io/docs/\nRequires=network-online.target\nAfter=network-online.target\nConditionFileNotEmpty=/etc/vault.d/vault.hcl\nStartLimitIntervalSec=60\nStartLimitBurst=3\n\n[Service]\nUser=vault\nGroup=vault\nProtectSystem=full\nProtectHome=read-only\nPrivateTmp=yes\nPrivateDevices=yes\nSecureBits=keep-caps\nAmbientCapabilities=CAP_IPC_LOCK\nCapabilityBoundingSet=CAP_SYSLOG CAP_IPC_LOCK\nNoNewPrivileges=yes\nExecStart=/usr/bin/vault server -config=/etc/vault.d/vault.hcl\nExecReload=/bin/kill --signal HUP \\$MAINPID\nKillMode=process\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=5\nTimeoutStopSec=30\nLimitNOFILE=65536\nLimitMEMLOCK=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p><pre><code>## change &lt;node-x-ip&gt;\n## auto-unseal is active on AWS KMS for detail check auto-unseal section. if dont need delete seal \"awskms\" lines\n\nsudo tee /etc/vault.d/vault.hcl &lt;&lt;EOF\nstorage \"raft\" {\n  path    = \"/opt/vault/data\"\n  node_id = \"vault-node-2\"\n\n  retry_join {\n    leader_api_addr = \"http://&lt;node-1-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-2-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-3-ip&gt;:8200\"\n  }\n}\n\nlistener \"tcp\" {\naddress = \"&lt;node-2-ip&gt;:8200\" \ncluster_address = \"&lt;node-2-ip&gt;:8201\"\ntls_disable = 1\n}\n\napi_addr = \"http://&lt;node-2-ip&gt;:8200\"\ncluster_addr = \"http://&lt;node-2-ip&gt;:8201\"\nui = true\nlog_level = \"INFO\"\n\ndisable_mlock=true\n\nseal \"awskms\" {\n  region     = \"eu-central-1\"\n  access_key = \"xxx\"\n  secret_key = \"xxx\"\n  kms_key_id = \"xxx\"\n}\n\naudit \"file\" {\n  path = \"/var/log/vault/audit.log\"\n  log_requests = true\n  log_responses = true\n}\nEOF\n</code></pre> - create data and log path </p> <pre><code># Create required directories\nsudo mkdir -p /opt/vault/data\nsudo mkdir -p /var/log/vault\n\n# Set ownership and permissions\nsudo chown -R vault:vault /opt/vault\nsudo chown -R vault:vault /var/log/vault\nsudo chown vault:vault /etc/vault.d/vault.hcl\nsudo chmod 640 /etc/vault.d/vault.hcl\nsudo chmod 750  /var/log/vault\n\n# Enable and start vault service\nsudo systemctl daemon-reload\nsudo systemctl stop vault\nsudo systemctl start vault\nsudo systemctl status vault\nsudo systemctl enable vault \n\nexport VAULT_ADDR=http://&lt;node-2-ip&gt;:8200\nvault status\n\n# After service starts, node will automatically join cluster via retry_join\n# Check cluster status and unseal if needed\nvault operator unseal &lt;unsealkey1&gt;\nvault operator unseal &lt;unsealkey2&gt;\n</code></pre>"},{"location":"devops/devsecops/vault/#run-on-node-3","title":"Run on NODE-3","text":"<ul> <li>Using package manager</li> </ul> <pre><code>wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vault\n</code></pre> <pre><code>Not: # binary path is /usr/bin/vault check that \"which vault\"\n\nsudo tee /lib/systemd/system/vault.service &lt;&lt;EOF\n[Unit]\nDescription=\"HashiCorp Vault - A tool for managing secrets\"\nDocumentation=https://www.vaultproject.io/docs/\nRequires=network-online.target\nAfter=network-online.target\nConditionFileNotEmpty=/etc/vault.d/vault.hcl\nStartLimitIntervalSec=60\nStartLimitBurst=3\n\n[Service]\nUser=vault\nGroup=vault\nProtectSystem=full\nProtectHome=read-only\nPrivateTmp=yes\nPrivateDevices=yes\nSecureBits=keep-caps\nAmbientCapabilities=CAP_IPC_LOCK\nCapabilityBoundingSet=CAP_SYSLOG CAP_IPC_LOCK\nNoNewPrivileges=yes\nExecStart=/usr/bin/vault server -config=/etc/vault.d/vault.hcl\nExecReload=/bin/kill --signal HUP \\$MAINPID\nKillMode=process\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=5\nTimeoutStopSec=30\nLimitNOFILE=65536\nLimitMEMLOCK=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p><pre><code>## change &lt;node-x-ip&gt;\n## auto-unseal is active on AWS KMS for detail check auto-unseal section. if dont need delete seal \"awskms\" lines\n\nsudo tee /etc/vault.d/vault.hcl &lt;&lt;EOF\nstorage \"raft\" {\n  path    = \"/opt/vault/data\"\n  node_id = \"vault-node-3\"\n\n  retry_join {\n    leader_api_addr = \"http://&lt;node-1-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-2-ip&gt;:8200\"\n  }\n  retry_join {\n    leader_api_addr = \"http://&lt;node-3-ip&gt;:8200\"\n  }\n}\n\nlistener \"tcp\" {\naddress = \"&lt;node-3-ip&gt;:8200\" \ncluster_address = \"&lt;node-3-ip&gt;:8201\"\ntls_disable = 1\n}\n\napi_addr = \"http://&lt;node-3-ip&gt;:8200\"\ncluster_addr = \"http://&lt;node-3-ip&gt;:8201\"\nui = true\nlog_level = \"INFO\"\n\ndisable_mlock=true\n\nseal \"awskms\" {\n  region     = \"eu-central-1\"\n  access_key = \"xxx\"\n  secret_key = \"xxx\"\n  kms_key_id = \"xxx\"\n}\n\naudit \"file\" {\n  path = \"/var/log/vault/audit.log\"\n  log_requests = true\n  log_responses = true\n}\nEOF\n</code></pre> - create data and log path </p> <pre><code># Create required directories\nsudo mkdir -p /opt/vault/data\nsudo mkdir -p /var/log/vault\n\n# Set ownership and permissions\nsudo chown -R vault:vault /opt/vault\nsudo chown -R vault:vault /var/log/vault\nsudo chown vault:vault /etc/vault.d/vault.hcl\nsudo chmod 640 /etc/vault.d/vault.hcl\nsudo chmod 750  /var/log/vault\n\n# Enable and start vault service\nsudo systemctl daemon-reload\nsudo systemctl stop vault\nsudo systemctl start vault\nsudo systemctl status vault\nsudo systemctl enable vault \n\nexport VAULT_ADDR=http://&lt;node-3-ip&gt;:8200\nvault status\n\n# After service starts, node will automatically join cluster via retry_join\n# Check cluster status and unseal if needed\nvault operator unseal &lt;unsealkey1&gt;\nvault operator unseal &lt;unsealkey2&gt;\n</code></pre> <ul> <li>verify complete cluster setup (final step)</li> </ul> <pre><code># Login with root token and verify all nodes joined the cluster\nvault login &lt;root token&gt;\n\nvault operator raft list-peers\n\n# Expected output showing all 3 nodes:\n# vault-node-1    &lt;node-1-ip&gt;:8201    leader      true\n# vault-node-2    &lt;node-2-ip&gt;:8201    follower    true  \n# vault-node-3    &lt;node-3-ip&gt;:8201    follower    true\n\n# Verify cluster health\nvault status\n</code></pre>"},{"location":"devops/devsecops/vault/#auto-unseal","title":"auto-unseal","text":""},{"location":"devops/devsecops/vault/#for-k8s-deploymnet","title":"for k8s deploymnet","text":"<pre><code>vi ext-vault-values.yaml \nand \nadd helm values config section this lines\n</code></pre> <p><pre><code>  ha:\n    enabled: true\n    replicas: 3\n    raft: \n      enabled: true\n    config: |\n      ui = true\n\n      listener \"tcp\" {\n        tls_disable = 1\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n      }\n      storage \"raft\" {\n          path = \"/vault/data\"\n\n          retry_join {\n            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n          }\n          retry_join {\n            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n          }\n          retry_join {\n            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n          }\n        }\n\n      service_registration \"kubernetes\" {}\n      seal \"awskms\" {\n        region     = \"eu-central-1\"\n        access_key = \"xxx\"\n        secret_key = \"xxx\"\n        kms_key_id = \"xxx\"\n      }\n</code></pre> - helm upgrade  <pre><code>helm upgrade --install vault hashicorp/vault --version 0.30.0 -f ext-vault-values.yaml\n</code></pre></p> <pre><code>kubectl exec -it vault-0 -n vault -- sh\n\n# for one more time for unseal \n\nvault operator unseal -migrate\n\nenter &lt;unsealkey1&gt;\nenter &lt;unsealkey2&gt;\n\nsystemctl restart vault.service \nvault status\n\n# show sealed: false\n\nNot: Need root token every time for Vault ui login\n</code></pre>"},{"location":"devops/devsecops/vault/#for-systemd-service","title":"for systemd service","text":"<p>Not: In the /etc/vault.d/vault.hcl document for the Systemd service installation mentioned above, it's mandatory to enter the relevant auto unseal area. Afterward, the following steps must be taken:</p> <pre><code># for one more time for unseal \n\nvault operator unseal -migrate\n\nenter &lt;unsealkey1&gt;\nenter &lt;unsealkey2&gt;\n\nsystemctl restart vault.service \nvault status\n\n# show sealed: false\n\nNot: Need root token every time for Vault ui login\n</code></pre>"},{"location":"devops/devsecops/vault/#use-tls-for-systemd-service","title":"use tls (for systemd service)","text":"<pre><code>sudo nano /etc/vault.d/vault.hcl\n\n# modified adress, listener, api_addr, cluster_addr section\n\nlistener \"tcp\" {\n address = \"0.0.0.0:443\" # port 443\n cluster_address = \"0.0.0.0:8201\"\n tls_cert_file = \"/etc/vault.d/certs/vault.crt\" # cert path \n tls_key_file  = \"/etc/vault.d/certs/vault.key\" # key path\n tls_disable   = 0 # set 0 for tls enable\n}\napi_addr = \"https://xxx.test.com:8200\" # use new dns\ncluster_addr = \"https://xxx.test.com:8201\" # use new dns\n\n# permission for bind service and run on 443, if not use port 8200 like https://xxx.test.com:8200\nsudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/vault\n\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/vault.d/certs/vault.key -out /etc/vault.d/certs/vault.crt -subj \"/CN=xxx.test.com\"\n\nsudo nano  /lib/systemd/system/vault.service\n\n# modified below lines\nCapabilityBoundingSet=CAP_IPC_LOCK CAP_NET_BIND_SERVICE\nAmbientCapabilities=CAP_IPC_LOCK CAP_NET_BIND_SERVICE\n\n\n# restart vault service\nsudo systemctl daemon-reload\nsudo systemctl restart vault\nsudo systemctl status vault\n\n# verify tls connection\n\nhttps://xxx.test.com\n</code></pre>"},{"location":"devops/devsecops/vault/#ldap-integration-for-systemd-service","title":"LDAP integration (for systemd service)","text":"<p>The ldap auth method allows authentication using an existing LDAP server and user/password credentials. This allows Vault to be integrated into environments using LDAP without duplicating the user/pass configuration in multiple places.</p> <pre><code>vault auth enable ldap\n\nvault write auth/ldap/config \\\n    url=\"ldap://ldap.server.address\" \\\n    bind_dn=\"cn=admin,dc=example,dc=com\" \\\n    bind_password=\"admin_password\" \\\n    user_dn=\"ou=users,dc=example,dc=com\" \\\n    group_dn=\"ou=groups,dc=example,dc=com\" \\\n    user_attr=\"uid\" \\\n    group_attr=\"cn\" \\\n    group_filter=\"(objectClass=groupOfNames)\" \\\n    group_base_dn=\"ou=groups,dc=example,dc=com\" \\\n    insecure_tls=true\n</code></pre> <p><pre><code>url: The address of the LDAP server.\nbind_dn: The user used to bind (authenticate) to the LDAP directory.\nbind_password: The password for the LDAP user specified in bind_dn.\nuser_dn: The distinguished name (DN) where user entries are located in the LDAP directory.\ngroup_dn: The distinguished name (DN) where group entries are located in the LDAP directory.\nuser_attr: The LDAP attribute used to identify the user (e.g., uid).\ngroup_attr: The LDAP attribute used to identify the group name (e.g., cn).\ngroup_filter: The filter to identify LDAP group objects (e.g., (objectClass=groupOfNames)).\ngroup_base_dn: The base distinguished name for groups in the LDAP directory.\ninsecure_tls: If you are not using TLS, set this to true.\n</code></pre> <pre><code>nano ldap-users-policy.hcl\n\n--\npath \"auth/ldap/login/*\" { \n  capabilities = [\"read\", \"update\"]\n}\npath \"kv/*\" {\n  capabilities = [\"read\"]\n}\npath \"kv/data/*\" {\n  capabilities = [\"read\"]\n}\npath \"kv/metadata/*\" {\n  capabilities = [\"read\"]\n}\npath \"sys/mounts/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"sys/mounts\" {\n  capabilities = [\"read\"]\n}\n--\nvault policy write ldap-users-policy ldap-users-policy.hcl\n\nvault write auth/ldap/groups/&lt;ldap_groups&gt; policies=\"default,ldap-users-policy\"\n\n#test ldap user login \nvault login -method=ldap username=ldap-username password=password\n</code></pre></p>"},{"location":"devops/devsecops/vault/#auth-kubernetes-enable","title":"auth-kubernetes enable","text":""},{"location":"devops/devsecops/vault/#kubernetes-side-configuration","title":"Kubernetes side configuration","text":"<p>Prerequisites: - External Secrets Operator (ESO) must be installed on Kubernetes cluster</p>"},{"location":"devops/devsecops/vault/#step-1-create-service-account-and-rbac","title":"Step 1: Create Service Account and RBAC","text":"<pre><code># Define service account for ESO-Vault integration with proper RBAC permissions\n# This service account allows ESO to authenticate with Vault and manage Kubernetes secrets\n\nvi vault-sa.yaml\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-secrets-sa\n  namespace: eso\n\n---\n# ClusterRoleBinding links the service account to the cluster role\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-secrets-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: external-secrets-sa\n  namespace: eso\n</code></pre> <p>Apply the service account configuration:</p> <pre><code>kubectl apply -f vault-sa.yaml\n</code></pre>"},{"location":"devops/devsecops/vault/#step-2-create-service-account-token","title":"Step 2: Create Service Account Token","text":"<pre><code># For Kubernetes 1.24+, manual token creation is required\n# This secret contains the JWT token for service account authentication\n\nvi vault-sa-token.yaml\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: external-secrets-sa-token\n  namespace: eso\n  annotations:\n    kubernetes.io/service-account.name: external-secrets-sa\ntype: kubernetes.io/service-account-token\n</code></pre> <p>Apply the token secret:</p> <pre><code>kubectl apply -f vault-sa-token.yaml\n\n# Verify token creation\nkubectl get secret external-secrets-sa-token -n eso\n</code></pre>"},{"location":"devops/devsecops/vault/#vault-server-side-configuration-only-using-by-single-cluster","title":"Vault server side configuration (only using by single cluster)","text":"<p>Note: The following configuration is for single cluster setup only. If you need to connect multiple Kubernetes clusters to Vault, skip this section and go directly to Multiple Clusters Configuration.</p>"},{"location":"devops/devsecops/vault/#step-3-enable-kubernetes-auth-method","title":"Step 3: Enable Kubernetes Auth Method","text":"<pre><code># Enable Kubernetes authentication method in Vault\nvault auth enable kubernetes\n</code></pre>"},{"location":"devops/devsecops/vault/#step-4-configure-kubernetes-auth","title":"Step 4: Configure Kubernetes Auth","text":"<pre><code># Configure Vault with Kubernetes cluster information\n# Replace placeholders with actual values from your environment\n\nvault write auth/kubernetes/config \\\n    token_reviewer_jwt=\"$(kubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.token}' | base64 -d)\" \\\n    kubernetes_host=\"https://YOUR-K8S-API-SERVER:6443\" \\\n    kubernetes_ca_cert=\"$(cat /tmp/k8s-ca.crt)\" \\\n    disable_iss_validation=true\n\n# Verify configuration\nvault read auth/kubernetes/config\n</code></pre>"},{"location":"devops/devsecops/vault/#step-5-create-vault-policy","title":"Step 5: Create Vault Policy","text":"<pre><code># Create policy file defining permissions for ESO\nvi eso-policy.hcl\n\npath \"kv/data/*\" {\n  capabilities = [\"read\"]\n}\npath \"kv/metadata/*\" {\n  capabilities = [\"read\"]\n}\n\n# Apply the policy to Vault\nvault policy write eso-policy eso-policy.hcl\n\n# Verify policy creation\nvault read sys/policy/eso-policy\n</code></pre>"},{"location":"devops/devsecops/vault/#step-6-create-kubernetes-role","title":"Step 6: Create Kubernetes Role","text":"<pre><code># Create role binding service account to Vault policy\n# This role allows the specified service account to authenticate with Vault\nvault write auth/kubernetes/role/eso-role \\\n    bound_service_account_names=external-secrets-sa \\\n    bound_service_account_namespaces=eso \\\n    policies=eso-policy \\\n    ttl=24h\n\n# Verify role creation\nvault read auth/kubernetes/role/eso-role\n</code></pre>"},{"location":"devops/devsecops/vault/#step-7-test-authentication","title":"Step 7: Test Authentication","text":"<pre><code># Test authentication with the service account token\nTOKEN=$(kubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.token}' | base64 -d)\n\nvault write auth/kubernetes/login role=eso-role jwt=\"$TOKEN\"\n</code></pre>"},{"location":"devops/devsecops/vault/#multiple-clusters-configuration","title":"Multiple Clusters Configuration","text":"<p>When you need to connect multiple Kubernetes clusters to a single Vault instance, each cluster requires its own auth path, policy, and role configuration.</p>"},{"location":"devops/devsecops/vault/#cluster-1-configuration","title":"Cluster 1 Configuration","text":""},{"location":"devops/devsecops/vault/#step-1-enable-kubernetes-auth-for-cluster-1","title":"Step 1: Enable Kubernetes Auth for Cluster 1","text":"<pre><code># Enable Kubernetes authentication method for cluster 1\nvault auth enable -path=kubernetes/cluster1 kubernetes\n</code></pre>"},{"location":"devops/devsecops/vault/#step-2-create-policy-for-cluster-1","title":"Step 2: Create Policy for Cluster 1","text":"<pre><code># Create policy file for cluster 1\nvi cluster1-eso-policy.hcl\n</code></pre> <pre><code># Policy for cluster1 - allows access to cluster1 specific secrets\npath \"cluster1/data/*\" {\n  capabilities = [\"read\"]\n}\npath \"cluster1/metadata/*\" {\n  capabilities = [\"read\"]\n}\n</code></pre> <pre><code># Apply the policy to Vault\nvault policy write cluster1-eso-policy cluster1-eso-policy.hcl\n</code></pre>"},{"location":"devops/devsecops/vault/#step-3-configure-kubernetes-auth-for-cluster-1","title":"Step 3: Configure Kubernetes Auth for Cluster 1","text":"<pre><code># Get cluster 1 credentials (from cluster 1 context)\nkubectl config use-context cluster1-context\nkubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.token}' | base64 -d &gt; /tmp/cluster1-token.jwt\nkubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; /tmp/cluster1-ca.crt\nCLUSTER1_HOST=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\n\n# Configure Vault with Cluster 1 information\nvault write auth/kubernetes/cluster1/config \\\n    token_reviewer_jwt=\"$(cat /tmp/cluster1-token.jwt)\" \\\n    kubernetes_host=\"$CLUSTER1_HOST\" \\\n    kubernetes_ca_cert=\"$(cat /tmp/cluster1-ca.crt)\" \\\n    disable_iss_validation=true\n</code></pre>"},{"location":"devops/devsecops/vault/#step-4-create-role-for-cluster-1","title":"Step 4: Create Role for Cluster 1","text":"<pre><code># Create role for cluster 1\nvault write auth/kubernetes/cluster1/role/cluster1-eso-role \\\n    bound_service_account_names=external-secrets-sa \\\n    bound_service_account_namespaces=eso \\\n    policies=cluster1-eso-policy \\\n    ttl=24h \\\n    audience=vault\n</code></pre>"},{"location":"devops/devsecops/vault/#step-5-test-authentication-for-cluster-1","title":"Step 5: Test Authentication for Cluster 1","text":"<pre><code># Test authentication for cluster 1\nvault write auth/kubernetes/cluster1/login \\\n    role=cluster1-eso-role \\\n    jwt=\"$(cat /tmp/cluster1-token.jwt)\"\n</code></pre>"},{"location":"devops/devsecops/vault/#cluster-2-configuration","title":"Cluster 2 Configuration","text":""},{"location":"devops/devsecops/vault/#step-1-enable-kubernetes-auth-for-cluster-2","title":"Step 1: Enable Kubernetes Auth for Cluster 2","text":"<pre><code># Enable Kubernetes authentication method for cluster 2\nvault auth enable -path=kubernetes/cluster2 kubernetes\n</code></pre>"},{"location":"devops/devsecops/vault/#step-2-create-policy-for-cluster-2","title":"Step 2: Create Policy for Cluster 2","text":"<pre><code># Create policy file for cluster 2\nvi cluster2-eso-policy.hcl\n</code></pre> <pre><code># Policy for cluster2 - allows access to cluster2 specific secrets\npath \"cluster2/data/*\" {\n  capabilities = [\"read\"]\n}\npath \"cluster2/metadata/*\" {\n  capabilities = [\"read\"]\n}\n</code></pre> <pre><code># Apply the policy to Vault\nvault policy write cluster2-eso-policy cluster2-eso-policy.hcl\n</code></pre>"},{"location":"devops/devsecops/vault/#step-3-configure-kubernetes-auth-for-cluster-2","title":"Step 3: Configure Kubernetes Auth for Cluster 2","text":"<pre><code># Get cluster 2 credentials (from cluster 2 context)\nkubectl config use-context cluster2-context\nkubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.token}' | base64 -d &gt; /tmp/cluster2-token.jwt\nkubectl get secret external-secrets-sa-token -n eso -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; /tmp/cluster2-ca.crt\nCLUSTER2_HOST=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\n\n# Configure Vault with Cluster 2 information\nvault write auth/kubernetes/cluster2/config \\\n    token_reviewer_jwt=\"$(cat /tmp/cluster2-token.jwt)\" \\\n    kubernetes_host=\"$CLUSTER2_HOST\" \\\n    kubernetes_ca_cert=\"$(cat /tmp/cluster2-ca.crt)\" \\\n    disable_iss_validation=true\n</code></pre>"},{"location":"devops/devsecops/vault/#step-4-create-role-for-cluster-2","title":"Step 4: Create Role for Cluster 2","text":"<pre><code># Create role for cluster 2\nvault write auth/kubernetes/cluster2/role/cluster2-eso-role \\\n    bound_service_account_names=external-secrets-sa \\\n    bound_service_account_namespaces=eso \\\n    policies=cluster2-eso-policy \\\n    ttl=24h \\\n    audience=vault\n</code></pre>"},{"location":"devops/devsecops/vault/#step-5-test-authentication-for-cluster-2","title":"Step 5: Test Authentication for Cluster 2","text":"<pre><code># Test authentication for cluster 2\nvault write auth/kubernetes/cluster2/login \\\n    role=cluster2-eso-role \\\n    jwt=\"$(cat /tmp/cluster2-token.jwt)\"\n</code></pre>"},{"location":"devops/devsecops/vault/#verification-commands-for-multiple-clusters","title":"Verification Commands for Multiple Clusters","text":"<pre><code># List all authentication methods\nvault auth list\n\n# Check cluster 1 config\nvault read auth/kubernetes/cluster1/config\nvault read auth/kubernetes/cluster1/role/cluster1-eso-role\n\n# Check cluster 2 config\nvault read auth/kubernetes/cluster2/config\nvault read auth/kubernetes/cluster2/role/cluster2-eso-role\n</code></pre>"},{"location":"devops/devsecops/vault/#clustersecretstore-configuration-for-each-cluster","title":"ClusterSecretStore Configuration for Each Cluster","text":""},{"location":"devops/devsecops/vault/#cluster-1-clustersecretstore","title":"Cluster 1 ClusterSecretStore","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-cluster1-secretstore\nspec:\n  provider:\n    vault:\n      server: \"http://vault-ip:8200\"  # CHANGE ME: Replace with actual Vault server IP or FQDN\n      path: \"cluster1\"\n      version: v2\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes/cluster1\"\n          role: \"cluster1-eso-role\"\n          serviceAccountRef:\n            name: external-secrets-sa\n            namespace: eso\n          secretRef:\n            name: external-secrets-sa-token              # Token secret reference\n            namespace: eso\n            key: token\n</code></pre>"},{"location":"devops/devsecops/vault/#cluster-2-clustersecretstore","title":"Cluster 2 ClusterSecretStore","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-cluster2-secretstore\nspec:\n  provider:\n    vault:\n      server: \"http://vault-ip:8200\"  # CHANGE ME: Replace with actual Vault server IP or FQDN\n      path: \"cluster2\"\n      version: v2\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes/cluster2\"\n          role: \"cluster2-eso-role\"\n          serviceAccountRef:\n            name: external-secrets-sa\n            namespace: eso\n          secretRef:\n            name: external-secrets-sa-token              # Token secret reference\n            namespace: eso\n            key: token\n</code></pre>"},{"location":"devops/devsecops/vault/#secret-engines-for-each-cluster","title":"Secret Engines for Each Cluster","text":"<pre><code># Enable KV secret engines for each cluster\nvault secrets enable -path=cluster1 kv-v2\nvault secrets enable -path=cluster2 kv-v2\n\n# Create sample secrets for testing\nvault kv put cluster1/database username=\"cluster1-db-user\" password=\"cluster1-db-pass\"\nvault kv put cluster2/database username=\"cluster2-db-user\" password=\"cluster2-db-pass\"\n</code></pre>"},{"location":"devops/devsecops/vault/#kubernetes-side-create-clustersecretstore","title":"Kubernetes side - Create ClusterSecretStore","text":""},{"location":"devops/devsecops/vault/#step-8-create-clustersecretstore","title":"Step 8: Create ClusterSecretStore","text":"<pre><code># Define ClusterSecretStore for ESO-Vault integration\n# ClusterSecretStore can be accessed from all namespaces (unlike SecretStore which is namespace-scoped)\n\nvi clustersecretstore-sa.yaml\n\napiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-nonprod-serviceaccount\nspec:\n  provider:\n    vault:\n      server: http://your-vault-server.example.com # Vault server URL\n      path: \"kv\"                                         # Vault secret engine path\n      version: v2                                        # KV engine version\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"                        # Vault auth method path\n          role: \"eso-role\"                              # Vault role name\n          serviceAccountRef:\n            name: external-secrets-sa                    # Kubernetes service account\n            namespace: eso\n          secretRef:\n            name: external-secrets-sa-token              # Token secret reference\n            namespace: eso\n            key: token\n</code></pre> <p>Apply the ClusterSecretStore:</p> <pre><code>kubectl apply -f clustersecretstore-sa.yaml\n\n# Verify ClusterSecretStore creation\nkubectl get ClusterSecretStore\n\n# Check status - should show \"Valid\"\nkubectl describe ClusterSecretStore vault-nonprod-serviceaccount\n</code></pre>"},{"location":"devops/devsecops/vault/#testing-the-integration","title":"Testing the Integration","text":""},{"location":"devops/devsecops/vault/#step-9-create-test-secret-in-vault","title":"Step 9: Create Test Secret in Vault","text":"<pre><code># Create a test secret in Vault for validation\nvault kv put kv/test vault-username=\"test-username-123\"\n\n# Verify secret creation\nvault kv get kv/test\n</code></pre>"},{"location":"devops/devsecops/vault/#step-10-create-externalsecret-for-testing","title":"Step 10: Create ExternalSecret for Testing","text":"<pre><code># ExternalSecret resource converts Vault secrets to Kubernetes secrets\nvi externalsecret.yaml\n\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: vault-example\n  namespace: eso\nspec:\n  refreshInterval: \"30s\"                              # Secret refresh interval\n  secretStoreRef:\n    name: vault-nonprod-serviceaccount               # ClusterSecretStore reference\n    kind: ClusterSecretStore\n  target:\n    name: devops-test                                # Kubernetes secret name to create\n    creationPolicy: Owner                            # Secret ownership policy\n  data:\n  - secretKey: k8s-username                              # Key name in Kubernetes secret\n    remoteRef:\n      key: test                                      # Vault secret path (kv/test)\n      property: vault-username                                # Vault secret field name\n</code></pre> <p>Apply the ExternalSecret:</p> <pre><code>kubectl apply -f externalsecret.yaml\n</code></pre>"},{"location":"devops/devsecops/vault/#step-11-verify-integration","title":"Step 11: Verify Integration","text":"<pre><code># Check ExternalSecret status\nkubectl get externalsecret vault-example -n eso\n\n# View ExternalSecret details\nkubectl describe externalsecret vault-example -n eso\n\n# Verify Kubernetes secret creation\nkubectl get secret devops-test -n eso -o yaml\n\n# Decode and view secret content\nkubectl get secret devops-test -n eso -o jsonpath='{.data.k8s-username}' | base64 -d\n# Expected output: test-username-123\n\n# Check ESO controller logs for troubleshooting\nkubectl logs -n eso -l app.kubernetes.io/name=external-secrets --tail=50\n</code></pre>"},{"location":"devops/devsecops/vault/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Check ClusterSecretStore status: Must show \"Valid\"</li> <li>Verify Vault connectivity: Ensure Vault server is accessible from Kubernetes</li> <li>Check ESO logs: Look for authentication or permission errors</li> <li>Validate token: Ensure service account token is not expired</li> <li>Test Vault authentication manually: Use <code>vault write auth/kubernetes/login</code> command</li> </ol> <p>Common issues: - Token expiration (recreate token secret) - Network connectivity between Kubernetes and Vault - Incorrect Vault policy permissions - Wrong service account or namespace bindings</p>"},{"location":"devops/devsecops/vault/#database-secret-engine","title":"database secret engine","text":""},{"location":"devops/devsecops/vault/#running-postgres-pod","title":"running postgres pod","text":"<pre><code>kubectl get secret --namespace default postgre-vault-test-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d\nMQEvdzuWw6\n\nkubectl exec -it postgre-vault-test-postgresql-0 -- bash\n\n\npsql -U vault -d postgres\npass: vault\n\nCREATE ROLE vault WITH LOGIN SUPERUSER PASSWORD 'vault';\n</code></pre>"},{"location":"devops/devsecops/vault/#enable-secret-engine","title":"enable secret engine","text":"<p>vault secrets enable database</p> <p>vault write database/config/postgres plugin_name=postgresql-database-plugin allowed_roles=\"sql-role\" connection_url=\"postgresql://{{username}}:{{password}}@postgre-vault-test-postgresql.default.svc:5432/postgres?sslmode=disable\" username=\"vault\" password=\"vault\"</p> <p>vault write database/roles/sql-role db_name=postgres creation_statements=\"CREATE ROLE \\\"{{name}}\\\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO \\\"{{name}}\\\";\" default_ttl=\"1h\" max_ttl=\"24h\"</p> <p>vault read database/creds/sql-role</p>"},{"location":"devops/devsecops/vault/#define-policy-for-database-role","title":"define policy for database role","text":"<p>kubectl -n vault-example exec -it vault-example-0 sh</p> <p>cat &lt; /home/vault/postgres-app-policy.hcl path \"database/creds/sql-role\" {   capabilities = [\"read\"] } EOF <p>vault policy write postgres-app-policy /home/vault/postgres-app-policy.hcl</p>"},{"location":"devops/devsecops/vault/#regenerate-new-root-token","title":"regenerate new root-token","text":"<pre><code>vault operator generate-root -init\n\n# show the output and use nonce and OTP with code\nA One-Time-Password has been generated for you and is shown in the OTP field.\nYou will need this value to decode the resulting root token, so keep it safe.\nNonce         ed835311-96a5-b34b-b0b5-dab778cad815\nStarted       true\nProgress      0/3\nComplete      false\nOTP           HH8Yi0nYyKmIHV9c2J2v9LH7qFG4\nOTP Length    28\n\nvault operator generate-root -nonce=&lt;nonce-number&gt;\nthen enter unseal-1\nvault operator generate-root -nonce=&lt;nonce-number&gt;\nthen enter unseal-2\n...\nlast output get &lt;Encoded-Token&gt;\n\n\nvault operator generate-root -decode=&lt;Encoded-Token&gt; -otp=&lt;OTP-Token&gt;\n\n\nget new root-token\n\n\"hvs.xxx\"\n\n\nNot: not revoke first root token. use both\n\nif you want to revoke other root token \n\nvault token revoke &lt;old-root-token&gt;\n</code></pre>"},{"location":"devops/devsecops/vault/#vault-policy-group-and-user-configuration","title":"Vault Policy, Group, and User Configuration","text":"<p>This document outlines the steps for creating policies, groups, and users in Vault, including how to assign policies to groups and users to groups, using both Vault CLI and Vault UI.</p> <p>Vault Policy Creation In Vault, policies are created to manage appropriate paths within secret engines. These policies define access permissions for specific paths. The following examples demonstrate how to create policies for different projects and environments using the kv secret engine.</p> <p>Example Policy Structure Policies can be created individually for each project and environment combination, or with a more general approach. Here, it's assumed you have different secret engines like parasut, zirve, mikro. If these are not separate secret engines but rather paths under the kv secret engine, they should be structured as path \"secret/data/parasut/...\".</p> <ul> <li>mikro-all-policy.hcl (Example: A more general policy for Mikro)</li> </ul> <pre><code>path \"auth/ldap/login/*\" { \n  capabilities = [\"read\", \"update\"]\n}\npath \"zirve/metadata/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"zirve/data/&lt;project-name&gt;/*\" {\n  capabilities = [\"read\", \"update\", \"delete\", \"list\"]\n}\n</code></pre>"},{"location":"devops/devsecops/vault/#methods-to-create-policies","title":"Methods to Create Policies","text":"<p>Vault policies can be created using a few different methods:</p> <ol> <li>Via HCL Files and Vault CLI (Recommended for Version Control) This method involves defining policies in .hcl files and then uploading them to Vault using the CLI. This is generally recommended for version control and automation.</li> </ol> <p>After creating each .hcl file, you can upload these policies to Vault using the Vault CLI:</p> <pre><code># Upload project policy\nvault policy write &lt;project-policy-name&gt; mikro-all-policy.hcl\n</code></pre> <ol> <li>Via Vault UI For users who prefer a graphical interface, policies can also be created and managed directly through the Vault UI.</li> </ol> <p>Access your Vault UI (http://your-vault-server.example.com/).</p> <p>Navigate to Policies in the left-hand menu.</p> <ul> <li>Click on Create new policy.</li> </ul> <p>Provide a Policy Name and paste the HCL policy definition into the Policy HCL text area. <pre><code>path \"auth/ldap/login/*\" { \n  capabilities = [\"read\", \"update\"]\n}\npath \"zirve/metadata/*\" {\n  capabilities = [\"read\", \"list\"]\n}\npath \"zirve/data/&lt;project-name&gt;/*\" {\n  capabilities = [\"read\", \"update\", \"delete\", \"list\"]\n}\n</code></pre> - Click Create policy.</p>"},{"location":"devops/devsecops/vault/#group-creation-and-policy-assignment","title":"Group Creation and Policy Assignment","text":"<p>Vault groups are used to manage collections of entities (users or machines) and assign policies to them. This simplifies permission management by allowing policies to be assigned to groups rather than individual entities.</p> <ul> <li> <p>Methods for Group Creation and Policy Assignment</p> </li> <li> <p>Via Vault CLI First, ensure the identity secrets engine is enabled (it usually is by default). Then, create groups and assign policies to them.</p> </li> </ul>"},{"location":"devops/devsecops/vault/#create-project-team-group-and-assign-policy","title":"Create project-team group and assign policy","text":"<p>vault write auth/ldap/groups/ policies= <ol> <li>Via Vault UI Groups can also be created and managed through the Vault UI.</li> </ol> <p>Access your Vault UI.</p> <p>Navigate to Access &gt; LDAP in the left-hand menu.</p> <p>Go to the Groups tab.</p> <ul> <li>Click Create Group.</li> </ul> <p>Enter the Group Name (e.g., shopside-policy).</p> <p>Select the Policies to attach to this group.</p> <ul> <li>Click Save Group.</li> </ul>"},{"location":"devops/devsecops/vault/#user-creation-and-group-assignment","title":"User Creation and Group Assignment","text":"<p>Users (referred to as \"entities\" in Vault's Identity system) can be created and then associated with authentication methods. These entities can then be assigned to groups to inherit policies. Here, we'll use the userpass auth method for demonstration.</p> <ul> <li> <p>Methods for User Creation and Group Assignment</p> </li> <li> <p>Via Vault CLI First, create a user with LDAP auth method. Then, create an identity entity for this user and link it to the group.</p> </li> </ul> <pre><code># Create a user in the userpass auth method\nvault write auth/ldap/users/&lt;LDAP-Username&gt; groups=&lt;groupname&gt;\n</code></pre> <ol> <li>Via Vault UI You can create users and assign them to groups directly via the Vault UI.</li> </ol> <p>Access your Vault UI.</p> <p>Create User (via Auth Method):</p> <p>Navigate to Access &gt; LDAP in the left-hand menu.</p> <p>Go to the Users tab.</p> <ul> <li>Click Create User.</li> </ul> <p>Enter a Username (e.g., name.lastname)</p> <p>Enter the Group Name (e.g., shopside-policy).</p> <p>If need enter special Policies to attach to this user</p> <ul> <li>Click Save User</li> </ul>"},{"location":"devops/devsecops/vault/#vault-secret-creation-and-referencing-in-deployments","title":"Vault Secret Creation and Referencing in Deployments","text":"<p>This document describes how to create secrets within the Vault UI and how to reference these secrets in your application's deployment configuration, specifically within a values.yaml file for Kubernetes.</p> <ul> <li>Creating Secrets via Vault UI A user with appropriate policy permissions can create secrets within the relevant secret engine paths. These secrets can be entered as key-value pairs or as a JSON object, containing environment variables or other sensitive application data.</li> </ul> <p>Access your Vault UI.</p> <p>Navigate to the Secrets section in the left-hand menu.</p> <ul> <li>Click on the specific secret engine you want to use (e.g., parasut/, zirve/).</li> </ul> <p>Browse or create the desired path (e.g., parasut/project1/dev).</p> <ul> <li>Click Create secret.</li> </ul> <p>Enter the Key and Value pairs directly, or toggle to JSON format to paste a JSON object containing multiple key-value pairs.</p> <pre><code>Example (Key-Value):\n\nKey: DATABASE_URL\n\nValue: jdbc:postgresql://db.example.com:5432/mydb\n\nExample (JSON):\n\n{\n  \"API_KEY\": \"your_api_key_here\",\n  \"SERVICE_ACCOUNT_EMAIL\": \"service@example.com\"\n}\n</code></pre> <ul> <li>Click Save.</li> </ul>"},{"location":"devops/devsecops/vault/#referencing-secrets-in-deployment-configuration-valuesyaml","title":"Referencing Secrets in Deployment Configuration (values.yaml)","text":"<p>After creating the secret in Vault, you can reference it in your application's deployment repository, typically within the dev-test-staging directories' values.yaml files. This is done using the externalSecretVault configuration, which leverages an External Secrets Operator (ESO) in Kubernetes to fetch secrets from Vault.</p> <p>Locate the values.yaml file for your specific deployment environment (e.g., deployments/dev/values.yaml) and add the following section:</p> <pre><code>externalSecretVault:\n  name: projectname-clustername-external-secret # Unique name for the ExternalSecret resource\n  secretStoreRef:\n    name: vault-nonprod-xxx # Reference to your ClusterSecretStore or SecretStore\n  dataFrom:\n    - key: secretengine/projead\u0131/deployment-environment # Path to your secret in Vault (e.g., kv/parasut/project1/dev)\n\n##----------UYARI---------##\n# Bu b\u00f6l\u00fcmdeki extraSecret ve extraConfigMap b\u00f6l\u00fcmleri art\u0131k kullan\u0131lmamaktad\u0131r.\n# Bunun yerine externalSecretVault kullan\u0131lmaktad\u0131r.\n# Ekleme yapmak i\u00e7in Devops ekibi ile ileti\u015fime ge\u00e7iniz.\n##-----------UYARI--------##\n</code></pre>"},{"location":"devops/devsecops/vault/#creating-a-clustersecretstore-for-a-new-secret-engine","title":"Creating a ClusterSecretStore for a New Secret Engine","text":"<p>If you are creating a new secret engine in Vault and need to integrate it with Kubernetes via External Secrets Operator (ESO), you will need to create a corresponding <code>ClusterSecretStore</code> resource in your Kubernetes cluster. There are generally two methods to achieve this:</p>"},{"location":"devops/devsecops/vault/#method-1-cloning-an-existing-clustersecretstore","title":"Method 1: Cloning an Existing ClusterSecretStore","text":"<p>You can clone an existing <code>ClusterSecretStore</code> definition via the Rancher UI and modify its <code>name</code> and <code>path</code> fields to match your new secret engine. This is often the quickest way if you have a template and prefer a graphical interface.</p> <ol> <li>Navigate to the relevant cluster in the Rancher interface.</li> <li>In the left-hand sidebar, go to More Resources --&gt; external-secrets.io and then enter the ClusterSecretStore section.</li> <li>On the right side of the <code>ClusterSecretStore</code> you wish to clone, click on the <code>...</code> (three dots) icon and select Clone.</li> <li>In the pop-up window, change the <code>name</code> and <code>path</code> fields to create the new one.</li> </ol>"},{"location":"devops/devsecops/vault/#method-2-applying-a-new-clustersecretstore-crd","title":"Method 2: Applying a New ClusterSecretStore CRD","text":"<p>You can directly apply a new <code>ClusterSecretStore</code> Custom Resource Definition (CRD) to your Kubernetes cluster. This method gives you full control over the definition.</p> <p>Create a YAML file (e.g., <code>vault-new-secretstore.yaml</code>) with the following content and apply it:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-nonprod-xxx\nspec:\n  provider:\n    vault:\n      server: \"http://your-vault-server.example.com/\"\n      path: \"&lt;parasut&gt;\"   # CHANGE ME. Path to the secrets in Vault\n      # Version is the Vault KV secret engine version.\n      # This can be either \"v1\" or \"v2\", defaults to \"v2\"\n      version: \"v2\"\n      auth:\n        tokenSecretRef:\n          name: \"vault-nonprod-token\" # Name of the Kubernetes secret containing the Vault token\n          key: \"token\"   # Key in the Kubernetes secret that contains the Vault token\n          namespace:  eso # Namespace where the secret is located\n\n\n# Apply the CRD to your cluster:\nkubectl apply -f vault-new-secretstore.yaml\n\n# After applying, verify the status of your new ClusterSecretStore:\n\nkubectl get ClusterSecretStore &lt;your-new-store-name&gt;\n\n# Ensure the status indicates it's Valid.\n</code></pre>"},{"location":"devops/docker/portainer/","title":"Docker Management with Portainer","text":""},{"location":"devops/docker/portainer/#overview","title":"Overview","text":"<p>Portainer simplifies containerized application management across Docker,  Docker Swarm, Kubernetes, and Podman, whether in the Cloud, Hybrid Cloud, On-Premise, or Data Centers.  As a universal container management platform, it provides an intuitive interface that streamlines deployment  and monitoring, making containerization accessible to all. By removing complexity, Portainer enables users  to focus on innovation and efficiency.</p>"},{"location":"devops/docker/portainer/#why-portainer","title":"Why Portainer?","text":""},{"location":"devops/docker/portainer/#simplicity","title":"Simplicity","text":"<p>Portainer\u2019s user-friendly interface is designed with simplicity at its core. It eliminates the need for  extensive training, allowing teams to get up and running quickly.</p>"},{"location":"devops/docker/portainer/#universality","title":"Universality","text":"<p>Portainer is agnostic, supporting a wide range of container technologies and environments. It provides a  single pane of glass to manage your entire container ecosystem, regardless of the underlying infrastructure.</p>"},{"location":"devops/docker/portainer/#empowerment","title":"Empowerment","text":"<p>Portainer democratizes container management. It empowers developers, DevOps teams, and IT operations to  take control of their environments without needing to become container experts.</p>"},{"location":"devops/docker/portainer/#community-support","title":"Community &amp; Support","text":"<p>Built on a foundation of open-source collaboration, Portainer is backed by a passionate community and a  robust support network, ensuring that you\u2019re never alone on your container journey.</p>"},{"location":"devops/docker/portainer/#demo-prerequisites","title":"Demo - Prerequisites","text":"<ul> <li> <p>AWS Account</p> </li> <li> <p>VS Code with Remote SSH (Optional)</p> </li> </ul>"},{"location":"devops/docker/portainer/#demo","title":"Demo","text":""},{"location":"devops/docker/portainer/#part-1-setting-up-the-environment","title":"Part 1: Setting up the environment","text":"<ul> <li> <p>Sign in to <code>AWS Management Console</code> and go to <code>EC2</code>.</p> </li> <li> <p>From the menu on the left, choose <code>Instances</code>. Then, choose <code>Launch instances</code>.</p> </li> <li> <p>Launch an EC2 with the following configuration:</p> </li> </ul> <pre><code>Number of instances        : 1\nName and tags              : portainer-demo-instance\nAmazon Machine Image (AMI) : Ubuntu Server 24.04 LTS (HVM)\nInstance type              : t3.medium\nKey pair                   : &lt;your-key-pair&gt;\nVPC                        : Default VPC\nSecurity Group             : 22, 80, 443, 8000, 8080, 9000, 9443\nStorage                    : 30 GiB, gp3\nUser data                  : &lt;content-of-user-data&gt;\n</code></pre> <ul> <li> <p>You can find the content of <code>user-data.sh</code> in the same directory with <code>README.md</code>.</p> </li> <li> <p>Choose <code>Launch instance</code>.</p> </li> <li> <p>After the launch, choose the instance you created from the EC2 instance list.</p> </li> <li> <p>Then, choose <code>Connect</code>. The <code>Connect to instance</code> page is opened.</p> </li> <li> <p>Under the <code>EC2 Instance Connect</code> section, choose <code>Connect</code>.</p> </li> <li> <p>Alternatively, you can connect to EC2 using its Public IP from VS Code.</p> </li> </ul>"},{"location":"devops/docker/portainer/#part-2-configuring-the-portainer","title":"Part 2: Configuring the Portainer","text":"<ul> <li>Ensure that Docker and Docker Compose are safely installed.</li> </ul> <pre><code>docker --version\ndocker-compose --version\n</code></pre> <ul> <li>Create a working directory.</li> </ul> <pre><code>mkdir demo\ncd demo\n</code></pre> <ul> <li>In the working directory, create <code>docker-compose.yml</code>.</li> </ul> <pre><code>vi docker-compose.yml\n</code></pre> <ul> <li> <p>You can find the content of <code>docker-compose.yml</code> in the same directory with <code>README.md</code>.</p> </li> <li> <p>Copy and paste the content to <code>docker-compose.yml</code>.</p> </li> <li> <p>Create a container for Portainer.</p> </li> </ul> <pre><code>sudo docker-compose up -d\n</code></pre> <ul> <li>Copy and paste the following in a new tab on browser.</li> </ul> <pre><code>http://&lt;public-ip-of-ec2&gt;:9000\n</code></pre> <ul> <li>The login page of Portainer is opened.</li> </ul> <p></p> <ul> <li>Enter a username and a password as required.</li> </ul> <pre><code>Username: admin\nPassword: password1234\n</code></pre>"},{"location":"devops/docker/portainer/#part-3-exploring-the-features","title":"Part 3: Exploring the features","text":"<ul> <li> <p>After successful login, choose <code>Home</code> from the menu on the left.</p> </li> <li> <p>Notice the local environment for Docker.</p> </li> </ul> <p></p> <ul> <li>Choose the local environment for Docker and analyze the dashboard.</li> </ul> <p></p> <ul> <li>From the menu on the left, choose <code>Templates -&gt; Application</code>.</li> </ul> <p></p> <ul> <li>Notice the commonly used templates. Search for <code>Nginx</code> and choose it.</li> </ul> <pre><code>Name: demo-container\nShow advanced options -&gt; Port mapping: Host (8080) -&gt; Container (80)\n</code></pre> <ul> <li> <p>Choose <code>Deploy the container</code>. Notice that two containers are running.</p> </li> <li> <p>Copy and paste the following in a new tab on browser.</p> </li> </ul> <pre><code>http://&lt;public-ip-of-ec2&gt;:8080\n</code></pre> <ul> <li> <p>The <code>Welcome to nginx!</code> page is opened.</p> </li> <li> <p>Alternatively, you can list the containers from the terminal of instance.</p> </li> </ul> <pre><code>sudo docker ps\n</code></pre> <ul> <li> <p>In the <code>Container list</code> page of Portainer, locate the <code>Quick Actions</code> column.</p> </li> <li> <p>The icons for <code>Logs</code>, <code>Inspect</code>, <code>Stats</code>, <code>Exec Console</code> and <code>Attach Console</code> are given from left to right for each container, respectively.</p> </li> <li> <p>Choose <code>Logs</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the logs for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Inspect</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the configurations for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Stats</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the statistics for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Exec Console</code> icon for <code>demo-container</code>. Then, choose <code>Connect</code>.</p> </li> </ul> <p></p> <ul> <li> <p>The terminal for container is opened.</p> </li> <li> <p>Don't forget to destroy the resources you created from AWS Management Console.</p> </li> </ul>"},{"location":"devops/falcon-logscale/agent-installation/","title":"Falcon LogScale Agent(Log Collector) Setup","text":"<p>First of all, you need to create a new repo from <code>Repositories and View</code>.After creating the repo, you will go into the repo.You need to enter <code>Settings</code> from the upper menus.You need to enter <code>Ingest Tokens</code> under the <code>Ingest</code> category in the left menu.Then you will create a new token with <code>Add New Token</code>.</p> <p>Remember that this token will be useful later.</p>"},{"location":"devops/falcon-logscale/agent-installation/#download-collector","title":"Download Collector","text":"<p>You should follow the steps below to reach the download page, select the package suitable for your system and download it.</p> <p>Main Page \u27a1\ufe0f Fleet Management \u27a1\ufe0f LogScale Collector Download</p> <p>Important Installations Notes</p> <p>You may get an error while downloading.This is because the download URL is wrong or incorrect.</p> <p>For example: <code>http://10.40.140.2:8080/None/api/v1/log-collector/download/humio-log-collector_1.4.1_linux_amd64.deb</code>. </p> <p>The <code>NONE</code> here may cause you to download an incorrect file.Delete it !.After downloading the file, check its size with the <code>ls -alh</code> command.  </p> <p>Follow these steps after downloading the file.</p> <pre><code>dpkg -i humio-log-collector_x.x.x_linux_amd64.deb\n</code></pre> <p>By default, the humio-log-collector process will run as the humio-log-collector user, which is installed by the package and won't have access to logs in <code>/var/log</code>.</p> <p>This can be granted by adding the user to the adm group. <pre><code>sudo usermod -a -G adm humio-log-collector\n</code></pre></p>"},{"location":"devops/falcon-logscale/agent-installation/#you-can-run-the-logscale-collector-as-a-standalone-process-and-ignore-the-service-file-etc","title":"You can run the LogScale Collector as a standalone process and ignore the service file etc.","text":"<pre><code>/etc/humio-log-collector/config.yaml\n</code></pre> <p>Open the source field and enter the token and ip address you created in the relevant fields.</p> <p>Remember the source option specifies where you want to get the logs from</p> <pre><code>sources:\n  var_log:\n    type: file\n    include: /var/log/*\n    exclude: /var/log/*.gz\n    sink: humio\nsinks:\n  humio:\n    type: humio\n    token: &lt;Ingest Token&gt; # 210f4309-0d71-4d3d-b4e3-d503d46b93b9\n    url: &lt;host ip-address of the humio&gt; # http://52.91.72.78:8080/\n</code></pre> <p>Now you need to start and enable the services to apply the changes</p> <p><pre><code>sudo systemctl start humio-log-collector.service\n</code></pre> <pre><code>sudo systemctl enable humio-log-collector.service\n</code></pre></p> <p>Check the status of the service</p> <pre><code>sudo systemctl status humio-log-collector.service\n</code></pre> <p>Success</p> <p>If Active:active (running), you can go to the repository from the interface and check your logs</p> <p>Important Installations Notes</p> <p>If you get your logs as <code>Docker Container</code>, you can get your logs without installing an agent by using the following command with <code>Driver:Splunk</code></p> <pre><code>export YOUR_LOGSCALE_URL=\"&lt;10.40.140.2:8080&gt;\"\nexport INGEST_TOKEN=\"&lt;210f4309-0d71-4d3d-b4e3-d503d46b93b9&gt;\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\n</code></pre>"},{"location":"devops/falcon-logscale/agent-installation/#to-generate-logs-with-docker-for-testing-you-can-generate-test-logs-using-the-following-docker-compose-commands","title":"To generate logs with <code>docker</code> for testing, you can generate test logs using the following <code>docker-compose</code> commands.","text":"<pre><code>export YOUR_LOGSCALE_URL=\"\"\nexport INGEST_TOKEN=\"\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  chentex-random-logger:\n    image: chentex/random-logger:latest\n    command: [\"100\", \"300\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-apache-common:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"apache_common\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-rfc5424:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"rfc5424\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-json:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"json\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"]\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\ndocker-compose up -d\ndocker-compose logs\n</code></pre>"},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/","title":"Falcon LogScale Setup With Docker","text":""},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/#falcon-logscale-setup-requirements","title":"Falcon LogScale Setup Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>30GB</code>      STORAGE <p> The first step to install LogScale using <code>Docker</code> is to <code>install Docker</code> on the machine where you want to run <code>Docker</code> with LogScale. You can Download Docker from their site or by using a package installation program like yum or apt-get.</p> <p>Or You Can Use These Command For Ubuntu:</p> <pre><code>sudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt install docker.io\nsystemctl start docker\nsystemctl enable docker\ndocker --version\n</code></pre> <p>Now let's run a container on port 8080 using the following commands and watch it with <code>docker ps</code></p> <p>Important Installations Notes</p> <p>If your machine is not open to port 8080, make it open</p> <pre><code>export HOST_DATA_DIR=/home/ubuntu/mounts/data\nexport HOST_KAFKA_DATA_DIR=/home/ubuntu/mounts/kafka-data\nexport PATH_TO_READONLY_FILES=/home/ubuntu/mounts/readonly\nexport HOST_ENV_FILE=/home/ubuntu/mounts/\n\nmkdir -p $PATH_TO_READONLY_FILES\nmkdir -p $HOST_KAFKA_DATA_DIR\nmkdir -p $HOST_DATA_DIR\ntouch $HOST_ENV_FILE.env\n\ndocker run -v $HOST_DATA_DIR:/data  \\\n   -v $HOST_KAFKA_DATA_DIR:/data/kafka-data  \\\n   -v $PATH_TO_READONLY_FILES:/etc/humio:ro  \\\n   -e AUTHENTICATION_METHOD=single-user \\\n   -e SINGLE_USER_USERNAME=hepapi \\\n   -e SINGLE_USER_PASSWORD=123456 \\\n   --net=host \\\n   --name=humio \\\n   --ulimit=\"nofile=8192:8192\"  \\\n   --stop-timeout 300 \\\n   --env-file=$HOST_ENV_FILE  \\\n   -d \\\n   -p 8080:8080 \\\n   humio/humio\n</code></pre> 'HOST_DATA_DIR, HOST_KAFKA_DATA_DIR, HOST_ENV_FILE, PATH_TO_READONLY_FILES'<pre><code>We bind these exported variables to the downloaded HUMIO container.\n</code></pre> <p>Info</p> <p>SINGLE_USER_USERNAME= Your Username for login </p> <p>SINGLE_USER_PASSWORD= Your Password for login</p> <p>Success</p> <p>Let's go your http://ip-address:8080 and enter activation key and enter your <code>SINGLE_USER_USERNAME</code> and <code>SINGLE_USER_PASSWORD</code></p>"},{"location":"devops/falcon-logscale/humio/","title":"Humio Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"    # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"    # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"    # Change here with ip of kafka3 node\n\nEXTERNAL_IP=3.76.209.91         # Change here with ip of humio node\nINTERNAL_IP=3.76.209.91         # Change here with ip of humio node\n\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\nHUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\" # If you'll use here change here with path of humio\nHUMIO_PACKAGE_PATH=\"\"   # If you'll use here change here with path of humio package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\n\n#### Java version control ####\njava --version\nsleep 10\n\n\n#### Hunio user access ####\nadduser humio --shell=/bin/false --no-create-home --system --group\n\n#### Kafka folders create and access ####\nmkdir -p /opt/humio /etc/humio/filebeat /data/log/humio /data/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /data/log/humio /data/humio/data\nsleep 2\n\n#### Humio tar.gz package download or move ####\ncd /opt/humio/\nsleep 2\n\nif [ -n \"$HUMIO_DOWNLOAD_LINK\" ]; then\n\n    cd /opt/humio &amp;&amp; wget $HUMIO_DOWNLOAD_LINK\n    tar zxf \"$(basename $HUMIO_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\n\nelse \n\n    cd /opt/humio &amp;&amp; cp -R $HUMIO_PACKAGE_PATH .\n    tar zxf $HUMIO_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\nfi\nsleep 2\n\n\n#### Humio server.conf create ####\ncd /etc/humio/\ncat &lt;&lt;EOF &gt;&gt; server.conf\nAUTHENTICATION_METHOD=single-user\nSINGLE_USER_USERNAME=admin\nSINGLE_USER_PASSWORD=admin\nDIRECTORY=/data/humio/data\nHUMIO_AUDITLOG_DIR=/data/log/humio\nHUMIO_DEBUGLOG_DIR=/data/log/humio\nJVM_LOG_DIR=/data/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\n\nKAFKA_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092\nEXTERNAL_URL=http://$EXTERNAL_IP:8080\nPUBLIC_URL=http://$INTERNAL_IP:8080\nEOF\nsleep 2\n\n#### Humio system service create ####\ncd /etc/systemd/system/\ncat &lt;&lt;EOF &gt;&gt; humio.service\n[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/data/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\nTimeoutSec=900\n\n[Install]\nWantedBy=default.target\nEOF\nsleep 2\n\n#### User access ####\nchown -R humio:humio /opt/humio /etc/humio\n\nsleep 2\n\nchown -R humio:humio /data/log/humio /data/humio/data\n</code></pre>"},{"location":"devops/falcon-logscale/humioinstallation/","title":"LogScale Installation","text":"<p>First, you'll need to create a non-administrative user named, <code>humio</code> to run LogScale software in the background. You can do this by executing the following from the command-line:  <pre><code>adduser humio --shell=/bin/false --no-create-home --system --group\n</code></pre></p> <p>You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install Kafka.</p> <p>Next, create the LogScale system directories and give the <code>humio</code> user ownership of them: <pre><code>mkdir -p /opt/humio /etc/humio/filebeat /var/log/humio /var/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /var/log/humio /var/humio/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/humioinstallation/#installation","title":"Installation","text":"<p>You're now ready to download and install LogScale's software. You should go to the LogScale directory and use wget to download the LogScale Java Archive. You can do this from the command-line like so:  <pre><code>cd /opt/humio/\n\nwget https://repo.humio.com/repository/maven-releases/com/humio/server/1.112.0/server-1.112.0.tar.gz\n\ntar xzf /opt/humio/server-1.112.0.tar.gz\n</code></pre> The wget here is used to download the latest release from Download Humio Server. You'll have to adjust the lines for the correct directory and file name, based on the version at the time. After you've downloaded it, enter the last line here to create a symbolic link to it. </p>"},{"location":"devops/falcon-logscale/humioinstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, create the LogScale configuration file, server.conf in the <code>/etc/humio</code> directory. There are a few environment variables you will need to enter in this configuration file in order to run LogScale on a single server or instance. Below are those basic settings: <pre><code>AUTHENTICATION_METHOD=single-user\nSINGLE_USER_PASSWORD=&lt;Your-Password-Here&gt;\nSINGLE_USER_USERNAME=&lt;Your-Username-Here&gt;\nBOOTSTRAP_HOST_ID=1\nDIRECTORY=/var/humio/data\nHUMIO_AUDITLOG_DIR=/var/log/humio\nHUMIO_DEBUGLOG_DIR=/var/log/humio\nJVM_LOG_DIR=/var/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\nZOOKEEPER_URL=127.0.0.1:2181\nKAFKA_SERVERS=127.0.0.1:9092\nEXTERNAL_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;:8080\nPUBLIC_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;\nHUMIO_SOCKET_BIND=0.0.0.0\nHUMIO_HTTP_BIND=0.0.0.0\n</code></pre> Next you should set up a service file. Using a simple text editor, create a file named, humio.service in the <code>/etc/systemd/system/</code> sub-directory. Add these lines to that file: <pre><code>[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/var/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\n\n[Install]\nWantedBy=default.target\n</code></pre>  You will need to change the ownership of the LogScale files and start the LogScale service. To change the ownership, execute the following two lines from the command-line: <pre><code>chown -R humio:humio /opt/humio /etc/humio/filebeat\nchown -R humio:humio /var/log/humio /var/humio/data\n</code></pre> You're ready to start LogScale. <pre><code>systemctl start humio\n</code></pre> Just to be sure LogScale is running and everything is fine, check it with the journalctl tool. You can do this by entering the following from the command-line <pre><code>journalctl -fu humio\n</code></pre> If there are no errors, open a web browser and enter the domain name or IP address with port 8080. For example, you would enter something like http://example.com:8080 in the browser's address field.</p>"},{"location":"devops/falcon-logscale/humiosetup/","title":"Humio Single Node Installation Guide","text":""},{"location":"devops/falcon-logscale/humiosetup/#logscale-installers-are-available-for-several-linux-distributions","title":"LogScale installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Red Hat</li> </ul>"},{"location":"devops/falcon-logscale/humiosetup/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/falcon-logscale/humiosetup/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>16GB</code>  MEMORY <code>8CPU</code>  CPU <code>100GB</code>      STORAGE <p>Access Permissions</p> <p>The machine to be installed must have access to the following addresses.</p> <p>http://humio.com</p> <p>https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz </p> <p>https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz [downloads.apache.org]</p> <p>hkp://keyserver.ubuntu.com:80</p> <p>http://repos.azulsystems.com/ubuntu [repos.azulsystems.com]</p> <p>https://repo.humio.com/repository/maven-releases/com/humio/server/1.117.0/server-1.117.0.tar.gz</p> <p>The following ports must be open; <pre><code>80,443,8080,1514\n</code></pre></p> <p>You must follow the sequence below to install Falcon LogScale.</p> <ul> <li>Java Installation Page</li> <li>Zookeeper Installation Page</li> <li>Kafka Installation Page</li> <li>LogScale Installation Page</li> </ul>"},{"location":"devops/falcon-logscale/javainstallation/","title":"Java Installation","text":"<p>Install at least java version 17 in the following order</p> <p>Download java package <pre><code>https://cdn.azul.com/zulu/bin/zulu17.48.15-ca-jdk17.0.10-linux_amd64.deb\n</code></pre></p> <p>Import Azul\u2019s public key: <pre><code>sudo apt install gnupg ca-certificates curl\n</code></pre> <pre><code>curl -s https://repos.azul.com/azul-repo.key | sudo gpg --dearmor -o /usr/share/keyrings/azul.gpg\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/azul.gpg] https://repos.azul.com/zulu/deb stable main\" | sudo tee /etc/apt/sources.list.d/zulu.list\n</code></pre></p> <p>Install the required Azul Zulu package: <pre><code>sudo apt install zulu17-jdk\n</code></pre> Check java version <pre><code>java -version\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafka/","title":"Kafka Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"     # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"     # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"     # Change here with ip of kafka3 node\n\nKAFKA_NODE_NUMBER=1             # Change here with id of kafka1 node\n\nKAFKA_PACKAGE_PATH=\"\"           # If you'll use here change here with path of kafka package\nKAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"  # If you'll use here change here with path of kafka package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\njava --version\nsleep 10\n\n#### Add IP Addresses to hosts file\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\n\n#### Kafka user access ####\nadduser kafka --shell=/bin/false --no-create-home --system --group\nsleep 2\n\n#### Kafka tar.gz package download or move ####\nif [ -n \"$KAFKA_DOWNLOAD_LINK\" ]; then\n\n    cd /opt &amp;&amp; wget $KAFKA_DOWNLOAD_LINK\n    tar zxf \"$(basename $KAFKA_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nelse \n\n    cd /opt &amp;&amp; cp -R $KAFKA_PACKAGE_PATH .\n    tar zxf $KAFKA_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nfi\nsleep 2\n\n\n#### Kafka folders create and access ####\nmkdir -p /data/log/kafka\nmkdir -p /data/log/zookeeper\nmkdir -p /data/kafka/kafka\nmkdir -p /data/kafka/zookeeper\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka\nsleep 2\n\n\n#### Link for kafka ####\nln -s \"/opt/$KAFKA_PACKAGE_FILE\" /opt/kafka\nsleep 2\n\n\n#### Server properties config ####\ncd /opt/kafka/config\nFILE=\"server.properties\"\n\n# New variables\nBROKER_ID=\"broker.id=$KAFKA_NODE_NUMBER\"\nNEW_LOG_DIRS=\"log.dirs=/data/kafka/kafka\"\nsleep 2\n\n\n# Update specific values \nsed -i \"s/^broker.id=.*/$BROKER_ID/\" \"$FILE\"\nsed -i \"s|^log.dirs=.*|$NEW_LOG_DIRS|\" \"$FILE\"\necho \"delete.topic.enable = true\" &gt;&gt; \"$FILE\"\nsleep 2\n\n\n#### Kafka user chown ####\nchown -R kafka:kafka /opt/kafka\nsleep 2\n\n\n#### Zookeper Config ####\ncd /opt/kafka/config\nrm -rf zookeeper.properties\ncat &lt;&lt;EOF &gt;&gt; zookeeper.properties\ndataDir=/data/kafka/zookeeper\nclientPort=2181\nmaxClientCnxns=0\nadmin.enableServer=false\nserver.1=kafka1:2888:3888\nserver.2=kafka2:2888:3888\nserver.3=kafka3:2888:3888\n4lw.commands.whitelist=*\ntickTime=2000\ninitLimit=5\nsyncLimit=2\nEOF\nsleep 2\n\n\n#### Zookeper chown ####\necho $KAFKA_NODE_NUMBER &gt;/data/kafka/zookeeper/myid\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n\n#### Zookeeper service config ####\ncd /etc/systemd/system\nrm -rf zookeeper.service\ncat &lt;&lt;EOF &gt;&gt; zookeeper.service\n[Unit]\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/zookeeper\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\nsleep 2\n\n#### Zookeeper service start ####\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n#### Kafka service config ####\ncd /etc/systemd/system\nrm -rf kafka.service\ncat &lt;&lt;EOF &gt;&gt; kafka.service\n[Unit]\n\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsleep 2\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka /opt/kafka\n</code></pre>"},{"location":"devops/falcon-logscale/kafkainstallation/","title":"Kafka Installation","text":"<p>LogScale recommend that the latest Kafka version be used with your LogScale deployment. The latest version of Kafka is available at Kafka Downloads</p> <p><pre><code>apt-get update\napt-get upgrade\n</code></pre>  Next, create a non-administrative user named, <code>kafka</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser kafka --shell=/bin/false --no-create-home --system --group\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafkainstallation/#installation","title":"Installation","text":"<p>To install Kafka, you'll need to go to the /opt directory and download the latest release. You can do that like so with wget.  <pre><code>cd /opt\nwget https://www-us.apache.org/dist/kafka/x.x.x/kafka_x.x.x.x.tgz\n</code></pre> You would adjust this last line, change the Xs to the latest version number. Once it downloads, untar the file and then create the directories it needs like this:  <pre><code>tar zxf kafka_x.x.x.x.tgz\n\nmkdir /var/log/kafka\nmkdir /var/kafka/data\nchown kafka:kafka /var/log/kafka\nchown kafka:kafka /var/kafka/data\n\nln -s /opt/kafka_x.x.x.x /opt/kafka\n</code></pre></p> <p>The four lines in the middle here create the directories for Kafka's logs and data, and changes the ownership of those directories to the kafka user. The last line creates a symbolic to /opt/kafka. You would adjust that, though, replacing the Xs with the version number.</p>"},{"location":"devops/falcon-logscale/kafkainstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, open the Kafka properties file, server.properties, located in the kafka/config sub-directory. You'll need to set a few options \u2014 the lines below are not necessarily the order in which they'll be found in the configuration file: <pre><code>broker.id=1\nlog.dirs=/var/kafka/data\ndelete.topic.enable = true\n</code></pre> The first line sets the broker.id value to match the server number <code>(myid)</code> you set when configuring ZooKeeper. The second sets the data directory. The third line should be added to the end of the configuration file. When you're finished, save the file and change the owner to the kafka user:  <pre><code>chown -R kafka:kafka /opt/kafka_x.x.x.x\n</code></pre></p> <p>You'll have to adjust this to the version you installed. Note, changing the ownership of the link <code>/opt/kafka</code> doesn't change the ownership of the files in the directory.</p> <p>Now you'll need to create a service file for starting Kafka. Use a simple text editor to create a file named, kafka.service in the <code>/etc/systemd/system/</code> sub-directory. Then add the following lines to the service file: <pre><code>[Unit]\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/var/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Now you're ready to start the Kafka service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start kafka\nsystemctl status kafka\nsystemctl enable kafka\n</code></pre></p>"},{"location":"devops/falcon-logscale/readme/","title":"Kafka Installation (kafka.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Ex: KAFKA_HOST_1=\"172.296.22.10\"</p> <p>Usage of KAFKA_NODE_NUMBER</p> <p>If you are installing more than one kafka, you must specify a node number for each kafka. The value you provide indicates the rank of the kafka node.</p> <p><code>KAFKA_NODE_NUMBER=&lt;Kafka Node Number&gt;</code></p>"},{"location":"devops/falcon-logscale/readme/#3-cluster-kafka-example","title":"3 Cluster Kafka Example:","text":"<p><code>(Primary Node kafka.sh)</code>   : KAFKA_NODE_NUMBER=1</p> <p><code>(Secondary Node kafka.sh)</code> : KAFKA_NODE_NUMBER=2</p> <p><code>(Third Node kafka.sh)</code> : KAFKA_NODE_NUMBER=3</p> <p>Usage of KAFKA_DOWNLOAD_LINK / KAFKA_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"KAFKA_DOWNLOAD_LINK\"</code> variable and leave the <code>\"KAFKA_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"KAFKA_PACKAGE_PATH\"</code> variable and leave the <code>\"KAFKA_PACKAGE_URL\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>KAFKA_PACKAGE_PATH=\"\" KAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"</p> <p>Example Usage (We will use PATH):</p> <p>KAFKA_PACKAGE_PATH=\"/tmp/packages/kafka_2.13-3.7.0.tgz\" KAFKA_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#humio-setup-humiosh","title":"Humio Setup (humio.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code> KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code> KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Usage of EXTERNAL_IP / INTERNAL_IP</p> <p>To access the Humio interface:</p> <p>EXTERNAL_IP=<code>\"&lt;Humio External IP Address&gt;\"</code></p> <p>INTERNAL_IP=<code>\"&lt;Humio Internal IP Address&gt;\"</code></p> <p>Usage of HUMIO_DOWNLOAD_LINK / HUMIO_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable and leave the <code>\"HUMIO_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"HUMIO_PACKAGE_PATH\"</code> variable and leave the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>HUMIO_PACKAGE_PATH=\"\" HUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\"</p> <p>Example Usage (We will use PATH):</p> <p>HUMIO_PACKAGE_PATH=\"/tmp/packages/server-1.131.1.tar.gz\" HUMIO_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#run-the-services","title":"Run The Services","text":"<p>After the installation is completed, quickly run the following commands one by one.</p> <p>For each node (node1, node2, node3)</p> <p><code>systemctl enable zookeeper</code></p> <p><code>systemctl start zookeeper</code></p> <p><code>systemctl status zookeeper</code></p> <p><code>systemctl enable kafka</code></p> <p><code>systemctl start kafka</code></p> <p><code>systemctl status kafka</code></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/","title":"Zookeeper Installation","text":"<p><pre><code>apt-get update\napt-get upgrade\n</code></pre> Next, create a non-administrative user named, <code>zookeeper</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser zookeeper --shell=/bin/false --no-create-home --system --group\n</code></pre> You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install ZooKeeper.</p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#installation","title":"Installation","text":"<p>Navigate to opt directory and download a of ZooKeeper. The official release site is Apache Zookeeper Release <pre><code>cd /opt\nwget https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz\n</code></pre> After the file downloads, untar the ZooKeeper file and create a symbolic to <code>/opt/zookeeper</code> like so:  <pre><code>tar -zxf apache-zookeeper-x.x.x-bin.tar.gz\nln -s /opt/apache-zookeeper-x.x.x-bin /opt/zookeeper\n</code></pre> Navigate to zookeeper sub-directory and create a data directory for ZooKeeper:  <pre><code>cd /opt/zookeeper\nmkdir -p /var/zookeeper/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#configuration","title":"Configuration","text":"<p>Using a text editor, create the ZooKeeper configuration file in the conf sub-directory. Name the file, zoo.cfg. For example, <code>/opt/zookeeper/conf/zoo.cfg</code>. Copy the lines below into that file: <pre><code>tickTime = 2000\ndataDir = /var/zookeeper/data\nclientPort = 2181\ninitLimit = 5\nsyncLimit = 2\nmaxClientCnxns=60\nautopurge.purgeInterval=1\nadmin.enableServer=false\n4lw.commands.whitelist=*\nserver.1=127.0.0.1:2888:3888\nadmin.enableServer=false\n</code></pre> Create a myid file in the data sub-directory with just the number 1 as its contents. They you can start ZooKeeper to verify that the configuration is working:</p> <p><pre><code>bash -c 'echo 1 &gt; /var/zookeeper/data/myid'\n\n./bin/zkServer.sh start\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-x.x.x/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n</code></pre> Stop ZooKeeper and change the ownership of the zookeeper directory like so, adjusting for the version number you installed:  <pre><code>./bin/zkServer.sh stop\n\nchown -R zookeeper:zookeeper /opt/apache-zookeeper-x.x.x\nchown -R zookeeper:zookeeper /var/zookeeper/data\n</code></pre></p> <p>So that ZooKeeper will start when the server is rebooted, you'll need to create a ZooKeeper service file named zookeeper.service in the <code>/etc/systemd/system/</code> sub-directory. Use a text editor to create the file and copy the following lines into it. <pre><code>[Unit]\nDescription=ZooKeeper Daemon\nDocumentation=http://zookeeper.apache.org\nRequires=network.target\nAfter=network.target\n\n[Service]\nType=forking\nWorkingDirectory=/opt/zookeeper\nUser=zookeeper\nGroup=zookeeper\nExecStart=/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg\nExecStop=/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg\nExecReload=/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg\nTimeoutSec=30\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre> Start the ZooKeeper service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start zookeeper\nsystemctl status zookeeper\nsystemctl enable zookeeper\n</code></pre></p>"},{"location":"devops/git/Commands/","title":"Commands","text":"Command Explanation Usage push Pushing is the process of sending local commits to a remote repository. <code>git push origin branch_name</code> pull Pulling is the process of fetching and merging remote changes into the local repository. <code>git pull origin branch_name</code> add . Adds all modified and new files to the staging area. <code>git add .</code> add  Adds a specific file to the staging area. <code>git add file_name</code> commit A commit is a snapshot of changes made to the repository. <code>git commit -m \"Commit message\"</code> init Initializes a new Git repository in the current directory. <code>git init</code> log Log displays the commit history of the repository. <code>git log</code> status Status shows the current state of the repository. <code>git status</code> branch A branch is a separate line of development within a repository. <code>git branch branch_name</code> merge Merging combines changes from different branches into a single branch. <code>git merge branch_name</code> revert Revert creates a new commit that undoes changes from a previous commit. <code>git revert commit_hash</code> reset Reset moves the current branch pointer to a specific commit, potentially discarding commits. <code>git reset commit_hash</code> rm / remove Removes a file from the repository and the working directory. <code>git rm file_name</code> mv / move Renames or moves a file or directory within the repository. <code>git mv old_file_name new_file_name</code> clone Cloning creates a local copy of a remote repository. <code>git clone repository_url</code> echo Echo prints a message or value to the terminal or a file. <code>echo \"Hello, World!\"</code> touch Touch creates an empty file or updates the timestamp of an existing file. <code>touch file_name</code> ls / list List displays the files and directories in the current directory. <code>ls</code> or <code>list</code> cat Cat displays the contents of a file. <code>cat file_name</code> diff Diff shows the differences between different versions of files. <code>git diff</code> checkout Checkout allows you to switch between branches or restore files from previous commits. <code>git checkout branch_name</code> .gitignore A .gitignore file specifies files and patterns to be ignored by Git. Create a .gitignore file and list files/patterns to ignore repository A repository is a location where Git stores all the files, history, and changes for a project. <code>git init</code> fork Forking creates a copy of a repository under your GitHub account. Click on the \"Fork\" button in the GitHub UI pull request A pull request proposes changes from a forked repository to the original repository. Create a pull request through the GitHub UI PR (Pull Request) PR is an abbreviation for pull request. Use the term \"PR\" interchangeably with \"pull request\" master Master is the default branch in Git. The initial branch created in a repository (commonly used) config Config sets configuration options for Git. <code>git config --global user.name \"Your Name\"</code> remote Remote refers to a remote repository, typically on a server. <code>git remote add origin repository_url</code> stash Stash temporarily saves local modifications for later use. <code>git stash save \"Stash message\"</code> pop Pop applies the most recent stash and removes it from the stash list. <code>git stash pop</code> reflog Reflog shows a log of all reference updates in the repository. <code>git reflog</code> Action Explanation Usage Create and Switch to New Branch Creates a new branch and switches to it. <code>git checkout -b &lt;branch&gt;</code> Merge Branch into Main Branch Merges changes from a feature branch into the main branch. <code>git checkout &lt;main_branch&gt;</code><code>git merge &lt;feature_branch&gt;</code> Rebase Branch onto Main Branch Updates the feature branch with the latest changes from the main branch. <code>git checkout &lt;feature_branch&gt;</code><code>git rebase &lt;main_branch&gt;</code> Interactive Rebase Allows interactive modification, reordering, or squashing of commits. <code>git rebase -i HEAD~&lt;number_of_commits&gt;</code> Revert a Commit Creates a new commit that undoes the changes from a specific commit. <code>git revert &lt;commit_hash&gt;</code> Undo Last Commit (Keep Changes) Moves the branch pointer back one commit, keeping changes in the staging area. <code>git reset --soft HEAD~1</code> Discard Last Commit (Lose Changes) Moves the branch pointer back one commit, discarding changes in the working directory and staging area. <code>git reset --hard HEAD~1</code>"},{"location":"devops/git/Description/","title":"Description","text":""},{"location":"devops/git/Description/#version-control-system-vcs","title":"Version Control System (VCS)","text":"<p>A Version Control System (VCS) is a software tool that helps track changes made to files and directories over time. It allows multiple people to collaborate on a project, keeping a history of changes, and providing mechanisms to manage different versions of files. VCS enables teams to work concurrently, facilitating efficient collaboration and providing features like branching, merging, and conflict resolution.</p>"},{"location":"devops/git/Description/#git","title":"Git","text":"<p>Git is a widely used distributed version control system designed for speed, flexibility, and data integrity. It is free and open-source, offering powerful features that make it popular among individuals and large organizations. Git provides a decentralized approach, allowing users to have a local copy of the entire repository, including its history, branches, and tags.</p>"},{"location":"devops/git/installation/","title":"Installation","text":""},{"location":"devops/git/installation/#downloading-git","title":"Downloading Git","text":"<p>To download Git for different operating systems, follow these instructions:</p>"},{"location":"devops/git/installation/#windows","title":"Windows","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the latest Git version for Windows.</li> <li>Run the installer and follow the prompts.</li> <li>Choose the desired installation options and complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#macos","title":"macOS","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the macOS version of Git.</li> <li>Run the installer package and follow the prompts.</li> <li>Complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#linux-ubuntu","title":"Linux (Ubuntu)","text":"<ul> <li>Open the terminal.</li> <li>Install Git using the package manager:</li> <li>For Debian/Ubuntu-based systems: <code>sudo apt-get install git</code></li> <li>For Fedora: <code>sudo dnf install git</code></li> <li>For CentOS/RHEL: <code>sudo yum install git</code></li> </ul>"},{"location":"devops/git/installation/#linux-generic","title":"Linux (Generic)","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the source code archive for Linux.</li> <li>Extract the archive to a desired location.</li> <li>In the terminal, navigate to the extracted directory.</li> <li>Run the following commands:</li> <li><code>make prefix=/usr/local all</code></li> <li><code>sudo make prefix=/usr/local install</code></li> </ul>"},{"location":"devops/git/installation/#verification","title":"Verification","text":"<p>Once Git is installed on your system, you can verify the installation by opening a terminal and running <code>git --version</code> to display the installed version.</p>"},{"location":"devops/git/installation/#interfaces","title":"Interfaces","text":"<p>Git provides a command-line interface (CLI) and various graphical user interface (GUI) tools. You can choose the interface that suits your preference and start using Git for version control in your projects.</p>"},{"location":"devops/gitlab/gitlab-runner-installation/","title":"GitLab Runner Installation Guide","text":"<p>Hello everyone from Hepapi! Today, we will discuss how to install GitLab Runner on a self-hosted GitLab instance.  </p> <p>In modern DevOps workflows, GitLab Runner plays a critical role in executing CI/CD pipelines. Whether you need to run jobs on Kubernetes, Docker, or bare-metal environments, installing GitLab Runner correctly ensures that your builds, tests, and deployments are executed efficiently.  </p> <p>In this guide, we'll walk through the step-by-step installation process using Helm, making it easy to deploy, configure, and manage GitLab Runner in your Kubernetes cluster.  </p> <p>By the end of this tutorial, you'll have a fully functional GitLab Runner connected to your GitLab instance, ready to process CI/CD jobs seamlessly.  </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#1-add-the-gitlab-helm-repository","title":"1. Add the GitLab Helm Repository","text":"<p>Before installing GitLab Runner, ensure that the official GitLab Helm repository is added and updated in your system:  </p> <pre><code>helm repo add gitlab https://charts.gitlab.io\nhelm repo update\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#2-get-the-gitlab-runner-token","title":"2. Get the GitLab Runner Token","text":"<p>To register a new GitLab Runner, follow these steps:  </p> <ol> <li>Navigate to GitLab \u2192 Settings \u2192 CI/CD.  </li> <li>Scroll down to the Runners section.  </li> <li>Click on New instance runner.  </li> <li>Add a tag to the runner (e.g., <code>kubernetes-runner</code>).  </li> <li>Click Create Runner and save the generated token.  </li> </ol> <p>You'll need this runner token in the next step to connect GitLab Runner to your instance.  </p> <p> </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#3-install-gitlab-runner-with-helm","title":"3. Install GitLab Runner with Helm","text":"<p>Now, install GitLab Runner on your Kubernetes cluster using Helm. Replace <code>&lt;your-gitlab-url&gt;</code> with your GitLab instance URL and <code>&lt;your-runner-token&gt;</code> with the token you saved in the previous step.  </p> <pre><code>helm install --namespace gitlab gitlab-runner gitlab/gitlab-runner \\\n  --set gitlabUrl=&lt;your-gitlab-url&gt; \\\n  --set runnerToken=&lt;your-runner-token&gt;\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#example","title":"Example:","text":"<pre><code>helm install --namespace gitlab gitlab-runner gitlab/gitlab-runner \\\n  --set gitlabUrl=https://gitlab.example.com \\\n  --set runnerToken=your-unique-runner-token\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#4-verify-the-gitlab-runner-deployment","title":"4. Verify the GitLab Runner Deployment","text":"<p>Once the installation is complete, check if the GitLab Runner pods are running correctly:  </p> <pre><code>kubectl get pods -n gitlab\n</code></pre> <p>You should see an output similar to this:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\ngitlab-runner-56f7cbb6d8-tlqxm          1/1     Running   0          2m\n</code></pre> <p>If the runner is not running, check the logs for any issues:  </p> <pre><code>kubectl logs -n gitlab -l app=gitlab-runner\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#5-confirm-runner-registration-in-gitlab","title":"5. Confirm Runner Registration in GitLab","text":"<p>Go to GitLab \u2192 Settings \u2192 CI/CD \u2192 Runners, and you should see your new runner online with the tag you assigned earlier.  </p> <p>This means your GitLab Runner is successfully installed and ready to execute CI/CD jobs! \ud83c\udf89  </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#6-optional-customize-gitlab-runner-configuration","title":"6. Optional: Customize GitLab Runner Configuration","text":"<p>You can customize your GitLab Runner setup by modifying values in the Helm chart. For more details and how to enable docker in docker, refer to the GitLab Runner Helm Chart Configuration documentation. </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#conclusion","title":"Conclusion","text":"<p>By following this guide, you have successfully installed GitLab Runner in your Kubernetes cluster using Helm. This setup enables you to leverage scalable and automated CI/CD pipelines while keeping infrastructure maintenance minimal.  </p> <p>Feel free to explore additional configurations based on your requirements, such as autoscaling, custom runner tags, and integration with Kubernetes-native workloads.  </p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/","title":"GitLab Self-Hosted on Kubernetes - Installation Guide","text":"<p>Hello everyone from Hepapi. Today, we will discuss GitLab Self-Hosted on Kubernetes. We are in an era where DevOps automation and containerized applications play a crucial role in software development. Deploying and managing a self-hosted GitLab instance on Kubernetes provides scalability, flexibility, and security for your CI/CD pipelines. Compared to traditional single-node installations, a Kubernetes-based deployment ensures high availability and resilience. However, managing such deployments manually can be complex and time-consuming. Thanks to GitLab Helm Charts, we can simplify the installation and configuration process while maintaining consistency and reusability across different environments.</p> <p>In our modern development landscape, where applications are increasingly built as microservices, deploying GitLab in a self-hosted environment provides a robust and scalable solution for your DevOps needs. Instead of wrestling with complex installation procedures, this guide simplifies the process into clear, modular steps\u2014from adding the GitLab Helm repository to installing GitLab with your custom configurations, and finally verifying the deployment. With this streamlined approach, you can quickly harness GitLab\u2019s powerful features for continuous integration and delivery while keeping your setup clean and maintainable.</p> <p></p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#1-add-the-gitlab-helm-repository","title":"1. Add the GitLab Helm Repository","text":"<p>First, add the official GitLab Helm repository and update your local Helm chart list:</p> <pre><code>helm repo add gitlab https://charts.gitlab.io/\nhelm repo update\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#2-install-gitlab-with-helm","title":"2. Install GitLab with Helm","text":"<p>To install GitLab on your Kubernetes cluster using Helm, run:</p> <pre><code>helm install gitlab gitlab/gitlab \\\n  --set global.hosts.domain=example.com \\\n  --set certmanager-issuer.email=me@example.com\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#using-an-existing-cert-manager","title":"Using an Existing Cert-Manager","text":"<p>If you already have Cert-Manager installed in your cluster, disable its installation by adding:</p> <pre><code>--set certmanager.install=false\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#disabling-runner-install","title":"Disabling runner install","text":"<p>If you want to install gitlab-runner seperately you can disable runner install with:</p> <pre><code>--set gitlab-runner.install=false\n</code></pre> <p>You can checkout all deployment options from here.</p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#3-verify-the-deployment","title":"3. Verify the Deployment","text":"<p>Check if the GitLab pods are running with:</p> <pre><code>kubectl get pods -n gitlab\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#4-retrieve-the-gitlab-root-password","title":"4. Retrieve the GitLab Root Password","text":"<p>To get the initial GitLab root password, use the following command:</p> <pre><code>kubectl get secret gitlab-gitlab-initial-root-password -ojsonpath='{.data.password}' | base64 --decode ; echo\n</code></pre> <p></p> <p></p>"},{"location":"devops/jenkins/","title":"Jenkins","text":""},{"location":"devops/jenkins/jenkins-installation/","title":"Jenkins Install","text":""},{"location":"devops/jenkins/jenkins-installation/#jenkins-installers-are-available-for-several-linux-distributions","title":"Jenkins installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Fedora</li> <li>Red Hat/Alma/Rocky</li> </ul>"},{"location":"devops/jenkins/jenkins-installation/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/jenkins/jenkins-installation/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>256MB</code>  MEMORY <code>512MB</code>  CPU <code>1GB</code>      STORAGE <p>1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container)</p>"},{"location":"devops/jenkins/jenkins-installation/#recommended-hardware-configuration-for-a-small-team","title":"Recommended hardware configuration for a small team:","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>50GB</code>      STORAGE"},{"location":"devops/jenkins/jenkins-installation/#installation-of-java","title":"Installation of Java","text":"<p><code>Jenkins requires Java in order to run, yet certain distributions don\u2019t include this by default and some Java versions are incompatible with Jenkins. There are multiple Java implementations which you can use. OpenJDK is the most popular one at the moment, we will use it in this guide. Update the Debian apt repositories, install OpenJDK 11, and check the installation with the commands:</code></p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jre\njava -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment (build 11.0.12+7-post-Debian-2)\nOpenJDK 64-Bit Server VM (build 11.0.12+7-post-Debian-2, mixed mode, sharing)\n</code></pre> <p>After installing Java without any problems, we will install jenkins</p>"},{"location":"devops/jenkins/jenkins-installation/#jenkins-install_1","title":"Jenkins Install","text":"<pre><code>curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install jenkins\n</code></pre> <p>Quote</p> <p>If Jenkins fails to start because a port is in use, run -- systemctl edit jenkins -- and add the following ; <pre><code>[Service]\nEnvironment=\"JENKINS_PORT=8081\"\n</code></pre></p>"},{"location":"devops/jenkins/jenkins-installation/#start-jenkins","title":"Start Jenkins","text":"<p>You can enable the Jenkins service to start at boot with the command: <pre><code>sudo systemctl enable jenkins\n</code></pre></p> <p>You can start the Jenkins service with the command: <pre><code>sudo systemctl start jenkins\n</code></pre></p> <p>You can check the status of the Jenkins service using the command: <pre><code>sudo systemctl status jenkins\n</code></pre></p> <p>If everything has been set up correctly, you should see an output like this: <pre><code>Loaded: loaded (/lib/systemd/system/jenkins.service; enabled; vendor preset: enabled)\nActive: active (running) since Tue 2018-11-13 16:19:01 +03; 4min 57s ago\n</code></pre></p> <p>If you have a firewall installed, you must add Jenkins as an exception. You must change YOURPORT in the script below to the port you want to use. Port 8080 is the most common. <pre><code>YOURPORT=8080\nPERM=\"--permanent\"\nSERV=\"$PERM --service=jenkins\"\n\nfirewall-cmd $PERM --new-service=jenkins\nfirewall-cmd $SERV --set-short=\"Jenkins ports\"\nfirewall-cmd $SERV --set-description=\"Jenkins port exceptions\"\nfirewall-cmd $SERV --add-port=$YOURPORT/tcp\nfirewall-cmd $PERM --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n</code></pre></p> <ul> <li> After installing Jenkins, we go to the instance ip address e.g : <code>&lt; ip-address &gt;:8080</code> </li> <li> <p> On the page that opens, it asks us to enter a token. We take this token from the server with the <code>cat /var/lib/jenkins/secrets/initialAdminPassword</code> code and paste it into the input and log in.</p> </li> <li> <p> Jenkins gives you two options.</p> <ul> <li> <code>Install Suggested Plugins</code></li> <li> <code>Select Plugins To Install</code></li> </ul> </li> </ul> <p>If you don't see jenkins when you go to your server's ip address, allow port 8080</p> <p> You can login to Jenkins by choosing the one you want and creating Username and Password.</p> <p>For more installation details</p>"},{"location":"devops/jenkins/jenkins-installation/#happy-jenkins","title":"Happy Jenkins","text":""},{"location":"devops/jenkins/shared-library/","title":"Jenkins Shared Library","text":"<p>Hello everybody from Hepapi.We will talk about Jenkins Shared Library.Lets start. We are in a period where modern applications are generally divided into small components and run on a microservice architecture. Compared to a monolithic application, there are many extra pipelines in an application with microservice architecture. Therefore, it is very important to ensure modularity and reusability of the created pipelines. Thanks to Jenkins Shared Library, we can get rid of the code complexity in the pipeline and comply with the DRY (Don't Repeat Yourself) principle.</p> <p>Example: You can perform the docker build process that you perform jointly for more than one service by simply sending the docker image information to a single function for all services.</p> <pre><code>// var/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String REPOSITORY) {\n  String REGISTRY = \"ersinsari\"\n  sh \"docker build -t ${REGISTRY}/${REPOSITORY}:latest .\"\n}\n// Jenkinsfile\n@Library('mylibrary') _\npipeline {\n  stages {\n    stage('Docker Build') {\n      dockerBuild \"nodejs-helloworld\"    \n    }\n  }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#folder-structure-of-shared-library","title":"Folder Structure of Shared Library","text":"<pre><code>\u2514\u2500\u2500 jenkins-shared\n    \u251c\u2500\u2500 src\n    \u2502   \u251c\u2500\u2500 main\n    \u2502       \u2514\u2500\u2500 groovy\n    |           \u2514\u2500\u2500 *.groovy\n    \u251c\u2500\u2500 vars           \n    |   \u251c\u2500\u2500 *.groovy\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 config.properties\n        \u2514\u2500\u2500 template.xml\n</code></pre> <ul> <li> <p>The src directory is structured like a standard Java project. This means that you can use the import statement to import classes from other directories in the src directory.</p> </li> <li> <p>The vars directory is a special directory that contains global variables that are defined in the shared library. These variables can be accessed from any Jenkins job that imports the shared library.</p> </li> <li> <p>The resources directory is a regular directory that can contain any type of file. However, it is typically used to store static resources that are used by the shared library.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#global-shared-libraries","title":"Global Shared Libraries","text":"<p>There are several places where Shared Libraries can be defined, depending on the use-case</p> <ul> <li>Manage Jenkins \u00bb System \u00bb Global Pipeline Libraries as many libraries as necessary can be configured</li> </ul> <p>These libraries are considered \"trusted:\" they can run any methods in Java, Groovy, Jenkins internal APIs, Jenkins plugins, or third-party libraries. This allows you to define libraries which encapsulate individually unsafe APIs in a higher-level wrapper safe for use from any Pipeline. Beware that anyone able to push commits to this SCM repository could obtain unlimited access to Jenkins. You need the Overall/RunScripts permission to configure these libraries (normally this will be granted to Jenkins administrators).</p>"},{"location":"devops/jenkins/shared-library/#folder-level-shared-libraries","title":"Folder-level Shared Libraries","text":"<p>Any Folder created can have Shared Libraries associated with it. This mechanism allows scoping of specific libraries to all the Pipelines inside of the folder or subfolder.Folder-based libraries are not considered \"trusted:\" they run in the Groovy sandbox just like typical Pipelines.</p>"},{"location":"devops/jenkins/shared-library/#automatic-shared-libraries","title":"Automatic Shared Libraries","text":"<p>Other plugins may add ways of defining libraries on the fly. For example, the Pipeline: GitHub Groovy Libraries plugin allows a script to use an untrusted library named like github.com/someorg/somerepo without any additional configuration. In this case, the specified GitHub repository would be loaded, from the master branch, using an anonymous checkout.</p>"},{"location":"devops/jenkins/shared-library/#using-libraries","title":"Using Libraries","text":"<p>Shared Libraries marked Load implicitly allows Pipelines to immediately use classes or global variables defined by any such libraries. To access other shared libraries, the Jenkinsfile needs to use the @Library annotation, specifying the library\u2019s name:</p> <pre><code>@Library('my-shared-library') _\n\n/* Using a version specifier, such as branch, tag, etc */\n@Library('my-shared-library@1.0') _\n\n/* Accessing multiple libraries with one statement */\n@Library(['my-shared-library', 'otherlib@abc1234']) _\n</code></pre>"},{"location":"devops/jenkins/shared-library/#loading-libraries-dynamically","title":"Loading libraries dynamically","text":"<p>As of version 2.7 of the Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time during the build.</p> <p>If you are only interested in using global variables/functions (from the vars/ directory), the syntax is quite simple:</p> <pre><code>library 'my-shared-library'\n</code></pre> <p>Thereafter, any global variables from that library will be accessible to the script.</p> <p>Using classes from the src/ directory is also possible, but trickier. Whereas the @Library annotation prepares the \u201cclasspath\u201d of the script prior to compilation, by the time a library step is encountered the script has already been compiled. Therefore you cannot import or otherwise \u201cstatically\u201d refer to types from the library.</p>"},{"location":"devops/jenkins/shared-library/#demo","title":"DEMO","text":"<p>First of all we need jenkins server for this demo.There are some options to deploy jenkins-server</p> <pre><code>https://www.jenkins.io/doc/book/installing/\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-1-creating-shared-library","title":"Step-1 Creating Shared library","text":"<p>Let's first create the groovy scripts of the Jenkins shared library and push them to git. We have two simple scripts for this example. We will perform docker build and docker push operations using these scripts.</p> <pre><code>// vars/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n    dir(\"${WORKSPACE}\") {\n        sh \"docker build -t ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER) .\"\n    }\n}\n</code></pre> <pre><code>// vars/dockerPush.groovy\n#!/usr/bin/env groovy\n\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n\n    dir(\"${WORKSPACE}\") {\n        sh \"echo $DOCKERHUB_CRED_PSW | docker login -u $DOCKERHUB_CRED_USR --password-stdin\"\n        sh \"docker push ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER)\"\n    }\n}\n</code></pre> <pre><code>// vars/sayHello.groovy\n#!/usr/bin/env groovy\n\ndef call(String name = 'human') {\n  echo \"Hello, ${name}.\"\n}\n</code></pre> <p>Go to the Global Pipeline Library to Jenkins. After accessing the Jenkins dashboard, navigate to Manage Jenkins &gt;  System to find the Global Pipeline Libraries section and click on the add button to add a new library. Manage Jenkins ---&gt; System ---&gt; Global Pipeline Libraries ---&gt; Click Add button</p> <ul> <li>Library Name: my-shared-library</li> <li>Default version: main</li> <li>Retrivial method: Modern SCM</li> <li>Source Code Management</li> <li> <p>Project Repository: https://github.com/ersinsari13/jenkins-shared-library.git  #enter your repo name</p> </li> <li> <p>We can leave other parameters as default and save the configuration.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-2-creating-jenkinsfile","title":"Step-2 Creating jenkinsfile","text":"<p>Let's first create github repo named docker-build-push then create the Jenkinsfile and push them to git.</p> <pre><code>@Library('my-shared-library') _\npipeline {\n    agent any\n\n    environment {\n        DOCKERHUB_CRED=credentials('docker-hub-credential')\n    }\n    stages {\n        stage('Set Environment') {\n            steps {\n                script {\n                    REGISTRY = \"ersinsari\"\n                    REPOSITORY = \"docker-build-push\"\n                }\n            }\n        }\n        stage('Hello') {\n            steps {\n                sayHello \"hepapi\"\n            }\n        }\n        stage('Build') {\n            steps {\n                dockerBuild \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n        stage('Push') {\n            steps {\n                dockerPush \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-3-creating-pipeline","title":"Step-3 Creating Pipeline","text":"<ul> <li>Go to Jenkins Server Dashboard</li> <li>Click New Item</li> <li>Enter \"docker-build-push\" as a name and select pipeline and click Ok</li> <li> <p>Under Pipeline Section:</p> </li> <li> <p>Definition: Pipeline script from SCM</p> </li> <li>SCM: Git</li> <li>Repository URL: https://github.com/ersinsari13/docker-build-push.git  #enter your repository name</li> <li>Branch: main</li> </ul> <p>Since the Jenkinsfile in the repo is not under any folder, it will be sufficient to just specify its name. For example, if my Jenkinsfile file was located under a folder named build; I should have written build/Jenkinsfile in the Script Path section.</p> <ul> <li>Save the pipeline configuration</li> </ul>"},{"location":"devops/jenkins/shared-library/#step-4-credential-for-dockerhub","title":"Step-4 Credential for Dockerhub","text":"<p>In order for the images we created in Pipeline to be pushed to the registry, we need to provide registry credential information to the Jenkins server.</p> <ul> <li>Go to Jenkins Dashboard</li> <li>Select Manage Jenkins</li> <li>Click Credentials</li> <li>Select Global</li> <li> <p>Click Add Credentials button:</p> </li> <li> <p>Kind: Username with password</p> </li> <li>username: enter your dockerhub registry username</li> <li>password: enter your dockerhub registry password</li> <li>id: dockerhub-registry-credentials</li> <li> <p>description: dockerhub-registry-credentials</p> </li> <li> <p>Save credentials configuration</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-5-build-pipeline","title":"Step-5 Build Pipeline","text":"<p>After all the steps, the pipeline we created is now ready to be tested.</p> <ul> <li>Go to Jenkins Server Dashboard</li> <li>Select Pipeline was created</li> <li>Click Build Now</li> <li>You should see the pipeline result success</li> <li>You can check your new image on the Dockerhub registry</li> </ul>"},{"location":"devops/k8s-engine/k3s-installation/","title":"K3S Setup","text":""},{"location":"devops/k8s-engine/k3s-installation/#k3s-requirements","title":"K3S Requirements :","text":"Resources Limits <code>512MB</code>  MEMORY (we recommend at least 1GB) <code>1CPU</code>  CPU (we recommend at least 2CPU) <code>20GB</code>      STORAGE"},{"location":"devops/k8s-engine/k3s-installation/#cpu-and-memory","title":"CPU and Memory","text":"<p>The following are the minimum CPU and memory requirements for nodes in a high-availability K3S server</p> Deployment Size Nodes VCPUS RAM Small Up to 10 2 4 GB Medium Up to 100 4 8 GB Large Up to 250 8 16 GB X-Large Up to 500 16 32 GB XX-Large 500+ 32 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#disks","title":"Disks","text":"<p> The cluster performance depends on database performance. To ensure optimal speed, we recommend always using SSD disks to back your K3S cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS.</p>"},{"location":"devops/k8s-engine/k3s-installation/#database","title":"Database","text":"<p>K3S supports different databases including MySQL, PostgreSQL, MariaDB, and etcd, the following is a sizing guide for the database resources you need to run large clusters:</p> Deployment Size Nodes VCPUS RAM Small Up to 10 1 2 GB Medium Up to 100 2 8 GB Large Up to 250 4 16 GB X-Large Up to 500 8 32 GB XX-Large 500+ 16 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#single-master-node-k3s-install","title":"Single Master Node K3S Install","text":"<p>A single-node server installation is a fully-functional Kubernetes cluster, including all the datastore, control-plane, kubelet, and container runtime components necessary to host workload pods. It is not necessary to add additional server or agents nodes, but you may want to do so to add additional capacity or redundancy to your cluster.</p> <p><code>Run this command :</code> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#high-available-k3s-install","title":"High Available K3S Install","text":"<p>To run K3S in this mode, you must have an odd number of server nodes. We recommend starting with three nodes.</p> <p>To get started, first launch a server node with the <code>cluster-init</code> flag to enable clustering and a token that will be used as a shared secret to join additional servers to the cluster.</p> <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s.service</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 13:54:17 UTC; 37min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>After launching the first server, join the second and third servers to the cluster using the shared secret: <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://&lt;ip or hostname of server1&gt;:6443\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#cluster-access","title":"Cluster Access","text":"<p>The kubeconfig file stored at <code>/etc/rancher/k3s/k3s.yaml</code> is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path.This can be done by either exporting the <code>KUBECONFIG</code> environment variable or by invoking the <code>--kubeconfig</code> command line flag. Refer to the examples below for details.</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Check to see that the second and third servers are now part of the cluster: <pre><code>$ kubectl get nodes\n\nNAME        STATUS   ROLES                       AGE   VERSION\nserver1     Ready    control-plane,etcd,master   28m   vX.Y.Z\nserver2     Ready    control-plane,etcd,master   13m   vX.Y.Z\nserver3     Ready    control-plane,etcd,master   10m   vX.Y.Z\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#k3s-agent-install","title":"K3S Agent Install","text":"<p>To install additional agent nodes and add them to the cluster, run the installation script with the <code>K3S_URL</code> and <code>K3S_TOKEN</code> environment variables. Here is an example showing how to join an agent: <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -\n</code></pre></p> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s-agent</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s-agent.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 14:24:53 UTC; 7min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>Setting the <code>K3S_URL</code> parameter causes the installer to configure K3S as an agent, instead of a server. The K3S agent will register with the K3S server listening at the supplied URL. if you have not set a stable token the value to use for <code>K3S_TOKEN</code> is stored at <code>/var/lib/rancher/k3s/server/node-token</code> on your server node.</p> <p>Example</p> Leverage the KUBECONFIG environment variable:Or specify the location of the kubeconfig file in the command: <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n</code></pre> <pre><code>kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces\nhelm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces\n</code></pre>"},{"location":"devops/k8s-engine/k3s-installation/#uninstalling-k3s","title":"Uninstalling K3S","text":"<p>Example</p> Uninstalling Servers:Uninstalling Agents: <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre> <pre><code>/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre> <p>Make sure all required port numbers are open</p> <p>For More Details</p>"},{"location":"devops/k8s-engine/rke2-installation-ansible/","title":"RKE2 Cluster Installation With Ansible","text":"<p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#set-up-passwordless-ssh","title":"Set up passwordless SSH","text":"<ol> <li> <p>Generate ssh key</p> <pre><code>ssh-keygen\n</code></pre> <p>Simply press enter when prompted for default name and no password, this will create <code>~/.ssh/id_rsa</code></p> </li> <li> <p>Copy ssh keys to master and worker nodes</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa root@10.40.140.4\n</code></pre> <p>Enter the ssh password when prompted and repeat for all master and worker nodes.</p> </li> <li> <p>Add the ssh key to the ssh-agent</p> <pre><code>ssh-agent bash\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>If you get an error run the following and try again</p> <pre><code>eval `ssh-agent`\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#install-ansible-and-rke2-deployment-role","title":"Install Ansible and RKE2 deployment role","text":"<ol> <li> <p>Install ansible</p> <pre><code>sudo apt update\nsudo apt install ansible -y\n</code></pre> </li> <li> <p>Install the lablabs/rke2 ansible role (https://galaxy.ansible.com/lablabs/rke2)</p> <pre><code>ansible-galaxy install lablabs.rke2\n</code></pre> <p>This will install the role under the <code>~/.ansible</code> directory</p> </li> <li> <p>Navigate to the <code>~/.ansible</code> directory</p> <pre><code>cd ~/.ansible\n</code></pre> </li> <li> <p>Create inventory file in <code>~/.ansible</code> (can be copied from local with scp)</p> <pre><code>vi inventory\n</code></pre> <p>Go into insert mode by pressing i</p> <p>Copy the following and paste with ctrl+shift+v</p> <pre><code>[masters]\nmaster-01 ansible_host=10.40.140.4 rke2_type=server\nmaster-02 ansible_host=10.40.140.5 rke2_type=server\nmaster-03 ansible_host=10.40.140.6 rke2_type=server\n\n[workers]\nworker-01 ansible_host=10.40.140.7 rke2_type=agent\nworker-02 ansible_host=10.40.140.8 rke2_type=agent\nworker-03 ansible_host=10.40.140.9 rke2_type=agent\n\n[k8s_cluster:children]\nmasters\nworkers\n</code></pre> <p>Host names (ex: master-01) can be changed, however they must be lowercase.</p> <p>Save and quit by pressing Esc, then :wq!</p> </li> <li> <p>Create playbook.yaml file in <code>~/.ansible</code> (check previous step for help with vi)</p> <pre><code>- name: Deploy RKE2\n  hosts: all\n  become: yes\n  vars:\n    rke2_ha_mode: true\n    rke2_api_ip : 10.40.140.4\n    rke2_download_kubeconf: true\n    rke2_ha_mode_keepalived: false\n    rke2_server_node_taints:\n      - 'CriticalAddonsOnly=true:NoExecute'\n  roles:\n    - role: lablabs.rke2\n</code></pre> <p><code>rke2_api_ip</code> should point to your load balancer for master nodes, this should be a TCP load balancer on port 6443. If you don't have one it can point to one of the master nodes.</p> <p>Alternatively you can use keepalived by removing the <code>rke2_ha_mode_keepalived</code> line, and pointing the <code>rke2_api_ip</code> to an unused static IP such as <code>10.40.140.100</code></p> </li> <li> <p>Confirm inventory is working</p> <pre><code>ansible all -i inventory -m ping\n</code></pre> <p>There should be no errors.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#run-the-playbook-and-confirm-the-rke2-cluster-is-up","title":"Run the playbook and confirm the RKE2 cluster is up","text":"<ol> <li> <p>Run the ansible playbook</p> <pre><code>ansible-playbook -i inventory playbook.yaml\n</code></pre> <p>If you're ssh'ing to the other machines as a non-root user, run the following instead:</p> <pre><code>ansible-playbook -i inventory playbook.yaml -K\n</code></pre> <p>Provide the password for the user you're logging in as when prompted.</p> <p>If it hangs at 'Wait for remaining nodes to be ready', check if rke2 is installed on all machines, if not the <code>rke2.sh</code> script may not be running. To fix this, edit the <code>~/.ansible/roles/lablabs.rke2/tasks/rke2.yml</code> by removing the <code>Check RKE2 version</code> task and replacing it with:</p> <pre><code>- name: Check rke2 bin exists\n  ansible.builtin.stat:\n    path: \"{{ rke2_bin_path }}\"\n  register: rke2_exists\n\n- name: Check RKE2 version\n  ansible.builtin.shell: |\n    set -o pipefail\n    {{ rke2_bin_path }} --version | grep -E \"rke2 version\" | awk '{print $3}'\n  args:\n    executable: /bin/bash\n  changed_when: false\n  register: installed_rke2_version\n  when: rke2_exists.stat.exists\n</code></pre> </li> <li> <p>Install kubectl</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Copy kubeconfig file to a better directory and export kubeconfig</p> <pre><code>cp /tmp/rke2.yaml ~/rke2.yaml\n</code></pre> <p>Now we can manage our cluster with <code>kubectl --kubeconfig ~/rke2.yaml</code>, or we can do the following to shorten our commands:</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm our cluster is running and with correct internal IP addresses</p> <pre><code>kubectl get nodes -o wide\n</code></pre> </li> <li> <p>Check the health of our pods</p> <pre><code>kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#cleanup","title":"Cleanup","text":""},{"location":"devops/k8s-engine/rke2-installation-ansible/#jump","title":"Jump","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Kubectl and Ansible</p> <pre><code>sudo snap remove kubectl\nsudo apt remove ansible\n</code></pre> </li> <li> <p>Remove remaining artifacts</p> <pre><code>rm -rf /root/.ansible\nrm /root/.ssh/id_rsa /root/.ssh/id_rsa.pub\nrm ~/rke2.yaml\nrm -rf /root/.kube\napt autoremove -y\n</code></pre> <p>If you followed the installation guide as a non-root user the directories might be different.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#master-and-worker-nodes","title":"Master and Worker nodes","text":"<p>Repeat the following on all master and worker nodes</p> <ol> <li> <p>ssh into the node</p> <pre><code>ssh root@10.40.140.4\n</code></pre> </li> <li> <p>Run the RKE2 uninstall script</p> <pre><code>sudo /usr/local/bin/rke2-uninstall.sh\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation/","title":"RKE2 Setup","text":""},{"location":"devops/k8s-engine/rke2-installation/#rke2-requirements","title":"RKE2 Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY (we recommend at least 8GB) <code>2</code>  CPU (we recommend at least 4CPU) <code>60GB</code>      STORAGE <p> We turn off the firewall to avoid problems in the future. We update the packages and clean up any files left over from previous installations.</p> <p>Ubuntu:</p> <pre><code># Ubuntu instructions \n# stop the software firewall\nsystemctl disable --now ufw\n# get updates, install nfs, and apply\napt update\napt install nfs-common -y  \napt upgrade -y\n# clean up\napt autoremove -y\n</code></pre> <p>Now that we have all the nodes up to date, let's focus on <code>rancher1</code>. While this might seem controversial, <code>curl | bash</code> does work nicely. The install script will use the tarball install for Ubuntu and the RPM install for Rocky/Centos. Please be patient, the start command can take a minute. Here are the rke2 docs and install options for reference.</p>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-server-install","title":"RKE2 Server Install","text":"<pre><code>#rancher1\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=server sh -\n\nmkdir -p /etc/rancher/rke2/\ncat &lt;&lt; EOF &gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\nEOF\n\n# enable and start\nsystemctl enable --now rke2-server.service\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status rke2-server</code> and make sure it is <code>active</code>.</p> <p>Quote</p> <p>If you want to install a specific version use the following command ;</p> <pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.24 INSTALL_RKE2_TYPE=server sh -\n</code></pre> <p>Perfect!  Now we can start talking Kubernetes. We need to symlink the <code>kubectl</code> cli on <code>rancher1</code> that gets installed from RKE2.</p> <pre><code># add kubectl conf\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# check node status\nkubectl get node\n</code></pre> <p>Quote</p> <p>In addition these commands can be used for KUBECONFIG</p> <pre><code># simlink all the things - kubectl\nln -s $(find /var/lib/rancher/rke2/data/ -name kubectl) /usr/local/bin/kubectl\n\n# add kubectl conf\nexport KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n</code></pre> <p>We will also need to get the token from <code>rancher1</code>.</p> <pre><code># save this for rancher2 and rancher3\ncat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-agent-install","title":"RKE2 Agent Install","text":"<p>The agent install is VERY similar to the server install. Except that we need an agent config file before starting. We will start with <code>rancher2</code>. We need to install the agent and setup the configuration file.</p> <pre><code># we add INSTALL_RKE2_TYPE=agent\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=agent sh -\n\n# create config file\nmkdir -p /etc/rancher/rke2/ \n\n# change the ip to reflect your rancher1 ip\necho \"server: https://$RANCHER1_IP:9345\" &gt; /etc/rancher/rke2/config.yaml\n\n# change the Token to the one from rancher1 /var/lib/rancher/rke2/server/node-token \necho \"token: $TOKEN\" &gt;&gt; /etc/rancher/rke2/config.yaml\n\n# enable and start\nsystemctl enable --now rke2-agent.service\n</code></pre> <p>If you want to add your Node as control plane, etcd replace here with below code</p> <p>Example</p> Orjinal CodeChange Code <pre><code>systemctl enable --now rke2-agent.service\n</code></pre> <pre><code>systemctl enable --now rke2-server.service\n</code></pre> <p>Rinse and repeat. Run the same install commands on <code>rancher2</code>, <code>rancher3</code>. Next we can validate all the nodes are playing nice by running kubectl get node -o wide on rancher1. </p>"},{"location":"devops/k8s-engine/rke2-installation/#run-this-code","title":"Run this code !","text":"'To check that Kubernetes is running'<pre><code>kubectl get node\n</code></pre> <p>Important Installations Notes</p> <p>If you are having problems with installations, make sure there are no problems with instances' accessing each other <code>(For Example --Ssh connection--: Permission Denied)</code></p> <p>Check below steps if RKE2-Server or RKE2-Agent is not working </p> <ul> <li> Rancher1 instances Node Token is correct ? </li> <li> Instances IP address is Correct ? </li> <li> Instance Ports is open (9345, 6443) ?</li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/","title":"Restoring RKE2 Clusters","text":"<p>During this process, the cluster will be unavailable. </p> <ul> <li>ETCD should be restored on the first master node that was created in the cluster.</li> <li>ETCD restoration process starts by disabling all rke2 services on all nodes.</li> <li>Don't forget to remove the <code>etcd database directory</code> on secondary master nodes before starting the <code>rke2-server</code>.</li> <li>ETCD restoration process starts with limitin etcd to single node. </li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-existing-nodes","title":"Restoring RKE2 Cluster ETCD on Existing Nodes","text":"<p>This process will restore the ETCD on the existing nodes. </p> <p>If you're adding new nodes to the cluster, you should follow the Restoring RKE2 Cluster ETCD on New Nodes section.</p> <p>1) On ALL nodes, disable the <code>rke2-server</code> service: <pre><code>systemctl disable rke2-server -now\n</code></pre></p> <p>2) On ALL nodes, kill all remaining processes: <pre><code>rke2-killall.sh\n</code></pre> 3 On FIRST NODE, restore etcd: <pre><code>rke2 server --cluster-reset \\\n    --cluster-reset-restore-path=$PATH_TO_SNAPSHOT_FILE\n</code></pre> 4) On FIRST NODE, start the rke2: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p> <p>5) On SECONDARY MASTER NODES, delete the etcd data directory: <pre><code># delete the old etcd data directory\n# make sure the directory is correct for your installation\nrm -rf /var/lib/rancher/rke2/server/db\n</code></pre> 6) On SECONDARY MASTER NODES, start the rke2 server: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-new-nodes","title":"Restoring RKE2 Cluster ETCD on New Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/","title":"Deploy RKE2 Highly Available Cluster","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prerequisites","title":"Prerequisites","text":"a b c 3 Linux Hosts RKE2 System Requirements"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prepare-the-hosts","title":"Prepare the hosts","text":"<p>[ATTENTION]: Do this on all of the master nodes</p> <p>1) Become root</p> <pre><code>sudo su\n</code></pre> <p>2) Install the OS dependencies</p> <pre><code>apt update -y &amp;&amp; apt upgrade -y\n\nsystemctl disable --now ufw\napt install nfs-common jq libselinux1 curl apparmor-profiles -y\napt autoremove -y\n\napparmor_status # check if apparmor is running\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-nodes","title":"MASTER NODES","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-1","title":"MASTER NODE 1","text":"<p>Notes</p> <p>[ATTENTION] Run this steps on the first master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation.</p> <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> <p>2) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\n\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n#etcd-s3: true\n#etcd-s3-endpoint: \"s3.amazonaws.com\"\n#etcd-s3-access-key: \"AKIAAAAAAAAAAAAAAAA\"\n#etcd-s3-secret-key: \"BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\"\n#etcd-s3-region: \"eu-central-1\"\n#etcd-s3-bucket: \"---my-s3-bucket-where-i-store-etcd-backups\"\n#etcd-s3-folder: \"---myclustername-etcd-backups-folder/\"\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>3) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>4) Start the <code>rke2-server</code></p> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> <p>5) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre> <p>6) [ATTENTION] Copy <code>node-token</code> to a safe place for joining other master nodes later on</p> <pre><code>cat /var/lib/rancher/rke2/server/node-token\n# output should look like this:\n# K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-2-3","title":"MASTER NODE 2 &amp; 3","text":"<p>Prerequisites</p> Required Name Description \u2705 <code>MASTER_IP</code> Static endpoint for the first master node \u2705 <code>MASTER_NODE_TOKEN</code> Should get this from first master node installation \u2705 <code>INSTALL_RKE2_VERSION</code> The RKE2 release version that you want to upgrade to <p>Notes</p> <p>[ATTENTION] Run this steps on the second and third master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Prerequisites. <pre><code>export MASTER_IP=\"172.31.15.113\" # TODO: change this to your master-1 IP\nexport MASTER_NODE_TOKEN=\"K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\"\n</code></pre> 2) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation. <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> 3) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\n# IMPORTANT\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nserver: https://$MASTER_IP:9345\ntoken: $MASTER_NODE_TOKEN\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>4) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>5) Start the <code>rke2-server</code> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> 6) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#adding-worker-nodes","title":"Adding Worker Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/","title":"System Upgrade Controller","text":"<p>rancher/system-upgrade-controller</p> <p>RKE2 Docs: Automatic Upgrades</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Highly Available RKE2 Cluster</li> <li>Install the <code>system-upgrade-controller</code> on the cluster</li> </ul> <p>Visit for the latest installation instructions: RKE2 Docs: Install the system-upgrade-controller</p> <p>For now, the command is: (the version might change, better to follow the docs) <pre><code>kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#upgrading-master-nodes-1-by-1","title":"Upgrading Master Nodes 1-by-1","text":"<p>1) [ATTENTION] Decide on which version of RKE2 to upgrade to. <pre><code># Kubernetes Label: remove '+' and replace with '-' or it doesnt work\n\n# TODO: SET THIS TO A RELEASE NUMBER or `latest` or `stable` works as well\nexport RKE2_UPGRADE_VERSION=\"v1.21.14+rke2r1\" \nexport RKE2_UPGRADE_VERSION_SAFE_STRING=$(echo -n $RKE2_UPGRADE_VERSION | tr '+' '-')\n# \"v1.21.14+rke2r1\" --becomes--&gt; \"v1.21.14-rke2r1\"\n</code></pre></p> <p>2) [ATTENTION] Create Master Node Upgrade Plan yaml file.</p> <p>This command will create a file named <code>rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml</code></p> <p>This plan can only be applied to master nodes that have the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p>You can also add the label <code>rke2-upgrade: disabled</code> to a master node to prevent it from being upgraded. <pre><code># create the upgrade-plan for it \ncat &lt;&lt; EOF &gt; rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: server-plan-for-$RKE2_UPGRADE_VERSION_SAFE_STRING\n  namespace: system-upgrade\n  labels:\n    rke2-upgrade: server\nspec:\n  concurrency: 1\n  nodeSelector:\n    matchExpressions:\n      - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]}\n      - {key: rke2-upgrade, operator: Exists}\n      - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]}\n      - {key: rke2-upgrade-to, operator: In, values: [\"$RKE2_UPGRADE_VERSION_SAFE_STRING\"]}\n  serviceAccountName: system-upgrade\n  cordon: true\n  drain:\n    force: true\n  upgrade:\n    image: rancher/rke2-upgrade\n  version: \"$RKE2_UPGRADE_VERSION\"\nEOF\n</code></pre></p> <p>3) Apply the <code>Plan</code> to the cluster</p> <p>This will not upgrade the nodes. That'll happen when we label the nodes correctly.</p> <p><code>Plan</code> will create <code>Job</code>s for each master node that has the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p><pre><code># apply the plan and check it\nkubectl apply -f rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL.yaml\n\n# check the plan and jobs\nkubectl -n system-upgrade get plans,jobs\n</code></pre> 4) [OPTIONAL] Disable a node from upgrading <pre><code>kubectl label nodes &lt;node-name&gt; rke2-upgrade=disabled --overwrite\n</code></pre></p> <p>5) [ATTENTION] Labeling &amp; Upgrading the master nodes 1-by-1</p> <p>This step should be done for each master node one after other.</p> <p>Labeling the master nodes will trigger an automatic upgrade.</p> <p>Do this step for all master nodes one after other.</p> <pre><code>echo \"RKE2_UPGRADE_VERSION=$RKE2_UPGRADE_VERSION\"\necho \"RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL=$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL\"\n\n# label the a master node to upgrade\nkubectl label nodes &lt;node-name&gt; rke2-upgrade=enabled --overwrite\nkubectl label nodes &lt;node-name&gt; rke2-upgrade-to=$RKE2_UPGRADE_VERSION_SAFE_STRING --overwrite\n\n# check if a job is created for the upgrade of the node\nkubectl -n system-upgrade get jobs,plans\n\n# watch as the node is upgraded\nkubectl get nodes --watch\n</code></pre>"},{"location":"devops/k8s-storage/longhorn/","title":"Longhorn Block Storage","text":"<p>Hello everybody! \ud83e\udee1</p> <p>Today I'm going to tell you about Longhorn Block Storage. First of all, what is Longhorn? What is it for? Why is it used? Let's find out... \ud83d\udcaa</p> <p>Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes.</p> <p>Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation.</p> <p>With Longhorn, you can:</p> <ul> <li>Use Longhorn volumes as persistent storage for the distributed stateful applications in your Kubernetes cluster</li> <li>Partition your block storage into Longhorn volumes so that you can use Kubernetes volumes with or without a cloud provider</li> <li>Replicate block storage across multiple nodes and data centers to increase availability</li> <li>Store backup data in external storage such as NFS or AWS S3</li> <li>Create cross-cluster disaster recovery volumes so that data from a primary Kubernetes cluster can be quickly recovered from backup in a second Kubernetes cluster</li> <li>Schedule recurring snapshots of a volume, and schedule recurring backups to NFS or S3-compatible secondary storage</li> <li>Restore volumes from backup</li> <li>Upgrade Longhorn without disrupting persistent volumes</li> </ul> <p>So what have we learned about Longhorn so far? Now let's see how we install K8s on it.</p> <p>\ud83d\udccc First of all, we need to set up the <code>helm</code> in our cluster.</p> <p>Okay, let's go on now.</p> <p>Now let's add <code>Longhorn</code> to the helm repository and update our repom. <pre><code>helm repo add longhorn https://charts.longhorn.io \nHelm repo update\n</code></pre></p> <p>If we get here smoothly, we can go on.</p> <p>Next, we have to set up the Longhorn. Let's create a new namespace and install Longhorn. \ud83d\udc47</p> <pre><code>kubectl create namespace longhorn-system helm install longhorn longhorn/longhorn --namespace longhorn-system\n</code></pre> <p>Installation finished \u2705</p> <p>How easy it was, wasn't it? It would be nice to see Longhorn as the UI at once,is it? Let's do that.</p> <pre><code>kubectl get service -n longhorn-system\n</code></pre> <p>Let's list <code>longhorn-frontend</code> with the command above. And with the command below, let's get this service <code>port-forward</code>.</p> <pre><code>kubectl port-forward -n longhorn-system service/longhorn-frontend 8080:80\n</code></pre> <p>Now visit <code>localhost:8080</code> and enjoy <code>longhorn</code>.</p> <p>We need to test the last <code>Longhorn</code> that we set up, right?</p> <pre><code>helm install my-release oci://registry-1.docker.io/bitnamicharts/mysql\n</code></pre> <p>Let's use the following command to display bitnamicharts/mysql <code>PersistentVolume, PersistentVolumeClaim</code> and the <code>Longhorn StorageClass</code> that we have installed.</p> <pre><code>kubectl get pv,pvc -n default\nkubectl get storageclass -n default\n</code></pre> <p>In fact, the <code>Longhorn</code> installation is so simple. See you on the next blog. \ud83c\udf88</p>"},{"location":"devops/k8s-storage/nfs-install/","title":"NFS Setup Requirements","text":"<ul> <li>1 Master Node</li> <li>1 Worker Node (NFS)</li> </ul> <p>First of all, we save the ip address of our NFS server in the <code>/etc/hosts</code> file on all our nodes.(<code>master - nfs</code>)</p> <pre><code>/etc/hosts\n\n172.31.18.194 master\n172.31.20.138 k8s-ankara-nfs01\n</code></pre> <p>Then we install the necessary applications for our NFS structure.(<code>nfs</code>)</p> <pre><code>sudo apt-get update\nsudo apt-get install -y nfs-kernel-server\n</code></pre> <p>Let's create a directory on our server to store files.(<code>nfs</code>) <pre><code>sudo mkdir /k8s-data &amp;&amp; sudo mkdir /k8s-data/ankara-data\nsudo chmod 1777 /k8s-data/ankara-data\ntouch /k8s-data/ankara-data/ankara-cluster.txt\n</code></pre> We need to edit the NFS server file for the directory we just created. At this stage, we will share the directory with all our nodes.(<code>nfs</code>) <pre><code>sudo nano /etc/exports\n</code></pre></p> <p>After opening, add the following values at the end.(<code>nfs</code>) <pre><code># /etc/exports: the access control list for filesystems which may be exported\n#               to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n\n/k8s-data/ankara-data *(rw,sync,no_root_squash,subtree_check)\n</code></pre></p> <p>Then run the following command to re-read exportfs and confirm the changes.(<code>nfs</code>) <pre><code>sudo exportfs -ra\n</code></pre></p> <p>Now we will do the following operations on all our Kubernetes nodes.(<code>master - nfs</code>) <pre><code>sudo apt-get -y install nfs-common\n</code></pre></p> <p>After the installation is finished, we check whether there is an access problem by running the following command on all our nodes.(<code>master</code>) <pre><code>showmount -e k8s-ankara-nfs01\n</code></pre></p> <p>There does not seem to be any problem. Now we can mount the folder we opened on our NFS server to our nodes.(<code>master</code>) <pre><code>sudo mount k8s-ankara-nfs01:/k8s-data/ankara-data /mnt\nls -l /mnt\n</code></pre></p> <p>We have made all the setup and adjustments for NFS. We can store the data of our pods without any problems.</p> <p>To make our NFS available to pods, the sample Persistent Volume, Persistent Volume Claim and Pod yaml file should be as follows. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ankara-cluster-test-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /k8s-data/ankara-data\n    server: k8s-ankara-nfs01\n    readOnly: false\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ankara-pv-claim\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: data-generator-pod\nspec:\n  containers:\n  - name: data-generator-container\n    image: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      while true; do\n        echo \"$(date) - Data content: $(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 10)\" &gt; /data/data-$(date +%s).txt\n        sleep 1\n      done\n    volumeMounts:\n    - name: data-volume\n      mountPath: /data\n  volumes:\n  - name: data-volume\n    persistentVolumeClaim:\n      claimName: ankara-pv-claim\n</code></pre></p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/","title":"Using an NFS Server as a StorageClass Backend in Amazon EKS","text":"<p>Modern Kubernetes clusters running on Amazon EKS often require shared, ReadWriteMany (RWX)\u2013capable storage for applications such as logging, CI/CD systems, caching layers, or shared configuration. While AWS provides multiple storage options, using an existing on-prem or cloud-hosted NFS server remains a simple and powerful solution when RWX support is required with minimum operational overhead.</p> <p>In this guide, we walk through how to use an existing NFS server as the storage backend for dynamic provisioning in EKS by installing the NFS Subdir External Provisioner via Helm. This documentation assumes that:</p> <ul> <li>You already have an accessible NFS server.</li> <li>One of its exported directories (e.g., <code>/pvcdata</code>) is mounted on a secondary disk.</li> <li>Your EKS worker nodes can reach the NFS server over the network.</li> </ul> <p>We will therefore focus on validations, configuration, Helm installation, and real-world test scenarios rather than provisioning the underlying NFS server from scratch.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#prerequisites-key-considerations","title":"Prerequisites &amp; Key Considerations","text":"<p>Before configuring the StorageClass, ensure:</p> <ul> <li>You have a functional Amazon EKS cluster.</li> <li>Worker nodes have network reachability to the NFS server (routing, firewalls, SGs, NACLs, VPN, or Direct Connect).</li> <li>NFS server is exporting a shared directory (e.g., <code>/pvcdata</code>).</li> <li>Correct NFS permissions, export rules, and versions are configured.</li> <li>Worker nodes have nfs-utils installed.</li> </ul>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#step-by-step-configuration","title":"Step-by-Step Configuration","text":""},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#ensure-that-nfs-server-is-running","title":"Ensure that NFS Server is Running","text":"<p>Verify that the server is active and that port 2049 (NFS service port) is listening:</p> <pre><code>systemctl status nfs-server\nsudo netstat -tulpn | grep 2049\n</code></pre> <p>If the service is down, start and enable it:</p> <pre><code>sudo systemctl enable --now nfs-server\n</code></pre>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#validate-nfs-version-configuration","title":"Validate NFS Version Configuration","text":"<p>Check the supported NFS protocol versions:</p> <p><pre><code>cat /etc/nfs.conf | grep vers\n</code></pre> For EKS workloads, NFSv4+ is preferred. Expected configuration:</p> <pre><code>  # vers4=y\n  # vers4.0=y\n  # vers4.1=y\n  # vers4.2=y\n</code></pre>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#verify-available-disks-and-mounted-paths","title":"Verify Available Disks and Mounted Paths","text":"<p>Confirm the mounted directory that will be used to store Kubernetes PV data:</p> <p><pre><code>lsblk -f\n</code></pre> For this documentation, we assume the exported path is <code>/pvcdata</code></p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#validate-nfs-exports","title":"Validate NFS Exports","text":"<p>Check the export list:</p> <p><pre><code>showmount -e &lt;NFS-SERVER-IP&gt;\n</code></pre> You should see:</p> <pre><code>Export list for &lt;NFS-SERVER-IP&gt;:\n/pvcdata 172.28.120.0/24\n</code></pre> <p>Instead of defining each worker node\u2019s IP address individually, it is more efficient to authorize the entire worker node subnet (CIDR range) in the export configuration.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#review-export-settings-for-subdirectory-creation","title":"Review Export Settings for Subdirectory Creation","text":"<p>Ensure your export flags allow dynamic folder creation by the provisioner:</p> <pre><code>cat /etc/exports\n</code></pre> <p>The entry should look like:</p> <pre><code>/pvcdata 172.28.120.0/24 (rw,sync,no_subtree_check,no_root_squash)\n</code></pre> <p>If any required export options are missing (such as <code>no_root_squash</code>), add them to your <code>/etc/exports</code> file and reload the NFS export configuration:</p> <pre><code>sudo exportfs -rav\n</code></pre>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#check-directory-ownership-permissions","title":"Check Directory Ownership &amp; Permissions","text":"<p>To ensure the NFS subdir-external-provisioner can write PVC folders correctly, verify the parent directory permissions:</p> <pre><code>ls -ld /pvcdata\n</code></pre> <p>You should see <code>drwxrwxrwx</code> or <code>drwxr-xr-x</code>. If the permissions are too restrictive, you can adjust them using:</p> <pre><code>sudo chmod 775 /pvcdata\nsudo chown -R nobody:nogroup /pvcdata\n</code></pre> <p>In certain environments, especially when multiple clients or services need unrestricted write access, you may temporarily require 777 permissions, but use this only when necessary as it grants full access to everyone.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#install-nfs-utilities-on-worker-nodes","title":"Install NFS Utilities on Worker Nodes","text":"<p>Worker nodes must have NFS client dependencies installed:</p> <pre><code>rpm -qa | grep nfs-utils\nsudo yum install -y nfs-utils\n</code></pre> <p>We assume the operating system is RHEL; if you're using a different distribution, adjust the package manager commands accordingly.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#install-nfs-subdir-external-provisioner-on-eks","title":"Install NFS Subdir External Provisioner on EKS","text":"<p>We\u2019ll now continue by installing the <code>nfs-subdir-external-provisioner</code> in our cluster using Helm.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#add-the-helm-repository","title":"Add the Helm Repository","text":"<pre><code>helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm repo update\n</code></pre>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#install-the-provisioner","title":"Install the provisioner","text":"<p>Create a custom <code>nfs-values.yaml</code> file to define your NFS configuration, StorageClass behavior, and deployment strategy.</p> <p>Below is an example configuration you can adapt:</p> <pre><code>replicaCount: 1\nstrategyType: Recreate\n\nnfs:\n  server: &lt;NFS-SERVER-IP&gt;\n  path: /pvcdata\n\nstorageClass:\n  create: true\n  defaultClass: true\n  name: nfs-client\n  reclaimPolicy: Retain\n  provisionerName: cluster.local/nfs-subdir-external-provisioner\n  allowVolumeExpansion: true\n  accessModes: ReadWriteMany\n  onDelete: retain\n</code></pre> <p>Important fields explained:</p> <ul> <li><code>nfs.server</code> \u2013 The IP address of your NFS server.</li> <li><code>nfs.path</code> \u2013 Base directory where PVC subdirectories will be created.</li> <li><code>storageClass.name</code> \u2013 Name used when binding PVCs (storageClassName: nfs-client).</li> <li><code>reclaimPolicy: Retain</code> \u2013 Keeps data even if the PVC is deleted.</li> <li><code>accessModes: ReadWriteMany</code> \u2013 Enables multi-node read/write access, the main reason to use NFS.</li> </ul> <p>Once your <code>nfs-values.yaml</code> file is ready, install the provisioner using Helm:</p> <p><pre><code>helm upgrade -i nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  -n nfs-storage \\\n  -f nfs-values.yaml \\\n  --create-namespace\n</code></pre> This will deploy the provisioner, create the StorageClass, and enable dynamic PVC provisioning backed by your NFS server.</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#validate-the-deployment","title":"Validate the Deployment","text":"<pre><code>kubectl get pods -n nfs-storage\nkubectl get storageclass\n</code></pre> <p>You should now see:   - Pods in <code>Running</code> state   - A StorageClass named <code>nfs-client</code> (as default)</p>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#testing-nfs-in-the-cluster","title":"Testing NFS in the Cluster","text":"<p>After installing the NFS Subdir External Provisioner, you can validate that dynamic provisioning and RWX access work correctly by deploying a simple test workload.</p> <p>This test will:</p> <ul> <li>Create a PVC using the nfs-client StorageClass</li> <li>Mount it on every node via a DaemonSet</li> <li>Write periodic timestamps into node-specific subdirectories</li> <li>Confirm that all nodes can read/write to the same NFS share</li> </ul>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#create-the-test-workload","title":"Create the Test Workload","text":"<p>Apply the following manifest:</p> <p><pre><code># test-nfs.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-test\n  namespace: nfs-storage\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: nfs-client\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nfs-test-ds\n  namespace: nfs-storage\nspec:\n  selector:\n    matchLabels:\n      app: nfs-test\n  template:\n    metadata:\n      labels:\n        app: nfs-test\n    spec:\n      containers:\n      - name: tester\n        image: registry.k8s.io/nginx:latest\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        command:\n          [\n            \"sh\",\n            \"-c\",\n            \"mkdir -p /data/${NODE_NAME}; \\\n             while true; do echo $(date) &gt;&gt; /data/${NODE_NAME}/test.txt; sleep 60; done\"\n          ]\n        volumeMounts:\n        - name: nfs-data\n          mountPath: /data\n      volumes:\n      - name: nfs-data\n        persistentVolumeClaim:\n          claimName: nfs-test\n</code></pre> Important fields explained:</p> <ul> <li><code>persistentVolumeClaim</code> \u2192 storageClassName -- Ensures the PVC uses the NFS StorageClass (nfs-client).</li> <li><code>DaemonSet</code> -- Creates one pod per node so you can test multi-node NFS RWX behavior.</li> <li><code>env.NODE_NAME</code> -- Injects the node name into each pod to create node-specific directories.</li> <li><code>command</code> -- Creates a folder per node and writes timestamps every 60 seconds.</li> <li><code>volumeMounts / volumes</code> -- Mounts the PVC at /data inside each tester pod.</li> </ul> <p>Then apply it:</p> <pre><code>kubectl apply -f test-nfs.yaml\nkubectl get pvc -n nfs-storage\nkubectl get pods -n nfs-storage\n</code></pre>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#check-logs-on-any-pod","title":"Check Logs on Any Pod","text":"<p>Pick any pod and check the logs:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -n nfs-storage -- tail -f /data/&lt;NODE-IP&gt;test.txt\n</code></pre> <p>Each node should write its own timestamps into its own subdirectory on the NFS server. The structure created on NFS should look like:</p> <pre><code>    /data/node1-name/test.txt\n    /data/node2-name/test.txt\n    /data/node3-name/test.txt\n    ...\n</code></pre> <p>and the content:</p> <p><pre><code>    Mon Dec 1 12:54:17 UTC 2025\n    Mon Dec 1 12:55:17 UTC 2025\n    Mon Dec 1 12:56:17 UTC 2025\n    ...\n</code></pre> Each folder contains a test.txt file with timestamps written by the corresponding node.</p> <p>This verifies that:</p> <ul> <li>The PVC was dynamically provisioned</li> <li>The DaemonSet successfully mounted NFS</li> <li>All nodes can write simultaneously via RWX</li> <li>The provisioner creates correct subdirectory structures</li> </ul>"},{"location":"devops/k8s-storage/nfs-storageclass-k8s/#validate-on-the-nfs-server","title":"Validate on the NFS Server","text":"<p>From the NFS host:</p> <pre><code>ls /pvcdata\ncat /pvcdata/nfs-storage-nfs-test-pvc-*/test.txt\n</code></pre> <p>Example output:</p> <pre><code>[root@NFS-SERVER ~]# ls /pvcdata\nnfs-storage-nfs-test-pvc-34f97028-6953-46ee-b6f4-3129f06023c9\nnfs-storage-nfs-test-pvc-98e5a1f7-c1c3-4172-a326-d0e248a715f0\nnfs-storage-nfs-test-pvc-a4b380b8-30b2-470e-8349-138d00b21c84\n...\n[root@NFS-SERVER ~]# cat /pvcdata/nfs-storage-nfs-test-pvc-*/test.txt\nMon Dec 1 12:41:25 UTC 2025\nMon Dec 1 12:41:25 UTC 2025\nMon Dec 1 12:41:30 UTC 2025\nMon Dec 1 12:42:25 UTC 2025\nMon Dec 1 12:42:25 UTC 2025\nMon Dec 1 12:42:30 UTC 2025\nMon Dec 1 12:43:25 UTC 2025\nMon Dec 1 12:43:25 UTC 2025\nMon Dec 1 12:43:30 UTC 2025\n...\n</code></pre> <p>This confirms:</p> <ul> <li>Subdirectories are created dynamically</li> <li>Data is written by each node</li> <li>NFS StorageClass is operational cluster-wide</li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/","title":"Ceph Installation Guide","text":"<p>Ceph is a highly scalable, open-source storage solution designed to provide unified storage for block, file, and object data. With its distributed architecture and strong data reliability features, Ceph eliminates single points of failure, making it a preferred choice for large-scale and dynamic storage environments.</p> <p></p> <p>This guide explains two approaches to setting up Ceph for Kubernetes:</p> <p>1- In-cluster Ceph using Rook: Integrating Ceph directly into a Kubernetes cluster by deploying Rook, a Kubernetes-native operator for managing Ceph. 2- External Ceph Cluster Setup: Installing Ceph on three Ubuntu nodes outside the Kubernetes cluster and integrating it as external storage.</p>"},{"location":"devops/k8s-storage/rook-ceph/#create-internal-ceph-cluster","title":"Create Internal Ceph Cluster","text":"<p>There are multiple ways to install Ceph. Rook deploys and manages Ceph clusters running in Kubernetes, while also enabling management of storage resources and provisioning via Kubernetes APIs. We recommend Rook as the way to run Ceph in Kubernetes or to connect an existing Ceph storage cluster to Kubernetes.</p> <p>Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for Ceph storage to natively integrate with cloud-native environments.</p>"},{"location":"devops/k8s-storage/rook-ceph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster with at least 3 worker nodes.(Kubernetes versions v1.27 through v1.32 are supported.)</li> <li>Helm installed on your local system.</li> <li>Ceph OSDs have a dependency on LVM in some scenario <pre><code># for ubuntu\nsudo apt-get install -y lvm2\n</code></pre></li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#steps","title":"Steps","text":"<p>A simple Rook cluster is created for Kubernetes with the following kubectl commands and example manifests.</p>"},{"location":"devops/k8s-storage/rook-ceph/#-deploy-the-rook-operator","title":"- Deploy the Rook Operator","text":"<p><pre><code>git clone --single-branch --branch v1.15.5 https://github.com/rook/rook.git\ncd rook/deploy/examples\n# create crds, common objects (ns, clusterroles..) and rook operator\nkubectl create -f crds.yaml -f common.yaml -f operator.yaml\n</code></pre> - verify the rook-ceph-operator is in the <code>Running</code> state before proceeding</p> <pre><code>kubectl config set-context --current --namespace rook-ceph\nkubectl -n rook-ceph get pod\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#-cluster-environments","title":"- Cluster Environments","text":"<p>The Rook documentation is focused around starting Rook in a variety of environments</p> <ul> <li>cluster.yaml: Cluster settings for a production cluster running on bare metal. Requires at least three worker nodes.</li> <li>cluster-on-pvc.yaml: Cluster settings for a production cluster running in a dynamic cloud environment.</li> <li>cluster-test.yaml: Cluster settings for a test environment such as minikube.</li> </ul> <p>We want to use production cluster with 3 worker nodes</p> <pre><code>kubectl create -f cluster.yaml\n\n#Verify the cluster is running by viewing the pods in the rook-ceph namespace.\nkubectl -n rook-ceph get pod\n\n# See how the cephcluster created successfully and HEALTH_OK\nkubectl get cephcluster -n rook-ceph\n</code></pre> <ul> <li>To verify that the cluster is in a healthy state, connect to the Rook toolbox and run the ceph status command.</li> </ul> <p><pre><code>kubectl create -f toolbox.yaml\n\n#Connect to the Rook toolbox\nkubectl get pod -n rook-ceph -l \"app=rook-ceph-tools\"\n\nkubectl exec -it rook-ceph-tools-7b67b65bd-kqjb6 -n rook-ceph -- bash\nceph status\nceph osd status\nceph osd ls\nceph osd crush rule dump\nceph df\nrados df\nceph mon stat    \n</code></pre> </p> <pre><code>ceph status: Overall health and operation of the cluster.\nceph osd status: Detailed status of the OSDs.\nceph df: High-level view of storage capacity.\nrados df: Object-level capacity and usage details.\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#-alternative-installment-of-rook","title":"- Alternative installment of rook","text":"<ul> <li> <p>The Rook Helm Chart is available to deploy the operator instead of example manifests</p> </li> <li> <p>Install Rook Helm Chart</p> </li> <li>Add the Rook Helm chart repository and install the operator:</li> </ul> <pre><code>helm repo add rook-release https://charts.rook.io/release  \nhelm repo update  \nhelm install rook-ceph rook-release/rook-ceph --namespace rook-ceph --create-namespace  \n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#login-ceph-dashoar-ui","title":"Login ceph-dashoar ui","text":"<p>https://rook.io/docs/rook/v1.16/Storage-Configuration/Monitoring/ceph-dashboard/?h=dashb</p> <ul> <li>Enable the Ceph Dashboard</li> </ul> <p><pre><code>[...]\nspec:\n  dashboard:\n    enabled: true\n</code></pre> - Viewing the Dashboard External to the Cluster (Ingress Controller) If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things:</p> <ul> <li>Exposes the dashboard on the Internet (using a reverse proxy)</li> <li>Issues a valid TLS Certificate for the specified domain name (using ACME)</li> <li>Tells the reverse proxy that the dashboard itself uses HTTPS</li> <li> <p>Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed)</p> </li> <li> <p>reach Ceph-dashboard over ssl</p> </li> <li> <p>install cert-managet to cluster</p> </li> </ul> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml\n\n#check cert pods is running\nkubectl get po --namespace cert-manager\n</code></pre> <ul> <li>create ClusterIssuer as issuer.yaml</li> </ul> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: test@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-prod-private-key\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <pre><code>kubectl apply -f issuer.yaml\nkubectl get ClusterIssuer\n\n**True**\n</code></pre> <ul> <li>then create ingress as ingress.yaml</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: \"nginx\"\n  tls:\n   - hosts:\n     - ceph.xxx.com\n     secretName: ceph.xxx.com\n  rules:\n  - host: ceph.xxx.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: rook-ceph-mgr-dashboard\n            port:\n              number: 8443\n</code></pre> <pre><code>kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre> <ul> <li> <p>create A record for ceph.xxx.com </p> </li> <li> <p>Login to https://ceph.xxx.com</p> </li> </ul> <pre><code>kubectl get secret\n\n* rook-ceph-dashboard-password\n</code></pre> <ul> <li>Get Credentials <pre><code>kubectl get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo\n</code></pre></li> <li>Overview of the status of Ceph cluster</li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-rbd-storage","title":"Create/build RBD storage","text":"<p>cd ~/rook/deploy/examples/csi/rbd</p> <pre><code>kubectl create -f storageclass.yaml \n## this also create replicapool (CephBlockPool) with 3 replicas\n\n# Verify created new storageclass for rbd\nkubectl get sc\n\n###\nNAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   xx\n###\n</code></pre> <ul> <li>Use this volume type in deployment. run wordpress app</li> </ul> <pre><code>cd ~/rook/deploy/examples/\nkubectl create -f mysql.yaml\nkubectl create -f wordpress.yaml\n\n#Verify the pod is up and running\nkubectl get pod\n\n#Verify the pvc and see bound \nkubectl get pvc,pv\n</code></pre> <ul> <li>Verify persistent volume</li> </ul> <p><pre><code>nodeport to service\n\nconnect to website\nhttp://ip:port\n\npublish new page\n\nkubectl rollout restart deploy wordpress\n</code></pre> <pre><code>kubectl exec -it &lt;wordpress-pod-name&gt; -- /bin/bash\n\ncd /var/www/html\n\necho \"hello\" &gt; test.txt\nls\n\nkubectl get pvc wp-pv-claim -o jsonpath='{.spec.volumeName}'\nor \nkubectl get pv\n#get pv name\n\nkubectl describe pv &lt;wordpress-pv-name&gt;\n#get volumename\n\n\nfindmnt | grep rbd\n#get mount path\n\ncd &lt;mount-path&gt;\nls\n\n#Verify exist test.txt\ncat test.txt\n</code></pre></p> <ul> <li>test exist persistent volume </li> </ul> <p><pre><code>kubectl get po \n\nkubectl delete &lt;pod-name&gt;\n#get new pod name\nkubectl get po \nkubectl exec -it &lt;pod-name&gt; -- cat /var/www/html/test.txt\n\n**hello**\n</code></pre> clean the sc <pre><code>kubectl delete -f mysql.yaml\nkubectl delete -f wordpress.yaml\n\n\ncd ~/rook/deploy/examples/csi/rbd\n\nkubectl delete -f storageclass.yaml\nk get sc\n ceph ui pools\n</code></pre></p>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-cephfs-storage","title":"Create/build CEPHFS storage","text":"<p>cd ~/rook/deploy/examples</p> <pre><code>kubectl create -f filesystem.yaml \n## this also create replicapool (ceph fs) with 3 replicas\n\n# crete cephfs storageclass\ncd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f storageclass.yaml\n\n# Verify created new storageclass for cephfile system\nkubectl get sc\n\n###\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   xx\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   xx\n###\n</code></pre> <ul> <li>Use this volume type in kube-registry deployment, replica:3 in kube-system namespace</li> </ul> <pre><code>cd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f kube-registry.yaml\n\n# pod mount path : /var/lib/registry\n\n#create new file on mounth path\nkubectl exec -it &lt;pod-replica-1&gt; -n &lt;namespace&gt; -- sh\necho \"test1\" &gt; /var/lib/registry/test.txt\n\n#verify the file exist on other pod \nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- ls /var/lib/registry/\n\nkubectl exec -it &lt;pod-replica-2&gt;  -- sh\necho \"test2\" &gt;&gt; /var/lib/registry/test.txt\n\nkubectl exec -it &lt;pod-replica-3&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n\n\n# veirfy volume is persistenet?\nkubectl rollout restart deployment kube-registry -n &lt;namespace&gt;\n\nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n# check volume path\n\nfindmnt | grep ceph\n\nsudo -i\ncd &lt;mount path&gt;\nls\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-object_storage","title":"Create/build OBJECT_STORAGE","text":"<pre><code>cd ~/rook/deploy/examples\n\nkubectl create -f object.yaml\n</code></pre> <ul> <li>After the CephObjectStore is created, the Rook operator will then create all the pools and other resources necessary to start the service. This may take a minute to complete.</li> </ul> <pre><code># can see ceph object storage\nkubectl get CephObjectStore\n\nNAME       PHASE   ENDPOINT                                         SECUREENDPOINT   AGE\nmy-store   Ready   http://rook-ceph-rgw-my-store.rook-ceph.svc:80                    xxx\n\n\n# To confirm the object store is configured, wait for the RGW pod(s) to start:\nkubectl -n rook-ceph get pod -l app=rook-ceph-rgw\n\nNAME                                        READY   STATUS    RESTARTS   AGE\nrook-ceph-rgw-my-store-a-xxxxxxx            2/2     Running   0          xxx\n</code></pre> <ul> <li>To consume the object store, Create a bucket:</li> </ul> <pre><code># Now that the object store is configured, next we need to create a bucket where a client can read and write objects\n# First create object storage class\n\nkubectl create -f storageclass-bucket-delete.yaml\n\nNAME                      PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block           rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   xxx\nrook-ceph-delete-bucket   rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  xxx\nrook-cephfs               rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   xxx\n\n\n# Based on this storage class, an object client can now request a bucket by creating an Object Bucket Claim (OBC). When the OBC is created, the Rook bucket provisioner will create a new bucket\n\nkubectl create -f object-bucket-claim-delete.yaml\n# Now that the claim is created, the operator will create the bucket as well as generate other artifacts to enable access to the bucket. also configure max object and max size inside yaml\n</code></pre> <ul> <li>Client Connections:</li> <li>The following commands extract key pieces of information from the secret and configmap:</li> </ul> <p>```bash</p>"},{"location":"devops/k8s-storage/rook-ceph/#config-map-secret-obc-will-part-of-default-if-no-specific-name-space-mentioned","title":"config-map, secret, OBC will part of default if no specific name space mentioned","text":"<p>export AWS_HOST=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_HOST}') export PORT=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_PORT}') export BUCKET_NAME=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_NAME}') export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode) export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode)  ```</p> <ul> <li>Consume the Object Storage</li> </ul> <p>This section will guide you through testing the connection to the CephObjectStore and uploading and downloading from it. Run the following commands after you have connected to the Rook toolbox.</p> <pre><code>#To test the CephObjectStore, set the object store credentials in the toolbox pod that contains the s5cmd tool\n#The default toolbox.yaml does not contain the s5cmd. The toolbox must be started with the rook operator image (toolbox-operator-image), which does contain s5cmd.\n\nkubectl create -f ~/rook/deploy/examples/toolbox-operator-image.yaml\nkubectl exec -it &lt;pod-name&gt; -n rook-ceph -- bash\n\nmkdir ~/.aws\ncat &gt; ~/.aws/credentials &lt;&lt; EOF\n[default]\naws_access_key_id = &lt;acceskey&gt;\naws_secret_access_key = &lt;secretkey&gt;\nEOF\n\n#Upload a file to the newly created bucket\necho \"Hello Rook\" &gt; /tmp/rookObj\n\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp /tmp/rookObj s3://$BUCKET_NAME\n\n#list bucket\ns5cmd --endpoint-url http://$AWS_HOST:$PORT  ls\n#print remote object content\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cat s3://$BUCKET_NAME/rookObj \n\n#Download and verify the file from the bucket\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp s3://$BUCKET_NAME/rookObj /tmp/rookObj-download\ncat /tmp/rookObj-download\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#create-external-ceph-cluster-and-use-it-exist-k8s-cluster","title":"Create External Ceph Cluster and use it exist K8S Cluster","text":""},{"location":"devops/k8s-storage/rook-ceph/#create-ceph-cluster-with-ceph-ansible","title":"Create ceph cluster with ceph-ansible","text":"<ul> <li> <p>pre-requirements</p> </li> <li> <p>all nodes</p> </li> </ul> <p><pre><code>sudo hostnamectl set-hostname ceph1\nsudo hostnamectl set-hostname ceph2\nsudo hostnamectl set-hostname ceph3\nsudo nano /etc/hosts\n172.18.11.55  ceph1\n172.18.11.56  ceph2\n172.18.11.57  ceph3\n</code></pre>   - on master ceph node <pre><code>sudo -i\nssh-keygen\ncat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys \ncat /root/.ssh/id_rsa.pub\n\nssh ceph2\nsudo -i\nsudo hostnamectl set-hostname ceph2\nnano .ssh/authorized_keys \n* paste id_rsa.pub\n\nssh ceph3\nsudo -i\nsudo hostnamectl set-hostname ceph3\nnano .ssh/authorized_keys\n* paste id_rsa.pub\n</code></pre></p>"},{"location":"devops/k8s-storage/rook-ceph/#install-ansible","title":"install ansible","text":"<pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository --yes --update ppa:ansible/ansible\nsudo apt install ansible -y\nansible --version\n</code></pre> <ul> <li>Clone cephadm-ansible repo</li> </ul> <pre><code>git clone https://github.com/ceph/cephadm-ansible.git\n\ncd cephadm-ansible\n\n# modified hosts\nnano hosts\n[ceph_servers]\nceph1 ansible_host=172.18.11.55 \nceph2 ansible_host=172.18.11.56 \nceph3 ansible_host=172.18.11.57\n\n[all:vars]\nansible_python_interpreter=/usr/bin/python3\nansible_ssh_private_key_file=/root/.ssh/id_ed25519\nansible_user=root\n# verify all node is reachable\nansible all -i hosts -m ping\n\nansible-playbook -i hosts cephadm-preflight.yml\n\n\n\nnano initial_config.yaml\n\n---\nservice_type: host\naddr: 172.18.11.55\nhostname: ceph1\n---\nservice_type: host\naddr: 172.18.11.56 \nhostname: ceph2\n---\nservice_type: host\naddr: 172.18.11.57\nhostname: ceph3\n---\nservice_type: mon\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3    \n---\nservice_type: mgr\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3\n---\nservice_type: osd\nservice_id: default_drive_group\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3\ndata_devices:\n  paths:\n    - /dev/sdb # if need, change disk name \n---\n\ncephadm bootstrap --mon-ip= 172.18.11.55 --apply-spec=initial_config.yaml --initial-dashboard-password=XK88Q4iPcDrJ --dashboard-password-noupdate # change mon-ip eand dashboard password\n\n- check current infra\nceph -s\nceph orch ls\nceph osd pool ls\nceph osd pool ls detail\nceph osd tree \nceph orch host ls\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-rbd-storage_1","title":"Create/build RBD storage","text":"<p>Ref: https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool</p> <ul> <li> <p>check current pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></p> </li> <li> <p>create new pool for rbd <pre><code>exm: ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]\n\nceph osd pool create rbd-pool replicated\n</code></pre></p> </li> <li>Associating a Pool with an Application Pools need to be associated with an application before they can be used</li> </ul> <pre><code>Exm: ceph osd pool application enable {pool-name} {application-name}\n\nceph osd pool application enable rbd-pool rbd\n\n## CephFS uses the application name cephfs, RBD uses the application name rbd, and RGW uses the application name rgw\n</code></pre> <ul> <li>check new pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></li> </ul> <pre><code>- some extra commands\nceph osd crush rule ls\nceph osd crush rule dump\n\n## Setting Pool Quotas\nceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]\n\n## Deleting a Pool\nceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]\n\n## Setting Pool Values\nceph osd pool set {pool-name} {key} {value}\nsize, pg_num, \n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-cephfs-storage_1","title":"Create/build CEPHFS storage","text":"<ul> <li> <p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph\u2019s distributed object store</p> </li> <li> <p>check current pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></p> </li> <li> <p>The Ceph Orchestrator will automatically create and configure MDS for your file system</p> </li> <li> <p>run on ceph master <pre><code>ceph fs volume create cephfs\n#create cephfs data and metadata pool automatically\n\n#or manual\n\nceph osd pool create cephfs_data 32\nceph osd pool create cephfs_metadata 1\n\n#check data pools\nceph osd pool ls\nceph osd pool ls detail\ncephfs.cephfs.meta\ncephfs.cephfs.data\n#anable data pool\nceph osd pool application enable cephfs_data cephfs\nceph osd pool application enable cephfs.cephfs.metacephfs\n\nceph osd pool ls detail\n\n#make file system from this data pool\nceph fs new cephfs-test cephfs_metadata cephfs_data\n\n#control file system\nceph fs ls\n\n#if need, delete file system: \"ceph fs rm  hepapi-cephfs --yes-i-really-mean-it\"\n\n#create mds\n\non ceph dashboard: services--&gt; create mds : 2 or cli: ceph fs volume create cephfs\n</code></pre></p> </li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#kubernetes-integration","title":"Kubernetes Integration","text":"<ul> <li> <p>Cluster export - import process</p> </li> <li> <p>Run on master ceph node</p> </li> </ul> <pre><code>git clone --single-branch --branch v1.15.5 https://github.com/rook/rook.git\n\ncd ~/rook/deploy/examples/external\n\npython3 create-external-cluster-resources.py --rbd-data-pool-name &lt;pool_name&gt; --cephfs-filesystem-name &lt;filesystem-name&gt; --rgw-endpoint  &lt;rgw-endpoint&gt; --namespace &lt;namespace&gt; --format bash\n\nsudo -i\ncd ~/home/ubuntu/rook/deploy/examples/external\nExm: python3 create-external-cluster-resources.py --cephfs-filesystem-name cephfs-hepapi --cephfs-data-pool-name cephfs_data --cephfs-metadata-pool-name cephfs_metadata --rbd-data-pool-name hepapi-ceph-replica --namespace rook-ceph --config-file config.ini --format bash\n````\n\n- Example output:\n\n```yaml\nexport ROOK_EXTERNAL_FSID=797f411a-aafe-11ec-a254-fa163e1539f5\nexport ROOK_EXTERNAL_USERNAME=client.healthchecker\nexport ROOK_EXTERNAL_CEPH_MON_DATA=ceph-rados-upstream-w4pdvq-node1-installer=10.0.210.83:6789\nexport ROOK_EXTERNAL_USER_SECRET=AQAdm0FilZDSJxAAMucfuu/j0ZYYP4Bia8Us+w==\nexport ROOK_EXTERNAL_DASHBOARD_LINK=https://10.0.210.83:8443/\nexport CSI_RBD_NODE_SECRET=AQC1iDxip45JDRAAVahaBhKz1z0WW98+ACLqMQ==\nexport CSI_RBD_PROVISIONER_SECRET=AQC1iDxiMM+LLhAA0PucjNZI8sG9Eh+pcvnWhQ==\nexport MONITORING_ENDPOINT=10.0.210.83\nexport MONITORING_ENDPOINT_PORT=9283\nexport RBD_POOL_NAME=replicated_2g\nexport RGW_POOL_PREFIX=default\n</code></pre> <ul> <li> <p>run on k8s cluster </p> </li> <li> <p>Prerequisites:   Rook operator is deployed</p> </li> </ul> <pre><code># Create common-external.yaml and cluster-external.yaml\ncd ~/rook/deploy/examples/\n# change namespace: rook-ceph-external in common-external.yaml and cluster-external.yaml then run\nkubectl create -f common-external.yaml\nkubectl create -f cluster-external.yaml\n\n# import external ceph cluster\nfirstly, Paste the above output from create-external-cluster-resources.py into your current shell to allow importing the source data.\n\n# then run\ncd ~/rook/deploy/examples/external\n# change namespace: rook-ceph-external in import-external-cluster.sh then run\n. import-external-cluster.sh\n\n#verify on k8s cluster cephcluster up and running, Health is OK\nkubectl -n rook-ceph  get CephCluster\n\nNAME                 DATADIRHOSTPATH   MONCOUNT   AGE    STATE       HEALTH\nrook-ceph-external   /var/lib/rook                xxx   Connected   HEALTH_OK\n\n#verify current storageclass\nkubectl get sc\n</code></pre> <ul> <li> <p>Verify created new cephfilesystem storageclass</p> </li> <li> <p>Use this volume type in kube-registry deployment, replica:3 in kube-system namespace</p> </li> </ul> <pre><code>cd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f kube-registry.yaml\n\n#verify pvc and pvbound\nkubectl get pv,pvc\n\n# pod mount path : /var/lib/registry\n\n#create new file on mounth path\nkubectl exec -it &lt;pod-replica-1&gt; -n &lt;namespace&gt; -- sh\necho \"test1\" &gt; /var/lib/registry/test.txt\n\n#verify the file exist on other pod \nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- ls /var/lib/registry/\n\nkubectl exec -it &lt;pod-replica-2&gt;  -- sh\necho \"test2\" &gt;&gt; /var/lib/registry/test.txt\n\nkubectl exec -it &lt;pod-replica-3&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n\n\n# verify volume is persistenet?\nkubectl rollout restart deployment kube-registry -n &lt;namespace&gt;\n\nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n# check volume path\n\nfindmnt | grep ceph\n\nsudo -i\ncd &lt;mount path&gt;\nls\n</code></pre>"},{"location":"devops/kubernetes/Velero/","title":"Simplify Cluster Backups with Velero","text":""},{"location":"devops/kubernetes/Velero/#overview","title":"Overview","text":"<p>Velero (formerly Heptio Ark) gives you tools to back up and restore your  Kubernetes cluster resources and persistent volumes. You can run Velero with a cloud provider  or on-premises. Velero lets you:</p> <ul> <li> <p>Take backups of your cluster and restore in case of loss.</p> </li> <li> <p>Migrate cluster resources to other clusters.</p> </li> <li> <p>Replicate your production cluster to development and testing clusters.</p> </li> </ul> <p>Velero consists of:</p> <ul> <li> <p>A server that runs on your cluster.</p> </li> <li> <p>A command-line client that runs locally.</p> </li> </ul>"},{"location":"devops/kubernetes/Velero/#how-velero-works","title":"How Velero Works?","text":"<p>Each Velero operation \u2013 on-demand backup, scheduled backup, restore \u2013 is a custom resource,  defined with a Kubernetes Custom Resource Definition (CRD) and stored in etcd. Velero also  includes controllers that process the custom resources to perform backups, restores, and all  related operations.</p> <p>You can back up or restore all objects in your cluster, or you can filter objects by type,  namespace, and/or label.</p> <p>Velero is ideal for the disaster recovery use case, as well as for snapshotting your application  state, prior to performing system operations on your cluster, like upgrades.</p>"},{"location":"devops/kubernetes/Velero/#demo-prerequisites","title":"Demo - Prerequisites","text":"<ol> <li> <p>Kubernetes Cluster</p> </li> <li> <p>AWS IAM User Account</p> </li> </ol>"},{"location":"devops/kubernetes/Velero/#demo","title":"Demo","text":"<ul> <li> <p>Go to Velero CLI Installation page to install <code>velero</code>.</p> </li> <li> <p>Go to <code>AWS Management Console</code> and sign in as <code>IAM-User</code>. </p> </li> <li> <p>Navigate to <code>S3</code>. From the menu on the left, choose <code>General purpose buckets</code>.</p> </li> <li> <p>Choose <code>Create Bucket</code> and follow these settings:</p> </li> </ul> <p><pre><code>Bucket name: velero-backup-&lt;YOUR_NAME&gt;\nLeave the rest as default.\n</code></pre> - Change <code>&lt;YOUR_NAME&gt;</code> with your name to make the bucket name unique.</p> <ul> <li> <p>Then, choose <code>Create bucket</code>. </p> </li> <li> <p>Choose the bucket you created. Then, choose <code>Create folder</code>.</p> </li> </ul> <pre><code>Folder name: backups\nLeave the rest as default.\n</code></pre> <ul> <li> <p>Choose <code>Create folder</code>. Then, go to main page of the bucket again.</p> </li> <li> <p>Choose <code>Create folder</code> again.</p> </li> </ul> <pre><code>Folder name: restores\nLeave the rest as default.\n</code></pre> <ul> <li>Choose <code>Create folder</code>. You should see the following structure:</li> </ul> <p></p> <ul> <li> <p>Now, go to <code>IAM</code>. From the menu on the left, choose <code>Users</code>.</p> </li> <li> <p>Choose your user and navigate to its page.</p> </li> <li> <p>Under <code>Permissions</code> section, choose <code>Add permissions -&gt; Create inline policy</code>.</p> </li> <li> <p>Choose <code>JSON</code> as <code>Policy editor</code> and enter the following:</p> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::velero-backup-mehmet\",\n                \"arn:aws:s3:::velero-backup-mehmet/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListAllMyBuckets\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <ul> <li> <p>Change <code>&lt;YOUR_BUCKET_NAME&gt;</code> with your bucket name.</p> </li> <li> <p>Choose <code>Next</code> ...</p> </li> </ul> <pre><code>Policy name: velero-role\nLeave the rest as default.\n</code></pre> <ul> <li> <p>Choose <code>Create policy</code>.</p> </li> <li> <p>Go to your main page of your IAM user account again.</p> </li> <li> <p>Under <code>Security credentials</code> section, choose <code>Create access key</code>.</p> </li> </ul> <pre><code>Use case: Command Line Interface (CLI)\n(Checked) Confirmation\n</code></pre> <ul> <li>Choose <code>Next</code> ...</li> </ul> <pre><code>Description tag value: Credentials for Velero\n</code></pre> <ul> <li> <p>Choose <code>Create access key</code> and choose <code>Download .csv file</code>.</p> </li> <li> <p>There is a <code>credentials.txt</code> file with the following content:</p> </li> </ul> <pre><code>[default]\naws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <ul> <li> <p>Enter the keys from the <code>.csv</code> file.</p> </li> <li> <p>Now, open the terminal. Check the status of cluster.</p> </li> </ul> <pre><code>kubectl version\n</code></pre> <ul> <li>Configure the <code>velero</code> in cluster.</li> </ul> <pre><code>velero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n    --bucket &lt;BUCKET_NAME&gt; \\\n    --backup-location-config region=&lt;REGION&gt; \\\n    --secret-file &lt;PATH_TO_AWS_CREDENTIALS_FILE&gt; \\\n    --pod-annotations iam.amazonaws.com/role=arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:user/&lt;AWS_IAM_USER&gt; \\\n    --use-volume-snapshots=false \n</code></pre> <ul> <li>Replace the related parts with your information. For example:</li> </ul> <pre><code>velero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n    --bucket velero-backup-mehmet-deneme \\\n    --backup-location-config region=us-east-1 \\\n    --secret-file ./credentials.txt \\\n    --pod-annotations iam.amazonaws.com/role=arn:aws:iam::123456789123:user/mehmet \\\n    --use-volume-snapshots=false\n</code></pre> <ul> <li>Run the following command to see the backup location.</li> </ul> <pre><code>velero backup-location get\n</code></pre> <ul> <li>The expected output will be: </li> </ul> <p></p> <ul> <li>After verifying that backup location is ready, take the backup.</li> </ul> <pre><code>velero create backup my-first-backup-demo\n</code></pre> <ul> <li>Open your bucket and check the backup.</li> </ul> <p></p> <ul> <li>Run the following command to see the backup status.</li> </ul> <pre><code>velero backup describe my-first-backup-demo\n</code></pre> <p></p> <ul> <li>Now, create a namespace.</li> </ul> <pre><code>kubectl create namespace velero-testing\n</code></pre> <ul> <li>Deploy a <code>nginx</code> pod in this namespace.</li> </ul> <pre><code>kubectl create deployment nginx --image=nginx --namespace=velero-testing\n</code></pre> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing \n</code></pre> <ul> <li>Take the backup of this cluster.</li> </ul> <pre><code>velero create backup testing-backup-do\n</code></pre> <ul> <li> <p>Check the bucket and notice the backup. </p> </li> <li> <p>Now, delete the pod.</p> </li> </ul> <pre><code>kubectl delete deploy nginx -n velero-testing\n</code></pre> <ul> <li>Then, delete the namespace.</li> </ul> <pre><code>kubectl delete namespace velero-testing\n</code></pre> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing\n</code></pre> <ul> <li>Now, restore the namespace and the pod.</li> </ul> <pre><code>velero restore create --from-backup testing-backup-do\n</code></pre> <ul> <li>Check the bucket for restoration files.</li> </ul> <p></p> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing\n</code></pre> <ul> <li>Also, you can create scheduled backups for every minute.</li> </ul> <pre><code>velero schedule create my-first-schedule --schedule=\"*/1 * * * *\"\n</code></pre> <ul> <li>Check the backup history.</li> </ul> <pre><code>velero get backup \n</code></pre> <p></p> <ul> <li>Delete the scheduled backups.</li> </ul> <pre><code>velero delete schedule my-first-schedule  \n</code></pre> <ul> <li>Delete the backups.</li> </ul> <pre><code>velero backup delete --all   \n</code></pre> <ul> <li>Don't forget to destroy the resources you created.</li> </ul>"},{"location":"devops/kubernetes/Velero/#references","title":"References","text":"<p>Mastering Kubernetes Backups with Velero</p>"},{"location":"devops/kubernetes/Velero/Cheat-Sheet/","title":"Velero Cheat Sheet","text":"<pre><code># Installation\nvelero install --provider &lt;provider&gt; --bucket &lt;bucket-name&gt; --secret-file ./credentials\n\n# Backup Management\nvelero backup create &lt;backup-name&gt;            # Create a Backup\nvelero backup get                             # List Backups\nvelero backup describe &lt;backup-name&gt;          # Describe Backup Details\nvelero backup delete &lt;backup-name&gt;            # Delete a Backup\nvelero backup logs &lt;backup-name&gt;              # View Backup Logs\n\n# Restore Management\nvelero restore create --from-backup &lt;backup-name&gt;  # Create a Restore\nvelero restore get                                 # List Restores\nvelero restore describe &lt;restore-name&gt;             # Describe Restore Details\nvelero restore delete &lt;restore-name&gt;               # Delete a Restore\nvelero restore logs &lt;restore-name&gt;                 # View Restore Logs\n\n# Schedule Management\nvelero schedule create &lt;schedule-name&gt; --schedule=\"*/5 * * * *\"  # Create a Backup Schedule\nvelero schedule get                                              # List Schedules\nvelero schedule describe &lt;schedule-name&gt;                         # Describe Schedule Details\nvelero schedule delete &lt;schedule-name&gt;                           # Delete a Schedule\n\n# Plugin Management\nvelero plugin get            # List Velero Plugins\n</code></pre>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/","title":"Upgrading an EKS Hybrid Cluster Using nodeadm","text":"<p>Recommend Way - <code>Cutover Migration</code>: - Create new hybrid nodes on your target Kubernetes version - Gracefully migrate your existing applications to the hybrid nodes on the new Kubernetes version - Remove the hybrid nodes on the old Kubernetes version from your cluster</p> <p>If you do not have spare capacity to create new hybrid nodes: - In-place upgrade - <code>nodeadm upgrade</code></p>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#prerequisites","title":"Prerequisites:","text":"<ul> <li>If you are following a cutover migration upgrade strategy, the new hybrid nodes you are installing on your target Kubernetes version must meet the Prerequisite setup for hybrid nodes requirements</li> <li>The version of your CNI must support the Kubernetes version you are upgrading to. </li> </ul> <p>\u2714 Cilium versions <code>v1.17.x</code> and <code>v1.18.x</code> are supported for EKS Hybrid Nodes for every Kubernetes version supported in Amazon EKS.</p> <p>Check the installed Cilium version (via Helm): <pre><code>helm list -n kube-system -o json | jq -r '.[] | select(.name==\"cilium\")\n</code></pre></p>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#in-place-upgrade","title":"In-place Upgrade:","text":"<p>The in-place upgrade process refers to using nodeadm upgrade to upgrade the Kubernetes version for hybrid nodes without using new physical or virtual hosts and a cutover migration strategy. The nodeadm upgrade process shuts down the existing older Kubernetes components running on the hybrid node, uninstalls the existing older Kubernetes components, installs the new target Kubernetes components, and starts the new target Kubernetes components. It is strongly recommend to upgrade one node at a time to minimize impact to applications running on the hybrid nodes. The duration of this process depends on your network bandwidth and latency.</p>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#steps","title":"Steps:","text":""},{"location":"devops/kubernetes/eks/hybrid-upgrade/#1-cordon-and-drain-the-node","title":"1- Cordon and drain the node","text":"<p><pre><code>export NODE_NAME=&lt;NODE_NAME&gt;\nkubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data --force\n</code></pre> This safely evicts running workloads and prevents new pods from being scheduled on the node.</p>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#2-validate-the-nodeconfig-and-verify-ssm-connectivity","title":"2- Validate the nodeConfig and verify SSM connectivity","text":"<p><pre><code>nodeadm debug -c file://nodeConfig.yaml\n</code></pre> Make sure the node can successfully communicate via the SSM Agent and that the configuration is valid.</p>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#3-upgrade-the-node-kubernetes-version-using-nodeadm","title":"3- Upgrade the node Kubernetes version using nodeadm","text":"<pre><code>nodeadm upgrade &lt;K8S_VERSION&gt; -c file://nodeConfig.yaml\n</code></pre>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#4-uncordon-the-node","title":"4- Uncordon the node","text":"<pre><code>kubectl uncordon $NODE_NAME\nkubectl get nodes -o wide -w\n</code></pre>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#5-check-the-status","title":"5- Check the status","text":"<pre><code>kubectl get node $NODE_NAME -o wide\nkubectl describe node $NODE_NAME\nkubectl get pods -A -o wide --field-selector spec.nodeName=$NODE_NAME\n</code></pre> <ul> <li>\u2714 Node is in Ready state</li> <li>\u2714 Correct Kubernetes version</li> <li>\u2714 No unusual taints or warnings</li> </ul>"},{"location":"devops/kubernetes/eks/hybrid-upgrade/#6-repeat-for-remaining-nodes-one-at-a-time","title":"6- Repeat for remaining nodes (one at a time)","text":"<p>Always upgrade one node only at a time to avoid capacity issues or service disruption.</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/","title":"KEDA (Kubernetes Event-driven Autoscaling) WITH CRON","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#the-midnight-madness-a-devops-scenario","title":"The Midnight Madness: A DevOps Scenario","text":"<p>The clock strikes 20:55. You are on a Zoom call with your DevOps team, and the silence is deafening. In exactly five minutes, at 21:00, the massive \"Flash Sale\" campaign begins. The marketing team has been pushing this for weeks, and push notifications are about to go out to millions of users.</p> <p>Normally, your system is protected by the standard Kubernetes HPA (Horizontal Pod Autoscaler). The logic is simple and reactive: Traffic increases -&gt; CPU usage spikes -&gt; HPA detects the spike -&gt; New Pods are requested.</p> <p>But at 21:00:01, the scenario you fear unfolds: The traffic doesn't climb like a gentle ramp; it hits like a vertical wall.</p> <p>In the critical 2-3 minutes it takes for HPA to wake up, request resources, and for new pods to pull images and reach \"Ready\" status, chaos ensues: * Existing pods are crushed under the load, spiraling into <code>CrashLoopBackOff</code>. * Users clicking \"Add to Cart\" are greeted with <code>503 Service Unavailable</code> errors. * Twitter starts flooding with complaints.</p> <p>As you watch the monitoring dashboard, waiting helplessly for the new pods to spin up, you ask yourself: \"We knew the exact time of the sale. Why did we wait for the traffic to hit us? Why weren't we there before the users?\"</p> <p>This is where the standard \"Reactive\" HPA fails. You need a \"Proactive\" solution. You need scaling based on the clock, not just the CPU.</p> <p>Enter KEDA (Kubernetes Event-Driven Autoscaling) and the Cron Scaler. In this guide, we will teach your Kubernetes cluster to say, \"Wake up at 20:55 and be ready for the storm.\"</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#what-is-keda","title":"What is KEDA ?","text":"<p>KEDA (Kubernetes Event-driven Autoscaling) is an open-source project that provides event-driven autoscaling capabilities in your Kubernetes environment. KEDA extends Kubernetes' HPA (Horizontal Pod Autoscaler) system, allowing you to scale your applications based on metrics beyond CPU and memory.</p> <p>[Image of KEDA architecture diagram]</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Installation</li> <li>Installation with Helm</li> <li>Installation with YAML</li> <li>Scaling with Cron</li> <li>Creating a ScaledObject</li> <li>Troubleshooting</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"devops/kubernetes/keda/keda-with-cron/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.16+)</li> <li><code>kubectl</code> CLI</li> <li>(Optional) Helm 3</li> <li>Cluster admin privileges</li> </ul>"},{"location":"devops/kubernetes/keda/keda-with-cron/#installation","title":"Installation","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#installation-with-helm","title":"Installation with Helm","text":"<pre><code># Add Helm repository\nhelm repo add kedacore [https://kedacore.github.io/charts](https://kedacore.github.io/charts)\nhelm repo update\n\n# Create KEDA namespace\nkubectl create namespace keda\n\n# Install KEDA\nhelm install keda kedacore/keda --namespace keda\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#installation-with-yaml","title":"Installation with YAML","text":"<pre><code># Install the latest version\nkubectl apply -f https://github.com/kedacore/keda/releases/download/v2.11.0/keda-2.11.0.yaml\n</code></pre> <p>Verify the installation:</p> <pre><code>kubectl get pods -n keda\n</code></pre> <p>Expected output: <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\nkeda-operator-7c8d65d96d-bzmqp            1/1     Running   0          30s\nkeda-operator-metrics-apiserver-7d9fd868b5-kvppj   1/1     Running   0          30s\n</code></pre></p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#scaling-with-cron","title":"Scaling with Cron","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#creating-a-scaledobject","title":"Creating a ScaledObject","text":"<p>ScaledObject is the core component KEDA uses to scale Kubernetes deployments.</p> <ol> <li>Create a sample deployment:</li> </ol> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cron-app\n  namespace: test\n  labels:\n    app: cron-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cron-app\n  template:\n    metadata:\n      labels:\n        app: cron-app\n    spec:\n      containers:\n      - name: cron-app\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cron-app-service\n  namespace: test\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: cron-app\n</code></pre> Apply the deployment:</p> <p><pre><code>kubectl apply -f deployment.yaml\n</code></pre> Verify the deployment:</p> <pre><code>kubectl get pods -n test\n</code></pre> <p>Expected output: <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\n\ncron-app-5df76bc88b-67krr   1/1     Running   0         33s\n</code></pre></p> <ol> <li>Define a ScaledObject:</li> </ol> <p><pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cron-app-scaledobject\n  namespace: test\nspec:\n  scaleTargetRef:\n    name: cron-app\n  minReplicaCount: 1\n  maxReplicaCount: 20\n  triggers:\n  - type: cron\n    metadata:\n      # Timezone\n      timezone: \"Europe/Amsterdam\"\n      # Scaling start time (e.g., starts every day at 08:45)\n      start: \"45 8 * * *\"\n      # Scaling end time (e.g., ends every day at 08:50)\n      end: \"50 8 * * *\"\n      # Desired number of pods during this time range\n      desiredReplicas: \"20\"\n</code></pre> Apply the ScaledObject:</p> <p><pre><code>kubectl apply -f scaleObject.yaml\n</code></pre> Verify the ScaledObject:</p> <pre><code>kubectl get so -n test\n</code></pre> <p>Expected output: <pre><code>NAME                    SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   READY   ACTIVE    FALLBACK   PAUSED    TRIGGERS   AUTHENTICATIONS   AGE\ncron-app-scaledobject   apps/v1.Deployment   cron-app          1     20    True    Unknown   False      Unknown                                18s\n</code></pre></p> <p>When the clock hits 08:45, the ScaledObject will activate and begin scaling the related service during the specified time range.</p> <p>Expected output when the ScaledObject is triggered: <pre><code>NAME                        READY   STATUS            RESTARTS    AGE\ncron-app-5df76bc88b-67krr   1/1     Running             0          7m8s\ncron-app-5df76bc88b-fqffn   0/1     Pending             0          0s\ncron-app-5df76bc88b-fqffn   0/1     Pending             0          0s\ncron-app-5df76bc88b-8llrz   0/1     Pending             0          0s\ncron-app-5df76bc88b-qk8qw   0/1     Pending             0          0s\ncron-app-5df76bc88b-fqffn   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-qk8qw   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-8llrz   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-6ltbp   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-dbkzx   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-smfnl   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-29854   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-8llrz   1/1     Running             0          2s\ncron-app-5df76bc88b-fqffn   1/1     Running             0          3s\ncron-app-5df76bc88b-smfnl   0/1     Pending             0          0s\ncron-app-5df76bc88b-smfnl   1/1     Running             0          2s\ncron-app-5df76bc88b-6ltbp   1/1     Running             0          3s\ncron-app-5df76bc88b-dbkzx   1/1     Running             0          4s\n...\n</code></pre> Current pod count after the ScaledObject runs within the specified time range: <pre><code>k get pod -n test\n\nNAME                        READY   STATUS    RESTARTS   AGE\ncron-app-5df76bc88b-29854   1/1     Running   0          69s\ncron-app-5df76bc88b-67krr   1/1     Running   0          9m20s\ncron-app-5df76bc88b-6ltbp   1/1     Running   0          69s\ncron-app-5df76bc88b-8llrz   1/1     Running   0          84s\ncron-app-5df76bc88b-9mxl8   1/1     Running   0          54s\ncron-app-5df76bc88b-c9lrj   1/1     Running   0          54s\ncron-app-5df76bc88b-dbkzx   1/1     Running   0          69s\n...\n</code></pre></p> <p>Expected output after the specified time range ends:</p> <p><pre><code>cron-app-5df76bc88b-c9lrj   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-jnpm6   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-8llrz   1/1     Terminating         0          9m46s\ncron-app-5df76bc88b-fqffn   1/1     Terminating         0          9m46s\ncron-app-5df76bc88b-67krr   1/1     Terminating         0          17m\ncron-app-5df76bc88b-pxwn4   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-mk6d2   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-xxhz9   1/1     Terminating         0          9m1s\n...\n\ncron-app-5df76bc88b-67krr   0/1     Completed           0          17m\ncron-app-5df76bc88b-pxwn4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-mk6d2   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-9mxl8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-jnpm6   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-xxhz9   0/1     Completed           0          9m3s\n...\n</code></pre> After the ScaledObject completes:</p> <pre><code>k get pod -n test\nNAME                        READY   STATUS    RESTARTS   AGE\ncron-app-5df76bc88b-smfnl   1/1     Running   0          10m\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#troubleshooting","title":"Troubleshooting","text":"<p>To troubleshoot KEDA issues:</p> <pre><code># Check KEDA operator logs\nkubectl logs -l app=keda-operator -n keda\n\n# Check metrics server logs\nkubectl logs -l app=keda-metrics-apiserver -n keda\n\n# Check ScaledObject status\nkubectl describe scaledobject cron-app-scaledobject -n test\n\n# Check HPA status\nkubectl get hpa -n test\nkubectl describe hpa keda-hpa-cron-app-scaledobject -n test\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#contributing","title":"Contributing","text":"<p>To contribute to the KEDA project, you can submit a pull request via GitHub.</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#license","title":"License","text":"<p>KEDA is distributed under the Apache 2.0 licance.</p>"},{"location":"devops/kubernetes/keda/keda/","title":"KEDA (Kubernetes Event-driven Autoscaling)","text":"<p>What is KEDA ?</p> <p>KEDA is a lightweight, open-source Kubernetes event-driven autoscaler used by DevOps, SRE, and Ops teams to horizontally scale pods based on external events or triggers. KEDA helps to extend the capability of native Kubernetes autoscaling solutions, which rely on standard resource metrics such as CPU or memory. You can deploy KEDA into a Kubernetes cluster and manage the scaling of pods using custom resource definitions (CRDs). Built on top of Kubernetes HPA, KEDA scales pods based on information from event sources such as AWS SQS, Kafka, RabbitMQ, etc. These event sources are monitored using scalers, which activate or deactivate deployments based on the rules set for them. KEDA scalers can also feed custom metrics for a specific event source, helping DevOps teams observe metrics relevant to them</p> <p> KEDA scales down the number of pods to zero in case there are no events to process. This is harder to do using the standard HPA, and it helps ensure effective resource utilization and cost optimization, ultimately bringing down the cloud bills..</p> <p>Quote</p> <p>KEDA supports a lot of built-in scalers and external scalers. External scalers include Redis, MYSQL,Prometheus,Rabbit MQ etc. Using external events as triggers aids efficient autoscaling, especially for message-driven microservices like payment gateways or order systems. Since KEDA can be extended by developing integrations with any data source, it can easily fit into any DevOps toolchain. </p>"},{"location":"devops/kubernetes/keda/keda/#keda-components","title":"KEDA Components","text":"<p>Event Sources</p> <p>These are the external event/trigger sources by which KEDA changes the number of pods. Prometheus, RabbitMQ, and Apache Pulsar are some examples of event sources.</p> <p>Metric Adapter</p> <p>Metrics adapter takes metrics from scalers and translates or adapts them into a form that HPA/controller component can understand.</p> <p>Controller</p> <p>The controller/operator acts upon the metrics provided by the adapter and brings about the desired deployment state specified in the ScaledObject (refer below).</p> <p>ScaledObject and ScaledJob:</p> <p>ScaledObject represents the mapping between event sources and objects, and specifies the scaling rules for a Deployment, StatefulSet, Jobs or any Custom Resource in a K8s cluster. Similarly, ScaledJob is used to specify scaling rules for Kubernetes Jobs.</p> <p>Below is an example of a ScaledObject which configures KEDA autoscaling based on Prometheus metrics. Here, the deployment object \u2018keda-test\u2019 is scaled based on the trigger threshold (50) from Prometheus metrics. KEDA will scale the number of replicas between a minimum of 1 and a maximum of 10, and scale down to 0 replicas if the metric value drops below the threshold.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: demo3\nspec:\n  scaleTargetRef:\n    apiVersion: argoproj.io/v1alpha1\n    kind: Rollout\n    name: keda-test-demo3\n  triggers:\n    - type: prometheus\n      metadata:\n      serverAddress:  http://&lt;prometheus-host&gt;:9090\n      metricName: http_request_total\n      query: envoy_cluster_upstream_rq{appId=\"300\", cluster_name=\"300-0\", container=\"envoy\", namespace=\"test\", response_code=\"200\" }\n      threshold: \"50\"\n  idleReplicaCount: 0                       \n  minReplicaCount: 1\n  maxReplicaCount: 10\n</code></pre> <p>Deploying KEDA on any Kubernetes cluster is easy, as it doesn\u2019t need overwriting or duplication of existing functionalities. Once deployed and the components are ready, the event-based scaling starts with the external event source. The scaler will continuously monitor for events based on the source set in ScaledObject and pass the metrics to the metrics adapter in case of any trigger events. The metrics adapter then adapts the metrics and provides them to the controller component, which then scales up or down the deployment based on the scaling rules set in ScaledObject.</p> <p>Note that KEDA activates or deactivates a deployment by scaling the number of replicas to zero or one. It then triggers HPA to scale the number of workloads from one to n based on the cluster resources</p>"},{"location":"devops/kubernetes/keda/keda/#keda-deployment","title":"Keda Deployment","text":"<p>KEDA can be deployed in a Kubernetes cluster through Helm charts, operator hub, or YAML declarations</p> <p>Helm Chart</p> <ul> <li>Add Helm repo</li> </ul> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\n</code></pre> <ul> <li>Update Helm repo</li> </ul> <pre><code>helm repo update\n</code></pre> <ul> <li>Install keda Helm chart</li> </ul> <pre><code>helm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre> <p>Deploying with operator Hub</p> <p>On Operator Hub Marketplace locate and install KEDA operator to namespace keda Create KedaController resource named keda in namespace keda</p> <p>NOTE: Further information on Operator Hub installation method can be found in the following repository.</p> <p>https://github.com/kedacore/keda-olm-operator</p> <p>Deploying using the deployment YAML files</p> <p>Run the following command (if needed, replace the version, in this case 2.13.0, with the one you are using):</p> <pre><code># Including admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0.yaml\n# Without admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0-core.yaml\n</code></pre>"},{"location":"devops/linux/tips/","title":"Linux Tips","text":""},{"location":"devops/linux/tooling/","title":"Linux Tooling","text":""},{"location":"devops/linux/tooling/#fzf-fuzzy-finder","title":"FZF - Fuzzy Finder ().","text":"<ul> <li>Github: https://github.com/junegunn/fzf</li> <li>Description: A command-line fuzzy finder. Use <code>&lt;C-r&gt;</code> to search through your command history, <code>&lt;C-t&gt;</code> to search through your files.</li> </ul>"},{"location":"devops/linux/tooling/#tldr","title":"tldr ()","text":"<ul> <li>Website: https://tldr.sh/</li> <li>Description: Too Long Didn't Read. Use it to learn about a command and its most useful options. <code>tldr &lt;any command&gt;</code> (e.g. <code>tldr curl</code>).   </li> </ul>"},{"location":"devops/linux/shell/ampersand-nohup/","title":"nohup, &","text":""},{"location":"devops/linux/shell/ampersand-nohup/#nohup-and","title":"nohup and &amp;","text":"<p>Ampersand (&amp;)</p> <p>The ampersand (<code>&amp;</code>) is an operator that is used to send the execution of a command to the background.</p> <p>Syntax:</p> <p><code>[command] &amp;</code></p> <p>For example, the command <code>sleep 5 &amp;</code> will start the sleep command in the background. You can then run other commands without having to wait for the sleep command to finish.</p> <p>The nohup command</p> <p>The <code>nohup</code> command is used to run a command in the background without any interruption, even when the terminal session is closed.</p> <p>Syntax:</p> <p><code>nohup [command]</code></p> <p>For example, the command <code>nohup sleep 5 &amp;</code> will start the sleep command in the background and will continue to run even if you close the terminal session.</p>"},{"location":"devops/linux/shell/ampersand-nohup/#difference-between-nohup-and","title":"Difference between <code>nohup</code> and <code>&amp;</code>","text":"<p>The <code>nohup</code> and <code>&amp;</code> commands are both used to run commands in the background. However, there are some key differences between the two commands.</p> <ul> <li><code>nohup</code> prevents the command from being interrupted by the <code>HUP</code> signal, even if the terminal session is closed. The <code>HUP</code> signal is typically sent to a process when the terminal session is closed. This can cause the process to stop running. However, the <code>nohup</code> command catches the <code>HUP</code> signal and ignores it, so that the command continues to run even after the terminal session is closed.</li> <li><code>&amp;</code> does not prevent the command from being interrupted by the <code>HUP</code> signal. If you run a command with the <code>&amp;</code> operator and then close the terminal session, the command will stop running.</li> </ul> <p>Differences between the <code>nohup</code> and <code>&amp;</code> commands:</p> <code>nohup</code> <code>&amp;</code> Prevents <code>HUP</code> signal Yes No Redirects output to file Yes No Suitable for Running commands that need to continue running even after the terminal session is closed Running commands that don't need to continue running after the terminal session is closed"},{"location":"devops/linux/shell/cat/","title":"cat","text":"<ul> <li>Description: Concatenate files and print on the standard output. Can be used to create a file from the standard input.</li> </ul> <p>This would display the contents of the file in the terminal window.</p> <p>Options</p> <p>The <code>cat</code> command provides several options that can be used to modify its behavior. Here are some of the most useful options:</p> <ul> <li><code>-n</code>: Shows line numbers in the output.</li> <li><code>-e</code>: Shows end-of-line characters as \"$\".</li> <li><code>-t</code>: Shows tab characters as \"^I\".</li> <li><code>-v</code>: Shows non-printable characters as \"^M\".</li> <li><code>-s</code>: Squeezes consecutive empty lines into a single line.</li> <li><code>-E</code>: Prints a \"$\" character at the end of each line.</li> </ul> <p>Heredoc (Here Document)</p> <p>The <code>heredoc</code> (Here Document) is a type of redirection that allows you to pass multiple lines of input to a command.</p> <p>The basic syntax for <code>heredoc</code> looks like this:</p> <pre><code>cat &lt;&lt; LimitString\n  text...\nLimitString\n</code></pre> <p>Here, LimitString is any string you choose, and text... is the text you want to pass to the command.</p> <p>Inline file creation with redirection</p> <p>Same thing above applies here and it saves you from creating a file and then editing it.</p> <pre><code>cat &lt;&lt; EOF &gt; newfile.txt\nThis is line 1.\nThis is line 2.\nEOF\n</code></pre> <p>This command will create <code>newfile.txt</code> file with the two lines of text.</p>"},{"location":"devops/linux/shell/chtsh/","title":"cht.sh Command Tool","text":"<p>In Linux it can be hard to remember some commands or options and there is a great tool for that. <code>cht.sh</code></p> <p>For example :</p> <pre><code>curl cht.sh/cat\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/cat\n cheat.sheets:cat\n# POSIX way in which to cat(1); see cat(1posix).\ncat -u [FILE_1 [FILE_2] ...]\n\n# Output a file, expanding any escape sequences (default). Using this short\n# one-liner let's you view the boot log how it was show at boot-time.\ncat /var/log/boot.log\n\n# This is an ever-popular useless use of cat.\ncat /etc/passwd | grep '^root'\n# The sane way:\ngrep '^root' /etc/passwd\n\n# If in bash(1), this is often (but not always) a useless use of cat(1).\nBuffer=`cat /etc/passwd`\n# The sane way:\nBuffer=`&lt; /etc/passwd`\n\n cheat:cat\n# To display the contents of a file:\ncat &lt;file&gt;\n\n# To display file contents with line numbers\ncat -n &lt;file&gt;\n\n# To display file contents with line numbers (blank lines excluded)\ncat -b &lt;file&gt;\n\n tldr:cat\n# cat\n# Print and concatenate files.\n# More information: &lt;https://www.gnu.org/software/coreutils/cat&gt;.\n\n# Print the contents of a file to the standard output:\ncat path/to/file\n\n# Concatenate several files into an output file:\ncat path/to/file1 path/to/file2 ... &gt; path/to/output_file\n\n# Append several files to an output file:\ncat path/to/file1 path/to/file2 ... &gt;&gt; path/to/output_file\n\n# Copy the contents of a file into an output file without buffering:\ncat -u /dev/tty12 &gt; /dev/tty13\n\n# Write `stdin` to a file:\ncat - &gt; path/to/file\n</code></pre> <p>You may say, why do you need this when you already have the <code>man</code> command in Linux? The most important feature that makes <code>cht.sh</code> different is that it explains each option in the simplest and most simple way. It does not require installation.</p> <p>Here are a few more examples :</p> <pre><code>curl cht.sh/tail\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/tail\n cheat:tail\n# To show the last 10 lines of &lt;file&gt;:\ntail &lt;file&gt;\n\n# To show the last &lt;number&gt; lines of &lt;file&gt;:\ntail -n &lt;number&gt; &lt;file&gt;\n\n# To show the last lines of &lt;file&gt; starting with &lt;number&gt;:\ntail -n +&lt;number&gt; &lt;file&gt;\n\n# To show the last &lt;number&gt; bytes of &lt;file&gt;:\ntail -c &lt;number&gt; &lt;file&gt;\n\n# To show the last 10 lines of &lt;file&gt; and to wait for &lt;file&gt; to grow:\ntail -f &lt;file&gt;\n\n tldr:tail\n# tail\n# Display the last part of a file.\n# See also: `head`.\n# More information: &lt;https://www.gnu.org/software/coreutils/tail&gt;.\n\n# Show last 'count' lines in file:\ntail --lines count path/to/file\n\n# Print a file from a specific line number:\ntail --lines +count path/to/file\n\n# Print a specific count of bytes from the end of a given file:\ntail --bytes count path/to/file\n\n# Print the last lines of a given file and keep reading file until `Ctrl + C`:\ntail --follow path/to/file\n\n# Keep reading file until `Ctrl + C`, even if the file is inaccessible:\ntail --retry --follow path/to/file\n\n# Show last 'num' lines in 'file' and refresh every 'n' seconds:\ntail --lines count --sleep-interval seconds --follow path/to/file\n</code></pre> <pre><code>curl cht.sh/touch\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/touch\n cheat:touch\n# To change a file's modification time:\ntouch -d &lt;time&gt; &lt;file&gt;\ntouch -d 12am &lt;file&gt;\ntouch -d \"yesterday 6am\" &lt;file&gt;\ntouch -d \"2 days ago 10:00\" &lt;file&gt;\ntouch -d \"tomorrow 04:00\" &lt;file&gt;\n\n# To put the timestamp of a file on another:\ntouch -r &lt;refrence-file&gt; &lt;target-file&gt;\n\n tldr:touch\n# touch\n# Create files and set access/modification times.\n# More information: &lt;https://manned.org/man/freebsd-13.1/touch&gt;.\n\n# Create specific files:\ntouch path/to/file1 path/to/file2 ...\n\n# Set the file [a]ccess or [m]odification times to the current one and don't [c]reate file if it doesn't exist:\ntouch -c -a|m path/to/file1 path/to/file2 ...\n\n# Set the file [t]ime to a specific value and don't [c]reate file if it doesn't exist:\ntouch -c -t YYYYMMDDHHMM.SS path/to/file1 path/to/file2 ...\n\n# Set the file time of a specific file to the time of anothe[r] file and don't [c]reate file if it doesn't exist:\ntouch -c -r ~/.emacs path/to/file1 path/to/file2 ...\n</code></pre>"},{"location":"devops/linux/shell/jobs-bg-fg/","title":"jobs, bg, fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs-bg-and-fg","title":"jobs, bg, and fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs","title":"jobs","text":"<p>The <code>jobs</code> command will list all jobs on the system; active, stopped, or otherwise.</p>"},{"location":"devops/linux/shell/jobs-bg-fg/#example-usage","title":"Example usage:","text":"<p>1.Create a job with using</p> <p><code>sleep 500 &amp;</code></p> <p>and stop it with <code>ctrl + z</code>. </p> <p>2.List all the jobs with the command : <code>jobs</code></p> <p>You will see that you have a single stopped job identified by the job number [1].</p> <p>Other options to know for this command include:</p> <ul> <li><code>-l</code> - list PIDs in addition to default info</li> <li><code>-n</code> - list only processes that have changed since the last notification</li> <li><code>-p</code> - list PIDs only</li> <li><code>-r</code> - show only running jobs</li> <li><code>-s</code> - show only stopped jobs</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#background","title":"Background","text":"<p>The <code>bg</code> command restarts a suspended job, and runs it in the background.</p> <p><code>bg [JOB_SPEC]</code></p> <p>Where JOB_SPEC can be one of the following:</p> <ul> <li><code>%n</code>: where <code>n</code> is the job number.</li> <li><code>%abc</code>: refers to a job started by a command beginning with <code>abc</code>.</li> <li><code>%?abc</code>: refers to a job started by a command containing <code>abc</code>.</li> <li><code>%-</code>: specifies the previous job.</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#foreground","title":"Foreground","text":"<p>The <code>fg</code> command switches a job running in the background into the foreground.</p> <p><code>fg [JOB_SPEC]</code></p> <p>NOTE: If no <code>JOB_SPEC</code> is provided, <code>bg</code> and <code>fg</code> operate on the current job.</p> <p>For example, if you have two jobs running in the background, and you run the command <code>bg</code>, the job that was most recently started will be brought to the foreground.</p> <p>You can also use the <code>%</code> character to specify a job by its job number, or by a partial command name.</p>"},{"location":"devops/linux/shell/netstat/","title":"Netstat &amp; SS Command","text":"<p>You can check the listening ports and applications with netstat as follows.</p> <p>Prerequisite By default, netstat command may not be installed on your system. Hence, use the apk command on Alpine Linux, dnf command/yum command on RHEL &amp; co, apt command/apt-get command on Debian, Ubuntu &amp; co, zypper command on SUSE/OpenSUSE, pacman command on Arch Linux to install the netstat.</p> <pre><code>sudo apt update\nsudo apt install net-tools\n</code></pre> <p>Run the netstat command along with grep command to filter out port in LISTEN state:</p> <pre><code>netstat -tulpn | grep LISTEN\nnetstat -tulpn | more\n</code></pre> <p>Where netstat command options are:</p> <p><code>-t</code> : Select all TCP ports</p> <p><code>-u</code> : Select all UDP ports</p> <p><code>-l</code> : Show listening server sockets (open TCP and UDP ports in listing state)</p> <p><code>-p</code> : Display PID/Program name for sockets. In other words, this option tells who opened the TCP or UDP port. For example, on my system, Nginx opened TCP port 80/443, so I will /usr/sbin/nginx or its PID.</p> <p><code>-n</code> : Don\u2019t resolve name (avoid dns lookup, this speed up the netstat on busy Linux/Unix servers)</p> <p>The netstat command <code>deprecated</code> for some time on Linux. Therefore, you need to use the ss command as follows:</p> <pre><code>sudo ss -tulw\nsudo ss -tulwn\nsudo ss -tulwn | grep LISTEN\n</code></pre> <p><code>-t</code> : Show only TCP sockets on Linux</p> <p><code>-u</code> : Display only UDP sockets on Linux</p> <p><code>-l</code> : Show listening sockets. For example, TCP port 22 is opened by SSHD server.</p> <p><code>-p</code> : List process name that opened sockets</p> <p><code>-n</code> : Don\u2019t resolve service names i.e. don\u2019t use DNS</p>"},{"location":"devops/linux/shell/netstat/#ps-command","title":"PS Command","text":"<p>The ps command without any options displays information about processes that are bound by the controlling terminal. <pre><code>ps\n</code></pre> The command returns a similar output: <pre><code>PID TTY      TIME     CMD\n285 pts/2    00:00:00 zsh\n334 pts/2    00:00:00 ps\n</code></pre></p> <p>The default output of the ps command contains four columns that provide the following information:</p> <p><code>PID</code>: The process ID is your system\u2019s tracking number for the process. The PID is useful when you need to use a command like kill or nice, which take a PID as their input.</p> <p><code>TTY</code>: The controlling terminal associated with the process. Processes that do not originate from a controlling terminal and were initiated by the system at boot are displayed with a question mark.</p> <p><code>TIME</code>: The CPU usage of the process. Displays the amount of CPU time used by the process. This value is not the run time of the process.</p> <p><code>CMD</code>: The name of the command or executable that is running. The output only includes the name of the command or executable and does not display any options that were passed in.</p>"},{"location":"devops/linux/shell/netstat/#the-aux-shortcut","title":"The <code>aux</code> shortcut","text":"<p>Now that you understand the basics of the <code>ps</code> command, this section covers the benefits to the <code>ps</code> <code>aux</code> command. The <code>ps</code> <code>aux</code> displays the most amount of information a user usually needs to understand the current state of their system\u2019s running processes. Take a look at the following example: <pre><code>ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0    892   572 ?        Sl   Nov28   0:00 /init\nroot       227  0.0  0.0    900    80 ?        Ss   Nov28   0:00 /init\nroot       228  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nzaphod     229  0.0  0.1 749596 31000 pts/0    Ssl+ Nov28   0:15 docker\nroot       240  0.0  0.0      0     0 ?        Z    Nov28   0:00 [init] &lt;defunct&gt;\nroot       247  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nroot       248  0.0  0.1 1758276 31408 pts/1   Ssl+ Nov28   0:10 /mnt/wsl/docker-desktop/docker-desktop-proxy\nroot       283  0.0  0.0    892    80 ?        Ss   Dec01   0:00 /init\nroot       284  0.0  0.0    892    80 ?        R    Dec01   0:00 /init\nzaphod     285  0.0  0.0  11964  5764 pts/2    Ss   Dec01   0:00 -zsh\nzaphod     343  0.0  0.0  23764  9836 pts/2    T    17:44   0:00 vi foo\nroot       349  0.0  0.0    892    80 ?        Ss   17:45   0:00 /init\nroot       350  0.0  0.0    892    80 ?        S    17:45   0:00 /init\nzaphod     351  0.0  0.0  11964  5764 pts/3    Ss+  17:45   0:00 -zsh\nzaphod     601  0.0  0.0  10612  3236 pts/2    R+   18:24   0:00 ps aux\n</code></pre></p> <p>The <code>ps aux</code> command displays more useful information than other similar options. For example, the <code>UID</code> column is replaced with a human-readable <code>username</code> column. <code>ps aux</code> also displays statistics about your Linux system, like the percent of CPU and memory that the process is using. The <code>VSZ</code> column displays amount of virtual memory being consumed by the process. <code>RSS</code> is the actual physical wired-in memory that is being used. The <code>START</code> column shows the date or time for when the process was started. This is different from the CPU time reported by the <code>TIME</code> column.</p>"},{"location":"devops/linux/shell/nmap/","title":"NMap Command","text":"<p>Nmap (an acronym of Network Mapper) is an open-source command-line utility to securely manage the network. Nmap command has an extensive list of options to deal with security auditing and network exploration.</p> <p>Prerequisites To use the Nmap utility, the Nmap must be installed on your Ubuntu 22.04. Nmap is available on the official repository of Ubuntu 22.04. Before installation, it is a better practice to update the core libraries of Ubuntu 22.04 as follows:</p> <p><pre><code>sudo apt update\nsudo apt install nmap\n</code></pre> or <pre><code>sudo apt update\nsnap install nmap\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#syntax-of-nmap-command","title":"Syntax of Nmap command","text":"<p>The syntax of the Nmap command is given below: <pre><code>nmap [options] [IP-adress or web-address]\n</code></pre> The Nmap command can be used to scan through the open ports of the host. For instance, the following command will scan the \u201cxxx.xxx.xxx\u201d for open ports..</p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-scan-specific-ports","title":"How to use the Nmap command to scan specific port(s)","text":"<p>By default, the Nmap scans through only 1000 most used ports (these are not consecutive but important). However, there are a total of 65535 ports. The Nmap command can be used to scan a specific port or all the ports.</p> <p>To scan all ports: The -p- flag of the Nmap command helps to scan through all 65535 ports: <pre><code>nmap -p- 192.168.214.138\n</code></pre></p> <p>To scan a <code>specific port</code>: One can specify the port number as well. For instance, the following command will scan for port 88 only: <pre><code>nmap -p 88 88 192.168.214.138\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-get-the-os-information","title":"How to use the Nmap command to get the OS information","text":"<p>The Nmap command can be used to get the <code>Operating System\u2019s information</code>. For instance, the following command will get the information of the OS associated with the IP address.</p> <pre><code>sudo nmap -O 192.168.214.138\n</code></pre>"},{"location":"devops/linux/shell/nslookup/","title":"NSLOOKUP","text":""},{"location":"devops/linux/shell/nslookup/#what-is-the-nslookup","title":"What is the 'nslookup'","text":""},{"location":"devops/linux/shell/nslookup/#nslookup-stands-for-name-server-lookup-is-a-useful-command-for-getting-information-from-the-dns-server-it-is-a-network-administration-tool-for-querying-the-domain-name-system-dns-to-obtain-domain-name-or-ip-address-mapping-or-any-other-specific-dns-record-it-is-also-used-to-troubleshoot-dns-related-problems","title":"Nslookup (stands for \u201cName Server Lookup\u201d) is a useful command for getting information from the DNS server. It is a network administration tool for querying the Domain Name System (DNS) to obtain domain name or IP address mapping or any other specific DNS record. It is also used to troubleshoot DNS-related problems.","text":"<p>Syntax of the -<code>nslookup</code>- command in Linux System <pre><code>nslookup [option] [hosts]\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#options-of-nslookup-command","title":"Options of nslookup command:","text":"Options Description -domain=[domain-name] <code>allows you to change the default DNS name.</code> -debug <code>enables the display of debugging information.</code> -port=[port-number] <code>Use the -port option to specify the port number for queries. By default, nslookup uses port 53 for DNS queries</code> -timeout=[seconds] <code>you can specify the time allowed for the DNS server to respond. By default, the timeout is set to a few seconds</code> -type=a <code>Lookup for a record We can also view all the available DNS records for a particular record using the -type=a option</code> -type=any <code>Lookup for any record We can also view all the available DNS records using the -type=any option.</code> -type=hinfo <code>displays hardware-related information about the host. It provides details about the operating system and hardware platform</code> -type=mx <code>Lookup for an mx record MX (Mail Exchange) maps a domain name to a list of mail exchange servers for that domain. The MX record says that all the mails sent to \u201cgoogle.com\u201d should be routed to the Mail server in that domain.</code> -type=ns <code>Lookup for an ns record NS (Name Server) record maps a domain name to a list of DNS servers authoritative for that domain. It will output the name serves which are associated with the given domain.</code> -type=ptr <code>used in reverse DNS lookups. It retrieves the Pointer (PTR) records, which map IP addresses to domain names.</code> -type=soa <code>Lookup for a soa record SOA record (start of authority), provides the authoritative information about the domain, the e-mail address of the domain admin, the domain serial number, etc\u2026</code>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-service","title":"Examples For K8S Service","text":"<p><pre><code>kubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre> <code>kubectl exec busybox -- nslookup nginx-svc</code> <pre><code>Name:   nginx-svc.default.svc.cluster.local\nAddress: 10.100.245.19\n\nnslookup: can't resolve 'kubernetes.default'\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-pod","title":"Examples For K8S Pod","text":"<p><pre><code>pod-ip-address.my-namespace.pod.cluster-domain.example\n</code></pre> <code>kubectl exec busybox -- nslookup 10-244-1-2.default.pod.cluster.local</code> <pre><code>172-17-0-3.default.pod.cluster.local\n</code></pre></p>"},{"location":"devops/linux/shell/scp/","title":"SCP Command","text":"<p>SCP (secure copy) is a command-line utility that allows you to securely copy files and directories between two locations.</p> <p>From your local system to a remote system. <pre><code>scp -i \"pam.pem\" /home/kullanici/dizin/local_file.txt ubuntu@18.204.206.157:/home/ubuntu/\n</code></pre> From a remote system to your local system. <pre><code>scp -i \"pam.pem\" -r ubuntu@3.86.225.192:/home/ubuntu/  .\n</code></pre></p> <p>Attention  pam.pem is the password file of Ec2 instance</p> <p><code>scp</code> provides a number of options that control every aspect of its behavior. The most widely used options are:</p> <p><code>-P</code> - Specifies the remote host ssh port.</p> <p><code>-p</code> - Preserves files modification and access times.</p> <p><code>-q</code> - Use this option if you want to suppress the progress meter and non-error messages.</p> <p><code>-C</code> - This option forces scp to compresses the data as it is sent to the destination machine.</p> <p><code>-r</code> - This option tells scp to copy directories recursively.</p>"},{"location":"devops/linux/shell/script/","title":"<code>script</code> command","text":"<pre><code>man script\n</code></pre>"},{"location":"devops/linux/shell/script/#save-your-terminal-session","title":"Save your terminal session","text":"<pre><code>script -a -t 5 sav-my-session-name.log\n\n# do stuff\n\nexit\n# run cat sav-my-session-name.log to see the output\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/","title":"ELK Stack with FileBeat","text":"<p>The ELK Stack (Elasticsearch, Logstash, and Kibana) is the world\u2019s most popular open-source log analysis platform. ELK is quickly overtaking existing proprietary solutions and becoming companies\u2019 top choice for log analysis and management solutions. There is one more component \u2014 Beats \u2014 which collects the data and sends it to Logstash. This led Elastic to rename ELK as the Elastic Stack.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is a NoSQL database. It is based on Lucene search engine, and it is built with RESTful APIS. Elasticsearch offers simple deployment, maximum reliability, and easy management. It also offers advanced queries to perform detail analysis and stores all the data centrally. It is helpful for executing a quick search of the documents. Elasticsearch also allows you to store, search and analyze big volume of data. Modern web and mobile applications have adopted it in search engine platforms.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#why-use-elasticsearch","title":"Why use Elasticsearch?","text":"<ul> <li> <p>Search: The main advantage of using Elasticsearch is it\u2019s rapid and accurate search functionality. For large datasets, relational databases takes a lot more time for search queries because of the number of joins the query has to go through.</p> </li> <li> <p>Scaling: Distributed architecture of Elasticsearch allows you to scale a lot of servers and data. We can scale the clusters to hundreds of nodes and also we can replicate data to prevent data loss in case of a node failure.</p> </li> <li> <p>Analytical engine: Elasticsearch analytical use case has become more popular than the search use case. Elasticsearch is specifically used for log analysis</p> </li> </ul>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#logstash","title":"Logstash","text":"<p>Logstash is a  open-source powerful tool for obtaining, filtering, and normalizing log files. A wide range of plugins for input, output and filtering specifications gives the user a great opportunity to easily configure Logstash to collect, process and channel logs data in many different architectures.</p> <p>Working with log files is divided into one or more pipelines. In each configured pipeline, one or more input plugins retrieve or gather data that is then placed on an internal queue. Process handling threads read queued data in small data series and process these batches via specified filter plugins in sequence.</p> <p></p> <p>After finishing data processing, threads send the data to the related output plugins which in turn are responsible for formatting and sending data to Elasticsearch or any other corresponding engine.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#filebeat","title":"FileBeat","text":"<p>Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.</p> <p>How Filebeat works: When you start Filebeat, it starts one or more inputs that look in the locations you\u2019ve specified for log data. For each log that Filebeat locates, Filebeat starts a harvester. Each harvester reads a single log for new content and sends the new log data to libbeat, which aggregates the events and sends the aggregated data to the output that you\u2019ve configured for Filebeat.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#kibana","title":"Kibana","text":"<p>Kibana is a data visualization which completes the ELK stack. This tool is used for visualizing the Elasticsearch documents and helps developers to have a quick insight into it. Kibana dashboard offers various interactive diagrams, geospatial data, and graphs to visualize complex queries.</p> <p>It is used to search, view, and interact with data stored in Elasticsearch directories and helps you to perform advanced data analysis and visualize your data in a variety of tables, charts, and maps.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#demo","title":"Demo","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#create-aks-cluster","title":"Create AKS Cluster","text":"<p>To create an AKS cluster, use the az aks create command. The following example creates a cluster named myAKSCluster with one node and enables a system-assigned managed identity.</p> <pre><code>az group create -l westus -n MyResourceGroup\n</code></pre> <pre><code>az aks create --resource-group myResourceGroup --name myAKSCluster --enable-managed-identity --node-count 3 -l westus\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#connect-to-the-cluster","title":"Connect to the cluster","text":"<p>Configure kubectl to connect to your Kubernetes cluster using the az aks get-credentials command. This command downloads credentials and configures the Kubernetes CLI to use them.</p> <pre><code>az aks get-credentials --resource-group myResourceGroup --name myAKSCluster --overwrite-existing\n</code></pre> <p>Verify the connection to your cluster using the kubectl get command. This command returns a list of the cluster nodes.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#install-elk-stack-in-aks-cluster-using-helm","title":"Install ELK stack in AKS cluster using Helm:","text":"<ul> <li>Create  namespace with the name of elk.</li> </ul> <p><pre><code>kubectl create ns elk\nkubectl get ns\n</code></pre> - Install ELK Stack helm repo into your local repo with helm command.</p> <p><pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> - Update your repo after installation.</p> <p><pre><code>helm repo update\n</code></pre> - Liste your repo packages.</p> <pre><code>helm repo ls\n</code></pre> <ul> <li>List your helm chart and manifest files.</li> </ul> <pre><code>helm search repo elastic\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elk-stack-elasticsearch-filebeat-logstashkibana-installation-and-configuration-in-aks-cluster-with-helm","title":"ELK Stack (Elasticsearch, Filebeat, Logstash,Kibana) installation and configuration in AKS cluster with Helm:","text":"<ul> <li>Save your elastic/elasticsearch values and save it as elasticsearch.values file in order to make some configuration.</li> </ul> <p><pre><code>helm show values elastic/elasticsearch &gt;&gt; elasticsearch.values\n</code></pre> elasticsearch.values</p> <pre><code>---\nclusterName: \"elasticsearch\"\nnodeGroup: \"master\"\n\nroles:\n  - master\n  - data\n  - data_content\n  - data_hot\n  - data_warm\n  - data_cold\n  - ingest\n  - ml\n  - remote_cluster_client\n  - transform\n\nreplicas: 2\nminimumMasterNodes: 2\nimage: \"docker.elastic.co/elasticsearch/elasticsearch\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\nnetworkHost: \"0.0.0.0\"\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 30Gi\n\npersistence:\n  enabled: true\n  labels:\n    enabled: false\n  annotations: {}\n\nenableServiceLinks: true\n\nprotocol: https\nhttpPort: 9200\ntransportPort: 9300\n\nservice:\n  enabled: true\n  labels: {}\n  labelsHeadless: {}\n  type: ClusterIP\n  publishNotReadyAddresses: false\n  nodePort: \"\"\n  annotations: {}\n  httpPortName: http\n  transportPortName: transport\n  loadBalancerIP: \"\"\n  loadBalancerSourceRanges: []\n  externalTrafficPolicy: \"\"\n\nmaxUnavailable: 1\nsysctlVmMaxMapCount: 262144\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n\n\ntests:\n  enabled: true\n</code></pre> <pre><code>helm install elasticsearch elastic/elasticsearch -f elasticsearch.values -n elk\n</code></pre> <pre><code>helm ls -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-kibana-via-helm-into-aks-cluster","title":"Installation and configuration of Kibana via Helm into AKS Cluster:","text":"<p><pre><code>helm show values elastic/kibana &gt;&gt; kibana.values\n</code></pre> - Change the values configuration with LoadBalancer.</p> <pre><code>---\nelasticsearchHosts: \"https://elasticsearch-master:9200\"\n\nreplicas: 1\n\n\nimage: \"docker.elastic.co/kibana/kibana\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n\nprotocol: http\n\nserverHost: \"0.0.0.0\"\n\nhealthCheckPath: \"/app/kibana\"\n\nautomountToken: true\n\nhttpPort: 5601\n\nservice:\n  type: LoadBalancer\n  loadBalancerIP: \"\"\n  port: 5601\n  nodePort: \"\"\n  labels: {}\n  annotations: {}\n  loadBalancerSourceRanges: []\n  httpPortName: http\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n</code></pre> <pre><code>helm install kibana elastic/kibana -f kibana.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-logstash-via-helm-into-aks-cluster","title":"Installation and configuration of logstash via Helm into AKS Cluster:","text":"<ul> <li>Installation of logstash via Helm.</li> </ul> <pre><code>helm show values elastic/logstash &gt;&gt; logstash.values\n</code></pre> <ul> <li>logstash.values ---&gt; !!! update elasticsearch password you can access alestichsearcg password this command: </li> </ul> <pre><code>kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode\n</code></pre> <pre><code>---\nreplicas: 1\n\nlogstashConfig:\n  logstash.yml: |\n    http.host: 0.0.0.0\n    # xpack.monitoring.enabled: false\n\n\nlogstashPipeline:\n  logstash.conf: |\n    input {\n      beats {\n        port =&gt; 5044\n      }\n    }\n    output {\n      elasticsearch {\n        hosts =&gt; [ \"https://elasticsearch-master:9200\" ]\n        ssl =&gt; true\n        manage_template =&gt; false\n        ssl_certificate_verification =&gt; true\n        index =&gt; \"logstash-%{+YYYY.MM.dd}\"\n        document_type =&gt; \"%{[@metadata][type]}\"\n        cacert =&gt; \"/usr/share/logstash/certs/ca.crt\"\n        user =&gt; \"${ELASTICSEARCH_USERNAME}\"\n        password =&gt; \"${ELASTICSEARCH_PASSWORD}\"  #update password\n      }\n    }\n\n\nimage: \"docker.elastic.co/logstash/logstash\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nextraEnvs:\n  - name: \"ELASTICSEARCH_USERNAME\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: username\n  - name: \"ELASTICSEARCH_PASSWORD\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: password\n\n\nsecretMounts:\n  - name: elasticsearch-master-certs\n    secretName: elasticsearch-master-certs\n    path: /usr/share/logstash/certs/\n\n\nlogstashJavaOpts: \"-Xmx1g -Xms1g\"\n\nresources:\n  requests:\n    cpu: \"100m\"\n    memory: \"1536Mi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"1536Mi\"\n\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n\n\nhttpPort: 9600\n\nmaxUnavailable: 1\n\nlivenessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 300\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n\nreadinessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 3\n\nservice:\n  annotations: {}\n  type: ClusterIP\n  loadBalancerIP: \"\"\n  ports:\n    - name: beats\n      port: 5044\n      protocol: TCP\n      targetPort: 5044\n    - name: http\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n</code></pre> <pre><code>helm install logstash elastic/logstash -f logstash.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-filebeat-via-helm-into-aks-cluster","title":"Installation and configuration of Filebeat via Helm into AKS Cluster:","text":"<ul> <li>Installation of Filebeat via Helm.</li> </ul> <pre><code>helm show values elastic/filebeat &gt;&gt; filebeat.values\n</code></pre> <ul> <li>filebeat.values</li> </ul> <pre><code>---\ndaemonset:\n  enabled: true\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n      - type: container\n        paths:\n          - /var/log/containers/*.log\n        processors:\n        - add_kubernetes_metadata:\n            host: ${NODE_NAME}\n            matchers:\n            - logs_path:\n                logs_path: \"/var/log/containers/\"\n\n      output.logstash:\n        hosts: [\"logstash-logstash:5044\"]\n\n  maxUnavailable: 1\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n\ndeployment:\n\n  enabled: false\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n        - type: log\n          paths:\n            - /usr/share/filebeat/logs/filebeat\n\n      output.elasticsearch:\n        host: \"${NODE_NAME}\"\n        hosts: '[\"https://${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}\"]'\n        username: \"${ELASTICSEARCH_USERNAME}\"\n        password: \"${ELASTICSEARCH_PASSWORD}\"\n        protocol: https\n        ssl.certificate_authorities: [\"/usr/share/filebeat/certs/ca.crt\"]\n\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n  securityContext:\n    runAsUser: 0\n    privileged: false\n  resources:\n    requests:\n      cpu: \"100m\"\n      memory: \"100Mi\"\n    limits:\n      cpu: \"1000m\"\n      memory: \"200Mi\"\n\nreplicas: 1\n\nhostPathRoot: /var/lib\n\nimage: \"docker.elastic.co/beats/filebeat\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nimagePullSecrets: []\n\nlivenessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        curl --fail 127.0.0.1:5066\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nreadinessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        filebeat test output\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nmanagedServiceAccount: true\n\nclusterRoleRules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n      - nodes\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"apps\"\n    resources:\n      - replicasets\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre> <pre><code>helm install filebeat elastic/filebeat -f filebeat.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#deployment-of-sample-applications-into-aks-kubernetes-environment","title":"Deployment of sample applications into AKS kubernetes environment:","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: db-pv-vol\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/home/ubuntu/pv-data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: database-persistent-volume-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: manual\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mongo\n  template:\n    metadata:\n      labels:\n        name: mongo\n        app: todoapp\n    spec:\n      containers:\n      - image: mongo:5.0\n        name: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n          - name: mongo-storage\n            mountPath: /data/db\n      volumes:\n        #- name: mongo-storage\n        #  hostPath:\n        #    path: /home/ubuntu/pv-data\n        - name: mongo-storage\n          persistentVolumeClaim:\n            claimName: database-persistent-volume-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: db-service\n  labels:\n    name: mongo\n    app: todoapp\nspec:\n  selector:\n    name: mongo\n  type: ClusterIP\n  ports:\n    - name: db\n      port: 27017\n      targetPort: 27017\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: web\n  template:\n    metadata:\n      labels:\n        name: web\n        app: todoapp\n    spec:\n      containers: \n        - image: ersinsari/todo\n          imagePullPolicy: Always\n          name: myweb\n          ports: \n            - containerPort: 3000\n          env:\n            - name: \"DBHOST\"\n              value: db-service\n          resources:\n            limits:\n              memory: 500Mi\n              cpu: 100m\n            requests:\n              memory: 250Mi\n              cpu: 80m  \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\n  labels:\n    name: web\n    app: todoapp\nspec:\n  selector:\n    name: web \n  type: LoadBalancer\n  ports:\n   - name: http\n     port: 3000\n     targetPort: 3000\n     protocol: TCP\n</code></pre> <pre><code>kubectl apply -f to_do.yaml\n</code></pre> <pre><code>kubectl get all\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#part-5-kibana-dashboard-configuration-and-sample-app-log-monitoring","title":"Part-5 Kibana Dashboard configuration and sample app log monitoring:","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#dashboard-configuration-and-index-pattern-creation","title":"Dashboard configuration and index pattern creation:","text":"<ul> <li>Go to http://loadbalancer-ip:5601</li> </ul> <p>username: elastic password: $(kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode)</p> <ul> <li> <p>Discover: create an index pattern</p> </li> <li> <p>logstash-*</p> </li> <li> <p>select @timestamp</p> </li> <li> <p>create an index pattern.</p> </li> <li> <p>filter your data using KQL syntax : kubernetes.deployment.name web-deployment</p> </li> <li>You can explore logs info</li> </ul>"},{"location":"devops/logging/Index-Lifecycle-Management/","title":"Elasticsearch Index Lifecycle Management","text":""},{"location":"devops/logging/Index-Lifecycle-Management/#indexing","title":"Indexing:","text":"<p>Indices based on the indexname defined on Logstash need to be created to be visible on Kibana. To do this, the pattern specified in the left menu is written and created by clicking on the Create Index Pattern section under Stack management. </p> <p></p>"},{"location":"devops/logging/Index-Lifecycle-Management/#create-repository","title":"Create Repository:","text":"<p>However, a repo must be created and registered to keep snapshots.</p> <p>/Stack management/Snapshot and Restore/Repositories</p> <p>Here repo elasticsearch can be on a separate server as it could be on the built-in server. Importantly, this directory should be specified in the elasticsearch configuration file (elasticSearch.yaml, values.jaml). If it will be on another server, it should be mounted.</p> <p></p> <p>For example, commands executed on the Elasticsearch server for a separate server to be used as an NFS server:</p> <pre><code>sudo apt install nfs-common\n\nsudo apt install cifs-utils\n\nsudo mount.nfs &lt;path on nfs server&gt; &lt;path on elasticserach server&gt;\n\nsudo  mount.nfs :/mnt/disk2/elasticmount /mnt/elasticmount \n\nchown -R elasticsearch:elasticsearch elasticmount \n</code></pre>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-template","title":"Index Template:","text":"<p>A template should be created to manage the created indexes and define a lifecycle policy.</p> <p>/Stack management/ Index Management/Index Templates</p> <p>Here, click create template to create a template that belongs to a particular pattern.</p> <p></p> <p>Under Index settings section:</p> <p>Add:</p> <p>{ \"index\": {\"lifecycle\": { \"name\": \"kubernetes-pod-policy\" } } }</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-lifecycle-policy","title":"Index Lifecycle Policy:","text":"<p>A policy is created for what to do with the indexes of the specified pattern. </p> <p>For this;</p> <p>Click /Stack management/ Index Lifecycle Policies</p> <p>Here a new policy is created with create Policy.</p> <p></p> <p>This section specifies how long it will last in which phase and what to do during that time. </p> <p>For example, the policy of given pod-logs is to remove indexes with a one-hour lifetime in the warm phase after the replica numbers are drawn to 0 (to avoid holding space), and the policy is to delete indexes that have a seven-day lifecycle in the delete phase when the snapshot policy is applied.</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#snapshot-policy","title":"Snapshot Policy:","text":"<p>It can be deleted by taking a snapshot at certain intervals so that the specified indexes do not hold their place. If necessary, it can be restored from these snapshots. </p> <p>A policy is defined for taking these snapshots.</p> <p>/Stack management/Snapshot and Restore/Policies</p> <p></p> <p>For example, when creating a kubernetes-pod-daily-snapshot policy in the form;</p> <ul> <li> <p>The snapshot to be taken is created on a day-based basis, defined as , <li> <p>specified in which repository the snapshot to be taken will be held, defined by the schedule of the time of the day,</p> </li> <li> <p>specified which pattern index is to be taken,</p> </li> <li> <p>The validity period of this snapshot is specified (expiration - after which time deletion permission is given),</p> </li> <li> <p>This policy specifies the number of snapshots to hold min and max.</p> </li>"},{"location":"devops/logging/Index-Lifecycle-Management/#restore-snapshots","title":"Restore snapshots:","text":"<p>To restore snapshots taken on a specific date</p> <p>Stack management/Snapshot and Restore/snapshot</p> <p></p> <p>Here you click the snapshot of the day. The restore button will be clicked on the screen that opens. </p> <p>Here the snapshots will belong to more than a day. However, it should not be forgotten that it is taken incrementally.</p> <p>To restore a day's snapshot, untick Data streams and indices and click deselect all below. The restore is then done by clicking on the index of the desired day.</p>"},{"location":"devops/logging/efk/","title":"EFK Stack (Elasticsearch, Fluentbit, Kibana) via Minikube","text":"<p>EFK is a popular logging stack used to collect, store, and analyze logs in Kubernetes. \ud83d\udc49  Elasticsearch: Stores and indexes log data for easy retrieval. \ud83d\udc49  Fluentbit: A lightweight log forwarder that collects logs from different sources and sends them to Elasticsearch. \ud83d\udc49  Kibana: A visualization tool that allows users to explore and analyze logs stored in Elasticsearch.</p> <p></p>"},{"location":"devops/logging/efk/#step-by-step-setup-on-minikube","title":"Step-by-Step Setup on minikube","text":""},{"location":"devops/logging/efk/#create-cluster","title":"Create cluster","text":"<pre><code>minikube start\n</code></pre>"},{"location":"devops/logging/efk/#create-namespace-for-logging","title":"Create Namespace for Logging","text":"<pre><code>kubectl create namespace logging\n</code></pre>"},{"location":"devops/logging/efk/#install-elasticsearch-on-k8s","title":"Install Elasticsearch on K8s","text":"<pre><code>helm repo add elastic https://helm.elastic.co\n\nhelm install elasticsearch \\\n --set replicas=1 \\\n --set persistence.enabled=false elastic/elasticsearch -n logging\n</code></pre> <p>### Install Kibana</p> <pre><code>helm install kibana  elastic/kibana -n logging\nkubectl port-forward svc/kibana-kibana 5601:5601 -n logging\n</code></pre> <ul> <li>Go to localhost:5601 and login Kibana</li> </ul>"},{"location":"devops/logging/efk/#retrieve-elasticsearch-username-password","title":"Retrieve Elasticsearch Username &amp; Password","text":"<pre><code># for username\nkubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d\n# for password\nkubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"devops/logging/efk/#install-fluentbit-with-custom-valuesconfigurations","title":"Install Fluentbit with Custom Values/Configurations","text":"<p>\ud83d\udc49 Note: Please update the HTTP_Passwd field in the fluentbit-values.yml file with the password retrieved earlier in step 6: (i.e NJyO47UqeYBsoaEU)\"</p> <p>fluentbit-values.yaml</p> <pre><code>kind: DaemonSet\nreplicaCount: 1\n\nimage:\n  repository: cr.fluentbit.io/fluent/fluent-bit\n\nservice:\n  type: ClusterIP\n  port: 2020\n\n\n\nluaScripts: {}\n\n## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file\nconfig:\n  service: |\n    [SERVICE]\n        Daemon Off\n        Flush {{ .Values.flush }}\n        Log_Level info\n        Parsers_File /fluent-bit/etc/parsers.conf\n        Parsers_File /fluent-bit/etc/conf/custom_parsers.conf\n        HTTP_Server On\n        HTTP_Listen 0.0.0.0\n        HTTP_Port {{ .Values.metricsPort }}\n        Health_Check On\n\n  ## https://docs.fluentbit.io/manual/pipeline/inputs\n  inputs: |\n    [INPUT]\n        Name tail\n        Path /var/log/containers/webapp-deployment*.log\n        multiline.parser docker, cri\n        Tag kube.*\n        Mem_Buf_Limit 5MB\n        Skip_Long_Lines On\n\n\n\n  ## https://docs.fluentbit.io/manual/pipeline/filters\n  filters: |\n    [FILTER]\n        Name kubernetes\n        Match kube.*\n        Merge_Log On\n        Keep_Log Off\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n\n  # https://docs.fluentbit.io/manual/pipeline/outputs\n  outputs: |\n    [OUTPUT]\n        Name es\n        Match kube.*\n        Type  _doc\n        Host elasticsearch-master\n        Port 9200\n        HTTP_User elastic\n        HTTP_Passwd xB7UdQgmGkgRaCrH\n        tls On\n        tls.verify Off\n        Logstash_Format On\n        Logstash_Prefix ersin-fluntbit\n        Retry_Limit False\n        Suppress_Type_Name On\n\n\n  ## https://docs.fluentbit.io/manual/pipeline/parsers\n  customParsers: |\n    [PARSER]\n        Name docker_no_time\n        Format json\n        Time_Keep Off\n        Time_Key time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L\n\n\nvolumeMounts:\n  - name: config\n    mountPath: /fluent-bit/etc/conf\n\ndaemonSetVolumes:\n  - name: varlog\n    hostPath:\n      path: /var/log\n  - name: varlibdockercontainers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: etcmachineid\n    hostPath:\n      path: /etc/machine-id\n      type: File\n\ndaemonSetVolumeMounts:\n  - name: varlog\n    mountPath: /var/log\n  - name: varlibdockercontainers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: etcmachineid\n    mountPath: /etc/machine-id\n    readOnly: true\n\ncommand:\n  - /fluent-bit/bin/fluent-bit\n\nargs:\n  - --workdir=/fluent-bit/etc\n  - --config=/fluent-bit/etc/conf/fluent-bit.conf\n</code></pre> <pre><code>helm repo add fluent https://fluent.github.io/helm-charts\nhelm upgrade --install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging\n</code></pre> <p>Service Section Defines global configuration settings for the Fluent Bit service.</p> <p>Daemon Off: Runs Fluent Bit in the foreground. Flush {{ .Values.flush }}: Sets the flush interval for sending data, using a configurable Helm value. Log_Level info: Sets the logging level to info. Parsers_File: Specifies parser configuration files. HTTP_Server On: Enables the built-in HTTP server for metrics and health checks. HTTP_Listen 0.0.0.0: Sets the HTTP server to listen on all network interfaces. HTTP_Port {{ .Values.metricsPort }}: Sets the HTTP server port for metrics, using a Helm value.</p> <p>Inputs Section Defines where Fluent Bit collects logs from.</p> <p>[INPUT] Name tail: Specifies the input plugin tail to read log files. Path /var/log/containers/webapp-deployment.log: Specifies the log file path to monitor. multiline.parser docker, cri: Enables multi-line log parsing using Docker and CRI parsers. Tag kube.: Tags logs with a kube.* prefix for filtering. Mem_Buf_Limit 5MB: Sets the memory buffer limit to 5MB. Skip_Long_Lines On: Skips lines longer than the buffer limit.</p> <p>Filters Section Processes and enriches logs before output.</p> <p>[FILTER] Name kubernetes: Uses the kubernetes filter plugin for Kubernetes metadata enrichment. Match kube.: Filters logs with tags matching kube.. Merge_Log On: Combines partial log lines into a single entry. Keep_Log Off: Drops the original unparsed log after merging. K8S-Logging.Parser On: Uses parsers for logs based on Kubernetes metadata. K8S-Logging.Exclude On: Excludes logs that don't match certain Kubernetes metadata.</p> <p>Output Section</p> <p>[OUTPUT] Name es: Specifies the Elasticsearch output plugin. Match kube.: Sends logs with tags matching kube.. Type _doc: Specifies the document type (deprecated in modern Elasticsearch versions). Host elasticsearch-master, Port 9200: Sets the Elasticsearch host and port. HTTP_User, HTTP_Passwd: Provides authentication credentials for Elasticsearch. tls On: Enables TLS for secure communication. tls.verify Off: Disables certificate verification (not recommended in production). Logstash_Format On: Formats logs in Logstash-compatible JSON. Logstash_Prefix ersin-fluntbit: Sets the prefix for Elasticsearch index names. Retry_Limit False: Disables retrying on output failures.</p> <p>Custom Parsers Section Defines custom log parsing rules.</p> <p>[PARSER] Name docker_no_time: Names the parser docker_no_time. Format json: Specifies that the log format is JSON. Time_Keep Off: Ignores the timestamp from the original log. Time_Key time: Specifies the JSON key for extracting timestamps. Time_Format %Y-%m-%dT%H:%M:%S.%L: Defines the timestamp format with milliseconds.</p>"},{"location":"devops/logging/efk/#deploy-app-for-log","title":"Deploy App for log","text":"<p>python-app-service.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\n  namespace: app\n  labels:\n    app: webapp\nspec:\n  selector:\n    app: webapp\n  ports:\n  - port: 80\n    targetPort: 5005\n</code></pre> <p>python-app-deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-deployment\n  namespace: app\n  labels:\n    app: webapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: ersinsari/fluentbit-python:latest\n        volumeMounts:\n        - mountPath: /log\n          name: log-volume\n      volumes:\n      - name: log-volume\n        hostPath:\n          path: /var/log/webapp\n          type: DirectoryOrCreate\n</code></pre></p> <pre><code>kubectl create ns app\nkubectl apply -f python-app-service.yaml\nkubectl apply -f python-app-deployment.yaml\n</code></pre>"},{"location":"devops/logging/efk/#access-the-app-and-generate-log","title":"Access the App and generate log","text":"<pre><code>kubectl port-forward svc/webapp-service 5001:80 -n app\n</code></pre> <pre><code>username: ersin\npassword: password\n</code></pre> <p>If you enter username and credential right generate info log but wrong generate warn log</p> <p>Go to Kibana UI --&gt; Stack Management -- &gt; Kibana --&gt; Data View create new data view</p> <pre><code>name: ersin-fluntbit\nfilter: ersin-fluntbit*\n</code></pre> <pre><code>{\"level\": \"INFO\", \"message\": \"Response sent with status: 200\", \"time\": \"2025-01-20 18:59:49,375\", \"logger\": \"app\", \"pathname\": \"/app/app.py\", \"lineno\": 55, \"funcname\": \"log_response_info\", \"request\": {\"method\": \"GET\", \"url\": \"http://localhost:5001/second_level_auth\", \"remote_addr\": \"127.0.0.1\", \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}}\n</code></pre> <pre><code>{\"level\": \"WARNING\", \"message\": \"Login failed for user: ersin\", \"time\": \"2025-01-20 18:59:49,364\", \"logger\": \"app\", \"pathname\": \"/app/app.py\", \"lineno\": 75, \"funcname\": \"login\", \"request\": {\"method\": \"POST\", \"url\": \"http://localhost:5001/login\", \"remote_addr\": \"127.0.0.1\", \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}}\n</code></pre> <ul> <li>Go to Discover section and check logs</li> </ul> <p>The application logs were sent to the index in JSON format using a parser, which split the logs into fields. This allows for faster queries and makes it easier to find specific logs.</p> <p></p>"},{"location":"devops/logging/elastalert2/","title":"Elastalert2","text":""},{"location":"devops/logging/elastalert2/#elastalert2","title":"elastalert2","text":"<ul> <li>Introduction</li> </ul> <p>ElastAlert 2 is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch and OpenSearch.</p> <ul> <li>As a Kubernetes deployment</li> </ul> <p>The Docker container for ElastAlert 2 can be used directly as a Kubernetes deployment, but for convenience, a Helm chart is also available. See the Chart Readme for more information on how to install, configure, and run the chart.</p> <ul> <li>install and modified values.yaml</li> </ul> <pre><code>wget https://raw.githubusercontent.com/jertel/elastalert2/refs/heads/master/chart/elastalert2/values.yaml\n\nmv values.yaml elastalert2.yaml \n\nvi elastalert2.yaml \n````\n\n```yaml\nenabledRules: [\"deadman_slack\" ] # should match the name in the rule field\nelasticsearch:\n  host: elasticsearch-master # elasticsearch service name\n  port: 9200\n  useSsl: \"True\"\n  username: \"\"\n  password: \"\"\n  credentialsSecret: \"elasticsearch-master-credentials\" # elasticsearch secret name for user-password credential\n  credentialsSecretUsernameKey: \"username\" # username key in elasticsearch secret\n  credentialsSecretPasswordKey: \"password\" # password key in elasticsearch secret\n  verifyCerts: \"True\" # true for connection to elacticsearch with cert\n  caCerts: \"/certs/ca.crt\" # ca.crt --&gt; name in the elasticsearch cert secret\n  certsVolumes:\n    - name: es-certs\n      secret:\n        defaultMode: 420\n        secretName: elasticsearch-master-certs # elasticsearch cert secret name\n  certsVolumes:\n  certsVolumeMounts:\n    - name: es-certs\n      mountPath: /certs\n      readOnly: true\n\nrules:\n  deadman_slack: |-\n    ---\n    name: Deadman Switch Slack  \n    type: frequency\n    index: \"&lt;index_name&gt;\" \n    num_events: 2\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"&lt;message include string&gt;&gt;\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n</code></pre>"},{"location":"devops/logging/elastalert2/#rules","title":"Rules","text":"<ul> <li>Several rule types with common monitoring paradigms are included with ElastAlert 2</li> </ul> <p>frequency, spike, flatline, blacklist, whitelist etc..</p> <p>*Match when there are more than X events in Y time use frequency Exmp:</p> <pre><code>rules:\n  deadman_slack: |-\n    ---\n    name: deadman_slack more log then x event in y minutes\n    type: frequency\n    index: \"devops-dev\"\n    num_events: 2\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"log text\"\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n</code></pre> <p>*Match when there are less than X events in Y time use flatline Exmp:</p> <pre><code>  deadman_slack2: |-\n    ---\n    name: deadman_slack2 less log then 3 event\n    type: flatline\n    index: \"&lt;index_name&gt;\"\n    threshold: 3\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"log text\"\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n    slack_msg_color: good\n</code></pre>"},{"location":"devops/logging/elastalert2/#alerts","title":"Alerts","text":"<ul> <li>Each rule may have any number of alerts attached to it. Alerts are subclasses of Alerter and are passed a dictionary, or list of dictionaries, from ElastAlert 2 which contain relevant information. They are configured in the rule configuration file similarly to rule types.</li> </ul> <p>exmp:</p> <pre><code>alert:\n - email\nfrom_addr: \"no-reply@example.com\"\nemail: \"customer@example.com\"\n</code></pre> <ul> <li>Alert types</li> </ul> <pre><code>alert:\n  - datadog\n  - debug\n  - dingtalk\n  - discord\n  - email\n  - gitter\n  - googlechat\n  - jira\n  - ms_teams\n  - opsgenie\n  - slack\n  - sns\n  - stomp\n  - telegram\n  - twilio\n  - zabbix\n  ...\n  ...\n</code></pre> <ul> <li>install elastalert2 </li> </ul> <pre><code>helm repo add elastalert2 https://jertel.github.io/elastalert2/\n\nhelm upgrade --install elastalert2 elastalert2/elastalert2 --create-namespace -n logging -f elastalert2.yaml \n</code></pre> <ul> <li>Alert Notification exmp. (more log then x event in y minutes, type: frequency )</li> </ul> <p></p>"},{"location":"devops/logging/elasticsearch-exporter/","title":"Elasticsearch-Exporter","text":"<p>https://github.com/prometheus-community/elasticsearch_exporter</p> <p>The Elasticsearch Exporter is a tool that may be utilized to check the performance and health of Elasticsearch. This application gathers metrics and information from Elasticsearch and makes them available to Prometheus, a widely used open-source monitoring system. The Elasticsearch Exporter enables you to monitor a range of metrics, including cluster, node, and index-level information. These metrics encompass CPU utilization, memory usage, indexing rate, search rate, and other relevant data.</p>"},{"location":"devops/logging/elasticsearch-exporter/#step-by-step-guide-to-configure-elasticsearch-exporter","title":"Step-by-step guide to configure Elasticsearch Exporter","text":""},{"location":"devops/logging/elasticsearch-exporter/#create-a-system-user-for-elasticsearch","title":"Create a system user for Elasticsearch","text":"<pre><code>sudo useradd elastic_search\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#download-elasticsearch-exporter","title":"Download Elasticsearch Exporter","text":"<pre><code>sudo wget https://github.com/prometheus-community/elasticsearch_exporter/releases/download/v1.7.0/elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#extract-the-targz-file","title":"Extract the tar.gz file","text":"<pre><code>sudo tar -xvzf elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#move-to-the-extracted-directory","title":"Move to the extracted directory","text":"<pre><code>cd elasticsearch_exporter-1.7.0.linux-amd64/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#copy-the-exporter-binary-to-usrlocalbin","title":"Copy the exporter binary to /usr/local/bin/","text":"<pre><code>sudo cp elasticsearch_exporter /usr/local/bin/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#change-ownership-of-the-exporter-binary","title":"Change ownership of the exporter binary","text":"<pre><code>sudo chown elastic_search:elastic_search /usr/local/bin/elasticsearch_exporter\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#create-a-systemd-service-file","title":"Create a systemd service file","text":"<p>Copy the /etc/elasticsearch/certs/http_ca.crt file to the /home/elastic_search directory and set the necessary permissions with the chown elastic_search:elasticsearch /home/elastic_search/http_ca.crt command.</p> <p><pre><code>sudo vim /etc/systemd/system/elasticsearch_exporter.service\n-----------------------------------------------------------\n[Unit]\nDescription=Prometheus ES_exporter\nAfter=local-fs.target network-online.target network.target\nWants=local-fs.target network-online.target network.target\n[Service]\nUser=elastic_search\nNice=10\nExecStart=/usr/local/bin/elasticsearch_exporter --es.uri=https://elastic:password@localhost:9200 --es.ca /home/elastic_search/http_ca.crt  --es.all --es.indices --es.timeout 20s\nExecStop=/usr/bin/killall elasticsearch_exporter\n[Install]\nWantedBy=default.target\n</code></pre> ! Note: Update your user and password for elasticsearch</p>"},{"location":"devops/logging/elasticsearch-exporter/#start-the-elasticsearch-exporter-service-and-enable-the-service-to-start-on-boot","title":"Start the Elasticsearch Exporter service and enable the service to start on boot","text":"<p><pre><code>sudo systemctl start elasticsearch_exporter.service\nsudo systemctl enable elasticsearch_exporter.service\nsudo systemctl status elasticsearch_exporter.service\n</code></pre> !Note: Elastic search exporter uses port 9114, therefore expose it within the VPC in the security group.</p>"},{"location":"devops/logging/elasticsearch-exporter/#configure-prometheusyml","title":"Configure Prometheus.yml","text":"<p>To pull data from the elasticsearch-exporter into Prometheus and Grafana, you should update the prometheus.yml file with the following code, replacing the Elasticsearch node IP addresses as needed</p> <pre><code>    additionalScrapeConfigs:\n    - job_name: 'elasticsearch-exporter'\n      static_configs:\n        - targets: ['&lt;elasticsearch-node-ip-1&gt;:9114', '&lt;elasticsearch-node-ip-2&gt;:9114', '&lt;elasticsearch-node-ip-3&gt;:9114']\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#add-grafana-dashboard-for-elasticsearch","title":"Add Grafana Dashboard for Elasticsearch","text":"<p>Go to Grafana WebUI Click Dashboard --&gt; New --&gt; New Dashboard --&gt; Import --&gt; Add 266 --&gt; load</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/logging/elasticsearch-exporter/#add-alert","title":"Add alert","text":"<p>You can use Alertmanager or Grafana Alert section.</p> <p>elasticsearch.rules.yml</p> <pre><code>groups:\n  - name: elasticsearch\n    rules:\n      - record: elasticsearch_filesystem_data_used_percent\n        expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)\n          / elasticsearch_filesystem_data_size_bytes\n      - record: elasticsearch_filesystem_data_free_percent\n        expr: 100 - elasticsearch_filesystem_data_used_percent\n      - alert: ElasticsearchTooFewNodesRunning\n        expr: elasticsearch_cluster_health_number_of_nodes &lt; 3\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          description: There are only {{$value}} &lt; 3 Elasticsearch nodes running\n          summary: Elasticsearch running on less than 3 nodes\n      - alert: ElasticsearchHeapTooHigh\n        expr: elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n          &gt; 0.9\n        for: 15m\n        labels:\n          severity: critical\n        annotations:\n          description: The heap usage is over 90% for 15m\n          summary: Elasticsearch node {{$labels.node}} heap usage is high\n</code></pre> <p>elasticsearch.rules</p> <pre><code># calculate filesystem used and free percent\nelasticsearch_filesystem_data_used_percent = 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes\nelasticsearch_filesystem_data_free_percent = 100 - elasticsearch_filesystem_data_used_percent\n\n# alert if too few nodes are running\nALERT ElasticsearchTooFewNodesRunning\n  IF elasticsearch_cluster_health_number_of_nodes &lt; 3\n  FOR 5m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"There are only {{$value}} &lt; 3 Elasticsearch nodes running\", summary=\"Elasticsearch running on less than 3 nodes\"}\n\n# alert if heap usage is over 90%\nALERT ElasticsearchHeapTooHigh\n  IF elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} &gt; 0.9\n  FOR 15m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"The heap usage is over 90% for 15m\", summary=\"Elasticsearch node {{$labels.node}} heap usage is high\"}\n</code></pre> <p>You can add slack,email or another connector for alert mechanism.</p>"},{"location":"devops/logging/elk-snapshot/","title":"Elasticsearch-Snapshot","text":"<p>\ud83d\udcc4 Elasticsearch-Snapshot-Doc-Link </p> <p>A snapshot is a backup of a running Elasticsearch cluster. You can use snapshots to:</p> <p>Regularly back up a cluster with no downtime Recover data after deletion or a hardware failure Transfer data between clusters Reduce storage costs by using searchable snapshots in the cold and frozen data tiers</p>"},{"location":"devops/logging/elk-snapshot/#self-managed-repository-types","title":"Self-managed repository types","text":"<p>If you manage your own Elasticsearch cluster, you can use the following built-in snapshot repository types:</p> <p>Azure Google Cloud Storage AWS S3 Shared file system Read-only URL Source-only</p>"},{"location":"devops/logging/elk-snapshot/#shared-file-system-snapshots","title":"Shared file system Snapshots","text":"<p>This guide explains how to configure an on-premises snapshot repository for Elasticsearch using an NFS shared directory. Follow each step carefully \u2014 every Elasticsearch node must be configured identically.</p>"},{"location":"devops/logging/elk-snapshot/#1-configure-elasticsearch-snapshot-directory-pathrepo","title":"1. Configure Elasticsearch Snapshot Directory (path.repo)","text":"<p>First, we must define a directory where Elasticsearch is allowed to write snapshots. \ud83d\udd39 IMPORTANT: Perform the following steps on every Elasticsearch node. Create the Snapshot Directory SSH into each Elasticsearch node and run:</p> <p><pre><code>sudo mkdir -p /mnt/es-snap\nsudo chown -R elasticsearch:elasticsearch /mnt/es-snap\nsudo chmod 770 /mnt/snap\n</code></pre> Add the Directory to elasticsearch.yml /etc/elasticsearch/elasticsearch.yml Add this line: <pre><code>path.repo: /mnt/es-snap\n</code></pre></p>"},{"location":"devops/logging/elk-snapshot/#1-configure-mount-path-on-elasticsaerch-vms","title":"1. Configure Mount-Path on Elasticsaerch VM's","text":"<p>Install NFS Client Packages on Elasticsearch VM's Ubuntu: <pre><code>sudo apt install nfs-common -y\n</code></pre></p> <p>RHEL / Rocky: <pre><code>sudo yum install nfs-utils -y\n</code></pre> Mount Path to the NFS Share <pre><code>sudo mount -t nfs &lt;NFS_IP&gt;:/mnt/es-snap /mnt/es-snap\nExample:\nsudo mount -t nfs 10.0.10.5:/mnt/es-snap /mnt/es-snap\n</code></pre></p> <p>Add Mount Point to fstab</p> <p>/etc/fstab</p> <pre><code>&lt;NFS_IP&gt;:/mnt/snap /mnt/snap nfs defaults 0 0 &gt; /etc/fstab\n</code></pre> <p>Restart Elasticsearch <pre><code>sudo systemctl restart elasticsearch\n</code></pre></p>"},{"location":"devops/logging/elk-snapshot/#5-register-the-snapshot-repository-in-elasticsearch","title":"5. Register the Snapshot Repository in Elasticsearch","text":"<p>Once NFS is mounted on all nodes and the directory is configured as path.repo, register the repository: Go to Elasticsearch DevTool and run below command <pre><code>PUT _snapshot/nfs_repo\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"/mnt/es-snap\",\n    \"compress\": true\n  }\n}\n</code></pre> Verify the Repository <pre><code>POST _snapshot/nfs_repo/_verify\n</code></pre></p> <p>If everything is configured correctly, Elasticsearch will return a success message.</p>"},{"location":"devops/logging/elk-snapshot/#create-snapshot-policy","title":"Create Snapshot Policy","text":"<p>Go to Elasticsearch DevTool and run below command <pre><code>PUT _slm/policy/daily-snap\n{\n  \"schedule\": \"0 30 1 * * ?\",\n  \"name\": \"&lt;daily-snap-{now/d}&gt;\",\n  \"repository\": \"nfs_repo\",\n  \"config\": {\n    \"indices\": [\"*\"],   #edit indices depends on your index name\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n  },\n  \"retention\": {\n    \"expire_after\": \"30d\",\n    \"min_count\": 5,\n    \"max_count\": 50\n  }\n}\n</code></pre></p>"},{"location":"devops/logging/elk-snapshot/#test-snapshot","title":"Test Snapshot","text":"<p>Go To Kibana UI Stack Manamement --&gt; Snapshot and Restore Check your Repositories and Policies  Is everyting ok go to policies and click run now button</p> <p></p> <p>And Check your Snapshots</p> <p></p>"},{"location":"devops/logging/elk-snapshot/#azure-blob-storage-snapshots","title":"Azure Blob Storage Snapshots","text":""},{"location":"devops/logging/elk-snapshot/#create-azure-storage-account","title":"Create Azure Storage Account","text":"<p>Log in to Azure Portal: https://portal.azure.com/ Go to Storage Accounts \u2192 Create Fill in the required fields: Subscription:         Select your Azure subscription Resource Group:       Choose existing or create a new one Storage Account Name: es-snapshots Location:              Performance:          Standard Replication:          Locally-redundant storage (LRS) Click Review + Create, then Create."},{"location":"devops/logging/elk-snapshot/#create-blob-container-and-sas-token","title":"Create Blob Container and SAS Token","text":"<p>Create Blob Container Navigate to: Storage Account \u2192 Containers \u2192 + Container Fill in: Container Name: es-backups Public Access Level: Private Click Create. Generate SAS Token Navigate to: Storage Account \u2192 Security + Networking \u2192 Shared Access Signature (SAS) Select: Allowed Services:        Blob Allowed Resource Types:  Container, Object Permissions:             Read, Write, Delete, List Click: Generate SAS and Connection String Copy the SAS Token for later.</p>"},{"location":"devops/logging/elk-snapshot/#configure-azure-credentials-on-elasticsearch-nodes","title":"Configure Azure Credentials on Elasticsearch Nodes","text":"<p>You must run these commands on every Elasticsearch node. Add Storage Account Name to Elasticsearch Keystore <pre><code>/usr/share/elasticsearch/bin/elasticsearch-keystore add azure.client.default.account\nEnter your storage account name, e.g.:\nes-snapshots\n</code></pre></p>"},{"location":"devops/logging/elk-snapshot/#add-sas-token-to-elasticsearch-keystore","title":"Add SAS Token to Elasticsearch Keystore","text":"<pre><code>/usr/share/elasticsearch/bin/elasticsearch-keystore add azure.client.default.sas_token\nPaste the SAS Token from Azure Portal.\nRestart Elasticsearch\nsudo /bin/systemctl daemon-reload\nsudo systemctl restart elasticsearch.service\n</code></pre>"},{"location":"devops/logging/elk-snapshot/#register-azure-snapshot-repository-in-elasticsearch","title":"Register Azure Snapshot Repository in Elasticsearch","text":"<p>Use Kibana Dev Tools <pre><code>PUT _snapshot/azure_repo\n{\n  \"type\": \"azure\",\n  \"settings\": {\n    \"container\": \"es-backups\",\n    \"client\": \"default\",\n    \"base_path\": \"snapshots\",\n    \"chunk_size\": \"64mb\",\n    \"compress\": true\n  }\n}\n</code></pre></p>"},{"location":"devops/logging/elk-snapshot/#verify-repository","title":"Verify Repository","text":"<pre><code>PUT _snapshot/azure_repo/_verify\n</code></pre> <p>If successful, it returns node information.</p>"},{"location":"devops/logging/elk-upgrade/","title":"Elasticsearch + Kibana Upgrade (Rolling, 3 Master Nodes)","text":"<p>This section is a minimal, step-by-step guide so someone can run the commands and complete a safe upgrade.</p> <ul> <li>Elastic Support Matrix (Elasticsearch + Kibana OS/JVM compatibility)</li> </ul> <p>Production reminder: run the same upgrade on a test/staging cluster first, and validate snapshot/restore. Where to run commands: - Kibana Dev Tools: commands starting with <code>GET/PUT/POST</code> - Node terminal: commands starting with <code>sudo</code>, <code>systemctl</code>, <code>wget</code>, <code>rpm</code>, <code>dpkg</code></p>"},{"location":"devops/logging/elk-upgrade/#recommended-path-production","title":"Recommended Path (Production)","text":"<pre><code>8.14.2\n  \u2193\n8.18.x   (run Upgrade Assistant, fix all issues)\n  \u2193\n9.2.4    (rolling upgrade)\n</code></pre> <p>You must reach the latest 8.x first, then upgrade to 9.x. In 8.18.x, open Kibana Upgrade Assistant and fix all warnings before moving to 9.x. Do not install 9.x packages until all Critical items are cleared. Upgrade Assistant docs Version rule: 8.x \u2192 latest 8.x before 9.x; prefer a stable 9.x minor (not 9.0.0). Always verify the exact target versions on elastic.co before running <code>wget</code> (avoid 404s).</p>"},{"location":"devops/logging/elk-upgrade/#pre-upgrade-checklist-must-do","title":"Pre-Upgrade Checklist (Must Do)","text":"<p>1) Cluster health must be green or yellow (red is not allowed):</p> <pre><code>GET _cat/health?v\n</code></pre> <p>2) Snapshot is mandatory (rollback safety):</p> <pre><code>GET _snapshot\n</code></pre> <p>If repository exists, take a snapshot:</p> <pre><code>PUT _snapshot/&lt;repo_name&gt;/pre_upgrade_8x_to_8_latest?wait_for_completion=true\nGET _snapshot/&lt;repo_name&gt;/pre_upgrade_8x_to_8_latest\n</code></pre> <p>You should see:</p> <pre><code>\"state\":\"SUCCESS\"\n</code></pre> <p>Rollback note: Elasticsearch does not support downgrade. If upgrade fails, the recovery path is restore from snapshot.</p>"},{"location":"devops/logging/elk-upgrade/#upgrade-order-golden-rule","title":"Upgrade Order (Golden Rule)","text":"<p>Order is mandatory:</p> <p>1) Data nodes (if tiers: frozen \u2192 cold \u2192 warm \u2192 hot) 2) Other nodes (ingest, ml, coordinating) 3) Master nodes last, one-by-one</p> <p>Never upgrade masters before data nodes. Never stop two masters at once.</p>"},{"location":"devops/logging/elk-upgrade/#node-upgrade-loop-apply-to-each-node-one-by-one","title":"Node Upgrade Loop (Apply to Each Node One-by-One)","text":"<p>Run this loop for every node in the order above.</p> <p>0) Before touching the next node, confirm health:</p> <pre><code>GET _cat/health?v\n</code></pre> <p>1) Disable replica allocation (recommended for data nodes; optional for masters/others):</p> <pre><code>PUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": \"primaries\"\n  }\n}\n</code></pre> <p>2) Flush before stop (recommended for data nodes):</p> <pre><code>POST /_flush\n</code></pre> <p>3) Back up config before upgrade (recommended):</p> <pre><code>sudo cp -r /etc/elasticsearch /etc/elasticsearch_backup_$(date +%F)\n</code></pre> <p>4) Stop Elasticsearch on the node:</p> <pre><code>sudo systemctl stop elasticsearch\n</code></pre> <p>5) Upgrade package (choose your OS)</p> <p>DEB (Debian/Ubuntu):</p> <p>Elasticsearch DEB docs</p> <p>Example:</p> <pre><code>&lt;ELASTICSEARCH_VERSION&gt; = 8.19.10\n</code></pre> <pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-amd64.deb\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-amd64.deb.sha512\nshasum -a 512 -c elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-amd64.deb.sha512\nsudo dpkg -i elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-amd64.deb\n</code></pre> <p>RPM (RHEL/Rocky/Alma):</p> <p>Elasticsearch RPM docs</p> <pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-x86_64.rpm\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-x86_64.rpm.sha512\nshasum -a 512 -c elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-x86_64.rpm.sha512\nsudo rpm -Uvh elasticsearch-&lt;ELASTICSEARCH_VERSION&gt;-x86_64.rpm\n</code></pre> <p>Use the exact target version you plan. Do not use <code>--install</code> for RPM. If prompted about <code>/etc/elasticsearch/elasticsearch.yml</code>, choose Keep the local version.</p> <p>6) Upgrade plugins (only if installed on this node):</p> <pre><code>/usr/share/elasticsearch/bin/elasticsearch-plugin list\n</code></pre> <p>Check compatibility first:</p> <ul> <li>Elasticsearch plugins compatibility</li> </ul> <p>If you see plugins, remove + install again (same name):</p> <pre><code>/usr/share/elasticsearch/bin/elasticsearch-plugin remove &lt;plugin_name&gt;\n/usr/share/elasticsearch/bin/elasticsearch-plugin install &lt;plugin_name&gt;\n</code></pre> <p>7) Start Elasticsearch:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start elasticsearch\n</code></pre> <p>8) Wait until the cluster is stable before the next node:</p> <pre><code>GET _cat/health?v\nGET _cat/shards?v=true&amp;h=index,shard,prirep,state,node,unassigned.reason&amp;s=state\nGET _cat/recovery\n</code></pre> <p>You can watch recovery progress:</p> <pre><code>watch -n 5 'curl -s \"localhost:9200/_cat/recovery?v&amp;active_only=true\"'\n</code></pre> <p>Continue only if: - health = green/yellow - primary shards are STARTED - initializing/relocating shards = 0</p> <p>9) Re-enable allocation (after the node is stable):</p> <pre><code>PUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": null\n  }\n}\n</code></pre>"},{"location":"devops/logging/elk-upgrade/#master-node-order-3-masters","title":"Master Node Order (3 Masters)","text":"<p>Example:</p> <p>1) master-1 2) master-3 3) master-2 (current active master last)</p> <p>Quorum warning: in a 3-master cluster, only one master may be offline at a time.</p> <p>Check active master:</p> <pre><code>GET _cat/master\n</code></pre>"},{"location":"devops/logging/elk-upgrade/#final-elasticsearch-checks","title":"Final Elasticsearch Checks","text":"<pre><code>GET _cat/nodes?h=name,version,master\nGET _cat/health?v\nGET /\n</code></pre> <p>If you see any <code>archived.*</code> settings:</p> <pre><code>GET _cluster/settings?include_defaults=true\n</code></pre> <p><code>archived.*</code> means Elasticsearch detected old/unsupported settings after upgrade and ignored them. You should remove them before moving on.</p> <p>Example output (if you see this, you must clean it):</p> <pre><code>{\n  \"persistent\": {\n    \"archived.cluster.routing.allocation.awareness.attributes\": \"rack_id\"\n  }\n}\n</code></pre> <p>Example cleanup:</p> <pre><code>PUT _cluster/settings\n{\n  \"persistent\": {\n    \"archived.cluster.routing.allocation.awareness.attributes\": null\n  }\n}\n</code></pre>"},{"location":"devops/logging/elk-upgrade/#kibana-upgrade-must-match-elasticsearch-version","title":"Kibana Upgrade (Must Match Elasticsearch Version)","text":""},{"location":"devops/logging/elk-upgrade/#rules","title":"Rules","text":"<ul> <li>Kibana upgrades after Elasticsearch</li> <li>Kibana version = Elasticsearch version</li> <li>No rolling upgrade for Kibana (stop all Kibana instances first)</li> </ul>"},{"location":"devops/logging/elk-upgrade/#stop-all-kibana-instances","title":"Stop all Kibana instances","text":"<pre><code>sudo systemctl stop kibana\n</code></pre>"},{"location":"devops/logging/elk-upgrade/#upgrade-package","title":"Upgrade package","text":"<p>RPM (RHEL/Rocky/Alma):</p> <p>Kibana RPM docs</p> <p>Example:</p> <pre><code>&lt;KIBANA_VERSION&gt; = 8.19.10\n</code></pre> <p>Backup config first:</p> <pre><code>sudo cp -r /etc/kibana /etc/kibana_backup_$(date +%F)\n</code></pre> <pre><code>wget https://artifacts.elastic.co/downloads/kibana/kibana-&lt;KIBANA_VERSION&gt;-x86_64.rpm\nwget https://artifacts.elastic.co/downloads/kibana/kibana-&lt;KIBANA_VERSION&gt;-x86_64.rpm.sha512\nshasum -a 512 -c kibana-&lt;KIBANA_VERSION&gt;-x86_64.rpm.sha512\nsudo rpm -Uvh kibana-&lt;KIBANA_VERSION&gt;-x86_64.rpm\n</code></pre> <p>DEB (Debian/Ubuntu):</p> <p>Kibana DEB docs</p> <pre><code>wget https://artifacts.elastic.co/downloads/kibana/kibana-&lt;KIBANA_VERSION&gt;-amd64.deb\nwget https://artifacts.elastic.co/downloads/kibana/kibana-&lt;KIBANA_VERSION&gt;-amd64.deb.sha512\nshasum -a 512 -c kibana-&lt;KIBANA_VERSION&gt;-amd64.deb.sha512\nsudo dpkg -i kibana-&lt;KIBANA_VERSION&gt;-amd64.deb\n</code></pre> <p>If prompted about <code>/etc/kibana/kibana.yml</code>, choose Keep the local version.</p>"},{"location":"devops/logging/elk-upgrade/#upgrade-kibana-plugins-if-any","title":"Upgrade Kibana plugins (if any)","text":"<pre><code>/usr/share/kibana/bin/kibana-plugin list\n</code></pre> <p>If plugins exist:</p> <pre><code>/usr/share/kibana/bin/kibana-plugin remove &lt;plugin_name&gt;\n/usr/share/kibana/bin/kibana-plugin install &lt;plugin_name&gt;\n</code></pre>"},{"location":"devops/logging/elk-upgrade/#start-kibana-and-verify","title":"Start Kibana and verify","text":"<pre><code>sudo systemctl start kibana\n</code></pre> <p>Watch logs for migration status:</p> <pre><code>sudo journalctl -u kibana -f\n</code></pre> <p>Look for:</p> <pre><code>Saved object migrations completed successfully\n</code></pre> <p>Check:</p> <pre><code>GET _cat/indices/.kibana*\n</code></pre> <p>If login page opens and Kibana indices are healthy, upgrade is complete.</p>"},{"location":"devops/logging/loki-distributed/","title":"Grafana Loki: Distributed Log Management and Collection","text":"<p>Grafana Loki is a set of open source components that can be composed into a fully featured logging stack. A small index and highly compressed chunks simplifies the operation and significantly lowers the cost of Loki.</p> <p>Unlike other logging systems, Loki is built around the idea of only indexing metadata about your logs\u2019 labels (just like Prometheus labels). Log data itself is then compressed and stored in chunks in object stores such as Amazon Simple Storage Service (S3) or Google Cloud Storage (GCS), or even locally on the filesystem.</p> <p>Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It\u2019s designed to be very cost-effective and easy to operate.</p> <p>Because all Loki implementations are unique, the installation process is different for every customer. But there are some steps in the process that are common to every installation.</p> <p>To collect logs and view your log data generally involves the following steps:</p> <pre><code>    Install Loki --&gt; Deploy Agents, Collect Logs --&gt; Deploy Grafana --&gt; Query Log Data\n</code></pre>"},{"location":"devops/logging/loki-distributed/#loki-features","title":"Loki features","text":"<ul> <li> <p>Scalability - Loki is designed for scalability, and can scale from as small as running on a Raspberry Pi to ingesting petabytes a day. In its most common deployment, \u201csimple scalable mode\u201d, Loki decouples requests into separate read and write paths, so that you can independently scale them, which leads to flexible large-scale installations that can quickly adapt to meet your workload at any given time. </p> </li> <li> <p>Multi-tenancy - Loki allows multiple tenants to share a single Loki instance. With multi-tenancy, the data and requests of each tenant is completely isolated from the others. Multi-tenancy is configured by assigning a tenant ID in the agent.</p> </li> <li> <p>Third-party integrations - Several third-party agents (clients) have support for Loki, via plugins. This lets you keep your existing observability setup while also shipping logs to Loki.</p> </li> <li> <p>Efficient storage - Loki stores log data in highly compressed chunks. Similarly, the Loki index, because it indexes only the set of labels, is significantly smaller than other log aggregation tools. By leveraging object storage as the only data storage mechanism, Loki inherits the reliability and stability of the underlying object store. The compressed chunks, smaller index, and use of low-cost object storage, make Loki less expensive to operate.</p> </li> <li> <p>LogQL, the Loki query language - LogQL is the query language for Loki. Users who are already familiar with the Prometheus query language, PromQL, will find LogQL familiar and flexible for generating queries against the logs. The language also facilitates the generation of metrics from log data, a powerful feature that goes well beyond log aggregation.</p> </li> <li> <p>Alerting - Loki includes a component called the ruler, which can continually evaluate queries against your logs, and perform an action based on the result. This allows you to monitor your logs for anomalies or events. Loki integrates with Prometheus Alertmanager, or the alert manager within Grafana.</p> </li> <li> <p>Grafana integration - Loki integrates with Grafana, Mimir, and Tempo, providing a complete observability stack, and seamless correlation between logs, metrics and traces.</p> </li> </ul>"},{"location":"devops/logging/loki-distributed/#loki-architecture","title":"Loki architecture","text":"<p>Grafana Loki has a microservices-based architecture and is designed to run as a horizontally scalable, distributed system. The system has multiple components that can run separately and in parallel. The Grafana Loki design compiles the code for all components into a single binary or Docker image. The -target command-line flag controls which component(s) that binary will behave as.</p> <p></p>"},{"location":"devops/logging/loki-distributed/#loki-components","title":"Loki components","text":"<p>Loki is a modular system that contains many components that can either be run together (in \u201csingle binary\u201d mode with target all), in logical groups (in \u201csimple scalable deployment\u201d mode with targets read, write, backend), or individually (in \u201cmicroservice\u201d mode)</p> <p>This table describes the responsibilities of each of these components.</p> <p></p> <ul> <li> <p>Distributor: The distributor service is responsible for handling incoming push requests from clients. It\u2019s the first step in the write path for log data. Once the distributor receives a set of streams in an HTTP request, each stream is validated for correctness and to ensure that it is within the configured tenant (or global) limits. Each valid stream is then sent to n ingesters in parallel, where n is the replication factor for data. The distributor determines the ingesters to which it sends a stream to using consistent hashing.</p> <p>A load balancer must sit in front of the distributor to properly balance incoming traffic to them. In Kubernetes, the service load balancer provides this service.</p> <p>The distributor is a stateless component. This makes it easy to scale and offload as much work as possible from the ingesters, which are the most critical component on the write path. The ability to independently scale these validation operations means that Loki can also protect itself against denial of service attacks that could otherwise overload the ingesters. It also allows us to fan-out writes according to the replication factor.</p> </li> <li> <p>Ingester: The ingester service is responsible for persisting data and shipping it to long-term storage (Amazon Simple Storage Service, Google Cloud Storage, Azure Blob Storage, etc.) on the write path, and returning recently ingested, in-memory log data for queries on the read path.</p> </li> <li> <p>Query frontend: The query frontend is an optional service providing the querier\u2019s API endpoints and can be used to accelerate the read path. When the query frontend is in place, incoming query requests should be directed to the query frontend instead of the queriers. The querier service will be still required within the cluster, in order to execute the actual queries.</p> <p>The query frontend internally performs some query adjustments and holds queries in an internal queue. In this setup, queriers act as workers which pull jobs from the queue, execute them, and return them to the query frontend for aggregation. Queriers need to be configured with the query frontend address (via the -querier.frontend-address CLI flag) in order to allow them to connect to the query frontends.</p> </li> <li> <p>Querier: The querier service is responsible for executing Log Query Language (LogQL) queries. The querier can handle HTTP requests from the client directly (in \u201csingle binary\u201d mode, or as part of the read path in \u201csimple scalable deployment\u201d) or pull subqueries from the query frontend or query scheduler (in \u201cmicroservice\u201d mode).</p> <p>It fetches log data from both the ingesters and from long-term storage. Queriers query all ingesters for in-memory data before falling back to running the same query against the backend store. Because of the replication factor, it is possible that the querier may receive duplicate data. To resolve this, the querier internally deduplicates data that has the same nanosecond timestamp, label set, and log message.</p> </li> <li> <p>Query scheduler: The query scheduler is an optional service providing more advanced queuing functionality than the query frontend. When using this component in the Loki deployment, query frontend pushes split up queries to the query scheduler which enqueues them in an internal in-memory queue. There is a queue for each tenant to guarantee the query fairness across all tenants. The queriers that connect to the query scheduler act as workers that pull their jobs from the queue, execute them, and return them to the query frontend for aggregation. Queriers therefore need to be configured with the query scheduler address (via the -querier.scheduler-address CLI flag) in order to allow them to connect to the query scheduler.</p> </li> <li> <p>Index Gateway: The index gateway service is responsible for handling and serving metadata queries. Metadata queries are queries that look up data from the index. The index gateway is only used by \u201cshipper stores\u201d, such as single store TSDB or single store BoltDB.</p> <p>The query frontend queries the index gateway for the log volume of queries so it can make a decision on how to shard the queries. The queriers query the index gateway for chunk references for a given query so they know which chunks to fetch and query.</p> </li> <li> <p>Compactor: The compactor service is used by \u201cshipper stores\u201d, such as single store TSDB or single store BoltDB, to compact the multiple index files produced by the ingesters and shipped to object storage into single index files per day and tenant. This makes index lookups more efficient.</p> <p>To do so, the compactor downloads the files from object storage in a regular interval, merges them into a single one, uploads the newly created index, and cleans up the old files. Additionally, the compactor is also responsible for log retention and log deletion.</p> </li> <li> <p>Ruler: The ruler service manages and evaluates rule and/or alert expressions provided in a rule configuration. The rule configuration is stored in object storage (or alternatively on local file system) and can be managed via the ruler API or directly by uploading the files to object storage.</p> <p>Alternatively, the ruler can also delegate rule evaluation to the query frontend. This mode is called remote rule evaluation and is used to gain the advantages of query splitting, query sharding, and caching from the query frontend.</p> </li> </ul>"},{"location":"devops/logging/loki-distributed/#loki-deployment-modes","title":"Loki deployment modes","text":"<ul> <li>Monolithic mode: The simplest mode of operation is the monolithic deployment mode. You enable monolithic mode by setting the -target=all command line parameter. This mode runs all of Loki\u2019s microservice components inside a single process as a single binary or Docker image.</li> </ul> <p>Monolithic mode is useful for getting started quickly to experiment with Loki, as well as for small read/write volumes of up to approximately 20GB per day.</p> <ul> <li>Simple Scalable: The simple scalable deployment is the default configuration installed by the Loki Helm Chart. This deployment mode is the easiest way to deploy Loki at scale. It strikes a balance between deploying in monolithic mode or deploying each component as a separate microservice. Simple scalable deployment is also referred to as SSD. </li> </ul> <p></p> <p>The three execution paths in simple scalable mode are each activated by appending the following arguments to Loki on startup:</p> <pre><code>-target=write - The write target is stateful and is controlled by a Kubernetes StatefulSet. It contains the following components:\nDistributor,\nIngester\n-target=read - The read target is stateless and can be run as a Kubernetes Deployment that can be scaled automatically (Note that in the official helm chart it is currently deployed as a stateful set). It contains the following components:\nQuery Frontend,\nQuerier\n-target=backend - The backend target is stateful, and is controlled by a Kubernetes StatefulSet. Contains the following components:\nCompactor,\nIndex Gateway,\nQuery Scheduler,\nRuler,\nBloom Planner (experimental),\nBloom Builder (experimental),\nBloom Gateway (experimental)\n</code></pre> <p>The simple scalable deployment mode requires a reverse proxy to be deployed in front of Loki, to direct client API requests to either the read or write nodes. The Loki Helm chart includes a default reverse proxy configuration, using Nginx.</p> <ul> <li> <p>Microservices mode: The microservices deployment mode runs components of Loki as distinct processes. The microservices deployment is also referred to as a Distributed deployment. Each process is invoked specifying its target. For release 3.2 the components are:</p> <p>Bloom Builder (experimental), Bloom Gateway (experimental), Bloom Planner (experimental), Compactor, Distributor, Index Gateway, Ingester, Overrides Exporter, Querier, Query Frontend, Query Scheduler, Ruler, Table Manager (deprecated)</p> </li> </ul> <p></p> <p>Running components as individual microservices provides more granularity, letting you scale each component as individual microservices, to better match your specific use case.</p> <p>Microservices mode deployments can be more efficient Loki installations. However, they are also the most complex to set up and maintain.</p>"},{"location":"devops/logging/loki-distributed/#install-loki","title":"Install Loki","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We can check all repo </li> </ul> <p><pre><code>helm search repo loki\n</code></pre> </p> <ul> <li>We will use grafana/loki helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values grafana/loki &gt; loki-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>global:\n  extraArgs: \n     - \"-log.level=debug\"\n\nloki:\n  schemaConfig:\n    configs:\n      - from: \"2024-04-01\"\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n\n  limits_config:\n    allow_structured_metadata: true\n    ingestion_rate_mb: 10\n    ingestion_burst_size_mb: 15\n    max_streams_per_user: 50000\n\n  ingester:\n    chunk_encoding: snappy\n\n  pattern_ingester:\n    enabled: true\n\n  querier:\n    max_concurrent: 4\n\n  auth_enabled: false\n\n  storage_config:\n    aws:\n      s3: s3://loki-example\n      region: eu-central-1\n      access_key_id: xxxxxxxxxxxxxxxx\n      secret_access_key: xxxxxxxxxxxxxxxx\n    tsdb_shipper:\n      active_index_directory: /var/loki/index\n      cache_location: /var/loki/index_cache\n      cache_ttl: 24h\n\n\n  storage:\n    type: s3\n    bucketNames:\n      chunks: loki-chunks\n      ruler: loki-ruler\n\ndeploymentMode: SimpleScalable\n\nchunksCache:\n  resources:\n    requests:\n      memory: \"2000Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"6000Mi\"\n      cpu: \"500m\"\n\nbackend:\n  replicas: 3\n  persistence:\n    volumeClaimsEnabled: false\n\nread:\n  replicas: 3\n\nwrite:\n  replicas: 3\n  persistence:\n    volumeClaimsEnabled: false\n\n# Disable minio storage\nminio:\n  enabled: false\n</code></pre> <ul> <li>Install loki Simple Scalable deployment mode.</li> </ul> <p><pre><code>helm upgrade --install loki grafana/loki -n logging --values loki-values.yaml --create-namespace \n</code></pre> </p>"},{"location":"devops/logging/loki-distributed/#deploy-agents-collect-logs-promtail-fluentbit-alloy-or-opentelemetry-collector","title":"Deploy Agents, Collect Logs --&gt; Promtail, FluentBit, Alloy or Opentelemetry Collector","text":""},{"location":"devops/logging/loki-distributed/#promtail","title":"Promtail","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We will use grafana/promtail helm chart so lets get values.yaml. Also we can add labels for spesific pod or app.</li> </ul> <pre><code>helm show values grafana/promtail &gt; promtail-values.yaml\n</code></pre> <ul> <li> <p>our logs format : <pre><code>2024-10-26 14:19:06.065 \n{\"log\":\"Sat Oct 26 11:19:06 UTC 2024 - Test log message from log-generator\\n\",\"stream\":\"stdout\",\"time\":\"2024-10-26T11:19:06.022721667Z\"}\n</code></pre></p> </li> <li> <p>We want to add 'time' and 'stream' section as a labels.</p> </li> <li> <p>Customize values.yaml for our scenario.</p> </li> </ul> <pre><code>config:\n  snippets:\n    pipelineStages:\n      - cri: {}\n      - match:\n          selector: '{app=\"log-generator\"}'\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n            - labels:\n                stream:\n                time:\n</code></pre> <ul> <li>Install Promtail.</li> </ul> <pre><code>helm upgrade --install promtail  grafana/promtail --values promtail-values.yaml\n</code></pre>"},{"location":"devops/logging/loki-distributed/#fluentbit","title":"FluentBit","text":"<ul> <li>Add Repo</li> </ul> <p><pre><code>helm repo add fluent https://fluent.github.io/helm-charts\nhelm repo update\n</code></pre> - We will use fluent/fluent-bit helm chart so lets get values.yaml</p> <pre><code>helm show values fluent/fluent-bit &gt; fluentbit-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>config:\n    outputs: |\n        [OUTPUT]\n            Name        loki\n            Match       *\n            Host        http://loki-gateway.logging:80\n            port        443\n            tls         on\n            tls.verify  on\n            http_user   XXX\n            http_passwd XXX\n            Labels agent=fluend-bit\n</code></pre> <ul> <li>Install FluentBit.</li> </ul> <pre><code>helm upgrade --install fluent-bit fluent/fluent-bit --values fluentbit-values.yaml\n</code></pre>"},{"location":"devops/logging/loki-distributed/#alloy","title":"Alloy","text":"<ul> <li> <p>SDK development is needed for Alloy this scenario.</p> </li> <li> <p>Add Repo</p> </li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We will use grafana/alloy helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values grafana/alloy &gt; alloy-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>alloy:\n  configMap:\n    content: |\n      // ===== CONFIGURATION =====\n\n      // Receive OpenTelemetry traces\n      otelcol.receiver.otlp \"otlp_receiver\" {\n        http {\n          endpoint = \"0.0.0.0:4318\"\n        }\n        grpc {\n          endpoint = \"0.0.0.0:4317\"\n        }\n\n        output {\n          logs = [otelcol.processor.batch.otlp_processor.input]\n        }\n      }\n\n      // Batch processor to improve performance\n      otelcol.processor.batch \"otlp_processor\" {\n        output {\n          logs = [otelcol.exporter.otlphttp.logs.input]\n        }\n      }\n\n      // Send logs to Loki\n      otelcol.exporter.otlphttp \"logs\" {\n        client {\n          endpoint = \"http://loki-gateway.logging:80/otlp\"\n        }\n      }\n\n      livedebugging {\n        enabled = true\n      } \n\n  extraPorts:\n    - name: otlp-grpc\n      port: 4317\n      targetPort: 4317\n      protocol: TCP\n    - name: otlp-http\n      port: 4318\n      targetPort: 4318\n      protocol: TCP\n\n  mounts:\n    # -- Mount /var/log from the host into the container for log collection.\n    varlog: true\n    # -- Mount /var/lib/docker/containers from the host into the container for log\n    # collection.\n    dockercontainers: true\n\n  serviceMonitor: \n    enabled: true\n    additionalLabels: \n      release: prometheus\n\ncontroller:\n  # -- Type of controller to use for deploying Grafana Alloy in the cluster.\n  # Must be one of 'daemonset', 'deployment', or 'statefulset'.\n  type: 'deployment'\n  replicas: 3\n</code></pre> <ul> <li>Install Alloy.</li> </ul> <pre><code> helm upgrade --install alloy grafana/alloy --values alloy-values.yaml\n</code></pre>"},{"location":"devops/logging/loki-distributed/#opentelemetry-collector","title":"Opentelemetry Collector","text":"<ul> <li> <p>SDK development is needed for Opentelemetry Collector this scenario.</p> </li> <li> <p>Add Repo</p> </li> </ul> <pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update\n</code></pre> <ul> <li>We will use open-telemetry/opentelemetry-collector helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values open-telemetry/opentelemetry-collector &gt; opentelemetry-collector-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>mode: deployment\nconfig:\n  exporters:\n    debug:\n      verbosity: detailed\n    otlphttp/gateway/loki:\n      endpoint: http://loki-gateway.logging:80/otlp\n  processors:\n    resource/loki:\n      attributes:\n        - action: upsert\n          key: service.instance.id # loki does not accept host.name (https://github.com/grafana/loki/issues/11786)\n          from_attribute: host.name\n    batch:\n      send_batch_size: 90\n      timeout: 30s\n    memory_limiter:\n      check_interval: 5s\n      limit_percentage: 80\n      spike_limit_percentage: 25\n\n  extensions:\n    health_check:\n      endpoint: ${env:MY_POD_IP}:13133\n  receivers:\n    otlp:\n      protocols:\n        http:\n          endpoint: \"0.0.0.0:4318\"\n        grpc:\n          endpoint: \"0.0.0.0:4317\"\n  service:\n    pipelines:\n      logs/gateway:\n        exporters: \n        - otlphttp/gateway/loki\n        processors: \n        - resource/loki\n        receivers: \n        - otlp\nimage:\n  repository: docker.io/otel/opentelemetry-collector-contrib\nreplicaCount: 3\nrevisionHistoryLimit: 10\n</code></pre> <ul> <li>Install Opentelemetry Collector.</li> </ul> <pre><code>helm upgrade --install opentelemetry-collector open-telemetry/opentelemetry-collector --values opentelemetry-collector-values.yaml\n</code></pre>"},{"location":"devops/logging/loki-distributed/#deploy-grafana-and-query-log-data","title":"Deploy Grafana and Query Log Data","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>Install grafana.</li> </ul> <pre><code>helm install grafana grafana/grafana \n</code></pre> <ul> <li>Port-forward grafana services and go to grafana ui</li> </ul> <pre><code>kubectl port-forward service/grafana 3000:80\n</code></pre> <ul> <li>You can get grafana admin-password via below command</li> </ul> <pre><code>kubectl get secret grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre> <ul> <li>Thanks to grafana/loki and grafana/grafana helm-chart you have to configure Loki's Data sources as shown below.</li> </ul> <ul> <li>We can check all pods logs grafana explore section.</li> </ul>"},{"location":"devops/logging/loki-distributed/#deploy-app-to-collect-logs","title":"Deploy App to Collect logs","text":"<ul> <li>Lets deploy a app and collect logs by using grafana-loki</li> </ul> <p>deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-generator\n  labels:\n    app: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n        - name: log-generator\n          image: busybox\n          command:\n            - /bin/sh\n            - -c\n            - |\n              while true; do\n                echo \"$(date) - Test log message from log-generator\"\n                sleep 5\n              done\n</code></pre> <pre><code>kubectl create ns app\nkubectl apply -f deployment.yaml -n app\n</code></pre> <ul> <li>Go to grafana ui and select explore section in left-hand-menu and select log generator pod.</li> </ul> <p> </p>"},{"location":"devops/logging/loki-distributed/#add-dashboard-grafana-to-loki-for-checking-logs","title":"Add Dashboard Grafana to loki for checking logs","text":"<ul> <li>Go to grafana webui and select dashboard left-hand-menu and click new and import</li> <li>Enter the template ID  --&gt; 15141  and click load</li> <li>Select Loki as a data source</li> </ul>"},{"location":"devops/logging/loki-single-binary/","title":"Install Loki,Promtail,Grafana","text":""},{"location":"devops/logging/loki-single-binary/#what-is-loki","title":"What is Loki?","text":"<p>Loki is a log aggregation system developed by Grafana Labs, designed specifically for storing and querying logs. Unlike traditional logging solutions, Loki is optimized for a \"cost-effective\" and \"lightweight\" approach by only indexing metadata (like labels) and not the full log content, making it more efficient and affordable to operate.</p>"},{"location":"devops/logging/loki-single-binary/#what-is-promtail","title":"What is Promtail","text":"<p>Promtail is an agent that collects logs from various sources and sends them to Loki for storage and querying. It\u2019s part of the Grafana Loki logging stack, designed to simplify log collection and forwarding.</p>"},{"location":"devops/logging/loki-single-binary/#loki-stack-helm-chart","title":"Loki-Stack Helm Chart","text":"<p>The Loki Stack Helm chart is a pre-configured set of resources that deploys the full Loki logging stack in Kubernetes. This Helm chart simplifies the deployment and management of Loki, Promtail, and other optional components like Grafana, making it easier to set up a complete logging solution within a Kubernetes cluster</p>"},{"location":"devops/logging/loki-single-binary/#what-are-we-going-to-need","title":"What are we going to need?","text":"<p>Kubernetes cluster. Grafana installation. Grafana Loki installation. Promtail agent on every node of the Kubernetes cluster.</p>"},{"location":"devops/logging/loki-single-binary/#create-kubernetes-cluster","title":"Create Kubernetes Cluster","text":"<p>Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node.</p> <ul> <li>First, install Docker Desktop on your Mac. The easiest way to do so is to get the .dmg file from Docker\u2019s website.</li> </ul> <pre><code>brew install minikube kubectl\nminikube start\n</code></pre>"},{"location":"devops/logging/loki-single-binary/#loki-stack-helm-chart_1","title":"Loki-Stack Helm Chart","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We can check all repo </li> </ul> <p><pre><code>helm search repo loki\n</code></pre> </p> <ul> <li>We will use grafana/loki-stack helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values grafana/loki-stack &gt; loki-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>loki:\n  enabled: true\n  isDefault: true\n  url: http://{{(include \"loki.serviceName\" .)}}:{{ .Values.loki.service.port }}\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  datasource:\n    jsonData: \"{}\"\n    uid: \"\"\n\n\npromtail:\n  enabled: true\n  config:\n    logLevel: info\n    serverPort: 3101\n    clients:\n      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push\n\n\ngrafana:\n  enabled: true\n  sidecar:\n    datasources:\n      label: \"\"\n      labelValue: \"\"\n      enabled: true\n      maxLines: 1000\n  image:\n    tag: latest\n</code></pre> <ul> <li>Install loki stack include promtail,loki and grafana</li> </ul> <p><pre><code>helm upgrade --install loki -f loki-values.yaml -n logging --create-namespace grafana/loki-stack \n</code></pre> </p> <ul> <li>Port-forward grafana services and go to grafana ui</li> </ul> <pre><code>kubectl port-forward service/loki-grafana 3000:80\n</code></pre> <ul> <li>You can get grafana admin-password via below command</li> </ul> <pre><code>kubectl get secret loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre> <pre><code>http://localhost:3000\nusername: admin\npassword: admin-password # add your password\n</code></pre> <ul> <li>Thanks to grafana/loki-stack helm-chart you can see Loki has been configured In Data sources  as shown below</li> </ul> <p></p> <ul> <li>We can check all pods logs grafana explore section.</li> </ul> <p> </p>"},{"location":"devops/logging/loki-single-binary/#deploy-app-to-collect-logs","title":"Deploy App to Collect logs","text":"<ul> <li>Lets deploy a app and collect logs by using grafana-loki</li> </ul> <p>deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-generator\n  labels:\n    app: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n        - name: log-generator\n          image: busybox\n          command:\n            - /bin/sh\n            - -c\n            - |\n              while true; do\n                echo \"$(date) - Test log message from log-generator\"\n                sleep 5\n              done\n</code></pre> <pre><code>kubectl create ns app\nkubectl apply -f deployment.yaml -n app\n</code></pre> <ul> <li>Go to grafana ui and select explore section  in left-hand-menu and select log generator pod</li> </ul> <p> </p> <ul> <li>We can add labels for spesific pod or app by manipulating promtail-config</li> </ul> <p><pre><code>kubectl get secret \nkubectl get secret loki-promtail -o jsonpath=\"{.data.promtail\\.yaml}\" | base64 --decode &gt; promtail-config.yaml\n</code></pre> promtail-config.yaml <pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}                \n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre></p> <ul> <li> <p>our logs format : <pre><code>2024-10-26 14:19:06.065 \n{\"log\":\"Sat Oct 26 11:19:06 UTC 2024 - Test log message from log-generator\\n\",\"stream\":\"stdout\",\"time\":\"2024-10-26T11:19:06.022721667Z\"}\n</code></pre></p> </li> <li> <p>We want to add 'time' and 'stream' section as a labels.</p> </li> <li> <p>Add below code-block into promtail-config.yaml</p> </li> </ul> <pre><code>      - match:\n          selector: '{app=\"log-generator\"}'\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:\n</code></pre> <ul> <li>new promtail-config.yaml as below.</li> </ul> <pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}\n\n      - match:                                         ##first\n          selector: '{app=\"log-generator\"}'            ## app label\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:                                  ### last \n\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre> <ul> <li>To apply the new Promtail configuration, the Loki secret needs to be deleted and recreated with the new configuration.</li> </ul> <p><pre><code>kubectl delete secret loki-promtail\n</code></pre> - Create secret by using new promtail-config</p> <p>loki-secret.yaml</p> <p><pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: loki-promtail\n  namespace: logging\n  labels:\n    app: promtail\ntype: Opaque\nstringData:\n  promtail.yaml: |\n    server:\n      log_level: info\n      log_format: logfmt\n      http_listen_port: 3101\n\n    clients:\n      - url: http://loki:3100/loki/api/v1/push\n\n    positions:\n      filename: /run/promtail/positions.yaml\n\n    scrape_configs:\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          - cri: {}\n          - match:\n              selector: '{app=\"log-generator\"}'\n              stages:\n                - json:\n                    expressions:\n                      stream: stream\n                      time: time\n                - labels:\n                    stream:\n                    time:\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_instance\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_node_name\n            target_label: node_name\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_namespace\n            target_label: namespace\n          - action: replace\n            replacement: $1\n            separator: /\n            source_labels:\n            - namespace\n            - app\n            target_label: job\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_name\n            target_label: pod\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_container_name\n            target_label: container\n          - action: replace\n            replacement: /var/log/pods/*$1/*.log\n            separator: /\n            source_labels:\n            - __meta_kubernetes_pod_uid\n            - __meta_kubernetes_pod_container_name\n            target_label: __path__\n          - action: replace\n            regex: true/(.*)\n            replacement: /var/log/pods/*$1/*.log\n            separator: /\n            source_labels:\n            - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n            - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n            - __meta_kubernetes_pod_container_name\n            target_label: __path__\n\n    limits_config:\n\n    tracing:\n      enabled: false\n</code></pre> <pre><code>kubectl apply -f loki-secret.yaml\n</code></pre> - To allow the Promtail pod to use the new configuration file, we need to restart the pod.</p> <p><pre><code>kubectl delete pod &lt;loki-promtail-pod-name&gt;\n</code></pre> - Wait until the new pod is up and running.</p> <ul> <li>Now we can see time and stream as a label in log </li> </ul> <p></p>"},{"location":"devops/logging/loki-single-binary/#add-dashboard-grafana-to-loki-for-checking-logs","title":"Add Dashboard Grafana to loki for checking logs","text":"<ul> <li>Go to grafana webui and select dashboard left-hand-menu and click new and import</li> <li>Enter the template ID  --&gt; 15141  and click load</li> <li>Select Loki as a data source</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/","title":"\ud83d\udd10 Monitoring TLS Certificate Expiration in Kubernetes with Alerting","text":"<p>This document explains how to monitor TLS certificate expiration dates in a Kubernetes cluster and generate alerts when they approach critical thresholds.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#objective","title":"\ud83c\udfaf Objective","text":"<ul> <li>Monitor the expiration dates of TLS certificates used within Kubernetes components.</li> <li>Automatically generate alerts when certificates are about to expire or if any read errors occur.</li> <li>Ensure continuous and secure cluster operation by proactively addressing certificate issues.</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#components-used","title":"\ud83e\uddf0 Components Used","text":"<ul> <li><code>x509-certificate-exporter</code> (deployed via Helm)</li> <li>Prometheus + Alertmanager (from kube-prometheus-stack)</li> <li>ArgoCD (with ApplicationSet for GitOps-style deployment)</li> <li>Grafana (for visualizing certificate metrics)</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#deployment-steps","title":"\ud83d\ude80 Deployment Steps","text":""},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#1-deploy-x509-certificate-exporter-with-argocd-helm","title":"1\ufe0f\u20e3 Deploy <code>x509-certificate-exporter</code> with ArgoCD (Helm)","text":"<p>The following <code>ApplicationSet</code> definition deploys the <code>x509-certificate-exporter</code> Helm chart to the cluster:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: x509-certificate-exporter\n  namespace: argocd\nspec:\n  generators:\n    - list:\n        elements:\n          - cluster: dev\n  template:\n    metadata:\n      name: '{{cluster}}-x509-certificate-exporter'\n    spec:\n      syncPolicy:\n        automated:\n          selfHeal: true\n        syncOptions:\n          - CreateNamespace=true\n      destination:\n        namespace: rke2-cert-monitoring\n        name: '{{cluster}}'\n      project: default\n      source:\n        repoURL: \"https://charts.enix.io\"\n        targetRevision: 3.18.1 # if need change me\n        chart: x509-certificate-exporter\n        helm:\n          values: |\n            hostPathsExporter:\n              podAnnotations:\n                prometheus.io/port: \"9793\"\n                prometheus.io/scrape: \"true\"\n              daemonSets:\n                cp:\n                  nodeSelector:\n                    node-role.kubernetes.io/control-plane: \"true\"\n                    beta.kubernetes.io/os: \"linux\"\n                  tolerations:\n                    - effect: \"NoExecute\"\n                      key: \"CriticalAddonsOnly\"\n                      operator: \"Exists\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/server/tls/client-admin.crt\n                    - /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt\n                    - /var/lib/rancher/rke2/server/tls/server-ca.crt\n                    - /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt\n                    - /var/lib/rancher/rke2/server/tls/client-ca.crt\n                etcd:\n                  nodeSelector:\n                    node-role.kubernetes.io/etcd: \"true\"\n                    beta.kubernetes.io/os: \"linux\"\n                  tolerations:\n                    - effect: \"NoExecute\"\n                      key: \"CriticalAddonsOnly\"\n                      operator: \"Exists\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/server/tls/etcd/server-client.crt\n                    - /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt\n                worker:\n                  affinity:\n                    nodeAffinity:\n                      requiredDuringSchedulingIgnoredDuringExecution:\n                        nodeSelectorTerms:\n                          - matchExpressions:\n                              - key: \"node-role.kubernetes.io/worker\"\n                                operator: In\n                                values:\n                                  - \"true\"\n                          - matchExpressions:\n                              - key: \"node-role.kubernetes.io/worker\"\n                                operator: In\n                                values:\n                                  - \"worker\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/agent/client-ca.crt\n                    - /var/lib/rancher/rke2/agent/client-kubelet.crt\n                    - /var/lib/rancher/rke2/agent/client-kube-proxy.crt\n                    - /var/lib/rancher/rke2/agent/client-rke2-controller.crt\n                    - /var/lib/rancher/rke2/agent/server-ca.crt\n                    - /var/lib/rancher/rke2/agent/serving-kubelet.crt\n            prometheusPodMonitor:\n              create: true\n            secretsExporter:\n              enabled: false\n</code></pre>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#2-prometheus-alert-rules","title":"2\ufe0f\u20e3 Prometheus Alert Rules","text":"<p>Add the following alert rules to the Prometheus alerting configuration to monitor the exporter and certificate expiration:</p> <pre><code>groups:\n  - name: x509-certificate-exporter.rules\n    rules:\n      - alert: X509ExporterReadErrors\n        annotations:\n          summary: Increasing read errors for x509-certificate-exporter\n          description: &gt;\n            Over the last 15 minutes, this x509-certificate-exporter instance\n            has experienced errors reading certificate files or querying the Kubernetes API.\n            This could be caused by a misconfiguration if triggered when the exporter starts.\n        expr: delta(x509_read_errors[15m]) &gt; 0\n        for: 5m\n        labels:\n          severity: warning\n\n      - alert: CertificateRenewal\n        annotations:\n          summary: Certificate should be renewed\n          description: &gt;\n            Certificate for \"{{ $labels.subject_CN }}\" should be renewed\n            {{if $labels.secret_name }}in Kubernetes secret \"{{ $labels.secret_namespace }}/{{ $labels.secret_name }}\"\n            {{else}}at location \"{{ $labels.filepath }}\"{{end}}\n        expr: ((x509_cert_not_after - time()) / 86400) &lt; 28\n        for: 15m\n        labels:\n          severity: warning\n\n      - alert: CertificateExpiration\n        annotations:\n          summary: Certificate is about to expire\n          description: &gt;\n            Certificate for \"{{ $labels.subject_CN }}\" is about to expire\n            {{if $labels.secret_name }}in Kubernetes secret \"{{ $labels.secret_namespace }}/{{ $labels.secret_name }}\"\n            {{else}}at location \"{{ $labels.filepath }}\"{{end}}\n        expr: ((x509_cert_not_after - time()) / 86400) &lt; 14\n        for: 15m\n        labels:\n          severity: critical\n</code></pre>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#grafana-dashboard-integration","title":"\ud83d\udcca Grafana Dashboard Integration","text":"<p>To visualize certificate metrics, import the official Grafana dashboard created for the <code>x509-certificate-exporter</code>.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#importing-via-grafanacom-dashboard-id","title":"\ud83d\udd27 Importing via Grafana.com Dashboard ID","text":"<ol> <li>Log in to your Grafana instance.</li> <li>Go to the left menu and select + Create &gt; Import.</li> <li>In the Import via grafana.com field, enter the following dashboard ID:</li> </ol> <pre><code>13922\n</code></pre> <ol> <li>Click Load.</li> <li>Select your Prometheus data source.</li> <li>Click Import to complete the setup.</li> </ol> <p>This dashboard provides a visual representation of certificate expiration dates, issuers, subjects, and alert states.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#summary","title":"\u2705 Summary","text":"<p>By completing the above steps, you will have:</p> <ul> <li>Deployed a certificate monitoring exporter to your Kubernetes cluster</li> <li>Configured Prometheus alert rules for expiring certificates</li> <li>Visualized certificate metrics in Grafana</li> <li>Ensured proactive alerting and observability for certificate expiration</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#references","title":"\ud83d\udce6 References","text":"<ul> <li>Enix x509-certificate-exporter</li> <li>Helm Chart Repository</li> <li>Grafana Dashboard 13922</li> </ul>"},{"location":"devops/networking/aws-onprem-network-connectivity/","title":"Validating End-to-End Network Connectivity in EKS Hybrid Architectures","text":"<p>In an EKS Hybrid architecture, the Kubernetes control plane is managed in AWS, while worker nodes may operate on-premises or across distinct network domains.</p> <p>Ensuring reliable, <code>bidirectional connectivity between on-premises pods and the AWS-hosted control plane network</code> is essential for cluster stability and troubleshooting.</p> <p>This documentation walks through a practical and reproducible method for validating ICMP reachability and routing paths (ping, traceroute) using a temporary EC2 instance as the AWS-side reference and an NGINX-based debug pod running on a designated hybrid node.</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#export-required-environment-variables","title":"Export Required Environment Variables","text":"<p>We start by exporting the AWS and network-related variables that will be reused throughout the guide.</p> <p>Note:   - VPC_ID must be the same VPC where the EKS cluster resides   - SUBNET_ID must be one of the cluster subnets</p> <pre><code>export AWS_PROFILE=AWS_PROFILE\nexport AWS_DEFAULT_REGION=AWS_DEFAULT_REGION\n\nexport VPC_ID=VPC_ID\nexport SUBNET_ID=SUBNET_ID\nexport KP_NAME=KP_NAME\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#create-a-security-group-for-network-testing","title":"Create a Security Group for Network Testing","text":"<p>We create a temporary security group that allows all inbound and outbound traffic. This is strictly for network diagnostics and must not be used in production workloads.</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#create-the-security-group","title":"Create the security group","text":"<pre><code>SG_ID=$(aws ec2 create-security-group \\\n  --group-name netshoot-test-all \\\n  --description \"Allow all -- network test for hybrid cluster\" \\\n  --vpc-id $VPC_ID \\\n  --query 'GroupId' \\\n  --output text)\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#allow-all-traffic","title":"Allow all traffic","text":"<pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id $SG_ID \\\n  --protocol -1 \\\n  --port -1 \\\n  --cidr 0.0.0.0/0\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#launch-a-temporary-ec2-instance","title":"Launch a Temporary EC2 Instance","text":"<p>To avoid hardcoding AMI IDs, we dynamically fetch the latest image from SSM.</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#fetch-latest-ami-id","title":"Fetch latest AMI ID","text":"<pre><code>AMI_ID=$(aws ssm get-parameter \\\n  --name /aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64 \\\n  --query 'Parameter.Value' \\\n  --output text)\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#create-the-ec2-instance","title":"Create the EC2 instance","text":"<pre><code>aws ec2 run-instances \\\n  --image-id $AMI_ID \\\n  --instance-type t2.small \\\n  --key-name $KP_NAME \\\n  --subnet-id $SUBNET_ID \\\n  --security-group-ids $SG_ID \\\n  --tag-specifications 'ResourceType=instance,Tags=[\n      {Key=Name,Value=netshoot-test-ec2},\n      {Key=DeleteAfterTest,Value=true},\n      {Key=Team,Value=DevOps}\n  ]'\n</code></pre> <p>This EC2 instance represents the AWS-side network plane of the EKS Hybrid Cluster.</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#deploy-an-nginx-debug-pod-on-a-specific-hybrid-node","title":"Deploy an NGINX Debug Pod on a Specific Hybrid Node","text":"<p>To perform deterministic network testing in a Hybrid EKS setup, it is important to control where the test pod is scheduled.</p> <p>\u26a0\ufe0f <code>spec.nodeName</code> is a static field and must be resolved before applying the manifest. As new hybrid worker nodes are added to the cluster, this value can be updated to point to the newly joined nodes.</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#apply-the-manifest","title":"Apply the manifest","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-net-debug\nspec:\n  nodeName: NODE_NAME\n  containers:\n  - name: nginx\n    image: public.ecr.aws/nginx/nginx:latest\n    command:\n      - sh\n      - -c\n      - |\n        apt-get update &amp;&amp; \\\n        apt-get install -y \\\n          iputils-ping \\\n          traceroute \\\n          telnet \\\n          netcat-openbsd \\\n          curl &amp;&amp; \\\n        nginx -g 'daemon off;'\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#run-network-tests-from-pod-ec2","title":"Run Network Tests from Pod \u2192 EC2","text":"<p>Exec into the pod:</p> <pre><code>kubectl exec -it nginx-net-debug -- bash\n</code></pre> <p>Run connectivity tests toward the EC2 instance:</p> <pre><code>ping EC2_PRIVATE_IP\ntraceroute EC2_PRIVATE_IP\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#run-network-tests-from-ec2-pod","title":"Run Network Tests from EC2 \u2192 Pod","text":"<p>SSH into the EC2 instance and test connectivity back to the pod:</p> <pre><code>ping POD_IP\ntraceroute POD_IP\n</code></pre> <p>Below is an example of a successful round-trip network test from the pod:</p>"},{"location":"devops/networking/aws-onprem-network-connectivity/#ping-output","title":"Ping output","text":"<pre><code>root@nginx-net-debug:/# ping 10.20.78.244\nPING 10.20.78.244 (10.20.78.244) 56(84) bytes of data.\n64 bytes from 10.20.78.244: icmp_seq=1 ttl=121 time=58.3 ms\n64 bytes from 10.20.78.244: icmp_seq=2 ttl=121 time=58.4 ms\n64 bytes from 10.20.78.244: icmp_seq=3 ttl=121 time=58.2 ms\n--- 10.20.78.244 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss\n</code></pre>"},{"location":"devops/networking/aws-onprem-network-connectivity/#traceroute-output","title":"Traceroute output","text":"<pre><code>root@nginx-net-debug:/# traceroute 10.20.78.244\ntraceroute to 10.20.78.244 (10.20.78.244), 30 hops max\n 1  * * *\n 2  10.15.78.158 (10.15.78.158)  0.592 ms\n 3  172.28.120.254 (172.28.120.254)  3.159 ms\n 4  172.28.3.253 (172.28.3.253)  3.095 ms\n 5  172.28.99.52 (172.28.99.52)  3.050 ms\n 6  192.168.30.2 (192.168.30.2)  2.928 ms\n 7  10.20.78.244 (10.20.78.244)  58.149 ms\n</code></pre> <p>This approach provides a clear, reproducible method to validate:</p> <ul> <li>Pod \u2192 AWS Control Plane</li> <li>AWS Control Plane \u2192 Pod connectivity</li> <li>Routing paths across hybrid boundaries</li> </ul> <p>All without custom images or private registries.</p>"},{"location":"devops/nexus/docker-hosted-repo/","title":"Docker Hosted Repository","text":""},{"location":"devops/nexus/docker-hosted-repo/#docker-hosted-repository","title":"Docker Hosted Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(hosted)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8082.</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul>"},{"location":"devops/nexus/docker-hosted-repo/#push-an-image-to-nexus","title":"Push an image to Nexus","text":"<p>Tag the image to repository url with HTTP connector port:</p> <pre><code>docker tag &lt;IMAGE&gt;:&lt;VERSION&gt; &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8082\" # hosted repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>If access to a repository requires the user to be authenticated, Docker will check for authentication access in the <code>.docker/config.json</code>file on your local machine. If authentication is not found, you will need to perform a <code>docker login</code> command.</p> <pre><code>docker login -u &lt;username&gt; &lt;EC2_PUBLIC_IP&gt;:8082\n</code></pre> <p>Then, push the image:</p> <pre><code>docker push &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre>"},{"location":"devops/nexus/docker-proxy-repo/","title":"Docker Proxy Repository","text":""},{"location":"devops/nexus/docker-proxy-repo/#docker-proxy-repository","title":"Docker Proxy Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(proxy)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8083.</li> <li>Enable Docker V1 API support if required by the remote repository.</li> <li>Add remote storage URL being proxied (e.g. https://registry-1.docker.io, https://gcr.io)</li> <li>If your remote repository is docker hub, select docker index as \"Use Docker Hub\". Otherwise, select \"Use proxy registry (specified above)\"</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8083\"  # proxy repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>From docker cli, pull an image but don't pull it from docker hub or gcr, pull it through the HTTP endpoint of your docker proxy repo that you have created above like so:</p> <pre><code>docker pull &lt;EC2_PUBLIC_IP&gt;:8083/example-image\n</code></pre> <p>This will create a pull request to your Nexus OSS, which will proxy the request to remote repository you specified before. The image from remote repository will be cached in your Nexus and will be delivered to you.</p>"},{"location":"devops/nexus/nexus-installation/","title":"Nexus Installation - Docker Private Registry","text":"<p>This guide shows how to install Nexus to an EC2 instance and run as a container.</p>"},{"location":"devops/nexus/nexus-installation/#requirements","title":"Requirements","text":"<ul> <li>EC2 Instance</li> <li>Docker</li> </ul>"},{"location":"devops/nexus/nexus-installation/#ec2-configurations","title":"EC2 Configurations","text":"<ul> <li>Min. 4 GB memory</li> <li>Edit security group rules to allow port range from 8080 to 8082.</li> <li>Enable Auto-assign public IP</li> </ul>"},{"location":"devops/nexus/nexus-installation/#installation","title":"Installation","text":"<p>Since docker volumes are persistent, a volume can be created specifically for this purpose. This is the recommended approach.</p> <pre><code>docker volume create --name nexus-data\n</code></pre> <p>The next step is to mount the volume with docker run command. We will use port 8081 to connect Nexus. Other port(s) are used for repository connections. Repository ports must be unique.</p> <p>In this guide, we open port 8082 as docker hosted repository connection and port 8083 as docker proxy connection.</p> <pre><code>docker run -d -p 8081:8081 -p 8082:8082 -p 8083:8083 --restart always --name nexus -v nexus-data:/nexus-data sonatype/nexus3\n</code></pre> <p>Browse following URL:</p> <pre><code>http://&lt;EC2_INSTANCE_PUBLIC_IP&gt;:8081\n</code></pre> <p>Default username is admin. To see the password, run the command:</p> <pre><code>sudo cat /var/lib/docker/volumes/nexus-data/_data/admin.password\n</code></pre> <p> After login, it is mandatory to set a new password.</p>"},{"location":"devops/nexus/nexus-user-and-roles/","title":"User And Roles","text":""},{"location":"devops/nexus/nexus-user-and-roles/#principle-of-least-privilege","title":"Principle of least privilege","text":"<p>For security purposes, we should use roles and users to grant permissions for specific tasks.</p>"},{"location":"devops/nexus/nexus-user-and-roles/#create-role-and-user","title":"Create Role and User","text":"<ul> <li>Type : Select Nexus role</li> <li>Privileges: Add <code>nx-repository-admin-*-*-*</code> This permission will allow all actions for all artifact and repository types. <ul> <li>First and second \"*\" represent recipe and repository type (docker hosted, docker proxy, apt hosted, apt proxy etc.) </li> <li>Last one represents actions (add,browse,read,edit,delete)</li> </ul> </li> <li>Create a new user using the role just created.</li> </ul> <p> In Nexus Repository, the <code>Docker Bearer Token Realm</code> is required in order to allow anonymous pulls from Docker repositories</p> <p>To allow anonymous pull:</p> <ul> <li>Go to <code>Realms</code> in Secutiry, add <code>Docker Bearer Token Realm</code> to active category.</li> <li>Edit the repo and click <code>Allow anonymous docker pull</code></li> </ul>"},{"location":"devops/nexus/pull-to-kubernetes/","title":"Pull Images to Kubernetes","text":""},{"location":"devops/nexus/pull-to-kubernetes/#copy-nexus-credentials-into-kubernetes","title":"Copy Nexus Credentials into Kubernetes","text":"<p>As we mentioned before, the login process creates or updates a config.json file that holds an authorization token.</p> <p>View the config.json file:</p> <pre><code>cat ~/.docker/config.json\n</code></pre> <p>The output contains a section similar to this:</p> <p><pre><code>{\n    \"auths\": {\n        \"&lt;EC2_PUBLIC_IP&gt;:8082\": {\n            \"auth\": \"c3R...zE2\"\n        }\n    }\n}\n</code></pre> A Kubernetes cluster uses the secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p> <p>If you already ran docker login, you can copy that credential into Kubernetes:</p> <pre><code>kubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=~/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then, add the secret to default service account.</p> <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"secret-name\"}]}'\n</code></pre> <p>Here is a manifest for an example Pod that needs access to your Docker credentials:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-private-pod\nspec:\n  containers:\n    - name: private\n      image: yourusername/privateimage:version\n  imagePullSecrets:\n    - name: secret-name\n</code></pre>"},{"location":"devops/nexus/registry-configuration/","title":"RKE2 Registry Configuration","text":"<p>Upon startup, RKE2 will check to see if a <code>registries.yaml</code> file exists at <code>/etc/rancher/rke2/</code> and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.</p> <p>Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the <code>registries.yaml</code> file and give different examples of using private registry configuration in RKE2.</p>"},{"location":"devops/nexus/registry-configuration/#configuration-file","title":"Configuration File","text":"<p>The file consists of two main sections:</p> <ul> <li>mirrors</li> <li>configs</li> </ul> <p>Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\n</code></pre> <p>When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one.</p> <p>The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define <code>auth</code> and/or <code>tls</code>. The credentials consist of either username/password or authentication token.</p>"},{"location":"devops/nexus/registry-configuration/#with-tls","title":"With TLS","text":"<p>Below are examples showing how you may configure <code>/etc/rancher/rke2/registries.yaml</code> on each node when using TLS.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: username # this is the registry username\n      password: password # this is the registry password\n    tls:\n      cert_file:            # path to the cert file used to authenticate to the registry\n      key_file:             # path to the key file for the certificate used to authenticate to the registry\n      ca_file:              # path to the ca file used to verify the registry's certificate\n      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate\n</code></pre> <p> If using a registry using plaintext HTTP without TLS, you need to specify <code>http://</code> as the endpoint URI scheme.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"http://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n</code></pre>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/","title":"OpenTelemetry Instrumentation: Manual vs. Automatic","text":""},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#what-is-opentelemetry-instrumentation","title":"What Is OpenTelemetry Instrumentation?","text":"<p>OpenTelemetry provides a set of APIs, libraries, and agents designed to capture, process, and export telemetry data from software applications. Instrumentation refers to ensuring an application\u2019s code generates traces, metrics, and logs that make it observable.</p>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#manual-instrumentation","title":"Manual instrumentation","text":"<p>Involves adding specific code snippets to your application to capture and send telemetry data. It can capture custom metrics specific to your application, like items in a shopping cart or the time it takes to perform a specific database query. Manual instrumentation provides a high level of control and flexibility but takes time to implement.</p> <p>Advantages</p> <ul> <li> <p>Full Control: You have complete authority over what gets measured, when, and in how much detail. This allows you to focus exclusively on critical business transactions and functions, maximizing the signal-to-noise ratio.</p> </li> <li> <p>Rich Business Context: You can enrich traces with valuable, domain-specific attributes that are crucial for debugging and performance analysis. For example, you can add tags like customer_id, cart_value, or subscription_tier='premium' to a trace. This helps answer complex questions like, \"Why do requests slow down only for premium users with a cart value over $500?\"</p> </li> <li> <p>Reliability &amp; Independence: The instrumentation code is a part of your application's source code. It is not dependent on the internal implementation details of third-party frameworks, making it more robust against framework updates that might otherwise break instrumentation.</p> </li> </ul> <p>Disadvantages</p> <ul> <li> <p>Time-Consuming &amp; High-Effort: Writing, adding, and maintaining instrumentation code across a large codebase requires significant developer time and effort. As the application evolves or is refactored, this code must also be updated.</p> </li> <li> <p>Requires Expertise: Developers must be knowledgeable in both the application's architecture and the specific API of the observability SDK being used.</p> </li> </ul>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#automatic-instrumentation","title":"Automatic instrumentation","text":"<p>Involves using pre-built libraries or agents that automatically capture and send telemetry data without requiring you to modify your application\u2019s code. It is typically used to capture standard metrics like CPU usage, memory usage, request latency, and error rates. This is less flexible than manual instrumentation but much simpler and quicker to implement.</p> <p>Advantages</p> <ul> <li> <p>Fast &amp; Easy Setup: Getting started is often as simple as including an agent in the application's startup command. You can begin collecting a wide range of telemetry data within minutes.</p> </li> <li> <p>No Code Changes Required: The application's source code remains untouched. This is a major benefit, especially for legacy systems, applications with low test coverage, or when you don't have direct access to the code.</p> </li> <li> <p>Standardization: It enforces a consistent baseline of observability across all services and teams in an organization, ensuring that everyone gets a standard set of telemetry.</p> </li> </ul> <p>Disadvantages</p> <p>The Version Compatibility Nightmare:</p> <ul> <li> <p>Framework Version: The agent must explicitly support the exact version of the framework (e.g., Spring Boot 3.1.x) your project uses. If not, it may fail to identify the framework's internal classes and methods.</p> </li> <li> <p>Library Version: The versions of libraries like database drivers or HTTP clients must be recognized by the agent.</p> </li> <li> <p>Language Runtime Version: The agent must be compatible with the Java, Python, or Node.js version you are running.</p> </li> </ul>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#demonstration","title":"Demonstration","text":""},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#opentelemtry-operator","title":"OpenTelemtry Operator","text":"<p>First, install the OpenTelemetry Operator into your cluster.</p> <ul> <li>opentelemetry-operator-values.yaml</li> </ul> <pre><code>manager:\n    collectorImage:\n        repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s\nadmissionWebhooks:\n    certManager:\n        enabled: false \n    autoGenerateCert:\n        enabled: true\n</code></pre> <pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update\nhelm upgrade --install opentelemetry-operator open-telemetry/opentelemetry-operator -n monitoring -f opentelemetry-operator-values.yaml\n</code></pre>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<ul> <li>opentelemetry-collector-values.yaml</li> </ul> <pre><code>config:\n  exporters:\n    otlphttp/tempo:\n      endpoint: http://tempo-gateway.monitoring:80 ## add tempo service address\n      tls:\n        insecure: true\n    prometheusremotewrite:\n      endpoint: http://prometheus-kube-prometheus-prometheus.monitoring:9090/api/v1/write ## add prometheus service address\n      target_info:\n        enabled: true\n  extensions:\n    health_check:\n      endpoint: ${env:MY_POD_IP}:13133\n  receivers:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: 0.0.0.0:4317\n        http:\n          endpoint: 0.0.0.0:4318\n  service:\n    pipelines:\n      metrics:\n        exporters:\n        - debug\n        - prometheusremotewrite\n        processors:\n        - memory_limiter\n        receivers:\n        - otlp\n      traces:\n        exporters:\n        - otlphttp/tempo\n        processors:\n        - memory_limiter\n        - batch\n        receivers:\n        - otlp\nimage:\n  repository: docker.io/otel/opentelemetry-collector-contrib\nmode: daemonset\nports:\n  otlp:\n    appProtocol: grpc\n    containerPort: 4317\n    enabled: true\n    hostPort: 4317\n    protocol: TCP\n    servicePort: 4317\n  otlp-http:\n    containerPort: 4318\n    enabled: true\n    hostPort: 4318\n    protocol: TCP\n    servicePort: 4318\nreplicaCount: 1\n</code></pre> <pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update\nhelm upgrade --install opentelemetry-collector open-telemetry/opentelemetry-collector -f opentelemetry-collector-values.yaml -n monitoring\n</code></pre>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#opentelemetry-instrumentation","title":"OpenTelemetry Instrumentation","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: java-instrumentation\n  namespace: monitoring\nspec:\n  java:\n    image: \"otel/opentelemetry-javaagent:2.19.0\"\n  exporter:\n    endpoint: http://opentelemetry-collector.monitoring:4318\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"1\"\n</code></pre> <p>** Excluding auto-instrumentation</p> <p>By default, the Java auto-instrumentation ships with many instrumentation libraries. This makes instrumentation easy, but could result in too much or unwanted data. If there are any libraries you do not want to use you can set the OTEL_INSTRUMENTATION_[NAME]ENABLED=false where [NAME] is the name of the library. If you know exactly which libraries you want to use, you can disable the default libraries by setting OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false and then use OTEL_INSTRUMENTATION[NAME]_ENABLED=true where [NAME] is the name of the library. For more details, see Suppressing specific instrumentation.</p> <p>Environment-list-link</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: demo-instrumentation\nspec:\n  exporter:\n    endpoint: http://demo-collector:4318\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: '1'\n  java:\n    env:\n      - name: OTEL_INSTRUMENTATION_KAFKA_ENABLED\n        value: false\n      - name: OTEL_INSTRUMENTATION_REDISCALA_ENABLED\n        value: false\n</code></pre>"},{"location":"devops/opentelemetry/auto-vs-manuel-inject/#application-annotation","title":"Application Annotation","text":"<p>The final step is to opt in your services to automatic instrumentation. This is done by updating your service\u2019s spec.template.metadata.annotations to include a language-specific annotation:</p> <ul> <li>.NET: instrumentation.opentelemetry.io/inject-dotnet: \"true\"</li> <li>Go: instrumentation.opentelemetry.io/inject-go: \"true\"</li> <li>Java: instrumentation.opentelemetry.io/inject-java: \"true\"</li> <li>Node.js: instrumentation.opentelemetry.io/inject-nodejs: \"true\"</li> <li>Python: instrumentation.opentelemetry.io/inject-python: \"true\"</li> </ul> <p>The possible values for the annotation can be \"true\" - to inject Instrumentation resource with default name from the current namespace. \"my-instrumentation\" - to inject Instrumentation CR instance with name \"my-instrumentation\" in the current namespace. \"my-other-namespace/my-instrumentation\" - to inject Instrumentation CR instance with name \"my-instrumentation\" from another namespace \"my-other-namespace\". \"false\" - do not inject</p> <p>Note: For our example, add as the annotation <pre><code>instrumentation.opentelemetry.io/inject-java: monitoring/java-instrumentation\n</code></pre></p> <p>doc-link</p>"},{"location":"devops/postgres/backup-restore/","title":"Backup","text":"<p>https://www.postgresql.org/docs/current/app-pgdump.html</p>"},{"location":"devops/postgres/backup-restore/#opsec","title":"OPSEC","text":"<p>About the versions</p> <p>Make sure to use the same version on all postgres tooling. Do this OPSEC every time!</p> <ul> <li>Check the postgres config, including the version number on the last line:   <pre><code>pg_config\n</code></pre></li> <li>Check the <code>pg_dump</code> version:   <pre><code>pg_dump --version\npg_dumpall --version\n</code></pre></li> <li>Check the <code>psql</code> version:   <pre><code>psql --version\n</code></pre></li> <li>Check the <code>pg_restore</code> version:   <pre><code>pg_restore --version\n</code></pre></li> </ul> <p>Make sure all the versions match. Otherwise you might run into issues when restoring the backup.</p>"},{"location":"devops/postgres/backup-restore/#choosing-the-right-backup-tool","title":"Choosing the right backup tool","text":"tool right time to use <code>pg_basebackup</code> [Physical Copy] Typically used for disaster recovery scenarios or when you need a complete copy of the database cluster. It allows for easy and efficient restoration of the entire database cluster to a specific point in time. <code>pg_dumpall</code> [All DBs in Server] Routine backups, database migration, or replicating the database structure and data to another server. <code>pg_dump</code> [Single DB in Server] Commonly used for routine backups of individual databases, database migration, and selective restoration of specific databases or objects."},{"location":"devops/postgres/backup-restore/#pg_dump","title":"pg_dump","text":"<ul> <li><code>pg_dump</code> only dumps a single database.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_dumpall","title":"pg_dumpall","text":"<ul> <li> <p>Use <code>pg_dumpall</code> to back up an entire cluster, or to back up global objects that are common to all databases in a cluster (such as roles and tablespaces).</p> </li> <li> <p>Used for logical backups, creating a script that contains SQL commands to recreate the database objects and data.</p> </li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_basebackup","title":"pg_basebackup","text":"<ul> <li>Primarily used for creating physical backups of the entire PostgreSQL database cluster, including all databases, tablespaces, and configuration files.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_restore","title":"pg_restore","text":"<p>TODO: add pg_restore docs</p>"},{"location":"devops/postgres/configuration/","title":"Postgres Configuration","text":""},{"location":"devops/postgres/configuration/#helpers","title":"Helpers","text":"<ul> <li>postgresqlco.nf/ Documentation for explanations of all the configuration options.<ul> <li>You can upload your <code>postgresql.conf</code> file and it will give you recommendations on how to improve it.</li> </ul> </li> <li>pgtune.leopard.in.ua/#/ for generating a <code>postgresql.conf</code> file based on your hardware and database usage.</li> </ul>"},{"location":"devops/postgres/configuration/#notes","title":"Notes","text":"<ul> <li> <p>Always set the <code>PGDATA</code> environment variable to the path of the data directory. As the default directory used by the postgres image could change in the future.</p> </li> <li> <p>Always set the <code>POSTGRES_PASSWORD</code> environment variable. If not set, a random password will be generated and printed in the logs. </p> </li> <li>Always manage the password externally, for example by using a Kubernetes Secret.</li> </ul> <p>Status of postgres process</p> <ul> <li>Check the status     <pre><code>sudo systemctl restart postgresql.service\n</code></pre></li> <li>[If Applicable] Enable the service to start on boot      <pre><code>sudo systemctl enable postgresql.service\n</code></pre></li> </ul>"},{"location":"devops/postgres/configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li> <p>Located at: <code>/etc/postgresql/&lt;version&gt;/main/*</code></p> </li> <li> <p>After changing configuration, you must apply it with      <pre><code>sudo systemctl restart postgresql.service\n</code></pre></p> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_identconf-os-user-identification","title":"pg_ident.conf (OS User Identification)","text":"<ul> <li>This file is used for Operation System User -&gt; Database User mapping in PostgreSQL. </li> <li>It allows you to define mappings between a local operating system user and a PostgreSQL database user. </li> </ul>"},{"location":"devops/postgres/configuration/#postgresqlconf-configuration","title":"postgresql.conf (Configuration)","text":"<ul> <li>This is the primary configuration file for PostgreSQL. </li> <li>It contains a wide range of settings that control the behavior of the database server. </li> <li>These settings include parameters such as: <ul> <li>database connection settings, </li> <li>memory allocation, </li> <li>logging options, </li> <li>security configurations,</li> <li>and performance-related settings.</li> </ul> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_hbaconf-host-based-authentication","title":"pg_hba.conf (Host Based Authentication)","text":"<ul> <li>This file controls client authentication in PostgreSQL. </li> <li>It specifies the rules for allowing or denying client connections based on various authentication methods such as: <ul> <li>password authentication, </li> <li>certificate-based authentication, </li> <li>or IP address-based authentication. </li> </ul> </li> <li>It is responsible for defining who can connect to the database and how they are authenticated.</li> </ul>"},{"location":"devops/postgres/configuration/#pg_statconf-statistics","title":"pg_stat.conf (Statistics)","text":"<ul> <li>This file is related to the statistics collection in PostgreSQL. </li> <li>It defines which statistics are collected and how they are stored. </li> <li>By configuring this file, you can control the collection of various statistics such as: <ul> <li>the number of tuples read or written, </li> <li>index usage, </li> <li>query execution time, and more.  These statistics help in monitoring and performance tuning of the database.</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/","title":"PoC","text":""},{"location":"devops/postgres/poc-backup-restore/#setup","title":"Setup","text":"<ul> <li>create an Ubuntu VM</li> <li>install postgres</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#backup","title":"Backup","text":"<ul> <li>Assumes you have a shell access running postgres instance.<ul> <li>If you're trying to do this over a network, you'll need to figure out other options for the commands run (e.g. host, port, etc.)</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#steps","title":"Steps","text":""},{"location":"devops/postgres/poc-backup-restore/#1-go-to-a-directory-where-the-postgres-user-has-write-access","title":"1. Go to a directory where the <code>postgres</code> user has write access","text":"<pre><code>cd /tmp\n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#2-create-a-backup-file","title":"2. Create a Backup file","text":"<p>!!! Note If you're not certain on which tool to use, try to use the <code>pg_dumpall</code>.</p> <p>2.A. Using <code>pg_dump</code></p> <p>Only dumps a single database named <code>postgres</code>. </p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dump -U postgres -d postgres -f db_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dump -Fc -U postgres postgres &gt; db_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul> <p>2.B. Using <code>pg_dumpall</code></p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#upload-the-backup-file-to-a-remote-storage","title":"Upload the backup file to a remote storage","text":"<ul> <li>Create a script that will upload the backup file to a remote storage (e.g. AWS S3, Azure Blob Storage, etc.)</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#restore","title":"Restore","text":""},{"location":"devops/postgres/poc-backup-restore/#restore-a-single-db","title":"Restore a Single DB","text":"<p>We are now working on the host machine that'll be backed up to.</p> <p>[IA] Delete the existing DB from the server <pre><code>sudo -u postgres -- dropdb existing-db-name\n</code></pre></p> <p>[IA] Create a new DB to restore to</p> <p>Create a new DB named <code>new-db-name</code> from the <code>template0</code> template. <pre><code>sudo -u postgres -- createdb -T template0 new-db-name\n</code></pre></p> <p>Restore a single DB from .sql file</p> <p>SQL files are restored with <code>psql</code>.</p> <pre><code>sudo -u postgres -- psql -U postgres -f db_backup.sql new-db-name \n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#restore-a-entire-db-server","title":"Restore a Entire DB Server","text":""},{"location":"devops/postgres/poc-backup-restore/#check-the-restoration","title":"Check the Restoration","text":"<p>Enter the psql shell to verify the restore was successful. <pre><code>sudo -u postgres -- psql -U postgres\n</code></pre> Run psql commands: <pre><code>-- list databases \n\\l\n-- connect to the new db\n\\c new-db-name\n-- list tables\n\\dt\n-- list rows in a table\nSELECT * FROM users;\n-- list users\n\\du\n-- list groups\n\\dg\n</code></pre></p>"},{"location":"devops/postgres/psql/","title":"psql CLI","text":""},{"location":"devops/postgres/psql/#installation","title":"Installation","text":"<p>Make sure to use the appropriate version of the client for the server</p> <p><code>bash     sudo apt install postgresql-client-&lt;version-number&gt;</code></p> <p>Go to official Download Page</p>"},{"location":"devops/postgres/psql/#running-commands","title":"Running commands","text":"<p>Accessing with the OS user</p> <p>Run your commands as the <code>postgres</code> user. This is the default user created by the postgres image.</p> <pre><code># sudo -u postgres:: will run the command as the postgres user\nsudo -u postgres psql &lt;database-name&gt;\n</code></pre> <p>The <code>postgres</code> user is the only user that can connect to the database without a password, create other users and databases.</p> <p>Using the credentials</p> <pre><code>psql --host &lt;your-servers-dns-or-ip&gt; \\\n    --username postgres \\\n    --password \\\n    --dbname template1\n</code></pre>"},{"location":"devops/postgres/psql/#psql-commands","title":"psql commands","text":"Command Description <code>\\conninfo</code> Details about the current db connection <code>\\l</code> List all databases <code>\\c &lt;db-name&gt;</code> Connect/Select a database <code>\\dt+</code> Show tables in the current database <code>\\dg+</code> Show users in the current database"},{"location":"devops/rancher/rancher-installation/","title":"Rancher Installation","text":"<p>This is a follow up to \"RKE2 Ansible Installation\" and assumes you're working on an RKE2 cluster similar to the one set up in that document.</p> <p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> <li> <p>Install kubectl if it's not already installed</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm that our nodes and pods are correct and health</p> <pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#install-rancher-on-the-rke2-cluster","title":"Install Rancher on the RKE2 cluster","text":"<ol> <li> <p>Install Helm</p> <pre><code>sudo snap install helm --classic\n</code></pre> </li> <li> <p>Add Helm chart repository (used latest here, can be latest, stable or alpha)</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> </li> <li> <p>Create a namespace for Rancher</p> <pre><code>kubectl create namespace cattle-system\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-rancher-generated-tls-cert","title":"Using Rancher-Generated TLS Cert","text":"<ol> <li> <p>Install cert-manager (needed if using Rancher-generated TLS cert or Let\u2019s Encrypt)</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.11.0\n</code></pre> </li> <li> <p>Verify that cert-manager is deployed correctly</p> <pre><code>kubectl get pods --namespace cert-manager\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --set hostname=10.40.140.8.nip.io \\\n  --set bootstrapPassword=admin \\\n  --set global.cattle.psp.enabled=false\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> <li> <p>Wait for Rancher to be rolled out</p> <pre><code>watch kubectl -n cattle-system get pods\n</code></pre> </li> <li> <p>In a web browser navigate to the DNS name that points to your load balancer (ex: <code>10.40.140.8:nip.io</code>), you should see the login page</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-your-own-certs","title":"Using your own Certs","text":""},{"location":"devops/rancher/rancher-installation/#formatting-certs","title":"Formatting Certs","text":"<p>It can cause complications while editing files on a machine running some form of Windows and uploading them to a Linux server. Windows-based text editors put special characters at the end of lines to denote a line return or newline. There is a simple way to correct this problem.</p> <ul> <li> <p>Install <code>dos2unix</code> and execute the command to convert line endings from DOS to Unix</p> <pre><code>sudo apt install -y dos2unix\n\ndos2unix /path/to/file/&lt;file-name&gt;\n</code></pre> </li> <li> <p>Windows servers use .pfx files which contain the public and private key. However, this can also be converted to .pem files to be used on Linux server</p> <pre><code>openssl pkcs12 -in cert.pfx -nocerts -out tls.key -nodes\nopenssl pkcs12 -in cert.pfx -nokeys -out tls.crt\n</code></pre> </li> </ul>"},{"location":"devops/rancher/rancher-installation/#validating-certs","title":"Validating Certs","text":"<p>Before you set up your certificates, it's a good idea to test them to ensure that they are correct and will work together.</p> <ol> <li> <p>Check to see if the private key and main certificate are in PEM format. <code>openssl</code> must be installed</p> <pre><code>sudo apt install openssl -y\n\nopenssl rsa -inform PEM -in /path/to/key/tls.key\nopenssl x509 -inform PEM -in /path/to/cert/tls.crt\n</code></pre> </li> <li> <p>Verify that the private key and main certificate match</p> <pre><code>openssl x509 -noout -modulus -in tls.crt | openssl md5\nopenssl rsa -noout -modulus -in tls.key | openssl md5\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Verify that the public keys contained in the private key file and the main certificate are the same</p> <pre><code>openssl x509 -in tls.crt -noout -pubkey\nopenssl rsa -in tls.key -pubout\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Check the validty of certificate chain</p> <pre><code>openssl verify -CAfile cacerts.pem tls.crt\n\n# Response must be OK.\n</code></pre> </li> <li> <p>Check if <code>Subject Alternative Names</code> contains <code>Common Name</code></p> </li> </ol> <p>Subject Alternative Name must contains the same value of the CN. If it does not, the certificate is not valid because the industry moves away from CN</p> <pre><code>openssl x509 -noout -subject -in tls.crt\n# subject= /CN=&lt;example.domain.com&gt;\nopenssl x509 -noout -in tls.crt -text | grep DNS\n# DNS:&lt;example.domain.com&gt;\n</code></pre>"},{"location":"devops/rancher/rancher-installation/#create-secrets-and-install","title":"Create Secrets and Install","text":"<ol> <li> <p>Create tls-ca secret with your private CA's root certificate</p> <pre><code>kubectl -n cattle-system create secret generic tls-ca \\\n    --from-file=cacerts.pem=./cacerts.pem\n</code></pre> </li> <li> <p>Create cert and key secrets</p> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n    --cert=tls.crt \\\n    --key=tls.key\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n    --namespace cattle-system \\\n    --set hostname=10.40.140.8.nip.io \\\n    --set bootstrapPassword=admin \\\n    --set global.cattle.psp.enabled=false \\\n    --set ingress.tls.source=secret \\\n    --set privateCA=true\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#cleanup","title":"Cleanup","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Rancher using helm</p> <pre><code>helm uninstall rancher -n cattle-system\n</code></pre> </li> <li> <p>Remove Helm repositories</p> <pre><code>helm repo remove jetstack\nhelm repo remove rancher-latest\n</code></pre> <p>Change <code>rancher-latest</code> with the version you used while installing (ex: <code>stable</code>, <code>alpha</code>)</p> </li> <li> <p>Remove Helm</p> <pre><code>sudo snap remove helm\n</code></pre> </li> </ol>"},{"location":"devops/sealed-secrets/sealed-secrets/","title":"Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets","text":"<p>Let's start by defining Sealed Secrets and addressing the problem it aims to solve.</p> <p>When working with Kubernetes manifests, we often store them in Git version control systems alongside our application code. However, a challenge arises when dealing with Kubernetes Secret manifests, as they contain data that needs to be hidden but is stored in base64 format, which can be easily decrypted.</p> <p>This is where Sealed Secrets comes to the rescue. It allows us to encrypt our Secrets into SealedSecrets, making them safe to store, even in public repositories. The decryption of SealedSecrets is only possible by the controller running in the target cluster, ensuring that not even the original author can obtain the original Secret from the SealedSecret.</p> <p>To achieve this, two components are required: kubeseal, which we install locally, and the controller running in Kubernetes. With kubeseal, we encrypt our Secret manifests before pushing them to the Git repository. The Kubernetes controller is responsible for decrypting these encrypted Secret objects.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#sealed-secret-example","title":"Sealed Secret Example","text":"<p>These encrypted secrets are encoded in a SealedSecret resource, which you can see as a recipe for creating a secret. Here is how it looks:</p> <p><pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\nspec:\n  encryptedData:\n    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n</code></pre> Once unsealed, this produces a Secret equivalent to the following: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> Having explored how a Secret is securely stored in a Git repository, let's proceed to the usage.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#installation","title":"Installation","text":"<p>Firstly, let's set up the Sealed Secrets controller in Kubernetes. I'll perform this task using Helm.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#helm-chart","title":"Helm Chart","text":"<p>The Sealed Secrets helm chart is now officially supported and hosted in this GitHub repo.</p> <p><pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\n</code></pre> <pre><code>helm install sealed-secrets -n kube-system sealed-secrets/sealed-secrets\n</code></pre></p> <p>When the controller in the cluster up-and-running, it will generate a key. We will perform the encryption process with kubeseal, which we will install locally. You can find the kubeseal installation below for Mac and linux.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#kubeseal","title":"Kubeseal","text":""},{"location":"devops/sealed-secrets/sealed-secrets/#for-mac-homebrew","title":"For Mac (Homebrew)","text":"<p>The <code>kubeseal</code> client is also available on homebrew:</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"devops/sealed-secrets/sealed-secrets/#for-linux","title":"For Linux","text":"<p>The <code>kubeseal</code> client can be installed on Linux, using the below commands:</p> <p><pre><code>KUBESEAL_VERSION='' # Set this to, for example, KUBESEAL_VERSION='0.23.0'\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION:?}/kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> If you have <code>curl</code> and <code>jq</code> installed on your machine, you can get the version dynamically this way. This can be useful for environments used in automation and such.</p> <p><pre><code># Fetch the latest sealed-secrets version using GitHub API\nKUBESEAL_VERSION=$(curl -s https://api.github.com/repos/bitnami-labs/sealed-secrets/tags | jq -r '.[0].name' | cut -c 2-)\n\n# Check if the version was fetched successfully\nif [ -z \"$KUBESEAL_VERSION\" ]; then\n    echo \"Failed to fetch the latest KUBESEAL_VERSION\"\n    exit 1\nfi\n\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION}/kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> With the tools in place, both the cluster-side controller/operator and the client-side utility kubeseal are ready.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#usage","title":"Usage","text":"<p>Assuming we have a Kubernetes Secret manifest file named <code>mysecret.yaml</code> as follows: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> We can encrypt the Kubernetes Secret file using the following command: <pre><code>kubeseal --controller-name sealed-secrets -o yaml -n kube-system &lt; mysecrets.yaml &gt; encrypted-mysecret.yaml\n</code></pre> Subsequently, we can apply the <code>encrypted-mysecret.yaml</code> file: <pre><code>kubectl apply -f encrypted-mysecret.yaml\n</code></pre> To summarize the process:</p> <ul> <li>The Sealed Secrets controller, installed with Helm, generated public and private keys.</li> <li>We encrypted the decrypted mysecret.yaml file locally using the kubeseal command.</li> <li>When deploying the encrypted-mysecret.yaml file into Kubernetes, the controller decrypted it with the private key, converting it into a Kubernetes Secret.</li> </ul> <p>Now, with peace of mind, we can store our Secret manifests in Git repositories, especially if you are using GitOps, where you can automate your work by planning the file directory containing your encrypted manifests as an ArgoCD application.</p>"},{"location":"devops/service-mesh/istio/","title":"What is Istio?","text":"<p>Istio is an open-source service mesh solution that provides secure and observable networking between services running on Kubernetes. It helps manage service-to-service communication, enhances security, and controls traffic flow with minimal changes to application code. This document covers the following aspects of Istio:</p> <ul> <li>How Istiod (Istio Control Plane) works</li> <li>What mutual TLS (mTLS) is and how it secures service-to-service communication</li> <li>The differences between installing Istio using Helm and istioctl</li> <li>Different methods for enabling sidecar injection, including exclusions</li> <li>Demonstrating mTLS with two example pods</li> </ul>"},{"location":"devops/service-mesh/istio/#how-istiod-works","title":"How Istiod Works","text":"<p>Istiod is the control plane of Istio. It manages the configuration and behavior of the data plane, which consists of Envoy proxies deployed as sidecars in pods. The main responsibilities of Istiod include:</p> <ul> <li>Service Discovery: Detecting and managing services within the cluster.</li> <li>Configuration Management: Dynamically configuring Envoy proxies based on Istio policies.</li> <li>Traffic Control: Routing traffic based on rules defined in VirtualService and DestinationRule objects.</li> <li>Security Enforcement: Handling authentication, authorization, and mTLS policies.</li> <li>Observability: Collecting telemetry data such as logs, metrics, and traces for better monitoring.</li> </ul>"},{"location":"devops/service-mesh/istio/#what-is-mutual-tls-mtls","title":"What is Mutual TLS (mTLS)?","text":"<p>Mutual TLS (mTLS) is a security feature of Istio that ensures encrypted and authenticated communication between services. With mTLS enabled:</p> <ul> <li>Each service is issued a cryptographic certificate to verify its identity.</li> <li>Traffic between services is encrypted, preventing eavesdropping.</li> <li>Unauthorized services cannot communicate with protected services.</li> </ul> <p>Istio supports different mTLS modes:</p> <ul> <li>PERMISSIVE: Allows both plaintext and mTLS communication (useful for migration).</li> <li>STRICT: Enforces mTLS for all service-to-service communication.</li> <li>DISABLE: Turns off mTLS.</li> </ul>"},{"location":"devops/service-mesh/istio/#installing-istio-helm-vs-istioctl","title":"Installing Istio (Helm vs istioctl)","text":"<p>Istio can be installed using Helm or istioctl, with each having different advantages:</p> Method Pros Cons Helm More control over Kubernetes manifests, easier GitOps integration More manual configuration required istioctl Simplifies installation, automatic best-practice settings Less control over individual manifests"},{"location":"devops/service-mesh/istio/#install-istio-with-helm","title":"Install Istio with Helm","text":"<pre><code>helm repo add istio-official https://istio-release.storage.googleapis.com/charts\nhelm repo update\nhelm install my-istiod istio-official/istiod --version 1.25.0 -n istio-system --create-namespace\n</code></pre>"},{"location":"devops/service-mesh/istio/#enabling-sidecar-injection","title":"Enabling Sidecar Injection","text":"<p>Sidecar injection ensures that each pod in a namespace gets an Envoy proxy automatically. Istio supports different methods of enabling injection:</p>"},{"location":"devops/service-mesh/istio/#1-namespace-based-auto-injection","title":"1. Namespace-Based Auto Injection","text":"<pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> <ul> <li>This method enables automatic sidecar injection for all new pods in the <code>default</code> namespace.</li> <li>Existing pods must be restarted to get the sidecar.</li> </ul>"},{"location":"devops/service-mesh/istio/#2-manual-sidecar-injection","title":"2. Manual Sidecar Injection","text":"<pre><code>kubectl get deployment nginx -o yaml | istioctl kube-inject -f - | kubectl apply -f -\n</code></pre> <ul> <li>This injects the Envoy proxy manually into the <code>nginx</code> deployment.</li> <li>Useful for cases where you don't want automatic injection.</li> </ul>"},{"location":"devops/service-mesh/istio/#3-excluding-pods-from-injection","title":"3. Excluding Pods from Injection","text":"<p>To exclude specific pods from sidecar injection, use annotations:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: no-sidecar-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\nspec:\n  containers:\n  - name: app\n    image: nginx\n</code></pre> <p>This ensures that the pod does not get an Envoy sidecar, even if injection is enabled for the namespace.</p>"},{"location":"devops/service-mesh/istio/#verifying-sidecar-injection","title":"Verifying Sidecar Injection","text":"<p>To test if sidecar injection is working correctly, deploy an Nginx pod and check its status:</p> <pre><code>kubectl create deployment nginx --image=nginx\nkubectl get pods\n</code></pre> <p>Expected output (<code>READY</code> should be <code>2/2</code>):</p> <pre><code>NAME                     READY   STATUS    RESTARTS   AGE\nnginx-xxxxxxx-xxxxx      2/2     Running   0          1m\n</code></pre>"},{"location":"devops/service-mesh/istio/#enabling-and-testing-mtls","title":"Enabling and Testing mTLS","text":"<p>To enforce STRICT mTLS, apply the following <code>PeerAuthentication</code> policy:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\n</code></pre> <p>Apply with: <pre><code>kubectl apply -f mtls-policy.yaml\n</code></pre></p>"},{"location":"devops/service-mesh/istio/#testing-mtls-with-two-pods","title":"Testing mTLS with Two Pods","text":"<ol> <li>Deploy two pods (<code>client</code> and <code>server</code>) in the <code>default</code> namespace:</li> </ol> <pre><code>kubectl run client --image=curlimages/curl -i --tty -- sh\nkubectl run server --image=nginx\n</code></pre> <ol> <li>Try sending a request from <code>client</code> to <code>server</code>:</li> </ol> <pre><code>kubectl exec -it client -- curl http://server.default.svc.cluster.local\n</code></pre> <p>If mTLS is enabled (STRICT mode), the request should fail because direct HTTP (plaintext) communication is blocked. If mTLS were PERMISSIVE, the request would succeed.</p>"},{"location":"devops/service-mesh/istio/#conclusion","title":"Conclusion","text":"<ul> <li>Istiod manages service discovery, security, and traffic routing.</li> <li>mTLS ensures encrypted and authenticated service-to-service communication.</li> <li>Helm vs istioctl: Helm provides more control, istioctl is easier.</li> <li>Sidecar Injection can be enabled per namespace, manually, or excluded per pod.</li> <li>mTLS can be tested using two pods to verify encryption enforcement.</li> </ul> <p>This completes the basic Istio setup with security best practices.</p>"},{"location":"devops/service-mesh/istio/#kaynaklar","title":"Kaynaklar","text":"<ul> <li>Istio Resmi Dok\u00fcmantasyonu</li> <li>YouTube: Dok\u00fcmantasyona \u00f6\u011fretti\u011fi bilgilerle y\u00f6n veren Hintli abi \ud83e\udd1d</li> </ul>"},{"location":"devops/sonarqube/advanced-installation/","title":"How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide","text":"<p>SonarQube is a top-tier source code quality management application that provides comprehensive code analysis and support for 17 programming languages. It is the preferred solution for static code analysis and code coverage, and it is extensively used by both developers and organizations.</p> <p>This article will provide you with a step-by-step guide to establishing SonarQube using Docker Compose, which is integrated with a PostgreSQL database and a Nginx proxy to redirect traffic to port 80 on your domain.</p> <p>Finally, we will configure SonarQube authentication and authorization to an LDAP server by configuring the appropriate values in <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code>.</p>"},{"location":"devops/sonarqube/advanced-installation/#prerequisites","title":"Prerequisites","text":"<p>Before we start, ensure you have the following:</p> <ul> <li>An instance with a minimum of 2 vCPUs and 4 GB RAM.</li> <li>Docker and Docker Compose are installed on your machine.</li> <li>Open port 80</li> </ul> <p>We may move further with the system settings now.</p>"},{"location":"devops/sonarqube/advanced-installation/#system-configuration","title":"System Configuration","text":"<p>SonarQube has to make some system adjustments because it uses Elasticsearch to store its indices in an MMap FS directory. You must ensure that:</p> <ul> <li>The process is allowed to have a maximum of 524288 memory map areas, as specified by the <code>vm.max_map_count</code> parameter.</li> <li>The value of the maximum number of open file descriptors <code>fs.file-max</code> is set to a minimum of 131072.</li> <li>The SonarQube user has a minimum capacity to open 131072 file descriptors.</li> <li>The SonarQube user has the capability to initiate a minimum of 8192 threads.</li> </ul> <p>Use the steps provided below according to your operating system:</p>"},{"location":"devops/sonarqube/advanced-installation/#for-red-hat-centos-or-amazon-linux","title":"For Red Hat, CentOS, or Amazon Linux","text":"<pre><code>sudo yum update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#for-ubuntu-or-debian","title":"For Ubuntu or Debian","text":"<pre><code>sudo apt update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#installation","title":"Installation","text":"<p>In order to establish our services, we require a <code>docker-compose.yml</code> file.</p> <pre><code>version: \"3\"\n\nservices:\n  sonarqube:\n    image: sonarqube:latest\n    container_name: sonarqube\n    restart: unless-stopped\n    depends_on:\n      - db\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar\n      SONAR_JDBC_USERNAME: sonar\n      SONAR_JDBC_PASSWORD: sonar\n    volumes:\n      - sonarqube_data:/opt/sonarqube/data\n      - sonarqube_extensions:/opt/sonarqube/extensions\n      - sonarqube_logs:/opt/sonarqube/logs\n    ports:\n      - \"9000:9000\"\n\n  db:\n    image: postgres:15\n    container_name: postgresql\n    environment:\n      POSTGRES_USER: sonar\n      POSTGRES_PASSWORD: sonar\n      POSTGRES_DB: sonar\n    volumes:\n      - postgresql:/var/lib/postgresql\n      - postgresql_data:/var/lib/postgresql/data\n\n  nginx:\n    image: nginx:latest\n    container_name: nginx\n    restart: unless-stopped\n    depends_on:\n      - sonarqube\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n    ports:\n      - \"80:80\"\n\nvolumes:\n  sonarqube_data:\n  sonarqube_extensions:\n  sonarqube_logs:\n  postgresql:\n  postgresql_data:\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#what-we-have-here","title":"What We Have Here?","text":"<p>Three services (SonarQube, PostgreSQL, and Nginx) are included in this compose file.</p> <ul> <li>SonarQube Service uses the latest SonarQube image, connects to the PostgreSQL database, and exposes port 9000 for the web interface. It includes volumes for persistent data, extensions, and logs.</li> <li>PostgreSQL Service sets up a PostgreSQL database with environment variables for user credentials and includes volumes for data persistence.</li> <li>Nginx Service acts as a reverse proxy using the latest Nginx image, mapping port 80 to the host. It relies on a custom Nginx configuration file.</li> </ul> <p>In order to redirect the traffic from <code>localhost:9000</code> to your domain on port 80, it is necessary to create a <code>nginx.conf</code> file. Make sure that this file is located in the same directory as the docker-compose.yml file.</p> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n\n    location / {\n        proxy_pass http://sonarqube:9000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#running-the-setup","title":"Running the Setup","text":"<p>Run the following command to start the setup:</p> <p><pre><code>sudo docker-compose up -d\n</code></pre> Docker Compose orchestrates and executes your complete application. To access SonarQube, use the domain indicated in your <code>nginx.conf</code> file, which is <code>yourdomain.com</code></p>"},{"location":"devops/sonarqube/advanced-installation/#ldap-configuration","title":"LDAP Configuration","text":"<p>Integrating LDAP (Lightweight Directory Access Protocol) with SonarQube is an essential process for firms seeking to centrally manage user authentication and authorization. LDAP integration enables the utilization of an already established LDAP directory, such as Active Directory, for the purpose of managing users. This streamlines administration by ensuring that there is just one authoritative source for user data. </p> <p>The main setup for LDAP integration in SonarQube is performed via the <code>sonar.properties</code> file. This file is usually located in  <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code></p> <pre><code># LDAP Configuration\n# Enable the LDAP feature\nsonar.security.realm=LDAP\n\n# General Configuration\nldap.url=ldap://ldap_server_ip:389\nldap.bindDn=CN=your_sonar_user,OU=Service Accounts,DC=your_domain,DC=top_level_domain\nldap.bindPassword=password_here\n\n# User Configuration\nldap.user.baseDn=DC=your_domain,DC=top_level_domain\nldap.user.request=(&amp;(objectClass=user)(sAMAccountName={login}))\nldap.user.realNameAttribute=cn\nldap.user.emailAttribute=mail\n\n# Group Configuration\nldap.server1.group.baseDn=DC=your_domain,DC=top_level_domain\nldap.server1.group.request=(&amp;(objectClass=group)(memberUid={uid}))\n</code></pre> <p>To implement the new configuration, restart the SonarQube server after saving the changes to the sonar.properties file.</p> <pre><code>sudo systemctl restart sonarqube\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#accessing-sonarqube","title":"Accessing SonarQube","text":"<p>Verify the LDAP configuration by attempting to log in with an LDAP user account after SonarQube has restarted. Ensure that the user attributes, such as email and name, are accurately populated from the LDAP directory.</p> <p> </p>"},{"location":"devops/sre/k8sgpt/","title":"k8sgpt","text":"<p>As Kubernetes continues to grow in popularity, managing and monitoring your clusters can become quite complex. That's where k8sgpt comes in. k8sgpt is a tool that uses advanced AI to make Kubernetes management easier. It helps you with tasks like monitoring your clusters and solving problems by understanding and responding to natural language commands. In this blog, I\u2019ll look at what k8sgpt does, how it can help you, and why it might be a great addition to your Kubernetes setup.</p> <p>The project already supports multiple installation options and different AI backends such as openAI,localAI,azureAI etc. In this blog, I will show you how to install and get started with K8sGPT, both the CLI tools and the Operator as well as how K8sGPT supports additional integrations.</p>"},{"location":"devops/sre/k8sgpt/#installation","title":"Installation","text":"<p>You can choose from several installation methods based on your operating system and personal preferences. The installation section of the documentation provides a list of these options.</p> <pre><code>https://docs.k8sgpt.ai/getting-started/installation/\n</code></pre> <p>The installation of K8sGPT requires that Homebrew is installed on your Mac.</p> <p>Run the following commands:</p> <pre><code>brew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n</code></pre> <ul> <li>To verify that K8sGPT is install correctly, you can check the version installed:</li> </ul> <pre><code>k8sgpt version\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-cli","title":"K8sGPT CLI","text":"<p>The K8sGPT CLI is a tool that lets you manage your Kubernetes clusters using simple, natural language commands. It makes it easier to control and troubleshoot your clusters by letting you type commands.</p> <ul> <li>To view all the commands provided by K8sGPT:</li> </ul> <pre><code>k8sgpt --help\n</code></pre> <p></p> <p>You can see an overview of the different commands also below link:</p> <pre><code>https://docs.k8sgpt.ai/reference/cli/\n</code></pre>"},{"location":"devops/sre/k8sgpt/#authorise-an-ai-backend","title":"Authorise an AI Backend","text":"<p>K8sGPT supports multiple AI providers, allowing you to choose from a variety of options to fit your needs. For example, you can use providers like OpenAI, localAI and AzureAI. By default, OpenAI is the primary provider, but keep in mind that analyzing data with OpenAI requires a minimum balance of $5. To avoid this cost and still utilize powerful AI capabilities, I opted to use LocalAI for my setup. This choice allowed me to seamlessly integrate AI without incurring additional expenses.</p> <p>Ollama is an easy-to-use tool that helps you quickly start using open source large language models (LLMs). You can download Ollama from https://ollama.com/ and install it on your computer. Once installed, launch Ollama and use the command provided to download a LLM. Note that the mistral model requires ~ 4GB of disk space.</p> <pre><code>brew update\nbrew install ollama\nbrew services start ollama\nollama pull mistral\n</code></pre>"},{"location":"devops/sre/k8sgpt/#configure-k8sgpt-to-use-localai-using-ollama-backend","title":"Configure k8sgpt to use LocalAI using Ollama backend","text":"<pre><code>k8sgpt auth add --backend localai --model mistral --baseurl http://localhost:11434/v1\n</code></pre> <p>You can list your backends with the following command:</p> <pre><code>k8sgpt auth list\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#install-a-malicious-deployment","title":"Install a malicious Deployment","text":"<p>Next, we will install a malicious Deployment into our Kubernetes cluster.This manifest is deliberately malicious as it includes an incorrect command (slep instead of sleep), causing the pod to continuously fail and enter a CrashLoopBackOff state, which makes it an ideal example for analyzing and debugging issues with K8sGPT. Here is the YAML:</p> <p>deployment yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\n  labels:\n    app: example-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-container\n        image: busybox\n        command: [\"/bin/sh\", \"-c\", \"slep 3600\"]  # not slep\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vulnerable-service\n  labels:\n    app: vulnerable-app\nspec:\n  selector:\n    app: vulnerable-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> <ul> <li>Next, we will install the Deployment</li> </ul> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> <p>Now you will see the pods throwing errors:</p> <p></p>"},{"location":"devops/sre/k8sgpt/#analyze-your-cluster-by-using-k8s","title":"Analyze your cluster by using k8s","text":"<p>Run the command below to get GenAI-generated insights about problems in your cluster.</p> <pre><code>k8sgpt analyze --explain --backend localai --namespace default --filter Pod\nk8sgpt analyze --explain --backend localai --namespace default --filter Service\n</code></pre> <p></p> <p>The analysis provided by K8sGPT identifies a CrashLoopBackOff issue with the pod, indicating that the container within the pod encountered an error. The suggested steps to troubleshoot this include checking container logs, verifying the Docker image, ensuring correct configuration, scaling the deployment, and reviewing Kubernetes events for further insights.</p>"},{"location":"devops/sre/k8sgpt/#integration","title":"Integration","text":"<p>Integrating K8sGPT with tools like Trivy, Grafana, or Prometheus can enhance its capabilities for Kubernetes security, monitoring, and analysis.Trivy is a vulnerability scanner for container images and file systems. Integrating K8sGPT with Trivy can help analyze vulnerabilities in your Kubernetes clusters.</p> <p>You can list all available integration with the following command:</p> <pre><code>k8sgpt integration list\n</code></pre> <p>Next, we want to activate the Trivy integration:</p> <p><pre><code>k8sgpt integration activate trivy\n</code></pre> This will install the Trivy Operator inside of your cluster</p> <ul> <li>Once the integration is activated, we can use the Vulnerability Reports created by Trivy as part of our K8sGPT analysis through k8sgpt filters:</li> </ul> <p></p> <ul> <li>The filters in K8sGPT are mapped to specific analyzers in the code, which focus on extracting and examining only the most relevant information, such as the most critical vulnerabilities.</li> </ul> <p>To use the VulnerabilityReport filter </p> <pre><code>k8sgpt analyse --filter=VulnerabilityReport\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-operator","title":"K8sGPT Operator","text":"<p>The CLI tool lets cluster admins run on-the-spot scans on their infrastructure and workloads. Meanwhile, the K8sGPT operator works continuously in your cluster, running all the time. It is built specifically for Kubernetes, using Custom Resources to create reports that are stored in your cluster as YAML files.</p> <ul> <li>To install the Operator, follow the documentation link (https://docs.k8sgpt.ai/getting-started/in-cluster-operator/) or the commands provided below:</li> </ul> <p>You will  need to install the kube-prometheus-stack Helm Chart to use Grafana and Prometheus. You can do this with the following commands:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prom prometheus-community/kube-prometheus-stack -n monitoring --create-namespace\n</code></pre> <p>In this setup, we instruct K8sGPT to install a ServiceMonitor, which sends scan report metrics to Prometheus and sets up a Dashboard for K8sGPT.</p> <p>k8sgpt-values.yaml <pre><code>serviceAccount:\n  create: true\n  name: \"k8sgpt\"\n  # -- Annotations for the managed k8sgpt workload service account\n  annotations: {}\nserviceMonitor:\n  enabled: true\n  additionalLabels:\n    release: prom\n  namespace: monitoring\ngrafanaDashboard:\n  enabled: true\n  # The namespace where Grafana expects to find the dashboard\n  # namespace: \"\"\n  folder:\n    annotation: grafana_folder\n    name: ai\n  label:\n    key: grafana_dashboard\n    value: \"1\"\n  # create GrafanaDashboard custom resource referencing to the configMap.\n  # according to https://grafana-operator.github.io/grafana-operator/docs/examples/dashboard_from_configmap/readme/\n  grafanaOperator:\n    enabled: false\n    matchLabels:\n      dashboards: \"grafana\"\ncontrollerManager:\n  kubeRbacProxy:\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: gcr.io/kubebuilder/kube-rbac-proxy\n      tag: v0.16.0\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 5m\n        memory: 64Mi\n  manager:\n    sinkWebhookTimeout: 30s\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: ghcr.io/k8sgpt-ai/k8sgpt-operator\n      tag: v0.1.7  # x-release-please-version\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 10m\n        memory: 64Mi\n  replicas: 1\n  ## Node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  #\n  nodeSelector: {}\nkubernetesClusterDomain: cluster.local\nmetricsService:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: https\n  type: ClusterIP\n</code></pre></p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace -f k8sgpt-values.yaml\n</code></pre> <ul> <li>Then, we need to configure the K8sGPT Operator to know which version of K8sGPT to use and which AI backend:</li> </ul> <p>k8sgpt-backend.yaml <pre><code>apiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-ollama\nspec:\n  ai:\n    enabled: true\n    model: mistral\n    backend: localai\n    baseUrl: http://host.docker.internal:11434/v1\n  noCache: false\n  filters: [\"Pod\", \"Service\"]\n  repository: ghcr.io/k8sgpt-ai/k8sgpt\n  version: v0.3.17\n  # sink:\n  #   type: slack\n  #   webhook: &lt;url&gt;\n</code></pre> - We need to apply this file to our K8sGPT cluster namespace</p> <pre><code>kubectl apply -n k8sgpt-operator-system -f k8sgpt-backend.yaml\n</code></pre> <ul> <li>That\u2019s all with the operator setup. The operator will watch for problems in the cluster and generate analysis results that you can view using the command below</li> </ul> <pre><code>kubectl get results -n k8sgpt-operator-system -o json | jq .\n</code></pre> <ul> <li>During the installation of K8sGPT, we set serviceMonitor to true, which ensures that a ServiceMonitor is created to send metrics from K8sGPT to Prometheus. Once this is configured correctly, you should be able to see a new target for K8sGPT under the Prometheus targets section, specifically within the Kubernetes targets. This integration allows Prometheus to continuously monitor K8sGPT's performance and activity within the cluster, providing valuable insights through metrics.</li> </ul> <pre><code>kubectl port-forward service/prom-kube-prometheus-stack-prometheus -n monitoring 9090:9090\n</code></pre> <ul> <li>Open localhost:9090 and navigate to Status --&gt; Targets and then you should see the serviceMonitor with thes result:</li> </ul> <p></p> <ul> <li>Lastly Go to Grafana Dashboard.</li> </ul> <pre><code>kubectl port-forward service/prom-grafana -n monitoring 3000:3000\n</code></pre> <ul> <li>Open localhost:3000 and then navigate to Dashboards&gt;K8sGPT Overview and then you will see the Dashboard with your results:</li> </ul> <p></p> <p>This blog  explained the main features of K8sGPT. First, we looked at the K8sGPT CLI, then how it works better with integrations, and finally installed the operator for continuous analysis inside the cluster with grafana dashboard.</p>"},{"location":"devops/sumo/sumo-linux-collector/","title":"Install a Sumo Logic Collector on Linux","text":""},{"location":"devops/sumo/sumo-linux-collector/#download-a-sumo-logic-collector-from-a-static-url","title":"Download a Sumo Logic Collector from a Static URL","text":"<p>Invoke a web request utility such as wget. For Linux 64-bit host, you can wget the Collector from the command line:</p> <pre><code>wget \"https://collectors.sumologic.com/rest/download/linux/64\" -O SumoCollector.sh &amp;&amp; chmod +x SumoCollector.sh\n</code></pre> <p>For other hosts choose the related one above:</p> Platform Download URL Linux 64 https://collectors.au.sumologic.com/rest/download/linux/64 Linux Debian https://collectors.au.sumologic.com/rest/download/deb/64 Linux RPM https://collectors.au.sumologic.com/rest/download/rpm/64 Tarball https://collectors.au.sumologic.com/rest/download/tar Windows 32 https://collectors.au.sumologic.com/rest/download/windows Windows 64 https://collectors.au.sumologic.com/rest/download/win64 <p>Important Note: The latest release of the Sumo Collector targets the Java 8 runtime. Java 6 and Java 7 are no longer supported as the Collector runtime, and Solaris is no longer supported. When you upgrade Collectors, JRE 8 or later is required. The Sumo Collector with a bundled JRE now ships with JRE 8.</p>"},{"location":"devops/sumo/sumo-linux-collector/#system-requirements","title":"System Requirements","text":"<p>The Sumo Logic Collector has the following system requirements:</p> <ul> <li>Operating System: Linux, major distributions 64-bit, or any generic Unix capable of running Java 1.8</li> <li>CPU: Single core</li> <li>RAM: 512MB</li> <li>Disk Space: 8GB</li> <li>TLS: Package installers require TLS 1.2 or higher</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#install-using-the-command-line-installer","title":"Install using the command line installer","text":"<ol> <li>Add execution permissions to the downloaded Collector file (.sh):</li> </ol> <p><pre><code>chmod +x SumoCollector.sh\n</code></pre> 2. Run the script with the parameters that you want to configure </p>"},{"location":"devops/sumo/sumo-linux-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<p><pre><code>sudo ./SumoCollector.sh -q -Vsumo.token_and_url=&lt;installationToken&gt; -Vsources=&lt;absolute_filepath&gt;\n</code></pre> By default, the Collector will be installed in either <code>/opt/SumoCollector</code> or <code>/usr/local/SumoCollector</code>.</p>"},{"location":"devops/sumo/sumo-linux-collector/#other-parameters-for-the-command-line-installer","title":"Other parameters for the command line installer","text":"<p>The command line installer also supports a number of other parameters, including:</p> <ul> <li>-dir [directory] : Sets a different installation directory than the default.</li> <li>-Vsumo.accessid=[accessId] : The access ID is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.accesskey=[accessKey] : The access key is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.token_and_url=[token] : The token can be either an Installation Token or Setup Wizard Token.</li> </ul> <p>The command line installer can also use all of the parameters available in the user.properties file. To use parameters from user.properties just add a <code>-V</code> to the beginning of the parameter without a space character.</p> <p>The following parameters have a different format in the command line installer:</p> user.properties cli name <code>-Vcollector.name</code> url <code>-Vcollector.url</code> proxyHost <code>-Vproxy.host</code> proxyPort <code>-Vproxy.port</code> proxyUser <code>-Vproxy.user</code> proxyPassword <code>-Vproxy.password</code>"},{"location":"devops/sumo/sumo-linux-collector/#other-userproperties-parameters","title":"Other user.properties parameters","text":"<p>The user.properties file can also be used to configure the following parameters:</p> <ul> <li>-Vdescription : Description of the collector</li> <li>-VhostName : Name of the host machine that the collector is installed</li> <li>-Vsources : The contents of the file or files are read upon Collector registration only, it is not synchronized with the Collector's configuration on an on-going basis.</li> <li>-VsyncSources : The Source definitions will be continuously monitored and synchronized with the Collector's configuration.</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#start-or-stop-a-collector-using-scripts","title":"Start or Stop a Collector using Scripts","text":"<p>To start, stop, check the status of the Collector or restart it, run one of the following commands from the Collector installation directory: <pre><code>sudo ./collector start\n</code></pre> <pre><code>sudo ./collector stop\n</code></pre> <pre><code>sudo ./collector status\n</code></pre> <pre><code>sudo ./collector restart\n</code></pre></p>"},{"location":"devops/sumo/sumo-linux-collector/#uninstall-using-the-command-line","title":"Uninstall using the command line","text":"<ol> <li> <p>In a terminal prompt, change the directory to the collector installation directory:</p> </li> <li> <p>Run the uninstall binary with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts:</p> <pre><code>sudo ./uninstall -q\n</code></pre> </li> </ol>"},{"location":"devops/sumo/sumo-local-file-management/","title":"Local Configuration File Management","text":""},{"location":"devops/sumo/sumo-local-file-management/#local-configuration-file-management","title":"Local Configuration File Management","text":"<p>With local configuration file management, you can configure Sources for an Installed Collector in one or more UTF-8 encoded JSON files.</p> <p>IMPORTANT NOTE: After you switch over to local configuration file management, you can no longer manage Sources through the Sumo web application or the Collector Management API.</p> <p>Local configuration file management is available on Collector version v19.108 and later.</p> <p>Benefits of local configuration file management</p> <ul> <li>You don't need to log in to the Sumo web app or use API calls. Instead, you edit the JSON configuration file(s), and they are read almost immediately by the Collector.</li> <li>If you have a large scale deployment, it can be impractical to add or edit Sources one at a time. Using local configuration management allows you to manage Sources more easily.</li> <li>You can use deployment tools so that established policies for deployments are not interrupted.</li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#options-for-specifying-sources-in-local-configuration-files","title":"Options for specifying Sources in local configuration file(s)","text":"<p>There are two ways to implement local configuration file management:</p> <ol> <li>Specify all Sources in a single UTF-8 encoded JSON file.</li> <li>Use multiple UTF-8 encoded JSON files to specify your Sources, and put all of those files in a single folder.</li> </ol> <p>Note: Each JSON file must have a <code>.json</code> extension.</p>"},{"location":"devops/sumo/sumo-local-file-management/#define-multiple-sources-in-a-json-file","title":"Define multiple Sources in a JSON file","text":"<p>When you define multiple Sources in a JSON file, you can define each Source in a <code>sources</code> JSON array. For example:</p> <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"LocalFile\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    },\n    {\n      \"sourceType\": \"RemoteFile\",\n      \"name\": \"Example2\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre> <p>To define a single source in a JSON file, you just have one source definition under the <code>sources</code> array. <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"DockerLogs\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"devops/sumo/sumo-local-file-management/#configure-the-location-of-json-file-or-folder","title":"Configure the location of JSON file or folder","text":"<p>When using local file configuration management, you specify the location of the JSON file or the folder that contains multiple JSON files in the Collector's <code>config/user.properties</code> file. You need to use the <code>syncSources</code> parameter to point to your configuration file or folder.</p> <ul> <li> <p>To point to a JSON file that defines Sources for a Collector:</p> <p><code>syncSources=/path/to/sources.json</code></p> </li> <li> <p>To point to a folder that contains JSON files that define Sources for a Collector: <code>syncSources=/path/to/sources-folder</code></p> </li> <li> <p>On Windows (note the escaped backslashes): <code>syncSources=C:\\path\\to\\sources-folder\\</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#type-of-sources","title":"Type of Sources","text":"<p>In our case, we are using the <code>\"LocalFile\"</code> type value for Local File Type.</p> <p>Note: You should add the parameter <code>\"sourceType\":\"LocalFile\"</code> in the source file.</p> <p>JSON Parameters for Installed Sources</p> Parameter Description sourceType The type of the data that the collector will collect. description Type a description of the Source. category Type a category of the source. cutoffTimestamp If you have a file that contains logs with timestamps spanning an entire week and set the cutoffTimestamp to two days ago, all of the logs from the entire week will be ingested since the file itself was modified more recent than the cutoffTimestamp pathExpression A valid path expression (full path) of the file to collect denylist Comma-separated list of valid path expressions from which logs will not be collected. encoding Defines the encoding form. Default is \"UTF-8\"; options include \"UTF-16\"; \"UTF-16BE\"; \"UTF-16LE\"."},{"location":"devops/sumo/sumo-local-file-management/#important-tip","title":"Important Tip:","text":"<p>While giving the <code>pathExpression</code> use a single asterisk wildcard <code>[*]</code> for file or folder names. For example:</p> <p><code>pathExpression: \"/var/foo/*.log\"</code></p> <p>Use two asterisks <code>[**]</code> to recurse within directories and subdirectories. For example:</p> <p><code>pathExpression: \"var/**/*.log\"</code></p>"},{"location":"devops/sumo/sumo-local-file-management/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>You can edit the JSON configuration file at any time to edit Source attributes or add new Sources. When you delete Sources from the file, they are deleted from the Collector.</p> <p>To make the changes take effect, you need to restart the Collector.</p> <p>To restart the Collector, use these commands:</p> <ul> <li> <p>Linux: <code>sudo ./collector restart</code></p> </li> <li> <p>Windows: <code>net restart sumo-collector</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#full-example-of-local-source-file","title":"Full example of Local Source file :","text":"<pre><code>{\n   \"api.version\":\"v1\",\n   \"sources\":[\n      {\n         \"name\":\"Hepapi-Test-Logs\",\n         \"category\":\"DockerLogs\",\n         \"automaticDateParsing\":true,\n         \"multilineProcessingEnabled\":false,\n         \"useAutolineMatching\":false,\n         \"forceTimeZone\":false,\n         \"timeZone\":\"UTC\",\n         \"cutoffTimestamp\":0,\n         \"encoding\":\"UTF-8\",\n         \"pathExpression\":\"/var/lib/docker/containers/*/*-json.log\",\n         \"sourceType\":\"LocalFile\"\n      } \n   ]\n}\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/","title":"Install a Sumo Logic Collector on Windows","text":""},{"location":"devops/sumo/sumo-windows-collector/#system-requirements-for-windows","title":"System Requirements for Windows","text":"<ul> <li>Windows 7, 32 or 64 bit</li> <li>Windows 8, 32 or 64 bit</li> <li>Windows 8.1, 32 or 64 bit</li> <li>Windows 10, 32 or 64 bit</li> <li>Windows 11, 32 or 64 bit</li> <li>Windows Server 2012</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> <li>Single core, 512MB RAM</li> <li>8GB disk space</li> <li>Package installers require TLS 1.2 or higher.</li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#download-a-collector-from-a-static-url","title":"Download a Collector from a Static URL","text":"<ol> <li>Open a terminal window or command prompt.</li> <li>If you're using PowerShell on a 64-bit Windows host, you can use Invoke-WebRequest:</li> </ol>"},{"location":"devops/sumo/sumo-windows-collector/#configure-usage-of-tls","title":"Configure usage of TLS","text":"<pre><code>[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Tls,Tls11,Tls12'\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/#download-the-installer","title":"Download the installer","text":"<pre><code>Invoke-WebRequest 'https://collectors.sumologic.com/rest/download/win64' -outfile '&lt;download_path&gt;\\SumoCollector.exe'\n</code></pre> <p>Replace the  with the location where you want to download the Collector. For example, <code>C:\\user\\example\\path\\to\\SumoCollector.exe</code>"},{"location":"devops/sumo/sumo-windows-collector/#install-the-sumo-logic-collector-using-the-command-line-installer","title":"Install the Sumo Logic Collector using the command line installer","text":"<p>From the command prompt, run the downloaded EXE file with the parameters that you want to configure. For example:</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<pre><code>./SumoCollector.exe -console -q \"-Vsumo.token_and_url=&lt;installationToken&gt;\" \"-Vsources=&lt;filepath&gt;\"\n</code></pre> <p>Reminder: You can pass other <code>user.properties</code> parameters as well inside <code>\"\"</code>.</p> <p>When you see the <code>Finishing installation...</code> message, you can close the command prompt window. The installation is complete.</p> Parameter Description <code>-console</code> Only has an effect when used with <code>-q</code>. Causes the installer to send progress messages to the console. On Windows, for this option to take effect, you must run the installer with start /wait. For example: <code>start /wait installer.exe -q -console</code> <code>-q</code> Causes the installer to run silently, which means you won't be prompted to supply installation parameters. For any installation parameter that you do not define at the command line, Sumo will use a default value. No output is sent to the console during installation, unless you also use the <code>-console</code> parameter."},{"location":"devops/sumo/sumo-windows-collector/#configuring-sources-for-collectors","title":"Configuring Sources for Collectors","text":"<p>After installing Collectors, you can configure Sources directly in Sumo Logic or by providing the Source settings in a JSON file.</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-a-json-file","title":"Using a JSON file","text":"<p>If you're using a UTF-8 encoded JSON file, you must provide the file before starting the Collector. The JSON file needs to be UTF-8 encoded.</p> <p>Important Note:</p> <p>In Windows Host by using double backslashes, the JSON parser will interpret each backslash as a literal character rather than an escape character. For example:</p> <ul> <li>Backslashes are NOT treated correctly:  <code>\"-Vsources=&lt;C:\\some\\path\\to\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\path\\to\\source.json&gt;\"</code></li> <li>Backslashes ARE  treated correctly:    <code>\"-Vsources=&lt;C:\\\\some\\\\path\\\\to\\\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\\\path\\\\to\\\\source.json&gt;\"</code></li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#uninstalling-from-the-command-line","title":"Uninstalling from the command line","text":"<p>From the command prompt, run the <code>uninstall.exe</code> file with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts.</p> <pre><code>./uninstall.exe -q -console\n</code></pre>"},{"location":"devops/terragrunt/Readme/","title":"Readme","text":""},{"location":"devops/terragrunt/Readme/#what-is-terragrunt","title":"What is Terragrunt?","text":"<p>Terragrunt is a popular open-source tool or \u2018thin wrapper\u2019 developed by Gruntwork, that helps manage Terraform configurations by providing additional features and simplifying workflow. It is often used to address common challenges in Terraform, such as keeping configurations DRY (Don\u2019t Repeat Yourself), managing remote state, handling multiple environments, and executing custom code before or after running Terraform.</p> <p>See Terragrunt vs Terraform for further information.</p>"},{"location":"devops/terragrunt/Readme/#terragrunt-features","title":"Terragrunt features","text":"<ol> <li> <p>Remote state management Terragrunt simplifies remote state management for Terraform projects. It can automatically configure and store state files remotely in services like Amazon S3, Google Cloud Storage, or any other backend supported by Terraform.</p> </li> <li> <p>DRY (Don\u2019t Repeat Yourself) configurations Terragrunt promotes DRY principles by allowing you to define and reuse common configurations across multiple Terraform modules. This helps reduce duplication and makes configurations more maintainable.</p> </li> <li> <p>Dependency management Terragrunt supports dependency management between different Terraform modules and states, ensuring that dependent resources are deployed in the correct order.</p> </li> <li> <p>Configuration inheritance Terragrunt allows you to create modular configurations that can inherit parameters and settings from parent configurations, making it easier to manage and organize your infrastructure code.</p> </li> <li> <p>Environment-specific configurations Terragrunt supports the creation of environment-specific configurations (e.g., dev, staging, prod) using HCL (HashiCorp Configuration Language) interpolation, making it easier to maintain consistent environments.</p> </li> <li> <p>Remote backend configurations Terragrunt allows you to specify backend configurations (e.g., S3 bucket, DynamoDB table) for each environment, enabling a more dynamic and flexible approach to state storage.</p> </li> <li> <p>Locking mechanism Terragrunt provides a locking mechanism to prevent concurrent executions that could potentially cause conflicts when modifying shared infrastructure.</p> </li> <li> <p>Secrets management Terragrunt can integrate with external secrets management tools like AWS Secrets Manager or HashiCorp Vault to handle sensitive data securely.</p> </li> <li> <p>Integration with CI/CD pipelines Terragrunt can be integrated into continuous integration and continuous deployment (CI/CD) pipelines to automate infrastructure deployments.</p> </li> <li> <p>Configurable hooks Terragrunt supports pre- and post-terraform hooks, allowing you to run custom scripts or commands before or after running Terraform commands.</p> </li> </ol>"},{"location":"devops/terragrunt/Readme/#how-does-terragrunt-work","title":"How does Terragrunt work?","text":"<p>Terragrunt relies on a configuration file called <code>terragrunt.hcl</code>. This file is placed in the root directory of your Terraform project or in the directories of specific modules. It contains settings and parameters that customize Terragrunt\u2019s behavior for your project or module.</p>"},{"location":"devops/terragrunt/Readme/#how-to-install-terragrunt","title":"How to install Terragrunt?","text":"<p>STEP 1: Install Terraform As Terragrunt is a wrapper around Terraform, you\u2019ll need to have Terraform installed first. You can download the appropriate version of Terraform for your operating system here.</p> <p>STEP 2: Extract the binary and place it in a directory included in your system\u2019s PATH After downloading Terraform, extract the binary and place it in a directory included in your system\u2019s <code>PATH</code>. The PATH tells a system where it should look for executables, making them accessible via command-line interfaces or scripts. To add a new folder to PATH in Windows, navigate to Advanced System Settings &gt; Environment Variables, select PATH, click \u201cEdit\u201d and then \u201cNew.\u201d</p> <p>STEP 3: Download Terragrunt Next, head over to the Terragrunt GitHub page to download it.</p> <p>STEP 4: Place the Terragrunt binary in a directory included in your system\u2019s PATH Once you have downloaded the Terragrunt binary, place it in a directory included in your system\u2019s <code>PATH</code>. You may also rename the binary to simply <code>terragrunt</code> (without the platform-specific suffix) for convenience.</p> <p>STEP 5: Verify the installation Lastly, verify the installation by running <code>terragrunt --version</code> on your console command line. It should show the currently installed version.</p> <pre><code>terragrunt --version\n</code></pre>"},{"location":"devops/terragrunt/Readme/#terragrunt-basic-commands","title":"Terragrunt basic commands","text":"<p>Terragrunt command should be run from the project directory that contains your <code>terragrunt.hcl</code> configuration file. Terragrunt has many of the same commands available you will be familiar with the Terraform workflow, (you just need to replace <code>terraform</code> with <code>terragrunt</code>).</p> <p>These include:</p> <ul> <li>terragrunt init</li> <li>terragrunt validate</li> <li>terragrunt plan</li> <li>terragrunt apply</li> <li>terragrunt destroy</li> <li>terragrunt graph</li> <li>terragrunt state</li> <li>terragrunt version</li> <li>terragrunt output</li> </ul> <p>Also, check out this Terraform cheat sheet.</p>"},{"location":"devops/terragrunt/Readme/#how-to-set-up-terragrunt-configurations","title":"How to set up Terragrunt configurations?","text":"<p>First, create your <code>terragrunt.hcl</code> file in the directory you want to use Terragrunt in. The <code>terragrunt.hcl</code> file consists of configuration blocks that define various settings for Terragrunt.</p> <p>Note that the Terragrunt configuration file uses the same HCL syntax as Terraform itself in <code>terragrunt.hcl</code>. Terragrunt also supports JSON-serialized HCL in a <code>terragrunt.hcl.json</code> file: where <code>terragrunt.hcl</code> is mentioned, you can always use <code>terragrunt.hcl.json</code> instead.</p> <p>The <code>terraform</code> block is used to configure how Terragrunt will interact with Terraform. You can configure things like before and after hooks for indicating custom commands to run before and after each terraform call or what CLI args to pass in for each command.</p> <p>The source attribute specifies where to find Terraform configuration files and uses the same syntax as the Terraform module source attribute.</p> <p>For example, you can pull modules directly from a Github repo:</p> <pre><code>terraform { \n  source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"\n}\n</code></pre> <p>Or modules from the local file system (Terragrunt will make a copy of the source folder in the Terragrunt working directory, typically '.terragrunt-cache'):</p> <pre><code>terraform {  \n  source = \"../modules/networking/vpc\"\n}\n</code></pre> <p>Other blocks you can configure in your <code>terraform.hcl</code> file include:</p> <ul> <li>remote_state</li> <li>include</li> <li>locals</li> <li>dependency</li> <li>dependencies</li> <li>generate</li> </ul>"},{"location":"devops/terragrunt/Readme/#example","title":"Example","text":""},{"location":"devops/terragrunt/Readme/#step-0","title":"STEP 0","text":"<p>You will need <code>dev</code> and <code>prod</code> accounts. You can create them using your main account. Then, you should create an IAM user account for logging in multiple accounts, namely SSO. In our example, our account is the \"Management Account\" which controls the other accounts. Similarly, <code>dev</code> and <code>prod</code> accounts are the \"Environment Accounts\" on which the resources are created. The IAM user account should be able to login to all account using \"Access Portal\".</p> <p>Before starting, make sure that AWS CLI is installed and configured on your desktop. If not, refer to download page of AWS CLI. After successful download, configure the CLI with IAM credentials of created IAM user. If you don't have, you can get credentials under IAM service.</p> <pre><code>aws configure\n</code></pre> <p>You will be prompted to enter your AWS Access Key ID, AWS Secret Access Key, default region name and default output format (e.g., json). Now, you are ready to proceed. Below, you can see the folder structure of the example: </p> <pre><code>modules/\n\u2514\u2500\u2500 vpc/\n    \u251c\u2500\u2500 main.tf\n    \u251c\u2500\u2500 versions.tf\n    \u251c\u2500\u2500 variables.tf\n    \u2514\u2500\u2500 outputs.tf\n\nenvironments/\n\u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for dev\n\u251c\u2500\u2500 prod/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for prod\n\u2514\u2500\u2500 terragrunt.hcl               # Top-level configuration linking all environments\n\ninitial_configs/                  \n\u251c\u2500\u2500 AWSTerraformInitialConfigs_Management.yaml\n\u2514\u2500\u2500 AWSTerraformInitialConfigs_Environment.yaml\n</code></pre> <p>As you can see, there is a <code>vpc</code> module under <code>modules</code> folder. Refer to vpc module page to get the module. Then, change the static values as variables and define the variables in <code>variables.tf</code>. Also, you can add <code>outputs.tf</code> to check if the module is successfully created. Note that there is no <code>.tfvars</code> file and we will see handle this issue in the next steps.</p>"},{"location":"devops/terragrunt/Readme/#step-1","title":"STEP 1","text":"<p>Let's analyze the files under <code>initial_configs</code> folder. Starting with <code>AWSTerraformInitialConfigs_Management.yaml</code>, this CloudFormation template defines resources and configurations necessary for managing Terraform state using AWS S3 and DynamoDB, and it provisions an IAM user with the required permissions. </p> <p>PARAMETERS:</p> <ul> <li>Serial: A value used to notify CloudFormation to rotate access keys.</li> <li>IaCUserName: The IAM user name (default: terraform).</li> <li>TerraformStateBucketPrefix: Prefix for the S3 bucket storing Terraform state.</li> <li>TerraformStateLockTableName: Name of the DynamoDB table for state locking.</li> </ul> <p>RESOURCES:</p> <ul> <li> <p>IaCUser (IAM User): Creates an IAM user with tags indicating its provision through CloudFormation and its usage for management purposes.</p> </li> <li> <p>IaCUserPolicy (IAM Policy): Grants the IAM user permissions to: Manage the Terraform S3 bucket (create, access, and configure it). Lock Terraform state via DynamoDB (create, read, update, and delete items). Assume the TerraformExecutionRole for executing tasks.</p> </li> <li> <p>IaCUserAccessKey &amp; IaCUserSecret (IAM Access Keys &amp; Secret): Creates and stores the access keys in AWS Secrets Manager for secure access.</p> </li> <li> <p>TerraformStateS3Bucket (S3 Bucket): Creates an S3 bucket to store Terraform state files. It enforces security policies like blocking public access and enabling versioning.</p> </li> <li> <p>TerraformStateS3BucketBucketPolicy (S3 Bucket Policy): Adds a policy to the S3 bucket that denies the deletion of Terraform state files.</p> </li> <li> <p>TerraformStateLockDynamoDBTable (DynamoDB Table): Creates a DynamoDB table (LockID as the key) for Terraform state locking to prevent concurrent modifications of the state.</p> </li> </ul> <p>NOTE: You should create a stack in CloudFormation and upload this file on the \"Management Account\". Now, let's proceed with the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. </p> <p>PARAMETERS:</p> <ul> <li>IaCUserARN: A string parameter representing the ARN of the IAM user responsible for running Terraform. This user will be allowed to assume the role defined in the template. By default, this value needs to be provided (though a placeholder \"ARN of the IaC User\" is set).</li> </ul> <p>RESOURCES:</p> <ul> <li>TerraformExecutionRole (IAM Role): The TerraformExecutionRole is an IAM role that grants specific AWS permissions for Terraform operations, allowing a specified IAM user (via IaCUserARN) to assume it for a maximum of 4 hours. It is associated with the AdministratorAccess policy, providing full administrative privileges, and includes tags for tracking its provisioning through CloudFormation.</li> </ul> <p>NOTE: Look at the \"IaCUserARN\" from the \"Management Account\" and assign this value to the \"Parameters\" section of the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. You should create a stack in CloudFormation and upload this file on the \"Environment Accounts\".</p>"},{"location":"devops/terragrunt/Readme/#step-2","title":"STEP 2","text":"<p>Now, we are ready to analyze the <code>environments</code> folder. There is a top-level <code>terragrunt.hcl</code> under the folder which includes important configurations. This Terragrunt configuration file sets up local variables and configurations for managing Terraform modules and remote state.</p> <p>Local Variables: - base_source_url: Points to the local module source directory. - environment_vars and region_vars: Load environment-specific and region-specific variables from env.hcl and region.hcl files, respectively. - target_account, target_region, remote_state_account, and remote_state_bucket: Define the AWS account and region for the remote state and specify the bucket for storing Terraform state files.</p> <p>AWS Provider Configuration: Generates an aws provider block, setting the region for both the remote state and the target account, with an assume role for accessing the target account's resources.</p> <p>Remote State Configuration: Configures Terraform to use an S3 bucket for remote state storage, with encryption enabled and a DynamoDB table for state locking.</p> <p>Global Parameters: Merges global inputs from env.hcl and region.hcl, allowing all resources to inherit these configurations, which is useful for multi-account setups.</p> <p>Let's move with the environment folders. The structure is similar in both of the environments, so it will be enough to analyze one of them. Under the environments, there are regions in which the resources are created. Also, there is a <code>env.hcl</code> which includes local variables. The variables in the <code>env.hcl</code> are important because they determine the account. Diving into one of the regions, you can see the <code>region.hcl</code> which specifies the region. In the same directory, you can see the folders of the modules, which is <code>vpc</code> in our example. Under the <code>vpc</code> folder, there is a <code>terragrunt.hcl</code> file. This Terragrunt configuration file includes the root <code>terragrunt.hcl</code> configuration, which contains common settings for remote state management across all components and environments.</p> <p>Include Block: The configuration references the root settings using the include directive, allowing access to shared configurations and exposing them for use in the current module.</p> <p>Local Variables: It defines base_source_url from the root configuration, along with the module_name as \"vpc\" and module_version as \"v0.0.1.\"</p> <p>Terraform Source: The source for the Terraform module is set based on the base source URL, module name, and version.</p> <p>Inputs: Specifies the input variables for the VPC module, including availability zones, VPC CIDR block, NAT and VPN gateway settings, subnet configurations, and tags for environment identification.</p> <p>NOTE: Don't forget to change the necessary values of variables in this folder. For example, the value of \"aws_account_id\" in the <code>env.hcl</code>. Similarly, you should check the values in the both top and root level <code>terragrunt.hcl</code> files.</p>"},{"location":"devops/terragrunt/Readme/#step-3","title":"STEP 3","text":"<p>After compliting the necessary adjustments in the files and on the AWS console, you are ready to create the resources. You can open the folder with VSCode and using the terminal, proceed to the <code>vpc</code> folders in which the root level <code>terragrunt.hcl</code> is located. Finally, enter the following command to create VPC in desired region.</p> <pre><code>terragrunt apply\n</code></pre> <p>If you want to delete the resources, you should proceed to directory of related root level <code>terragrunt.hcl</code> file and enter the following command.</p> <pre><code>terragrunt destroy\n</code></pre> <p>For example, if you want to create a <code>dev-vpc</code> in <code>the us-east-1</code> region, go to <code>~/environments/dev/us-east-1/vpc</code> using the terminal and enter <code>terragrunt apply</code> command. Similarly, you can delete this using <code>terragrunt destroy</code>. Note that the <code>.tfstate</code> files are stored in S3 and they are locked in DynamoDB.</p> <p>The advantage of this configuration is that you can control various resources in different regions. In our example, we show this by creating a VPC  in two different regions. We created and configured multiple VPCs in different accounts and regions by only making minor changes in the structure. The same can be done using Terraform, but if the number of resources and regions increase, it would be faster and more practical to create with Terragrunt. Furthermore, you can control all of them from one point, namely \"Management Account\".</p>"},{"location":"devops/terragrunt/Readme/#terragrunt-benefits","title":"Terragrunt benefits","text":"<p>Where Terraform allows you the freedom to structure your code in multiple ways, Terragrunt places restraints on how you can organize your Terraform code and forces you to use directory structure hierarchies and shared variable definition files to organize your code. These restraints force your code to be more consistent and make it harder to make mistakes. The trade-off is that the amount of flexibility you have is reduced.</p> <p>The key to using Terragrunt effectively is to carefully plan your directory structure in order to keep your code base DRY. Organizing your infrastructure code into reusable modules that represent logical components of your infrastructure is one way to achieve this. </p>"},{"location":"devops/terragrunt/Readme/#key-points","title":"Key points","text":"<p>Terragrunt is a powerful tool that helps you manage Terraform configurations more efficiently. To make the most out of Terragrunt and maintain a clean, scalable, and organized infrastructure codebase, be sure to follow the best practices and plan your folder structure and use of Terragrunt carefully.</p>"},{"location":"devops/terragrunt/Readme/#references","title":"References","text":"<p>Terragrunt Tutorial \u2013 Getting Started &amp; Examples</p>"},{"location":"devops/vagrant/vagrant-quickstart/","title":"Vagrant","text":""},{"location":"devops/vagrant/vagrant-quickstart/#what-is-vagrant","title":"What is Vagrant?","text":"<p>CLI tool for managing the life-cycle of VMs</p> <ul> <li>Reproducible local dev environments  </li> <li>Vagrantfile, akin to Dockerfile for VMs</li> </ul>"},{"location":"devops/vagrant/vagrant-quickstart/#installation","title":"Installation","text":"<p>See the official Vagrant downloads page Ubuntu/Debian: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vagrant\n</code></pre></p> <p>You also need VirtualBox, VMware or Hyper-V on your machine.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#important-commands","title":"Important Commands","text":"<p>Initialize a new Vagrant environment by creating a Vagrantfile: <code>vagrant init</code> Starts and provisions the Vagrant environment: <code>vagrant up</code> Connects to machine via ssh: <code>vagrant ssh</code> Outputs status of the machine: <code>vagrant status</code> Suspends the machine: <code>vagrant suspend</code> Stops the machine: <code>vagrant halt</code> Stops and deletes all traces of the machine: <code>vagrant destroy</code> </p>"},{"location":"devops/vagrant/vagrant-quickstart/#boxes","title":"Boxes","text":"<p>Vagrant base images are called boxes. See the official Vagrant boxes page</p>"},{"location":"devops/vagrant/vagrant-quickstart/#synced-folders","title":"Synced Folders","text":"<p>By default, Vagrant will share your project directory (the directory with the Vagrantfile) to <code>/vagrant</code>.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#vagrantfile","title":"Vagrantfile","text":"<p>Example Vagrantfiles:</p> Vagrantfile for Jenkins   This Vagrantfile installs Jenkins, AWS CLI, unzip and zip tools.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/focal64\"\n\n  # Port forwarding\n  config.vm.network \"forwarded_port\", guest: 8080, host: 8080\n\n  config.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n    # Update repositories\n    sudo apt-get update\n\n    # Install CA certificates (optional but recommended)\n    sudo apt-get install -y ca-certificates\n\n    # Install Java (a requirement for Jenkins)\n    sudo apt-get install -y openjdk-11-jdk\n\n    # Add Jenkins repository\n    wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n    sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'\n\n    sudo apt-get update\n\n    # Install Jenkins\n    sudo apt-get install -y jenkins\n\n    # Start Jenkins\n    sudo systemctl start jenkins\n\n    # Install necessary utilities\n    sudo apt-get install -y unzip\n    sudo apt-get install -y zip\n\n    # Install AWS CLI version 2\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    unzip awscliv2.zip\n    sudo ./aws/install\n  SHELL\nend\n</code></pre> Vagrantfile using Ansible for provisioning   This Vagrantfile uses an Ansible playbook for provisioning.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.box = \"ubuntu/bionic64\"\n  config.vm.network :forwarded_port, guest: 80, host: 8080\n  config.vm.network :forwarded_port, guest: 443, host: 8081\n  config.vm.network :forwarded_port, guest: 8080, host: 8082\n  config.vm.provision \"ansible\" do |ansible|\n    ansible.playbook = \"main.yml\"\n  end\n\nend\n</code></pre> Vagrantfile with advanced networking   This Vagrantfile brings up 2 VMs, assigns them static hostnames and IPs, allows root login and password authentication, and installs Ansible on one of the VMs.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  # Define VMs\n  (1..2).each do |i|\n    config.vm.define \"vm#{i}\" do |vmconfig|\n\n      # Use CentOS 8\n      vmconfig.vm.box = \"generic/centos8\"\n\n      # Set hostname\n      vmconfig.vm.hostname = \"vm#{i}\"\n\n      # Set private network\n      vmconfig.vm.network \"private_network\", ip: \"192.168.56.1#{i}\"\n\n      # Sync project directory to /vagrant\n      vmconfig.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\"\n\n      # Enable provisioning with a shell script\n      vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n        echo 'vagrant:vagrant' | chpasswd\n        sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config\n        sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config\n        systemctl restart sshd\n      SHELL\n\n      if i == 1\n        vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n          sudo yum update -y\n          sudo yum install -y epel-release\n          sudo yum install -y python3-pip gcc openssl-devel libffi-devel python3-devel\n          sudo pip3 install --upgrade pip\n          sudo pip3 install setuptools_rust\n          pip3 install ansible\n        SHELL\n      end\n    end\n  end\n\n  # Enable ssh agent forwarding\n  config.ssh.forward_agent = true\n\nend\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/","title":"Markdown Syntax","text":""},{"location":"misc/how-to-contribute/about-markdown/#links","title":"Links","text":"<pre><code>[Link Text](https://www.example.com)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#images","title":"Images","text":"<p>Put a <code>!</code> in front of the link syntax.</p> <pre><code>![Alt Text](https://www.example.com/image.png)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#syntax-highlighting","title":"Syntax Highlighting","text":"<p>Insert the language name after the first set of backticks.</p> <pre><code># ```python\nimport os\nos.system(\"echo 'Hello World'\")\n</code></pre> <pre><code># ```bash\nexport HELLO=\"world\"\ncat some-file | grep \"hello\"\n</code></pre> <pre><code># ```yaml\nsome: key\nanother: key\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#tables","title":"Tables","text":"<pre><code>| name | value |\n| ---- | ----- |\n| a    | b     |\n</code></pre> <p>You can align headers to the left, center, or right by adding colons to the header syntax.</p> <pre><code>| --name-- | --value-- | description |\n| :------- | :-------: | ----------: |\n| a        |     b     |           c |\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#headers","title":"Headers","text":"<pre><code># H1 Header (biggest)\n\n## H2 Header\n\n### H3 Header\n\n#### H4 Header\n\n##### H5 Header\n\n###### H6 Header (smallest)\n</code></pre>"},{"location":"misc/how-to-contribute/about-mkdocs/","title":"creating new mkdocs pages","text":""},{"location":"misc/how-to-contribute/about-mkdocs/#how-to-add-a-new-page","title":"How to add a new Page","text":"<ol> <li>Create a new <code>.md</code> file under in the <code>docs/</code> folder</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section</li> <li>Do not put <code>docs/</code> prefix on the filepath</li> <li>Commit and push your changes to the <code>main</code> branch</li> <li>GitHub Action will automatically build and deploy the changes to the website.</li> </ol>"},{"location":"misc/how-to-contribute/about-mkdocs/#about-this-website","title":"About this website","text":"<p>Stack</p> Name Description mkDocs Docs static site generator Material for MkDocs Material theme for mkDocs"},{"location":"misc/how-to-contribute/about-mkdocs/#steps-to-run-it-locally","title":"Steps to run it locally","text":"<ol> <li>Make sure <code>python3</code> is installed</li> <li>Install python requirements    <pre><code>pip install mkdocstrings[python]\npip install mkdocs-material\n</code></pre></li> <li>Clone the repository &amp; <code>cd</code> into it</li> <li>Run the local server    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"misc/how-to-contribute/mkdocs-features/","title":"MkDocs and Material Theme Features","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#to-do-lists","title":"TO-DO Lists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> Aenean pretium efficitur erat</li> </ul>"},{"location":"misc/how-to-contribute/mkdocs-features/#admonitions-or-notes","title":"Admonitions or Notes","text":"<ul> <li>Supported Icon Types: note, abstract, info, tip, success, question, warning, failure, danger, error, example, quote</li> </ul> <p>Warning Note Header</p> <p>You can write notes like this to provide additional information or warnings.</p> <p>We've done it!</p> <p>Works for code blocks as well <pre><code>sudo apt install postgresql-client-&lt;version-number&gt;\n</code></pre></p> Collapsible Note (collapsed) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> Collapsible Note (expanded) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> <p>Tabbed Note</p> UbuntuRHEL/CentOS/AL2 <pre><code>apt install vim\n</code></pre> <pre><code>yum install vim\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#code-block-with-title","title":"Code block with title","text":"here is a codeblock title<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#buttons","title":"Buttons","text":"<p>Button Primary Button Emoji Button </p>"},{"location":"misc/how-to-contribute/mkdocs-features/#mermaid-diagrams","title":"Mermaid Diagrams","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#flow-chart","title":"Flow Chart","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#state-diagram","title":"State Diagram","text":"<pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre>"},{"location":"qa/","title":"Welcome to Hepapi QA Knowledge Hub","text":""},{"location":"qa/#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This section has been created to promote knowledge sharing, team collaboration, and continuous learning about Hepapi Teknoloji's QA (Quality Assurance) processes.</p> <p>QA, as one of the cornerstones of delivering high-quality software, does not only identify defects but also improves processes to ensure quality at every stage of the software development lifecycle. At Hepapi, our QA team works across different platforms with both manual and automation testing to deliver the best user experience.</p>"},{"location":"qa/#what-will-you-find-here","title":"What will you find here?","text":"<ul> <li>Fundamentals of QA: Explore every aspect of QA, from test planning to defect management.</li> <li>Automation Testing Tools: Guides on Selenium, Appium, and other testing tools.</li> <li>Best Practices: Tips for more effective and efficient testing processes. -FAQs and Guides: Solutions to frequently asked questions and helpful documentation.</li> </ul> <p>This knowledge hub is a living, evolving resource. As part of Hepapi\u2019s dynamic QA team, we continue to learn and share. If you have valuable insights or suggestions to share, don\u2019t hesitate to contribute! Remember, what might seem obvious to you could be a new discovery for someone else.</p> <p> Follow us on Linkedin   hepapi.com </p> <p>Follow the docs on: How to contribute</p>"},{"location":"qa/#compendium","title":"Compendium","text":""},{"location":"qa/Fundamentals_of_QA/","title":"Fundamentals of Quality Assurance (QA)","text":"<p>Quality Assurance (QA) is a vital process in software development that ensures the delivery of reliable, high-performing, and user-friendly products. By systematically verifying that a product meets its design specifications, QA minimizes bugs and increases customer satisfaction. Let\u2019s explore the fundamental components of QA to understand how it contributes to the success of any project.</p>"},{"location":"qa/Fundamentals_of_QA/#1-test-planning-and-defining-strategy","title":"1. Test Planning and Defining Strategy","text":"<p>The QA process begins with meticulous planning, aligning testing procedures with the project\u2019s objectives. This phase defines the structure and scope of testing, ensuring efficiency and focus.</p>"},{"location":"qa/Fundamentals_of_QA/#defining-scope","title":"Defining Scope:","text":"<ul> <li>Identify areas to be tested and those excluded.</li> <li>Prevent unnecessary tests, save time, and focus on critical features.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#resource-allocation","title":"Resource Allocation:","text":"<ul> <li>Organize tools, equipment, software, and team members for a smooth QA process.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#timeline-creation","title":"Timeline Creation:","text":"<ul> <li>Establish deadlines for each phase of testing to align with the overall project schedule.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#risk-management","title":"Risk Management:","text":"<ul> <li>Anticipate potential issues and develop mitigation strategies.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#defining-strategy","title":"Defining Strategy:","text":"<ul> <li>Choose appropriate test types (e.g., functional, performance, security).</li> <li>Select tools like Selenium or Postman.</li> <li>Establish clear success criteria, such as thresholds for critical errors.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#2-writing-test-scenarios-and-test-cases","title":"2. Writing Test Scenarios and Test Cases","text":"<p>QA professionals break down complex system operations into manageable components through test scenarios and test cases.</p>"},{"location":"qa/Fundamentals_of_QA/#test-scenarios","title":"Test Scenarios:","text":"<ul> <li>High-level descriptions of user flows.</li> <li>Example: \"The user should be able to access their profile page after logging in.\"</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#test-cases","title":"Test Cases:","text":"<ul> <li>Detailed steps to validate functionality.</li> <li>Example:</li> <li>Open the login screen.</li> <li>Enter the username and password.</li> <li>Click the \"Login\" button.</li> <li>Expected Result: The user is redirected to the homepage.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#risk-analysis","title":"Risk Analysis:","text":"<ul> <li>Prioritize scenarios critical to the business (e.g., payment processes).</li> <li>Assign lower priority to rarely used modules.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#3-bug-reporting","title":"3. Bug Reporting","text":"<p>Effective bug reporting accelerates debugging, enabling developers to resolve issues promptly.</p>"},{"location":"qa/Fundamentals_of_QA/#a-well-documented-bug-report-includes","title":"A Well-Documented Bug Report Includes:","text":"<ul> <li>Bug Category: Impact on performance, user interface, or security.</li> <li>Severity: Impact level (e.g., \"User cannot log in\" vs. \"A button is misaligned\").</li> <li>Reproduction Steps: Step-by-step actions to reproduce the issue.</li> <li>Supporting Materials: Screenshots, error logs, or video recordings.</li> </ul> <p>Clear and thorough reports reduce ambiguity and enhance developer efficiency.</p>"},{"location":"qa/Fundamentals_of_QA/#4-test-tracking-and-reporting","title":"4. Test Tracking and Reporting","text":"<p>Continuous monitoring and reporting are essential to keep stakeholders informed about the QA process.</p>"},{"location":"qa/Fundamentals_of_QA/#test-tracking-tools","title":"Test Tracking Tools:","text":"<ul> <li>Platforms like Jira or TestRail enable real-time updates on test statuses and bug progress.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#reporting","title":"Reporting:","text":"<ul> <li>Regular updates (daily, weekly, or sprint-based) on testing outcomes and areas of concern.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#key-metrics","title":"Key Metrics:","text":"<ul> <li>Bug resolution rate.</li> <li>Remaining workload.</li> <li>Ratio of passed vs. failed tests.</li> </ul> <p>These metrics provide insight into project health and testing efficiency.</p>"},{"location":"qa/Fundamentals_of_QA/#5-preparing-test-environments","title":"5. Preparing Test Environments","text":"<p>A realistic and reliable test environment is crucial for accurate testing results.</p>"},{"location":"qa/Fundamentals_of_QA/#key-considerations","title":"Key Considerations:","text":"<ul> <li>Accurate Data: Use dummy data that mimics real-world usage.</li> <li>Accessibility: Ensure seamless access for all team members to avoid delays.</li> <li>Backup and Recovery: Contingency plans for data loss ensure testing continuity.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#conclusion","title":"Conclusion","text":"<p>Quality Assurance is more than just identifying bugs; it's a comprehensive process that ensures a product meets its functional, performance, and user experience goals. By following a structured QA process\u2014test planning, scenario writing, bug reporting, tracking, and preparing environments\u2014teams can deliver software that meets and exceeds customer expectations.</p> <p>Investing in QA from the beginning is not merely a cost but a strategic move to build trust, ensure reliability, and secure long-term success.</p>"},{"location":"qa/QA_Bug_Reporting/","title":"Bug Management","text":""},{"location":"qa/QA_Bug_Reporting/#1what-is-a-bug","title":"1.What is a Bug?","text":"<p>A bug is a mismatch between the expected behavior and the actual behavior of a system. Bugs can be categorized as follows:</p> <ul> <li>Coding Bugs: Issues originating from developer errors.</li> <li>Requirement Bugs: Problems caused by incorrect or incomplete requirements.</li> <li>UI Bugs: Design and usability issues in the user interface.</li> <li>Performance Bugs: System slowdowns or crashes under high load.</li> </ul>"},{"location":"qa/QA_Bug_Reporting/#2types-of-bugs","title":"2.Types of Bugs","text":"<p>Proper classification of bugs is crucial for determining resolution priorities.</p> <ul> <li>Critical: Bugs that cause system crashes or prevent main functions from working. Example: Payment system not operational.</li> <li>Major: Bugs that affect significant functions but can be temporarily worked around. Example: Login fails for some users.</li> <li>Minor: Bugs that affect user experience but do not break the system. Example: Misaligned button.</li> <li>Trivial: Low-priority bugs, often cosmetic. Example: Typo in text.</li> </ul>"},{"location":"qa/QA_Bug_Reporting/#3bug-tracking-tools","title":"3.Bug Tracking Tools","text":"<p>Bug tracking tools like Jira allow teams to: - Create and assign bugs to specific members. - Track the bug resolution process (open, in progress, resolved). - Share bug reports easily.</p>"},{"location":"qa/QA_Bug_Reporting/#steps-in-bug-management","title":"Steps in Bug Management","text":"<ol> <li>Detection: Identify the bug during testing.</li> <li>Reporting: Log a detailed report in the system.</li> <li>Analysis: Investigate the root cause.</li> <li>Resolution: Fix and re-test the bug.</li> <li>Closure: Close the bug once resolved.</li> </ol> <p>Transparency is critical throughout the process.</p>"},{"location":"qa/QA_Testing_Process/","title":"Fundamental Testing Process","text":""},{"location":"qa/QA_Testing_Process/#1test-planning-and-defining-strategy","title":"1.Test Planning and Defining Strategy","text":"<p>Testing begins with planning, aligning procedures with project objectives.</p>"},{"location":"qa/QA_Testing_Process/#key-steps","title":"Key Steps:","text":"<ul> <li>Defining Scope: Identify areas to be tested and those excluded.</li> <li>Resource Allocation: Organize equipment, tools, and team members.</li> <li>Timeline Creation: Set deadlines for each phase.</li> <li>Risk Management: Assess potential risks and create mitigation strategies.</li> </ul>"},{"location":"qa/QA_Testing_Process/#defining-strategy","title":"Defining Strategy:","text":"<ul> <li>Choose test types (functional, performance, security).</li> <li>Specify tools (e.g., Selenium, Postman).</li> <li>Clarify test success criteria.</li> </ul>"},{"location":"qa/QA_Testing_Process/#2writing-test-scenarios-and-test-cases","title":"2.Writing Test Scenarios and Test Cases","text":"<ul> <li>Test Scenarios: High-level user flows.</li> <li>Example: \"The user should access their profile page after logging in.\"</li> <li>Test Cases: Detailed steps.</li> <li>Example:<ol> <li>Open login screen.</li> <li>Enter username/password.</li> <li>Click \"Login\" button.</li> <li>Expected Result: Redirect to homepage.</li> </ol> </li> </ul>"},{"location":"qa/QA_Testing_Process/#risk-analysis","title":"Risk Analysis:","text":"<ul> <li>High-risk areas (e.g., payment processes) need rigorous testing.</li> <li>Less used modules have lower priority.</li> </ul>"},{"location":"qa/QA_Testing_Process/#3bug-reporting","title":"3.Bug Reporting","text":"<p>A good bug report includes: - Category: Performance, UI, security. - Severity: Impact level (e.g., login failure vs. button misalignment). - Reproduction Steps: Detailed actions to replicate the bug. - Supporting Materials: Screenshots, error logs, etc.</p>"},{"location":"qa/QA_Testing_Process/#4test-tracking-and-reporting","title":"4.Test Tracking and Reporting","text":"<p>Use tools like Jira or TestRail to manage test statuses and open bugs.</p>"},{"location":"qa/QA_Testing_Process/#reporting","title":"Reporting:","text":"<ul> <li>Regular updates (daily, weekly, sprint-based).</li> </ul>"},{"location":"qa/QA_Testing_Process/#metrics","title":"Metrics:","text":"<ul> <li>Resolved bugs ratio.</li> <li>Remaining workload.</li> </ul>"},{"location":"qa/QA_Testing_Process/#5preparing-test-environments","title":"5.Preparing Test Environments","text":"<p>Ensure a realistic and reliable test environment. - Accurate Data: Use dummy data. - Accessibility: Seamless team access. - Backup/Recovery: Prepare for data loss.</p>"},{"location":"qa/SSH_Config/","title":"Managing Multiple SSH Keys for QA Consultants","text":"<p>As QA consultants, managing SSH keys efficiently across projects is essential. Here are two approaches:</p>"},{"location":"qa/SSH_Config/#option-1-one-ssh-key-for-everything","title":"Option 1: One SSH Key for Everything","text":""},{"location":"qa/SSH_Config/#steps","title":"Steps:","text":"<ol> <li>Generate an SSH Key: <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre></li> <li>Add Key to Agent: <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre></li> <li>Add Key to Repository Platform: Copy and paste the key into platform settings.</li> </ol>"},{"location":"qa/SSH_Config/#option-2-multiple-ssh-keys-with-config","title":"Option 2: Multiple SSH Keys with Config","text":""},{"location":"qa/SSH_Config/#steps_1","title":"Steps:","text":"<ol> <li>Generate Multiple Keys: <pre><code>ssh-keygen -t rsa -b 4096 -C \"company1@example.com\" -f ~/.ssh/id_rsa_company1\nssh-keygen -t rsa -b 4096 -C \"company2@example.com\" -f ~/.ssh/id_rsa_company2\n</code></pre></li> <li>Add Keys to Agent: <pre><code>ssh-add ~/.ssh/id_rsa_company1\nssh-add ~/.ssh/id_rsa_company2\n</code></pre></li> <li> <p>Create SSH Config File: <pre><code>nano ~/.ssh/config\n</code></pre>    Example Config:    <pre><code># Company 1\nHost company1\n    HostName github.com\n    User git\n    IdentityFile ~/.ssh/id_rsa_company1\n\n# Company 2\nHost company2\n    HostName gitlab.com\n    User git\n    IdentityFile ~/.ssh/id_rsa_company2\n</code></pre></p> </li> <li> <p>Use the Config: Clone using the defined Host:    <pre><code>git clone git@company1:username/repo.git\ngit clone git@company2:username/repo.git\n</code></pre></p> </li> </ol>"},{"location":"qa/SSH_Config/#which-one-to-use","title":"Which One to Use?","text":"<ul> <li>Single Key: Simple, but less secure.</li> <li>Multiple Keys: Secure and clean for multiple clients.</li> </ul> <p>The second method is recommended for better management.</p>"},{"location":"qa/Software_Testing_Tools/","title":"Software Testing Tools","text":"<p>Software testing is the process of checking whether the functionality, quality, and performance of a software meet user expectations. Testing tools and technologies offer software that makes this process faster, more efficient, and effective. They enhance quality and reduce error rates by supporting testing processes either manually or through automation.</p>"},{"location":"qa/Software_Testing_Tools/#1manual-testing-tools","title":"1.Manual Testing Tools","text":"<p>Manual testing tools simplify the testing process of software systems, playing a crucial role in identifying defects, ensuring functionality, and evaluating performance.</p>"},{"location":"qa/Software_Testing_Tools/#jira","title":"Jira","text":"<ul> <li>Developed by Atlassian, Jira is a bug tracking and project management tool.</li> </ul> <p>Features:   - Create detailed bug records, including steps to reproduce, expected and actual behaviors, and severity levels.   - Prioritize bugs based on impact for efficient resolution.   - Integrates seamlessly with other testing and development tools.</p>"},{"location":"qa/Software_Testing_Tools/#testrail","title":"TestRail","text":"<ul> <li>A software test management tool.</li> </ul> <p>Features:   - Organize test scenarios, track results, and generate reports.   - Define test scope and visualize progress.   - Integrates with Jira for aligned bug tracking and project management.</p>"},{"location":"qa/Software_Testing_Tools/#2automation-testing-tools","title":"2.Automation Testing Tools","text":"<p>Automation testing tools conduct tests automatically without manual intervention, improving efficiency for frequent or complex test scenarios.</p>"},{"location":"qa/Software_Testing_Tools/#selenium","title":"Selenium","text":"<ul> <li>An open-source automation testing tool for web applications. Features:</li> <li>Supports various programming languages.</li> <li>Simulates user behavior and automates test scenarios.</li> <li>WebDriver enables testing across multiple browsers.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#appium","title":"Appium","text":"<ul> <li>An open-source tool for mobile application testing. Features:</li> <li>Works on Android and iOS platforms.</li> <li>Adapts Selenium's features for mobile testing.</li> <li>Supports multiple programming languages.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#junit","title":"JUnit","text":"<ul> <li>An open-source testing framework for unit testing in Java. Features:</li> <li>Creates, executes, and reports test scenarios.</li> <li>Ensures software accuracy and defect identification.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#testng","title":"TestNG","text":"<ul> <li>A Java-based testing tool with advanced features. Features:</li> <li>Supports parallel test execution and dependency management.</li> <li>Handles data-driven and database-driven scenarios.</li> <li>Provides robust reporting and integration capabilities.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#3api-testing-tools","title":"3.API Testing Tools","text":"<p>API testing tools ensure proper communication between software systems by testing functions like validation, security, and performance.</p>"},{"location":"qa/Software_Testing_Tools/#postman","title":"Postman","text":"<ul> <li>A widely used API testing tool. Features:</li> <li>Sends API requests and reviews responses.</li> <li>Automates test scenarios and validates API functionality.</li> <li>Generates reports for efficient testing.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#soapui","title":"SoapUI","text":"<ul> <li>An open-source tool for SOAP and REST API testing. Features:</li> <li>Facilitates request sending, response validation, and performance analysis.</li> <li>Conducts security tests with advanced automation capabilities.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#4performance-and-security-testing-tools","title":"4.Performance and Security Testing Tools","text":"<p>These tools test software speed, resilience under load, and vulnerabilities.</p>"},{"location":"qa/Software_Testing_Tools/#jmeter","title":"JMeter","text":"<ul> <li>An open-source tool for performance and load testing. Features:</li> <li>Simulates multiple users to analyze system behavior under traffic.</li> <li>Supports protocols like HTTP, HTTPS, REST API, and database queries.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#loadrunner","title":"LoadRunner","text":"<ul> <li>A performance testing tool for large-scale applications. Features:</li> <li>Simulates high user loads to evaluate speed, stability, and scalability.</li> <li>Provides detailed reports for comprehensive analysis.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#burp-suite","title":"Burp Suite","text":"<ul> <li>A tool for testing web application security. Features:</li> <li>Analyzes traffic to identify vulnerabilities like SQL injection and XSS.</li> <li>Offers automated scanning and manual testing.</li> <li>Provides detailed reports for enhanced security.</li> </ul>"},{"location":"qa/accessibility_tools_overview/","title":"Accessibility Testing Tools (Overview)","text":""},{"location":"qa/accessibility_tools_overview/#purpose-of-this-page","title":"Purpose of This Page","text":"<p>It deals with a real-world QA question that is posed on this page: 'What are the accessibility tools that we should actually use?'</p> <p>Emphasis will be placed on a central, realistic set of tools that the QA teams could use immediately rather than a complete list of tools that are available.</p> <p>For more information on accessibility principles and strategies for testing, see What is Accessibility Testing?</p>"},{"location":"qa/accessibility_tools_overview/#recommended-core-toolset","title":"Recommended Core Toolset","text":""},{"location":"qa/accessibility_tools_overview/#browser-extensions-quick-checks","title":"Browser Extensions (Quick Checks)","text":"Tool Purpose Best For axe DevTools Primary automated WCAG checker Specific problem identification WAVE Visual structure overview Quick Visual Audit <p>Such tools are very useful in development environments before conducting official QA tests.</p>"},{"location":"qa/accessibility_tools_overview/#screen-readers-essential-manual-testing","title":"Screen Readers (Essential Manual Testing)","text":"<p>Screen reader testing cannot be compromised when it comes to accessibility quality.</p> Platform Screen Reader Cost Windows NVDA Free macOS / iOS VoiceOver Built-in Android TalkBack Built-in Windows JAWS Paid"},{"location":"qa/accessibility_tools_overview/#essential-screen-reader-shortcuts","title":"Essential Screen Reader Shortcuts","text":"<p>NVDA (Windows)</p> Action Shortcut Start/Stop reading NVDA + \u2193 Next heading H Next link K Next form field F Elements list NVDA + F7 <p>VoiceOver (macOS)</p> Action Shortcut Start VoiceOver Cmd + F5 Next item VO + \u2192 Rotor (elements) VO + U Next heading VO + Cmd + H Next link VO + Cmd + L <p>VO = Control + Option</p>"},{"location":"qa/accessibility_tools_overview/#automated-testing-cicd","title":"Automated Testing &amp; CI/CD","text":"Tool Use Case axe-core Integration with test frameworks Pa11y CLI and CI/CD pipelines Lighthouse CI Performance + accessibility in pipelines"},{"location":"qa/accessibility_tools_overview/#playwright-axe-core-example","title":"Playwright + axe-core Example","text":"<p>import { test, expect } from '@playwright/test'; import AxeBuilder from '@axe-core/playwright';  </p> <p>test('checkout page accessibility', async ({ page }) =&gt; { await page.goto('/checkout');  </p> <p>const results = await new AxeBuilder({ page }) .withTags(['wcag2a', 'wcag2aa']) .analyze();  </p> <p>expect(results.violations).toEqual([]); });</p>"},{"location":"qa/accessibility_tools_overview/#cypress-axe-core-example","title":"Cypress + axe-core Example","text":"<p>describe('Accessibility Tests', () =&gt; { it('homepage should have no violations', () =&gt; { cy.visit('/'); cy.injectAxe(); cy.checkA11y(); }); });</p>"},{"location":"qa/accessibility_tools_overview/#color-contrast-tools","title":"Color &amp; Contrast Tools","text":"Tool Purpose WebAIM Contrast Checker Web-based contrast validation Colour Contrast Analyser Desktop app with eyedropper Stark Figma/Sketch plugin for designers <p>Contrast Requirements for WCAG:</p> <ul> <li> <p>Normal text: 4.5:1 (AA)</p> </li> <li> <p>Large text (18pt+): 3:1 (AA)</p> </li> <li> <p>UI components: 3:1 (AA)</p> </li> </ul>"},{"location":"qa/accessibility_tools_overview/#development-linters","title":"Development Linters","text":"Tool Framework eslint-plugin-jsx-a11y React vue-a11y Vue.js angular-eslint Angular <p>Angular Linters decrease the number of accessibility bugs that reach the QA phase.</p>"},{"location":"qa/accessibility_tools_overview/#practical-qa-workflow","title":"Practical QA Workflow","text":"<p>1. Automated Scan \u2192 axe DevTools / WAVE \u2193 2. Keyboard Testing \u2192 Tab, Enter, Escape, Arrow keys \u2193 3. Screen Reader \u2192 NVDA / VoiceOver on key flows \u2193 4. Visual Checks \u2192 Contrast, zoom (400%), reflow \u2193 5. CI/CD Integration \u2192 axe-core in test pipeline</p> <p>This layered approach balances speed, coverage, and real user validation.</p>"},{"location":"qa/accessibility_tools_overview/#what-tools-can-and-cannot-do","title":"What Tools Can and Cannot Do","text":""},{"location":"qa/accessibility_tools_overview/#automated-tools-can-detect","title":"Automated tools CAN detect:","text":"<ul> <li> <p>Missing alt text</p> </li> <li> <p>Color contrast issues</p> </li> <li> <p>Missing form labels</p> </li> <li> <p>Invalid ARIA attributes</p> </li> <li> <p>Broken skip links</p> </li> </ul>"},{"location":"qa/accessibility_tools_overview/#automated-tools-cannot-reliably-validate","title":"Automated tools CANNOT reliably validate:","text":"<ul> <li> <p>Meaningfulness of alt text content</p> </li> <li> <p>Logical reading order</p> </li> <li> <p>Real screen reader usability</p> </li> <li> <p>Keyboard trap scenarios</p> </li> <li> <p>Overall user comprehension</p> </li> </ul> <p>!!! warning \u201cImportant\u201d Automated tools catch ~30-40% of accessibility issues. Human evaluation remains essential.</p>"},{"location":"qa/accessibility_tools_overview/#tool-selection-guide","title":"Tool Selection Guide","text":"Scenario Recommended Tools Quick dev check axe DevTools, WAVE Sprint testing axe + NVDA/VoiceOver CI/CD pipeline axe-core, Pa11y, Lighthouse Mobile testing VoiceOver (iOS), TalkBack (Android) Design review Stark, Contrast Checker"},{"location":"qa/accessibility_tools_overview/#qa-perspective","title":"QA Perspective","text":"<p>Assistance tools help in QA judgment; they do not replace it.</p> <p>The best possible accessibility tests are achieved through the integration of the:</p> <ul> <li> <p>WCAG standards (what to test)</p> </li> <li> <p>Automated tooling (speed)</p> </li> <li> <p>Manual testing with assistive technologies (validation)</p> </li> </ul>"},{"location":"qa/accessibility_tools_overview/#conclusion","title":"Conclusion","text":"<p>Accessibility testing tools make it efficient for problems to be not only identified but prevented, although accessibility cannot be solely dependent on tools. The inclusion itself can be achieved by applying the right tool at the right time, as implemented by QA teams.</p> <p>!!! quote \u2018Final Reminder\u2019 The strongest tool in the field of accessibility is a QA expert well-versed in the technology and its effects.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/","title":"WCAG 2.1 Standards","text":""},{"location":"qa/accessibility_wcag_2_1_standards/#overview","title":"Overview","text":"<p>WCAG (Web Content Accessibility Guidelines) is a worldwide rule for web accessibility. It was built by the W3C (World Wide Web Consortium) Web Openness Activity. It permits a set of necessities, or victory criteria, which can be tried in order to guarantee that web content is accessible to clients with diverse disabilities. This is accomplished in WCAG 2.1.</p> <p>For QA groups, \"WCAG\" gives a lingua franca for analysts, engineers, planning groups, and other partners. This makes it conceivable for openness bugs to be categorized in a standardized manner.</p> <p>For common methodologies related to testing, see \u201cWhat is Openness Testing?\u201d.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#wcag-conformance-levels","title":"WCAG Conformance Levels","text":"Level Description Target Level A Minimum requirements. Failure often blocks users. Must have Level AA Addresses most common barriers. Recommended target. Should have Level AAA Highest level. Often aspirational. Nice to have <p>Most organizations aim for Level AA conformance as a realistic and legally defensible standard.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#the-four-principles-of-wcag-pour","title":"The Four Principles of WCAG (POUR)","text":""},{"location":"qa/accessibility_wcag_2_1_standards/#perceivable","title":"Perceivable","text":"<p>Users must be able to understand information in text, audio, or visuals. Examples: alternative text, captions, color contrast.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#operable","title":"Operable","text":"<p>Users ought to have the capability to interact with the interface using various types of input methods. Examples: keyboard access, focus sequence, absence of traps.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#understandable","title":"Understandable","text":"<p>The content, interaction, or user experience needs to be understandable and predictable. Examples include error messages, navigation, and text.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#robust","title":"Robust","text":"<p>The content should play well with assistive technology. Examples include valid HTML, semantic HTML, and using ARIA.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#whats-new-in-wcag-21","title":"What\u2019s New in WCAG 2.1","text":"<p>WCAG 2.1 (June 2018) introduced 17 new success criteria that mainly dealt with</p> <ul> <li> <p>Mobile accessibility</p> </li> <li> <p>Low vision users</p> </li> <li> <p>Cognitive disabilities</p> </li> </ul>"},{"location":"qa/accessibility_wcag_2_1_standards/#key-new-criteria-for-qa","title":"Key New Criteria for QA","text":"Criterion Level What to Test 1.3.4 Orientation AA Content works in both portrait and landscape 1.3.5 Identify Input Purpose AA Autocomplete attributes on form fields 1.4.10 Reflow AA No horizontal scroll at 320px width (400% zoom) 1.4.11 Non-text Contrast AA UI components have 3:1 contrast ratio 1.4.13 Content on Hover/Focus AA Tooltips dismissible, hoverable, persistent 2.1.4 Character Key Shortcuts A Single-key shortcuts can be disabled/remapped 2.5.1 Pointer Gestures A Multi-point gestures have single-pointer alternative 2.5.3 Label in Name A Visible label matches accessible name 2.5.4 Motion Actuation A Shake/tilt features have button alternative"},{"location":"qa/accessibility_wcag_2_1_standards/#commonly-tested-wcag-areas-in-qa","title":"Commonly Tested WCAG Areas in QA","text":"<p>Rather than test each criterion, the QA teams would usually target high-impact areas:</p> <ul> <li> <p>Text alternatives for non-text content</p> </li> <li> <p>Keyboard accessibility and focus management</p> </li> <li> <p>Color contrast and text resizing</p> </li> <li> <p>Form labels, directions, and error messages</p> </li> <li> <p>Page Structure and Navigation Consistency</p> </li> <li> <p>Touch target sizes (minimum 44x44px)</p> </li> </ul> <p>These areas account for the biggest chunk of defects in real-world projects.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#using-wcag-in-qa-testing","title":"Using WCAG in QA Testing","text":""},{"location":"qa/accessibility_wcag_2_1_standards/#practical-workflow","title":"Practical Workflow","text":"<ol> <li> <p>Set target level (normally Level AA)</p> </li> <li> <p>Testing criteria applicable to feature or user flow</p> </li> <li> <p>Map findings to WCAG references</p> </li> <li> <p>Prioritize based on user impact</p> </li> </ol>"},{"location":"qa/accessibility_wcag_2_1_standards/#example-defect-mapping","title":"Example Defect Mapping","text":"<p>Issue: \"Submit button not reachable via keyboard\" \u2192 WCAG 2.1.1 (Keyboard, Level A) \u2192 Severity: Blocker \u2192 Target Users: Keyboard-only users, screen reader users</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#common-failures-of-wcag","title":"Common Failures of WCAG","text":""},{"location":"qa/accessibility_wcag_2_1_standards/#critical-level-a","title":"Critical (Level A)","text":"<ul> <li> <p>Missing alternative tags for images</p> </li> <li> <p>Broken keyboard navigation</p> </li> <li> <p>Form fields without labels</p> </li> <li> <p>Keyboard traps</p> </li> </ul>"},{"location":"qa/accessibility_wcag_2_1_standards/#important-level-aa","title":"Important (Level AA)","text":"<ul> <li> <p>Color contrast insufficient (\\&lt; 4.5:1) for text</p> </li> <li> <p>Lacking visible focus indicators</p> </li> <li> <p>No skip navigation for repetitive content</p> </li> <li> <p>Content breaks at 400% zoom</p> </li> </ul>"},{"location":"qa/accessibility_wcag_2_1_standards/#aria-essentials-for-qa","title":"ARIA Essentials for QA","text":"<p>While testing the implementation of ARIA, the following:</p> Check Question Roles Is the role attribute appropriate? Labels Does aria-label or aria-labelledby exist? States Are aria-expanded, aria-selected updated? Live regions Is aria-live used for dynamic content? <p>Warning: \u201cARIA Rule\u201d Do not use poor-quality ARIA. Using poor-quality ARIA creates accessibility problems.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#wcag-22-note","title":"WCAG 2.2 Note","text":"<p>\u201cUpdate\u201d WCAG 2.2 has been released in October 2023, introducing new guidelines such as Focus Not Obscured (2.4.11) and Dragging Movements (2.5.7). However, it is recommended to check for updates.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#qa-perspective","title":"QA Perspective","text":"<p>The WCAG Guidelines should not be viewed as a checklist to complete at the conclusion of the development process. The Guidelines serve as a continuous assessment tool that can be employed to assist in attaining the following objectives:</p> <ul> <li> <p>Early risk detection</p> </li> <li> <p>Clear defect communication</p> </li> <li> <p>Consistent prioritization</p> </li> </ul> <p>Accessibility maturity is enhanced with the inclusion of WCAG guidelines in day-to-day QA processes.</p>"},{"location":"qa/accessibility_wcag_2_1_standards/#conclusion","title":"Conclusion","text":"<p>WCAG 2.1 offers a systematic and testable means for accessibility checking. By aiming for Level AA conformance and integrating WCAG accessibility checks into overall QA processes, developers can provide inclusive experiences.</p> <p>!!! quote \u201cRemember,\u201d WCAG is a guideline\u2014not the goal itself. The goal is usable, inclusive products.</p>"},{"location":"qa/accessibility_what_is/","title":"What is Accessibility Testing?","text":""},{"location":"qa/accessibility_what_is/#overview","title":"Overview","text":"<p>\"Accessibility Testing\" is utilized to verify that digital products such as websites, mobile applications, and software are usable by individuals with various disabilities. This enables individuals with visual, auditory, motor, or cognitive disabilities to easily access, navigate, and interact with an application.</p> <p>This is not something that the QA teams should consider as their secondary task but their primary task just like performance, security, or functionality.</p>"},{"location":"qa/accessibility_what_is/#why-accessibility-testing-matters","title":"Why Accessibility Testing Matters","text":""},{"location":"qa/accessibility_what_is/#legal-and-regulatory-framework","title":"Legal and Regulatory Framework","text":"<p>A number of Many nations have laws that enforce digital accessibility standards:</p> Region Framework United States ADA, Section 508 European Union EN 301 549 Canada AODA <p>The design of technology products should include accessibility by design in product development practices by organizations.</p>"},{"location":"qa/accessibility_what_is/#impact-on-business-and-users","title":"Impact on Business and Users","text":""},{"location":"qa/accessibility_what_is/#a-large-number-of-the-global-population-has-some-kind-of-disability-this-is-because-a-wheelchair-user-will-also-benefit-from-an-accessible-website-as-will-a-user-whose-hand-is-broken-and-is-using-a-phone-in-the-outdoors-an-accessible-website-will-improve-the-seo-ranking-of-the-website-as-well-as-the-trust-factor","title":"A large number of the global population has some kind of disability. This is because a wheelchair user will also benefit from an accessible website, as will a user whose hand is broken and is using a phone in the outdoors. An accessible website will improve the SEO ranking of the website as well as the trust factor.","text":""},{"location":"qa/accessibility_what_is/#quality-perspective","title":"Quality Perspective","text":"<p>A large number of the global population has some kind of disability. This is because a wheelchair user will also benefit from an accessible website, as will a user whose hand is broken and is using a phone in the outdoors. An accessible website will improve the SEO ranking of the website as well as the trust factor.</p>"},{"location":"qa/accessibility_what_is/#core-principles-pour","title":"Core Principles: POUR","text":"<p>Accessibility follows the guidelines set out in the WCAG guidelines, which consist of four principles:</p> Principle Description Example Perceivable Users must be able to perceive content Alt text, captions, sufficient contrast Operable Users must be able to navigate and interact Keyboard access, no traps Understandable Content must be clear and predictable Error messages, consistent UI Robust Content must work with assistive technologies Valid markup, ARIA"},{"location":"qa/accessibility_what_is/#accessibility-testing-approaches","title":"Accessibility Testing Approaches","text":""},{"location":"qa/accessibility_what_is/#manual-testing-essential","title":"Manual Testing (Essential)","text":"<p>Mimics real user behavior and is a critical component of real accessibility testing.</p> <ul> <li> <p>Keyboard-only navigation</p> </li> <li> <p>Screen reader use (NVDA, VoiceOver, TalkBack)</p> </li> <li> <p>Visual layout and contrast inspection</p> </li> <li> <p>Content clarity and error message review</p> </li> </ul>"},{"location":"qa/accessibility_what_is/#automated-testing-supportive","title":"Automated Testing (Supportive)","text":"<p>There are automated systems for detecting potential pitfalls.</p> <ul> <li> <p>Browser extensions (axe DevTools, WAVE)</p> </li> <li> <p>CI/CD integration (Lighthouse, Pa11y)</p> </li> <li> <p>Code-level linters (eslint-plugin-jsx-a11y)</p> </li> </ul> <p>Warning: \u201cImportant\u201d Automated testing speeds up detection but is not a substitute for manual verification using assistive technologies.</p> <p>For more information on the tools, refer to the Accessibility Testing Tools Overview.</p>"},{"location":"qa/accessibility_what_is/#qa-perspective-real-testing-scenario","title":"QA Perspective: Real Testing Scenario","text":"<p>Scenario: A new checkout form passes functional tests.</p> <ol> <li> <p>The automated scan is able to detect missing labels as well as low contrast.</p> </li> <li> <p>Keyboard testing \u201cProceed to Payment\u201d button is inaccessible \u2013 critical blocker issue.</p> </li> <li> <p>Screen reader testing reveals that validation messages are visible but not audible.</p> </li> </ol> <p>Result: Several critical accessibility problems that were not entirely captured by automated tools.</p>"},{"location":"qa/accessibility_what_is/#common-accessibility-issues-found-in-qa","title":"Common Accessibility Issues Found in QA","text":"<ul> <li> <p>Missing or generic alt text</p> </li> <li> <p>Lack of Color Contrast (WCAG-AA Failures)</p> </li> <li> <p>Keyboard traps or unreachable interactive elements</p> </li> <li> <p>Blank form fields without labels</p> </li> <li> <p>Announcement of error messages to the screen reader</p> </li> <li> <p>Illogically structured headings (for example, from H1 to H4)</p> </li> <li> <p>Links and Buttons Too Small (Less than 44x44 pixels)</p> </li> </ul>"},{"location":"qa/accessibility_what_is/#integrating-accessibility-into-the-qa-workflow","title":"Integrating Accessibility into the QA Workflow","text":"<p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Shift-Left \u2502 -&gt; \u2502 Development \u2502 -&gt; \u2502 QA Phase \u2502 -&gt; \u2502 CI/CD \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2022 A11y criteria \u2022 Dev checks \u2022 Automated scans \u2022 Quality gates \u2022 Design review \u2022 Code review \u2022 Manual testing \u2022 Regression</p>"},{"location":"qa/accessibility_what_is/#accessibility-defect-template","title":"Accessibility Defect Template","text":"<p>Title: [WCAG X.X.X] Brief description Severity: Blocker / Critical / Major / Minor WCAG Reference: X.X.X - Criterion Name (Level A/AA) Steps to Reproduce: ... Expected Behavior: ... Actual Behavior: ... Affected Users: Screen reader / Keyboard / Low vision Suggested Fix: ...</p>"},{"location":"qa/accessibility_what_is/#quick-start-checklist-for-qa","title":"Quick Start Checklist for QA","text":"<ul> <li> <p>Install axe DevTools or WAVE</p> </li> <li> <p>Familiarize themselves with the basics of screen readers (e.g., NVDA or VoiceOver)</p> </li> <li> <p>Test keyboard-based browsing functionality in critical flows</p> </li> <li> <p>Check color contrast with a contrast checker</p> </li> <li> <p>Add accessibility checks in automated tests</p> </li> <li> <p>Add accessibility to test plan and exit criteria</p> </li> </ul>"},{"location":"qa/accessibility_what_is/#conclusion","title":"Conclusion","text":"<p>Accessibility testing is a continuous process, and not a one-time activity. Accessibility tests are integrated using standards, automated testing tools, and manual testing using assistive technologies to make products accessible to everyone.</p> <p>\u201cRemember\u201d that if you can\u2019t test your product on someone with a disability, you can\u2019t use that product either.</p>"},{"location":"qa/api_karate_assertions_validations/","title":"Assertions &amp; Validations","text":"<p>Content will be added.</p>"},{"location":"qa/api_karate_cicd_integration/","title":"CI/CD Integration","text":"<p>Content will be added.</p>"},{"location":"qa/api_karate_data_driven_testing/","title":"Data-Driven Testing","text":"<p>Content will be added.</p>"},{"location":"qa/api_karate_feature_files_gherkin/","title":"Feature Files &amp; Gherkin Syntax","text":"<p>Content will be added.</p>"},{"location":"qa/api_karate_project_structure/","title":"Project Structure","text":"<p>Content will be added.</p>"},{"location":"qa/api_karate_what_is/","title":"What is Karate?","text":"<p>Content will be added.</p>"},{"location":"qa/api_postman_collections_environments/","title":"Collection &amp; Environment Management","text":"<p>Content will be added.</p>"},{"location":"qa/api_postman_newman_cli/","title":"CLI Usage with Newman","text":"<p>Content will be added.</p>"},{"location":"qa/api_postman_test_scripts/","title":"Test Scripts","text":"<p>Content will be added.</p>"},{"location":"qa/api_soapui_basic_usage/","title":"Basic Usage","text":"<p>Content will be added.</p>"},{"location":"qa/api_soapui_soap_vs_rest/","title":"SOAP vs REST","text":"<p>Content will be added.</p>"},{"location":"qa/approaches_black_white_grey_box/","title":"Black / White / Grey Box Testing","text":"<p>Seeing the Full Picture: The Three Perspectives of Quality</p> <p>Testing isn't just about finding errors; it's also about our perspective on how the problem occurred.We use three different way to make sure your software is flawless from every angle.</p> <ul> <li>Black Box Testing: Let's think about our favorite app. When we use it, we never think about the code written in the background, right? We just want the app to work correctly when we press the button. That's exactly what Black Box testing is. From a user's perspective, it's about exploring the interface without dealing with the code of the object being tested, checking clickable elements, scrolling if necessary, and entering input where it's possible.</li> <li>White Box Testing: While we prioritize user experience, what's happening behind the scenes is equally important. We examine the code to ensure it's soundly and cleanly designed, with robust security and a long lifespan for the application. It's like taking an X-ray of the software.</li> <li>Grey Box Testing: In some cases, we may need to use both techniques we mentioned earlier. We act as a user, but we also have documentation showing how the system works in the background. This helps us determine if specific actions performed by the user match a particular database rule in the background, and if not, identify any errors that may occur.</li> </ul>"},{"location":"qa/approaches_exploratory/","title":"Exploratory Testing","text":"<p>The Human Touch in a World of Algorithms</p> <p>While automated tests are great for checking repetitive tasks, they lack human curiosity. That's where Exploratory Testing comes in. Our experienced QA team sets aside command-based automated tests and tries to think like a distracted parent, a hasty manager, or a hacker.</p> <p>We act outside of normal behavior to find edge cases, searching for strange and unexpected errors that are in line with human nature but wouldn't occur to a machine or system.</p> <p>This approach not only finds bugs but also ensures the application is not only functional butalso enjoyabletouse.</p>"},{"location":"qa/approaches_risk_based/","title":"Risk-Based Testing","text":"<p>Testing with Intent: Focus Where It Matters Most</p> <p>Let's be honest. In a project that needs to move quickly, we sometimes don't have the opportunity to test every single pixel multiple times. In such scenarios, you need a strategy to protect your business and users at their most vulnerable points. That's where Risk-Based Testing comes in. We don't just test randomly; we test intelligently. We analyze the project and identify its \"high-risk\" areas. For example, for an e-commerce site, this might be the payment process. Or for a healthcare application, data privacy is crucial. If these processes fail, the impact is significant. By focusing on these critical areas in such scenarios, we ensure that these features remain robust and reliable, even under the tightest deadlines.</p>"},{"location":"qa/approaches_shift_left/","title":"Shift-Left Testing","text":"<p>What is the importance of early testing?</p> <p>In the early days of technology, testing was the final step before product launch, the last hurdle before the product went live. But waiting until the last minute is like checking the building plans after construction is finished. It can be quite stressful and equally expensive. Shift-Left is one of the best methods for such scenarios.</p> <p>So how can Shift-Left help you? The goal of Shift-Left is not to find bugs after development is complete, but to prevent these bugs before development. As a QA team, we meet with designers, analysts, and developers before any line of code is written and ask \"What if...\" questions early on. This ensures a healthier project progress, reduces the cost of future corrections, and allows us to reach the project completion deadline with confidence, not with last-minute panic.</p>"},{"location":"qa/automation_testing_and_best_practices/","title":"Automation Testing and Best Practices","text":"<p>Automation testing is an integral part of modern software development processes. It is utilized to ensure software quality, deliver faster releases, and minimize errors that could arise in manual testing. Below is an overview of the key steps of automation testing and how these should be implemented.</p>"},{"location":"qa/automation_testing_and_best_practices/#1automation-test-planning","title":"1.Automation Test Planning","text":"<p>Effective planning is critical to the success of automation testing. Automation test planning should be carried out as follows: - Defining Test Objectives: Evaluating which areas are suitable for automation. - Selecting Test Environment and Tools: Choosing tools such as Java, Ruby, Python, or UiPath that fit the project requirements. - Determining Test Scope: Identifying test scenarios to be automated and defining areas integrated with manual testing.</p>"},{"location":"qa/automation_testing_and_best_practices/#2identifying-test-scenarios","title":"2.Identifying Test Scenarios","text":"<p>The following principles should be followed when detailing test scenarios: - Prioritizing Test Cases: Starting with functionalities of critical importance. - Reusability: Creating templates that can be reused in future projects. - Writing with Gherkin Language: Especially when using the BDD approach, scenarios should be written in a format understandable by business units.</p>"},{"location":"qa/automation_testing_and_best_practices/#3code-structure-and-modularity","title":"3.Code Structure and Modularity","text":"<p>Structuring code in modular forms is a crucial factor in testing processes. Each test step should be designed as independent functions or classes. Key considerations include: - Ease of Maintenance: Ensuring the code can adapt to changes. - Reducing Redundancy: Writing each test function only once and reusing it across multiple tests if necessary. - OOP Principles: Adhering to object-oriented programming structures in Java and Python projects.</p>"},{"location":"qa/automation_testing_and_best_practices/#4automation-frameworks","title":"4.Automation Frameworks","text":"<p>Different frameworks should be employed to create tailored solutions for specific projects. The frameworks and approaches include:</p>"},{"location":"qa/automation_testing_and_best_practices/#page-object-model-pom","title":"Page Object Model (POM)","text":"<p>POM enhances code readability and ease of maintenance. This model should be implemented as follows: - Each web page is defined as a separate class. - Element definitions and methods related to the page are contained within that class. - Classes should be created in adherence to the POM structure using Selenium or Playwright frameworks.</p>"},{"location":"qa/automation_testing_and_best_practices/#behavior-driven-development-bdd-and-gherkin-language","title":"Behavior Driven Development (BDD) and Gherkin Language","text":"<p>BDD facilitates better communication between business units and developers. BDD scenarios should be handled as follows: - Scenarios are written in the \u201cGiven-When-Then\u201d format. - Tools like Cucumber with Ruby or JBehave with Java are used to support the Gherkin language.</p>"},{"location":"qa/automation_testing_and_best_practices/#5best-practices-for-automation-testing","title":"5.Best Practices for Automation Testing","text":"<p>The following best practices should be observed in the testing process: - Parallel Test Execution: Running tests quickly using Playwright or Selenium Grid. - Using Dynamic Data: Simulating real scenarios more effectively by using dynamic datasets instead of static ones. - Logging and Reporting: Utilizing tools like Allure and Extent Reports to analyze test reports in detail. - CI/CD Integration: Integrating automation tests into pipelines using Jenkins and GitHub Actions.</p>"},{"location":"qa/automation_testing_and_best_practices/#conclusion","title":"Conclusion","text":"<p>Modern automation testing approaches and solutions should be leveraged to maximize software quality and delivery speed. By effectively using languages such as Java, Ruby, Python, and UiPath, and frameworks like Selenium and Playwright, unique test scenarios tailored to different projects can be developed.</p>"},{"location":"qa/best_continuous_testing/","title":"Continuous Testing","text":""},{"location":"qa/best_continuous_testing/#continuous-testing-in-modern-development","title":"Continuous Testing in Modern Development","text":"<p>In modern software development, testing is no longer an activity performed only at the end. It is a continuous and integrated process.</p>"},{"location":"qa/best_continuous_testing/#what-continuous-testing-means","title":"What Continuous Testing Means","text":"<ul> <li>Automated tests after every commit</li> <li>Integration with CI/CD pipelines</li> <li>Fast feedback loops</li> <li>Early defect detection</li> </ul>"},{"location":"qa/best_continuous_testing/#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ul> <li>Reduces risks</li> <li>Improves release reliability</li> <li>Minimizes manual regression testing</li> </ul> <p>Test automation should be a natural and inseparable part of the delivery pipeline.</p>"},{"location":"qa/best_cross_browser_testing/","title":"Cross-Browser Testing","text":"<p>Users access applications through different browsers and devices. Ensuring functionality on only one browser is not enough.</p>"},{"location":"qa/best_cross_browser_testing/#how-cross-browser-testing-helps","title":"How Cross-Browser Testing Helps","text":"<ul> <li>Validate compatibility across Chrome, Firefox, Safari, and Edge</li> <li>Detect layout or styling issues</li> <li>Identify JavaScript inconsistencies</li> <li>Guarantee a consistent user experience</li> </ul>"},{"location":"qa/best_cross_browser_testing/#best-practices","title":"Best Practices","text":"<ul> <li>Define a browser matrix</li> <li>Run critical scenarios on all major browsers</li> <li>Use cloud device farms</li> </ul> <p>Before production releases, smoke and regression tests should run across multiple browsers.</p>"},{"location":"qa/best_flaky_test_management/","title":"Flaky Test Management","text":""},{"location":"qa/best_flaky_test_management/#flaky-tests-and-stability","title":"Flaky Tests and Stability","text":"<p>Flaky tests are tests that sometimes pass and sometimes fail without any code changes. They are one of the biggest challenges in automation because they reduce trust in the test suite.</p>"},{"location":"qa/best_flaky_test_management/#common-causes-of-flaky-tests","title":"Common Causes of Flaky Tests","text":"<ul> <li>Poor synchronization</li> <li>Unstable environments</li> <li>Dependencies between tests</li> <li>Dynamic elements</li> <li>Timing issues</li> </ul>"},{"location":"qa/best_flaky_test_management/#how-to-minimize-flakiness","title":"How to Minimize Flakiness","text":"<ul> <li>Use explicit waits</li> <li>Ensure test isolation</li> <li>Apply stable locator strategies</li> <li>Implement retry mechanisms</li> <li>Track and label flaky tests</li> </ul> <p>The goal is not to hide flaky tests, but to identify and fix the root cause permanently.</p>"},{"location":"qa/best_parallel_test_execution/","title":"Parallel Test Execution","text":"<p>As test suites grow, long execution times become a major problem. Parallel execution is a key solution.</p>"},{"location":"qa/best_parallel_test_execution/#benefits-of-running-tests-in-parallel","title":"Benefits of Running Tests in Parallel","text":"<ul> <li>Execution time is significantly reduced</li> <li>CI feedback becomes faster</li> <li>Releases can happen more frequently</li> </ul> <p>Tools like Playwright, Selenium Grid, and Cypress Cloud allow tests to run simultaneously across:</p> <ul> <li>Multiple browsers</li> <li>Different devices</li> <li>Various environments</li> </ul>"},{"location":"qa/best_parallel_test_execution/#requirements-for-reliable-parallel-execution","title":"Requirements for Reliable Parallel Execution","text":"<ul> <li>Tests must be independent</li> <li>Shared states must be avoided</li> <li>Test data conflicts must be prevented</li> </ul> <p>Otherwise, parallel runs may introduce additional failures.</p>"},{"location":"qa/best_test_automation_pyramid/","title":"Test Automation Pyramid","text":"<p>One of the most widely accepted and effective strategies in test automation is the Test Pyramid approach. This model illustrates how many tests should be written at each level of the system.</p> <p>At the base of the pyramid are Unit Tests. These tests run fast, are cost-effective, and detect defects early in the development cycle. The majority of tests should exist at this layer.</p> <p>The middle layer consists of API/Service Tests. These validate communication between system components and are generally faster and more stable than UI tests.</p> <p>At the top are UI (End-to-End) Tests. They simulate real user scenarios but tend to be slower, more fragile, and harder to maintain. Therefore, their number should be limited.</p>"},{"location":"qa/best_test_automation_pyramid/#benefits-of-following-this-structure","title":"Benefits of Following This Structure","text":"<ul> <li>Test execution time decreases</li> <li>Maintenance costs are reduced</li> <li>Feedback becomes faster</li> <li>CI/CD processes become more efficient</li> </ul> <p>In short: many unit tests, fewer UI tests.</p>"},{"location":"qa/best_test_code_review/","title":"Test Code Review","text":""},{"location":"qa/best_test_code_review/#code-review-in-test-automation","title":"Code Review in Test Automation","text":"<p>Test code is just as important as production code. Therefore, code reviews should also be mandatory for automation projects.</p>"},{"location":"qa/best_test_code_review/#why-it-matters","title":"Why It Matters","text":"<p>A proper review process helps:</p> <ul> <li>Eliminate duplicate code</li> <li>Improve readability and maintainability</li> <li>Catch incorrect assertions early</li> <li>Spread best practices across the team</li> </ul>"},{"location":"qa/best_test_code_review/#what-to-check-during-reviews","title":"What to Check During Reviews","text":"<ul> <li>Proper abstraction (Page Object Model, reusable methods)</li> <li>Avoiding hard-coded values</li> <li>Eliminating unnecessary waits or sleeps</li> <li>Ensuring test independence</li> <li>Clear and meaningful test names</li> </ul> <p>High-quality test code leads to sustainable automation.</p>"},{"location":"qa/best_test_maintenance_strategies/","title":"Test Maintenance Strategies","text":"<p>Writing tests is only the beginning. Keeping them maintainable over time is equally important. Otherwise, automation quickly turns into technical debt.</p>"},{"location":"qa/best_test_maintenance_strategies/#how-to-improve-maintainability","title":"How to Improve Maintainability","text":"<ul> <li>Use the Page Object Model (POM)</li> <li>Create reusable helper functions</li> <li>Centralize test data management</li> <li>Reduce code duplication</li> <li>Design modular test structures</li> </ul> <p>Regular refactoring and removing outdated or unused tests are also essential.</p> <p>Remember: automation that cannot be maintained becomes more expensive than manual testing.</p>"},{"location":"qa/bug_jira_usage/","title":"Jira Usage","text":""},{"location":"qa/bug_jira_usage/#how-qa-teams-use-jira-in-practice","title":"How QA Teams Use Jira in Practice","text":"<p>In our QA practices, Jira is used as a central workspace where testing activities, defect tracking, and quality visibility come together. Rather than serving only as a bug reporting tool, Jira supports the entire testing lifecycle and enables collaboration between QA, development, and product teams.</p> <p>From a QA standpoint, the purpose of using Jira is to ensure that quality-related information is visible, traceable, and continuously updated throughout the project. Clear tracking of defects and testing progress helps teams make informed decisions and avoid last-minute surprises.</p> <p>This page outlines how QA teams typically use Jira in day-to-day project work, based on real project experience.</p>"},{"location":"qa/bug_jira_usage/#about-jira-boards-and-visual-examples","title":"About Jira Boards and Visual Examples","text":"<p>The Jira board structures and visual examples shared in this documentation are intended to support understanding and knowledge transfer.</p> <p>There is no single \"correct\" Jira configuration. In practice, Jira boards may differ depending on:</p> <ul> <li>Company processes and internal standards</li> <li>Project scope and technical complexity</li> <li>Team size and role distribution</li> <li>Agile or hybrid delivery models</li> </ul> <p>All visual examples included here represent sample setups created for explanatory purposes. Teams should adapt Jira workflows according to their own project needs and organizational context.</p>"},{"location":"qa/bug_jira_usage/#qa-responsibilities-within-jira","title":"QA Responsibilities Within Jira","text":"<p>QA teams are responsible for validating that implemented features meet expected quality standards before release. This includes functional verification, risk identification, and communication of quality status.</p> <p>In Jira, QA engineers commonly:</p> <ul> <li>Create and manage bug tickets</li> <li>Track testing progress during sprints</li> <li>Share testing results with stakeholders</li> <li>Maintain traceability between requirements and defects</li> </ul> <p>In consultancy-based projects, QA engineers also contribute by guiding teams on testing discipline and improving visibility of quality-related risks.</p>"},{"location":"qa/bug_jira_usage/#managing-testing-activities-through-jira","title":"Managing Testing Activities Through Jira","text":"<p>Testing activities should be planned and tracked as part of the development lifecycle. Jira allows QA teams to follow testing status continuously rather than only at the end of development.</p> <p>Typical QA-related statuses tracked in Jira include:</p> <ul> <li>Items prepared for testing</li> <li>Features currently under test</li> <li>Items ready for UAT</li> <li>Failed tests requiring rework</li> </ul> <p>Tracking testing activities in this way helps ensure alignment with sprint goals and release plans.</p>"},{"location":"qa/bug_jira_usage/#example-qa-oriented-board-flow","title":"Example QA-Oriented Board Flow","text":"<p>The following workflow represents a commonly used QA-oriented board structure:</p> <ul> <li>Ready for Test: Development is complete and the item is available for QA validation.</li> <li>In Test: Test execution is in progress.</li> <li>Ready for UAT: QA validation is complete and the item is ready for business testing.</li> <li>In UAT: End users or stakeholders are validating the feature.</li> <li>Test Fail: Issues identified during testing require fixes.</li> <li>Done: All validation steps are completed and approved.</li> </ul> <p>This structure allows teams to quickly understand the quality status of work items at any time.</p>"},{"location":"qa/bug_jira_usage/#jira-access-and-permissions","title":"Jira Access and Permissions","text":"<p>Jira access is typically managed by client-side administrators in consultancy projects. Permissions are assigned according to role and responsibility.</p> <p>QA engineers usually have access to:</p> <ul> <li>Assigned project boards</li> <li>Relevant issue types</li> <li>Bug creation and update features</li> </ul> <p>Well-defined access rules support secure and transparent collaboration.</p>"},{"location":"qa/bug_jira_usage/#common-jira-issue-types-used-by-qa","title":"Common Jira Issue Types Used by QA","text":"<p>QA teams interact with different issue types depending on the nature of the work:</p> <ul> <li>Bug: Used to report defects discovered during testing</li> <li>Story: Describes a feature or requirement from a user perspective</li> <li>Task: Represents a specific unit of work</li> <li>Epic: Groups related stories and tasks</li> <li>Non-Development Task: Used for documentation or analysis activities</li> </ul> <p>Using appropriate issue types helps maintain structure and reporting consistency.</p>"},{"location":"qa/bug_jira_usage/#closing-notes","title":"Closing Notes","text":"<p>Using Jira effectively enables QA teams to maintain quality visibility throughout the project lifecycle. Clear workflows, consistent issue tracking, and active collaboration help reduce risks and improve delivery outcomes.</p> <p>This documentation is intended to support QA engineers in understanding practical Jira usage and aligning testing activities with project goals.</p>"},{"location":"qa/bug_life_cycle/","title":"Bug Life Cycle","text":""},{"location":"qa/bug_life_cycle/#overview","title":"Overview","text":"<p>The bug life cycle describes the journey of a defect from the moment it is identified until it is fully resolved and closed. Understanding and following a well-defined bug life cycle is essential for maintaining transparency, accountability, and efficiency across QA, development, and product teams.</p> <p>From a QA perspective, the bug life cycle ensures that defects are not only found, but also properly tracked, validated, and verified before being considered resolved.</p> <p>This document explains the typical stages of a bug life cycle and the role of QA at each stage.</p>"},{"location":"qa/bug_life_cycle/#what-is-a-bug","title":"What Is a Bug?","text":"<p>A bug is any behavior in the software that deviates from the expected result, requirements, or acceptance criteria. Bugs may affect functionality, performance, usability, security, or compatibility.</p> <p>QA engineers are responsible for identifying bugs through testing activities and ensuring they are communicated clearly and tracked until resolution.</p>"},{"location":"qa/bug_life_cycle/#typical-bug-life-cycle-stages","title":"Typical Bug Life Cycle Stages","text":"<p>Although workflows may differ across teams, a typical bug life cycle includes the following stages:</p>"},{"location":"qa/bug_life_cycle/#new","title":"New","text":"<p>This is the initial stage when a bug is first identified and reported by the QA team.</p> <p>At this stage:</p> <ul> <li>The bug is documented in the tracking tool</li> <li>Reproduction steps, environment details, and evidence are provided</li> <li>The bug has not yet been reviewed by the development team</li> </ul> <p>QA responsibility:</p> <ul> <li>Ensure the bug description is clear and complete</li> <li>Verify that the issue is reproducible</li> <li>Attach screenshots, videos, or logs when available</li> </ul>"},{"location":"qa/bug_life_cycle/#open","title":"Open","text":"<p>Once the bug is reviewed and accepted, it moves to the Open state.</p> <p>At this stage:</p> <ul> <li>The issue is acknowledged as valid</li> <li>It becomes ready for prioritization and assignment</li> </ul> <p>QA responsibility:</p> <ul> <li>Clarify details if additional information is requested</li> <li>Support prioritization discussions when needed</li> </ul>"},{"location":"qa/bug_life_cycle/#in-progress","title":"In Progress","text":"<p>The bug enters this state when a developer starts working on the fix.</p> <p>At this stage:</p> <ul> <li>Code changes are being implemented</li> <li>The issue is actively addressed</li> </ul> <p>QA responsibility:</p> <ul> <li>Stay available for clarification</li> <li>Avoid retesting until the fix is marked as completed</li> </ul>"},{"location":"qa/bug_life_cycle/#fixed","title":"Fixed","text":"<p>This status indicates that the developer has completed the fix and believes the issue has been resolved.</p> <p>At this stage:</p> <ul> <li>The fix is deployed to a testable environment</li> <li>The bug is ready for QA verification</li> </ul> <p>QA responsibility:</p> <ul> <li>Prepare for retesting</li> <li>Validate the fix in the appropriate environment</li> </ul>"},{"location":"qa/bug_life_cycle/#retest-ready-for-test","title":"Retest / Ready for Test","text":"<p>In this stage, the bug is handed back to QA for verification.</p> <p>At this stage:</p> <ul> <li>QA re-executes the original reproduction steps</li> <li>Regression impact may be evaluated</li> </ul> <p>QA responsibility:</p> <ul> <li>Confirm whether the issue is fully resolved</li> <li>Check for any side effects or regressions</li> </ul>"},{"location":"qa/bug_life_cycle/#closed","title":"Closed","text":"<p>The bug is closed once QA confirms that the issue has been successfully resolved.</p> <p>At this stage:</p> <ul> <li>Actual behavior matches expected behavior</li> <li>No regression is detected</li> </ul> <p>QA responsibility:</p> <ul> <li>Close the bug</li> <li>Add final verification notes if necessary</li> </ul>"},{"location":"qa/bug_life_cycle/#reopened","title":"Reopened","text":"<p>If the issue persists or reappears after being marked as fixed, the bug is reopened.</p> <p>At this stage:</p> <ul> <li>The issue returns to the development workflow</li> <li>Further investigation is required</li> </ul> <p>QA responsibility:</p> <ul> <li>Clearly explain why the bug is reopened</li> <li>Provide updated evidence if applicable</li> </ul>"},{"location":"qa/bug_life_cycle/#why-the-bug-life-cycle-is-important","title":"Why the Bug Life Cycle Is Important","text":"<p>A clearly defined bug life cycle provides several benefits:</p> <ul> <li>Prevents defects from being overlooked</li> <li>Improves communication between QA and development teams</li> <li>Clarifies ownership and responsibilities</li> <li>Enables accurate reporting and metrics</li> <li>Supports continuous quality improvement</li> </ul> <p>Without a structured bug life cycle, defect management becomes inconsistent and unreliable.</p>"},{"location":"qa/bug_life_cycle/#qa-perspective-on-bug-life-cycle-management","title":"QA Perspective on Bug Life Cycle Management","text":"<p>For QA engineers, managing the bug life cycle goes beyond opening and closing tickets. It involves:</p> <ul> <li>Reporting defects objectively and consistently</li> <li>Verifying fixes thoroughly</li> <li>Protecting the product from regressions</li> <li>Maintaining trust between teams</li> </ul> <p>A disciplined approach to bug life cycle management reflects the overall maturity of the QA process.</p>"},{"location":"qa/bug_life_cycle/#conclusion","title":"Conclusion","text":"<p>The bug life cycle is a fundamental component of effective defect management. When QA teams actively own and follow this process, defects are resolved faster, collaboration improves, and software quality increases.</p> <p>Understanding each stage and the QA responsibilities within it helps teams build predictable and reliable quality practices.</p>"},{"location":"qa/bug_reporting_best_practices/","title":"Bug Reporting Best Practices","text":""},{"location":"qa/bug_reporting_best_practices/#overview","title":"Overview","text":"<p>Bug reporting is one of the core responsibilities of a QA engineer. Detecting a defect is only the beginning of the quality process; how that defect is communicated has a direct impact on how quickly and correctly it will be resolved.</p> <p>A clear and well-structured bug report minimizes misunderstandings, prevents repeated clarification cycles, and helps development teams focus on fixing the issue rather than interpreting it. Effective bug reporting strengthens collaboration and improves overall product quality.</p> <p>This document describes practical and proven best practices for creating clear, consistent, and actionable bug reports from a QA perspective.</p>"},{"location":"qa/bug_reporting_best_practices/#purpose-of-a-bug-report","title":"Purpose of a Bug Report","text":"<p>The primary objective of a bug report is to support the development team in resolving an issue efficiently. A properly written bug report should enable the team to:</p> <ul> <li>Clearly understand the problem</li> <li>Reproduce the issue in a controlled environment</li> <li>Analyze the root cause</li> <li>Implement an accurate and complete fix</li> </ul> <p>A bug report should explain what is wrong, where it occurs, and how it can be reproduced, without speculation or personal interpretation.</p>"},{"location":"qa/bug_reporting_best_practices/#core-principles-of-effective-bug-reporting","title":"Core Principles of Effective Bug Reporting","text":"<p>An effective bug report follows a few fundamental principles:</p> <ul> <li>Clarity: Information is easy to read and unambiguous</li> <li>Reproducibility: Steps are detailed enough to recreate the issue</li> <li>Objectivity: Facts are presented without assumptions</li> <li>Completeness: All relevant technical and contextual data is included</li> </ul> <p>QA engineers should always assume that the reader has no prior knowledge of the issue.</p>"},{"location":"qa/bug_reporting_best_practices/#essential-elements-of-a-bug-report","title":"Essential Elements of a Bug Report","text":""},{"location":"qa/bug_reporting_best_practices/#summary-title","title":"Summary (Title)","text":"<p>The summary provides a short and precise description of the issue. A strong title allows teams to quickly understand the nature of the problem.</p> <p>Well-written summaries:</p> <ul> <li>Are concise and specific</li> <li>Clearly describe the defect</li> </ul> <p>Examples:</p> <ul> <li>\"Login button is unresponsive on mobile devices\"</li> <li>\"Incorrect validation message shown for invalid password input\"</li> </ul> <p>Poor examples:</p> <ul> <li>\"Login issue\"</li> <li>\"Not working\"</li> </ul>"},{"location":"qa/bug_reporting_best_practices/#description","title":"Description","text":"<p>The description section explains the issue in detail. It should clearly answer the following questions:</p> <ul> <li>What is the issue?</li> <li>Where does it occur?</li> <li>Under which conditions does it appear?</li> </ul> <p>Expected behavior and actual behavior should be explicitly separated to avoid confusion.</p>"},{"location":"qa/bug_reporting_best_practices/#environment-information","title":"Environment Information","text":"<p>Environment details are critical for accurate reproduction. This information may include:</p> <ul> <li>Application version or build number</li> <li>Test environment (QA, staging, production)</li> <li>Operating system</li> <li>Browser, device, or platform information</li> </ul> <p>Missing environment data often leads to delays or unresolved defects.</p>"},{"location":"qa/bug_reporting_best_practices/#steps-to-reproduce","title":"Steps to Reproduce","text":"<p>This is one of the most important sections of a bug report.</p> <p>Reproduction steps should be:</p> <ul> <li>Clearly numbered</li> <li>Simple and concise</li> <li>Written so that any team member can follow them</li> </ul> <p>Example:</p> <ol> <li>Navigate to the login page</li> <li>Enter a valid username and an invalid password</li> <li>Click the \"Login\" button</li> </ol>"},{"location":"qa/bug_reporting_best_practices/#expected-result","title":"Expected Result","text":"<p>This section describes how the system is supposed to behave based on requirements or acceptance criteria.</p> <p>Example: \"The user should see a validation message indicating that the credentials are incorrect.\"</p>"},{"location":"qa/bug_reporting_best_practices/#actual-result","title":"Actual Result","text":"<p>This section explains what actually happens when the steps are followed.</p> <p>Example: \"No validation message is displayed and the page remains unchanged.\"</p>"},{"location":"qa/bug_reporting_best_practices/#attachments-and-supporting-evidence","title":"Attachments and Supporting Evidence","text":"<p>Whenever possible, bug reports should include supporting materials such as:</p> <ul> <li>Screenshots</li> <li>Screen recordings</li> <li>Log files</li> </ul> <p>Visual and technical evidence reduces ambiguity and accelerates the investigation process.</p>"},{"location":"qa/bug_reporting_best_practices/#priority-and-severity","title":"Priority and Severity","text":"<p>QA engineers should clearly understand the distinction between severity and priority:</p> <ul> <li>Severity refers to the impact of the bug on system functionality</li> <li>Priority indicates how urgently the issue should be addressed</li> </ul> <p>While QA teams often suggest severity and priority, final decisions are typically made in collaboration with product and development stakeholders.</p>"},{"location":"qa/bug_reporting_best_practices/#common-bug-reporting-mistakes","title":"Common Bug Reporting Mistakes","text":"<p>To maintain quality and efficiency, QA engineers should avoid:</p> <ul> <li>Reporting multiple unrelated issues in a single bug</li> <li>Using vague or incomplete descriptions</li> <li>Including subjective opinions instead of facts</li> <li>Reporting expected system behavior as a defect</li> </ul> <p>Clear and accurate communication is more valuable than the number of reported bugs.</p>"},{"location":"qa/bug_reporting_best_practices/#qa-responsibility-in-bug-communication","title":"QA Responsibility in Bug Communication","text":"<p>Bug reporting is not only a technical activity but also a communication skill. QA engineers act as a bridge between users, developers, and product teams.</p> <p>Well-prepared bug reports:</p> <ul> <li>Improve cross-team collaboration</li> <li>Reduce misunderstandings and friction</li> <li>Increase confidence in QA findings</li> <li>Enable faster and more reliable fixes</li> </ul>"},{"location":"qa/bug_reporting_best_practices/#conclusion","title":"Conclusion","text":"<p>High-quality bug reporting is a strong indicator of QA maturity. When defects are documented clearly and consistently, teams spend less time analyzing issues and more time resolving them.</p> <p>By applying these best practices, QA engineers can significantly improve defect resolution efficiency and contribute directly to higher software quality.</p>"},{"location":"qa/bug_sprint_planning_qa/","title":"Sprint Planning &amp; QA","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_artifacts_reporting/","title":"Artifacts &amp; Reporting","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_azure_devops_pipelines/","title":"Azure DevOps Pipelines","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_github_actions/","title":"GitHub Actions","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_jenkins/","title":"Jenkins","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_test_automation_integration/","title":"Test Automation Integration","text":"<p>Content will be added.</p>"},{"location":"qa/cicd_yaml_pipeline_examples/","title":"YAML Pipeline Examples","text":"<p>Content will be added.</p>"},{"location":"qa/fundamentals_agile_scrum_qa_role/","title":"QA Role in Agile / Scrum","text":"<p>In the modern software world, QA is a strategic partner involved from the very beginning.</p>"},{"location":"qa/fundamentals_agile_scrum_qa_role/#agile-and-scrum-processes","title":"Agile and Scrum Processes","text":"<p>In the Agile model, development and testing activities are performed concurrently. Within Scrum, QA helps the team focus on all aspects of the software by providing transparency, inspection, and adaptation. In the Kanban model, it ensures the visualization of tasks.</p>"},{"location":"qa/fundamentals_agile_scrum_qa_role/#importance-of-manual-testing","title":"Importance of Manual Testing","text":"<p>Manual testing plays a critical role in areas outside the scope of automation:</p> <ul> <li>Execution: Test cases are executed manually without any tools.</li> <li>Advantages: It is easy to test visuals and quick to check minor changes.</li> <li>Flexibility: Ad-hoc and Exploratory Testing are easily performed manually.</li> <li>Defect Management: Helps find critical bugs and UX flaws. Defects are reported to developers; if uncorrected, they may lead to Failure.</li> </ul>"},{"location":"qa/fundamentals_agile_scrum_qa_role/#as-a-strategic-approach-test-prioritization","title":"As a Strategic Approach: Test Prioritization","text":"<p>This is the process of assigning tactical value to test cases based on their importance or the likelihood of uncovering defects. The five levels are:</p> <ul> <li>Critical: Risk of data loss or system corruption; must be tested immediately in the latest build.</li> <li>Major: Unacceptable loss of functionality; must be tested as soon as possible.</li> <li>High: Loss of functionality exists, but workarounds are available; must be tested once development is complete.</li> <li>Normal: Non-critical loss of functionality; testing once is sufficient.</li> <li>Low: Minor issues; tested only if time permits.</li> </ul>"},{"location":"qa/fundamentals_agile_scrum_qa_role/#qa-mindset-and-methodology","title":"QA Mindset and Methodology","text":"<p>QA follows the PDCA (Plan, Do, Check, Act) cycle. A QA engineer's mindset is as vital as their technical knowledge:</p> <ul> <li>Proactivity: Start test design early to find bugs before coding ends.</li> <li>Critical Perspective: Approach software with the view that \"it is buggy until proven otherwise,\" but explain issues constructively.</li> <li>Communication: Be critical toward the software, but kind and solution-oriented toward developers.</li> </ul>"},{"location":"qa/fundamentals_sdlc_stlc/","title":"SDLC &amp; STLC (High-level)","text":""},{"location":"qa/fundamentals_sdlc_stlc/#software-and-test-life-cycles","title":"Software and Test Life Cycles","text":"<p>Quality is not a final act; it is the result of the seamless integration between development (SDLC) and testing (STLC) processes throughout the project.</p>"},{"location":"qa/fundamentals_sdlc_stlc/#a-software-development-life-cycle-sdlc","title":"A. Software Development Life Cycle (SDLC)","text":"<p>SDLC is the framework that defines the tasks performed at each step in the software development process to produce high-quality software.</p> <ul> <li>Planning &amp; Risk Analysis: Defining scope, resource allocation, and identifying potential project risks.</li> <li>Requirement Analysis: Gathering business and technical needs to create documents like BRD (Business Requirements Document) and FRD (Functional Requirements Document).</li> <li>Design (DDS): Architects create the Design Document Specification, outlining the system's structure and data flow.</li> <li>Development (Coding): The actual phase where developers write code based on the design specifications.</li> <li>Testing: Verifying that the application is bug-free and meets the initial requirements.</li> <li>Deployment &amp; Maintenance: Releasing the product to the market and providing updates or fixes as needed.</li> </ul>"},{"location":"qa/fundamentals_sdlc_stlc/#b-software-testing-life-cycle-stlc","title":"B. Software Testing Life Cycle (STLC)","text":"<p>STLC is a sequence of specific activities conducted during the testing process to ensure software quality goals are met.</p> Phase Core Objective Deliverables Test Planning Determining strategy, tools, and timelines. Test Plan, Resource Plan Test Analysis Identifying what to test by reviewing requirements. Test Conditions, RTM (Traceability Matrix) Test Design Creating detailed steps and preparing data. Test Cases, Test Scripts, Test Data Environment Setup Preparing the hardware/software \"test bed.\" Test Environment Ready Test Execution Running the tests and reporting defects. Bug Reports, Test Logs Test Closure Evaluating exit criteria and summarizing results. Test Summary Report"},{"location":"qa/fundamentals_sdlc_stlc/#c-the-golden-rule-verification-vs-validation","title":"C. The Golden Rule: Verification vs. Validation","text":"<p>In QA, we distinguish between building the system correctly and building the correct system.</p>"},{"location":"qa/fundamentals_sdlc_stlc/#verification-process-oriented","title":"Verification (Process-Oriented)","text":"<ul> <li>The Question: \"Are we building the product right?\"</li> <li>Method: Static testing. It involves reviewing documents, designs, and code without executing the software.</li> <li>Focus: Compliance with specifications and standards.</li> </ul>"},{"location":"qa/fundamentals_sdlc_stlc/#validation-product-oriented","title":"Validation (Product-Oriented)","text":"<ul> <li>The Question: \"Are we building the right product?\"</li> <li>Method: Dynamic testing. It involves running the actual software to see if it meets user expectations.</li> <li>Focus: End-user satisfaction and real-world functionality.</li> </ul> <p>Pro Tip: In the V-Model, Verification activities happen on the left side (Requirement Analysis, Design), while Validation activities happen on the right side (Unit, Integration, System, and UAT testing).</p>"},{"location":"qa/fundamentals_test_levels/","title":"Test Levels (Unit / Integration / System / Acceptance)","text":""},{"location":"qa/fundamentals_test_levels/#the-hierarchy-from-unit-testing-to-user-acceptance","title":"The Hierarchy from Unit Testing to User Acceptance","text":"<p>Software testing is conducted in a hierarchical structure, typically within the V-Model framework, to isolate errors as early as possible, reduce costs, and ensure every part of the system is built correctly.</p> <p>Unit Testing: A white-box technique that verifies the smallest building blocks of the software (functions, methods, or classes) work as expected. Usually performed by developers during the coding phase, it aims to isolate a section of code and verify its correctness. It is the level where bugs are caught most early and cheaply.</p> <p>Integration Testing: Aims to expose defects in data communication and interaction between individually tested units when they are combined. It verifies if modules work harmoniously as a whole. Three main strategies are used:</p> <ul> <li>Big Bang Approach: All components are integrated simultaneously and tested as a single unit. Suitable for small systems, but defect isolation is difficult.</li> <li>Incremental Approach: Modules are merged in a logical order. Two sub-methods are used:<ul> <li>Top-Down: High-level modules are tested first; \"Stubs\" (dummy programs) are used for low-level modules not yet ready.</li> <li>Bottom-Up: Low-level modules are tested first; \"Drivers\" (calling programs) are used to simulate high-level modules.</li> </ul> </li> <li>Sandwich (Hybrid) Approach: A combination of Top-Down and Bottom-Up; it focuses on critical modules using both Stubs and Drivers.</li> </ul> <p>System Testing: The level that measures the compliance of the fully integrated system against the functional and non-functional requirements (SRS) defined at the start. The software undergoes a holistic evaluation before being presented to the end user.</p> <p>User Acceptance Testing (UAT): The final stage of testing to verify the software workflow. Often involving multiple users, this process determines if the software meets real-world needs.</p> <ul> <li>Requirement: Since developers might build software based on their own understanding, UAT ensures alignment with customer expectations.</li> <li>V-Model Correspondence: In the V-Model, UAT directly corresponds to the \"Requirement Analysis\" phase at the very beginning of the SDLC.</li> </ul>"},{"location":"qa/fundamentals_test_types/","title":"Test Types (Functional / Non-Functional / Regression)","text":""},{"location":"qa/fundamentals_test_types/#functional-and-non-functional-parameters","title":"Functional and Non-Functional Parameters","text":"<p>The success of software depends on both its functional accuracy and its structural robustness (performance, security, etc.).</p>"},{"location":"qa/fundamentals_test_types/#functional-testing-and-techniques","title":"Functional Testing and Techniques","text":"<p>Functional testing verifies if the system performs correctly according to requirements. Black Box techniques, which do not require knowledge of the internal code structure, are typically used:</p> <ul> <li>Equivalence Partitioning: Dividing inputs into logical groups.</li> <li>Decision Table: Testing combinations of inputs.</li> <li>State Transition: Examining changes in states.</li> <li>All-pairs Testing: Covering input pairs.</li> <li>Regression Testing: Verifies that a change has not broken existing functionality. It can focus on only the changed unit (URT) or the entire product (FRT).</li> </ul>"},{"location":"qa/fundamentals_test_types/#non-functional-testing","title":"Non-Functional Testing","text":"<p>Tests conducted to increase the efficiency and reliability of the software:</p> <ul> <li>Performance Testing: Measures the 3S (Speed, Scalability, Stability). It includes Load Testing (measuring daily use) and Stress Testing (pushing limits).</li> <li>Security Testing: Identifies system vulnerabilities through scans, penetration tests, and risk assessments.</li> <li>Accessibility Testing: Measures the usability of the system for individuals with disabilities using assistive technologies like screen readers.</li> </ul>"},{"location":"qa/fundamentals_what_is_qa_qa_vs_qc/","title":"What is QA? (QA vs QC)","text":""},{"location":"qa/fundamentals_what_is_qa_qa_vs_qc/#quality-and-assurance","title":"Quality and Assurance","text":"<ul> <li>Quality: Quality refers to a product's ability to meet customer expectations in terms of usability, design, reliability, and durability. The definition of quality can be expressed as \"fitness for use\" or \"fitness for purpose.\" It is based on satisfying the customer's expectations regarding the product's utility, design, reliability, durability, and price.</li> <li>Assurance: This is a guarantee provided that a product or service will operate without any issues in line with expectations.</li> </ul>"},{"location":"qa/fundamentals_what_is_qa_qa_vs_qc/#distinction-between-quality-assurance-qa-and-quality-control-qc","title":"Distinction Between Quality Assurance (QA) and Quality Control (QC)","text":"<p>Although these two concepts are often confused, their focal points and implementation methods differ:</p> <ul> <li>Quality Assurance (QA): A proactive procedure established to ensure an organization provides the best possible product to its customers. It is based on the philosophy of \"Right Process = Quality Software\" and aims to prevent defects by examining processes. It is also known as QA testing\u2014a procedure to ensure the organization provides the best product or service.</li> <li>Quality Control (QC): Quality Control (QC): This process involves inspecting the final results to analyze the quality of the specific output. Rather than being proactive, it is a reactive approach that checks if a product satisfies customer-defined specifications.Within the realm of software engineering, it is utilized to guarantee that services and products consistently meet quality benchmarks.</li> <li>Core Difference: While QA focuses on processes and guidance, QC concentrates on the product and inspection. QA examines processes and makes changes to those leading to the final product; QC does not deal with processes but inspects the final output to verify its quality.</li> </ul>"},{"location":"qa/perf_gatling_assertions_reporting/","title":"Assertions &amp; Reporting","text":"<p>Content will be added.</p>"},{"location":"qa/perf_gatling_load_models/","title":"Load Models","text":"<p>Content will be added.</p>"},{"location":"qa/perf_gatling_test_structure/","title":"Test Structure","text":"<p>Content will be added.</p>"},{"location":"qa/perf_gatling_what_is/","title":"What is Gatling?","text":"<p>Content will be added.</p>"},{"location":"qa/perf_jmeter_listeners_reporting/","title":"Listeners &amp; Reporting","text":"<p>Content will be added.</p>"},{"location":"qa/perf_jmeter_test_plan_structure/","title":"Test Plan Structure","text":"<p>Content will be added.</p>"},{"location":"qa/perf_jmeter_thread_groups_samplers/","title":"Thread Groups &amp; Samplers","text":"<p>Content will be added.</p>"},{"location":"qa/perf_jmeter_what_is/","title":"What is JMeter?","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_cloud_grafana_integration/","title":"k6 Cloud / Grafana Integration","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_metrics_reporting/","title":"Metrics &amp; Reporting","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_scenarios_executors/","title":"Scenarios &amp; Executors","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_script_writing/","title":"Script Writing","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_studio/","title":"k6 Studio","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_thresholds_checks/","title":"Thresholds &amp; Checks","text":"<p>Content will be added.</p>"},{"location":"qa/perf_k6_what_is/","title":"What is k6?","text":"<p>Content will be added.</p>"},{"location":"qa/rca_example_and_qa_perspective/","title":"RCA Example &amp; QA Perspective","text":""},{"location":"qa/rca_example_and_qa_perspective/#scenario-timeout-error-in-payment-service","title":"Scenario: Timeout Error in Payment Service","text":"<p>Problem Description: 15% of users in the production environment are unable to complete payment transactions. Users receive \"Transaction timed out\" error. The issue is observed more frequently during peak hours.</p> <p>Impact:</p> <ul> <li> <p>Daily transaction loss</p> </li> <li> <p>Customer complaints</p> </li> <li> <p>Increased ticket volume to support team</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#5-whys-technique-application","title":"5 Whys Technique Application","text":"Step Question Answer 1 Why are users unable to make payments? Payment API returns timeout error 2 Why does the API timeout? Database query responds late 3 Why does the query respond late? Full table scan is performed on the Orders table 4 Why is a full table scan performed? Index is not defined on the relevant column 5 Why is the index not defined? Table growth was not anticipated, performance testing was not conducted <p>Root Cause: user_id column lacks an index, and performance testing under load was not performed.</p>"},{"location":"qa/rca_example_and_qa_perspective/#fishbone-analysis-ishikawa-diagram","title":"Fishbone Analysis (Ishikawa Diagram)","text":"<p>The following table categorizes the factors contributing to the payment timeout error:</p> Category Factors PEOPLE \u2022 Performance check was skipped during code review \u2022 DBA approval was not obtained METHOD \u2022 Load testing was not performed \u2022 DB migration checklist was incomplete SYSTEM \u2022 Index is missing \u2022 Connection pool is insufficient ENVIRONMENT \u2022 Peak hour traffic increase \u2022 Concurrent user count increased 3x MEASUREMENT \u2022 DB response time was not monitored \u2022 Alert threshold was not defined <p>Main Problem: PAYMENT TIMEOUT ERROR</p>"},{"location":"qa/rca_example_and_qa_perspective/#actions-taken","title":"Actions Taken","text":""},{"location":"qa/rca_example_and_qa_perspective/#short-term-immediate-fix","title":"Short-Term (Immediate Fix)","text":"<ul> <li> <p>Index added to orders.user_id column</p> </li> <li> <p>Connection pool size increased</p> </li> <li> <p>API timeout duration optimized</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#medium-term-preventive-actions","title":"Medium-Term (Preventive Actions)","text":"<ul> <li> <p>Index review step added to DB migration checklist</p> </li> <li> <p>Load testing process integrated into CI/CD pipeline</p> </li> <li> <p>Monitoring and alerts defined for database response time</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#long-term-process-improvement","title":"Long-Term (Process Improvement)","text":"<ul> <li> <p>Performance testing strategy documented</p> </li> <li> <p>Performance criteria added to code review checklist</p> </li> <li> <p>Capacity planning process established</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#qas-role-in-the-rca-process","title":"QA's Role in the RCA Process","text":"<p>QA engineers should take an active role in the RCA process and ask the following questions:</p>"},{"location":"qa/rca_example_and_qa_perspective/#test-coverage-analysis","title":"Test Coverage Analysis","text":"<ul> <li> <p>Was this scenario within test coverage?</p> </li> <li> <p>Was load testing performed? If so, under what conditions?</p> </li> <li> <p>Were edge cases evaluated?</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#test-strategy-assessment","title":"Test Strategy Assessment","text":"<ul> <li> <p>Why did the current testing approach fail to catch this bug?</p> </li> <li> <p>Which type of testing was missing? (Performance, Load, Stress)</p> </li> <li> <p>How accurately did the test environment reflect production?</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#regression-strategy","title":"Regression Strategy","text":"<ul> <li> <p>Which areas carry regression risk after this fix?</p> </li> <li> <p>What should be added to automation coverage?</p> </li> <li> <p>Should the smoke test suite be updated?</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#defect-pattern-analysis","title":"Defect Pattern Analysis","text":"<ul> <li> <p>Have similar bugs occurred before?</p> </li> <li> <p>Is there a common pattern?</p> </li> <li> <p>Which module or component is at risk?</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#rca-report-template","title":"RCA Report Template","text":"<p>An RCA report should include the following sections:</p>"},{"location":"qa/rca_example_and_qa_perspective/#summary","title":"Summary","text":"<ul> <li> <p>Incident ID: INC-2024-0892</p> </li> <li> <p>Date: 2024-01-15</p> </li> <li> <p>Severity: P1</p> </li> <li> <p>Affected System: Payment Service</p> </li> <li> <p>Impact Duration: 3 hours 20 minutes</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#problem-description","title":"Problem Description","text":"<p>[Description of the problem and how it was detected]</p>"},{"location":"qa/rca_example_and_qa_perspective/#timeline","title":"Timeline","text":"<ul> <li> <p>09:15 - First alert triggered</p> </li> <li> <p>09:22 - Incident opened</p> </li> <li> <p>09:45 - Root cause identified</p> </li> <li> <p>10:30 - Hotfix deployed</p> </li> <li> <p>12:35 - Service stabilized</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#root-cause","title":"Root Cause","text":"<p>[Root cause identified through 5 Whys or other technique used]</p>"},{"location":"qa/rca_example_and_qa_perspective/#impact-analysis","title":"Impact Analysis","text":"<ul> <li> <p>Number of affected users</p> </li> <li> <p>Number of failed transactions</p> </li> <li> <p>Financial impact (if any)</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#actions-taken-table","title":"Actions Taken Table","text":"Action Owner Due Date Status Add index DB Team 2024-01-16 Completed Add load testing QA Team 2024-01-22 In Progress Set up monitoring DevOps Team 2024-01-25 Planned Update checklist QA Team 2024-01-20 Completed"},{"location":"qa/rca_example_and_qa_perspective/#lessons-learned","title":"Lessons Learned","text":"<p>[Conclusions drawn regarding process, system, and team]</p>"},{"location":"qa/rca_example_and_qa_perspective/#approval","title":"Approval","text":"<ul> <li> <p>Prepared by: [Name]</p> </li> <li> <p>Approved by: [Name]</p> </li> <li> <p>Date: [Date]</p> </li> </ul>"},{"location":"qa/rca_example_and_qa_perspective/#lessons-learned_1","title":"Lessons Learned","text":"<p>Questions to be evaluated as a team at the end of each RCA process:</p> <p>From a Process Perspective:</p> <ul> <li> <p>Which control points were missing?</p> </li> <li> <p>Which existing processes should be updated?</p> </li> <li> <p>Is there an automation opportunity?</p> </li> </ul> <p>From a Technical Perspective:</p> <ul> <li> <p>Is an architectural or design change required?</p> </li> <li> <p>Is monitoring and alerting sufficient?</p> </li> <li> <p>Is documentation up to date?</p> </li> </ul> <p>From a Team Perspective:</p> <ul> <li> <p>Was there a knowledge gap?</p> </li> <li> <p>Was a training need identified?</p> </li> <li> <p>Were communication processes adequate?</p> </li> </ul> <p>Lessons learned outputs should be shared with the entire team and stored as a reference to prevent similar errors.</p>"},{"location":"qa/rca_what_is_rca/","title":"What is Root Cause Analysis (RCA)?","text":""},{"location":"qa/rca_what_is_rca/#definition","title":"Definition","text":"<p>RCA (Root Cause Analysis) is a systematic investigation process aimed at identifying the actual source of a problem rather than addressing its superficial symptoms. It is known as Root Cause Analysis in software engineering and quality assurance processes.</p> <p>Instead of providing an immediate answer to the question \"Why did this error occur?\", RCA seeks a permanent answer to the question \"What is the true source of this error and how do we prevent its recurrence?\"</p>"},{"location":"qa/rca_what_is_rca/#distinguishing-symptoms-from-root-causes","title":"Distinguishing Symptoms from Root Causes","text":"<p>The most common mistake when solving a problem is accepting the visible symptom as the root cause.</p> Symptom Possible Root Cause Users cannot log in Database connection pool exhausted Page loads slowly Unoptimized SQL query Same bug reopened Insufficient test coverage or incomplete code review Payment transaction fails randomly Third-party service timeout mismatch Service crashed after deployment Missing environment variable definition <p>RCA aims to eliminate the source of the problem rather than temporarily alleviating symptoms.</p>"},{"location":"qa/rca_what_is_rca/#core-principles-of-rca","title":"Core Principles of RCA","text":""},{"location":"qa/rca_what_is_rca/#1-data-driven-approach","title":"1. Data-Driven Approach","text":"<p>Progress is made with concrete data, not assumptions. Log records, metrics, test results, and error reports are the primary inputs of the analysis process.</p>"},{"location":"qa/rca_what_is_rca/#2-systems-thinking","title":"2. Systems Thinking","text":"<p>The problem is evaluated as part of the system, not as an isolated event. Dependencies and interactions between components are taken into consideration.</p>"},{"location":"qa/rca_what_is_rca/#3-learning-not-blaming","title":"3. Learning, Not Blaming","text":"<p>RCA is conducted to identify process and system vulnerabilities, not to find someone to blame. The focus is not \"Who made the mistake?\" but rather \"Why did the system allow this error to occur?\"</p>"},{"location":"qa/rca_what_is_rca/#4-documentation-and-reusability","title":"4. Documentation and Reusability","text":"<p>Identified root causes and applied solutions are documented. This creates organizational memory and provides a reference for similar issues.</p>"},{"location":"qa/rca_what_is_rca/#common-rca-techniques","title":"Common RCA Techniques","text":""},{"location":"qa/rca_what_is_rca/#5-whys","title":"5 Whys","text":"<p>The \"Why?\" question is asked repeatedly to dig deeper into the problem. It is a simple and quickly applicable technique.</p>"},{"location":"qa/rca_what_is_rca/#fishbone-diagram-ishikawa","title":"Fishbone Diagram (Ishikawa)","text":"<p>Visualizes factors affecting the problem by categorizing them. Categories typically include: People, Method, System, Environment, Measurement. It is effective for multi-factor problems.</p>"},{"location":"qa/rca_what_is_rca/#fault-tree-analysis","title":"Fault Tree Analysis","text":"<p>Starting from a top-level error, it models possible causes in a tree structure using logical gates (AND/OR). It is preferred in safety-critical systems and risk analysis.</p>"},{"location":"qa/rca_what_is_rca/#rcas-place-in-software-processes","title":"RCA's Place in Software Processes","text":"<p>RCA is applied in software development and QA processes in the following situations:</p> <ul> <li> <p>Production Incidents: Outages or critical errors occurring in the live environment</p> </li> <li> <p>Recurring Defects: Bugs that are closed and reopened or appear in different forms</p> </li> <li> <p>Security Vulnerabilities: Identified vulnerabilities or breaches</p> </li> <li> <p>Performance Anomalies: Unexpected slowdowns or resource consumption issues</p> </li> <li> <p>Post-Release Issues: Unexpected behaviors that emerge following deployment</p> </li> </ul> <p>From a QA perspective, RCA means analyzing why the bug was not caught during the testing process and updating the test strategy accordingly.</p>"},{"location":"qa/rca_why_and_when/","title":"Why &amp; When RCA is Needed","text":"<p>The primary purpose of RCA is to prevent the recurrence of problems and produce permanent solutions. Fixing an error is an immediate intervention; understanding why that error occurred is a strategic investment.</p> <p>Core motivations for RCA:</p> <ul> <li> <p>Prevent the same error from recurring in different forms</p> </li> <li> <p>Apply permanent fixes instead of temporary solutions (workarounds)</p> </li> <li> <p>Minimize error cost at an early stage</p> </li> <li> <p>Identify process and system vulnerabilities</p> </li> <li> <p>Create awareness and a learning culture across the team</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#what-happens-if-rca-is-not-performed","title":"What Happens If RCA Is Not Performed?","text":"<p>When RCA is not applied, problems are superficially closed but the source is not eliminated. This leads to serious consequences in the medium and long term:</p> Situation Consequence Same bug reopened with different tickets Development and testing effort is wasted Production incidents recur Customer trust is shaken, SLA violations occur Team deals with the same issue repeatedly Motivation drops, productivity decreases Source of the problem remains unclear Technical debt accumulates, system complexity increases Errors are not documented Organizational memory is not formed, same mistakes are repeated by new team members"},{"location":"qa/rca_why_and_when/#when-should-rca-be-performed","title":"When Should RCA Be Performed?","text":"<p>It is not practical to perform RCA for every error. The RCA process should be initiated in the following situations:</p>"},{"location":"qa/rca_why_and_when/#critical-production-incidents","title":"Critical Production Incidents","text":"<ul> <li> <p>Service outage or loss of user access</p> </li> <li> <p>Data loss or data inconsistency</p> </li> <li> <p>Errors in financial transactions</p> </li> <li> <p>SLA violation</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#recurring-defects","title":"Recurring Defects","text":"<ul> <li> <p>Errors that appear multiple times in the same module or component</p> </li> <li> <p>Bugs that are closed and reopened</p> </li> <li> <p>Similar issues manifesting with different symptoms</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#security-breaches","title":"Security Breaches","text":"<ul> <li> <p>Identified security vulnerabilities</p> </li> <li> <p>Authorization or authentication errors</p> </li> <li> <p>Data leakage or unauthorized access</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#high-impact-customer-complaints","title":"High-Impact Customer Complaints","text":"<ul> <li> <p>Same feedback from multiple customers</p> </li> <li> <p>Functional errors with high business impact</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#critical-post-release-issues","title":"Critical Post-Release Issues","text":"<ul> <li> <p>Unexpected behaviors emerging after deployment</p> </li> <li> <p>Situations requiring rollback</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#when-is-rca-not-required","title":"When Is RCA Not Required?","text":"<p>RCA is not mandatory in every situation. Initiating a detailed analysis process in the following scenarios may create unnecessary effort:</p> <ul> <li> <p>One-time, low-impact errors: Minor bugs that affect an isolated user and do not recur</p> </li> <li> <p>Cases with obvious causes: If the root cause is clearly visible (e.g., typo, missing null check), a simple fix is sufficient</p> </li> <li> <p>Configuration errors: Environment-specific, easily correctable setting deficiencies</p> </li> <li> <p>Temporary third-party issues: Momentary outages originating from external services and beyond control</p> </li> </ul> <p>In these cases, the standard bug fix process is sufficient. However, if such errors become frequent, it should be evaluated whether a pattern is forming.</p>"},{"location":"qa/rca_why_and_when/#benefits-of-rca","title":"Benefits of RCA","text":"<p>A regularly and correctly applied RCA process provides the following benefits:</p>"},{"location":"qa/rca_why_and_when/#quality-improvement","title":"Quality Improvement","text":"<ul> <li> <p>Prevention of recurring errors</p> </li> <li> <p>Targeted expansion of test coverage</p> </li> <li> <p>Increased code quality</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#cost-optimization","title":"Cost Optimization","text":"<ul> <li> <p>Reduced error correction cost (early detection, permanent solution)</p> </li> <li> <p>Prevention of repeated effort</p> </li> <li> <p>Decreased number of production incidents</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#process-improvement","title":"Process Improvement","text":"<ul> <li> <p>Identification and strengthening of weak points</p> </li> <li> <p>Maturation of code review, testing, and deployment processes</p> </li> <li> <p>Identification of automation opportunities</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#team-development","title":"Team Development","text":"<ul> <li> <p>Establishment of a learning culture</p> </li> <li> <p>Increased knowledge sharing</p> </li> <li> <p>Development of problem-solving competency</p> </li> </ul>"},{"location":"qa/rca_why_and_when/#rca-and-continuous-improvement","title":"RCA and Continuous Improvement","text":"<p>RCA should be part of the continuous improvement cycle, not an isolated activity.</p> <p>Problem Detection \u2192 RCA \u2192 Root Cause \u2192 Action \u2192 Implementation \u2192 Verification \u2192 Monitoring</p> <p>The outputs feed into:</p> <ul> <li> <p>Process Updates</p> </li> <li> <p>Test Strategy Revision</p> </li> <li> <p>Automation Development</p> </li> </ul> <p>RCA in Agile and DevOps processes:</p> <ul> <li> <p>RCA outputs are evaluated in sprint retrospectives</p> </li> <li> <p>Incidents are analyzed in post-mortem meetings</p> </li> <li> <p>Preventive controls are added to CI/CD pipelines</p> </li> <li> <p>Monitoring and alerting strategies are updated</p> </li> </ul> <p>RCA outputs should be added to the backlog and improvement actions should be tracked.</p>"},{"location":"qa/reporting_allure/","title":"Allure Report","text":"<p>Allure Report is a reporting tool commonly used to visualize the results of automated test executions. It transforms raw test output into structured, easy-to-read reports that help teams quickly understand what was tested, what failed, and why.</p> <p>Rather than acting as a simple pass/fail summary, Allure focuses on providing detailed context around test executions.</p>"},{"location":"qa/reporting_allure/#how-allure-reports-are-used","title":"How Allure Reports Are Used","text":"<p>Allure reports are typically generated after an automated test run, often as part of a CI/CD pipeline. The report groups test results by features, test suites, or labels, making it easier to analyze failures and overall test coverage.</p> <p>Each test case includes execution details such as:</p> <ul> <li>Step-by-step test flow</li> <li>Assertions and failure points</li> <li>Logs and attachments like screenshots</li> <li>Execution time and environment information</li> </ul> <p>This level of detail helps reduce the time spent investigating test failures.</p>"},{"location":"qa/reporting_allure/#benefits-for-qa-teams","title":"Benefits for QA Teams","text":"<p>Allure provides clear visibility into automation results without requiring direct access to logs or test code. QA engineers, developers, and other stakeholders can review the same report and reach a shared understanding of test outcomes.</p> <p>It is especially useful when:</p> <ul> <li>Investigating flaky or unstable tests</li> <li>Comparing results across multiple runs</li> <li>Reviewing test results before a release</li> </ul>"},{"location":"qa/reporting_allure/#integration-and-flexibility","title":"Integration and Flexibility","text":"<p>Allure supports integration with many automation frameworks and can be easily added to CI/CD workflows. Reports can be generated locally during development or automatically published after pipeline executions.</p> <p>Its flexible structure allows teams to adapt reporting to their testing strategy without adding unnecessary complexity.</p>"},{"location":"qa/reporting_allure/#why-allure-report-matters","title":"Why Allure Report Matters","text":"<p>Clear and accessible reporting is essential for effective automated testing. Allure helps bridge the gap between automated test execution and human-readable results, making test outcomes easier to understand, share, and act on.</p> <p>Over time, consistent use of Allure reporting improves transparency, speeds up debugging, and supports more confident release decisions.</p>"},{"location":"qa/reporting_custom_reporting_approaches/","title":"Custom Reporting Approaches","text":"<p>Custom reporting approaches are used when standard reporting tools do not fully meet a project\u2019s needs. In such cases, teams design tailored reporting solutions that focus on the metrics, formats, and insights most relevant to their product and stakeholders.</p> <p>These approaches allow teams to move beyond generic pass/fail results and highlight quality indicators that matter most to the business.</p>"},{"location":"qa/reporting_custom_reporting_approaches/#when-custom-reporting-is-needed","title":"When Custom Reporting Is Needed","text":"<p>Custom reporting is typically preferred when:</p> <ul> <li>Project-specific KPIs are required</li> <li>Standard reports lack necessary context</li> <li>Stakeholders need simplified or targeted summaries</li> <li>Quality metrics must align closely with business flows</li> </ul> <p>In complex systems, off-the-shelf reports may not reflect real user impact or critical paths.</p>"},{"location":"qa/reporting_custom_reporting_approaches/#common-custom-reporting-examples","title":"Common Custom Reporting Examples","text":"<p>Custom reporting solutions may include:</p> <ul> <li>Dashboards built from test execution data</li> <li>Aggregated reports combining manual and automated results</li> <li>Release-focused summaries highlighting critical failures</li> <li>Trend analysis across multiple test cycles</li> </ul> <p>These reports are often generated using test outputs, database queries, or pipeline artifacts.</p>"},{"location":"qa/reporting_custom_reporting_approaches/#integration-with-cicd-and-team-tools","title":"Integration with CI/CD and Team Tools","text":"<p>Custom reports are frequently integrated into CI/CD pipelines and shared automatically through internal tools such as dashboards, messaging platforms, or documentation portals.</p> <p>This ensures that quality insights are delivered at the right time without manual effort, supporting faster feedback and decision-making.</p>"},{"location":"qa/reporting_custom_reporting_approaches/#benefits-of-custom-reporting","title":"Benefits of Custom Reporting","text":"<p>By tailoring reports to project needs, teams gain:</p> <ul> <li>Better visibility into real quality risks</li> <li>More meaningful release insights</li> <li>Improved communication with non-technical stakeholders</li> </ul> <p>Custom reporting helps position QA not only as a testing function, but as a contributor to product quality strategy.</p>"},{"location":"qa/reporting_custom_reporting_approaches/#why-custom-reporting-matters","title":"Why Custom Reporting Matters","text":"<p>Every project has different quality priorities. Custom reporting allows teams to adapt their reporting strategy to those priorities, ensuring that test results remain relevant, actionable, and aligned with both technical and business goals.</p> <p>Over time, this flexibility supports more informed decisions and more predictable releases.</p>"},{"location":"qa/reporting_extent/","title":"Extent Reports","text":"<p>Extent Reports is a reporting library used to generate detailed and visually structured test execution reports for automated tests. It is commonly preferred in UI automation projects where clear step-level visibility is important.</p> <p>Unlike simple console outputs or raw logs, Extent Reports present test results in a format that is easy to review and share.</p>"},{"location":"qa/reporting_extent/#how-extent-reports-are-used","title":"How Extent Reports Are Used","text":"<p>Extent Reports are generated during or after automated test execution and produce an HTML-based report. Each test scenario is displayed with its execution status, test steps, and relevant details captured during runtime.</p> <p>Typical information included in an Extent Report:</p> <ul> <li>Test status (pass, fail, skip)</li> <li>Step-level execution details</li> <li>Error messages and assertion failures</li> <li>Screenshots or additional logs when failures occur</li> </ul> <p>This structure makes it easier to understand exactly where and why a test failed.</p>"},{"location":"qa/reporting_extent/#strengths-of-extent-reports","title":"Strengths of Extent Reports","text":"<p>Extent Reports offer a high level of customization. Teams can adjust the layout, naming conventions, and level of detail based on their reporting needs. This makes it suitable for projects that require reports aligned with internal standards or stakeholder expectations.</p> <p>It is often used in:</p> <ul> <li>Selenium-based automation frameworks</li> <li>Playwright or similar UI test setups</li> <li>Projects where reports are shared outside the QA team</li> </ul>"},{"location":"qa/reporting_extent/#integration-and-maintenance","title":"Integration and Maintenance","text":"<p>Extent Reports can be integrated directly into test frameworks and CI pipelines. Reports are typically generated automatically after each execution and stored as build artifacts.</p> <p>Because of its flexible structure, maintaining and extending reports is straightforward, especially when test frameworks evolve over time.</p>"},{"location":"qa/reporting_extent/#why-extent-reports-matter","title":"Why Extent Reports Matter","text":"<p>Clear reporting plays a key role in making automation results actionable. Extent Reports help teams quickly assess test outcomes, identify failure points, and communicate results effectively.</p> <p>When used consistently, they improve transparency across teams and support faster feedback cycles during development and release processes.</p>"},{"location":"qa/reporting_jira_integrations/","title":"Jira Integrations","text":"<p>Jira integration connects test management and test execution activities with issue tracking workflows. This integration helps teams link testing results directly to development tasks, defects, and release planning.</p> <p>By keeping testing and issue tracking connected, teams gain better visibility into quality status and development progress.</p>"},{"location":"qa/reporting_jira_integrations/#how-jira-integration-works","title":"How Jira Integration Works","text":"<p>With Jira integration in place, test cases, test runs, and defects can be linked to Jira issues such as user stories, tasks, or bugs. Failed test results can be associated with existing issues or used to create new defect tickets.</p> <p>This ensures that test findings are immediately actionable and traceable.</p>"},{"location":"qa/reporting_jira_integrations/#benefits-for-daily-qa-and-development-work","title":"Benefits for Daily QA and Development Work","text":"<p>Jira integration allows teams to:</p> <ul> <li>Link test cases to user stories and requirements</li> <li>Create defects directly from failed test results</li> <li>Track defect status alongside test execution progress</li> <li>Maintain clear traceability between testing and development</li> </ul> <p>This reduces communication gaps and prevents issues from being lost between tools.</p>"},{"location":"qa/reporting_jira_integrations/#supporting-release-and-sprint-management","title":"Supporting Release and Sprint Management","text":"<p>By connecting test results to Jira tickets, teams can easily review quality status during sprint reviews and release planning. Stakeholders can quickly see which stories are fully tested, which have open defects, and what risks remain before release.</p> <p>This supports more informed decisions and smoother release cycles.</p>"},{"location":"qa/reporting_jira_integrations/#visibility-across-teams","title":"Visibility Across Teams","text":"<p>Jira integration improves collaboration between QA, development, and product teams. Everyone works with the same data, reducing manual updates and status meetings.</p> <p>Test progress and defect status become transparent and accessible throughout the development lifecycle.</p>"},{"location":"qa/reporting_jira_integrations/#why-jira-integration-matters","title":"Why Jira Integration Matters","text":"<p>Strong integration between testing tools and Jira helps ensure that quality is embedded into the development process. It improves traceability, speeds up feedback loops, and keeps testing aligned with delivery goals.</p> <p>Over time, this integration contributes to more predictable releases and better overall product quality.</p>"},{"location":"qa/reporting_tms_automation_integrations/","title":"TMS + Automation Integrations","text":"<p>TMS (Test Management System) and automation integrations are used to connect automated test executions with test case management tools. This integration helps teams keep test results, coverage, and execution history centralized and up to date.</p> <p>Instead of managing manual and automated tests separately, teams gain a unified view of overall test quality.</p>"},{"location":"qa/reporting_tms_automation_integrations/#how-the-integration-works","title":"How the Integration Works","text":"<p>In an integrated setup, automated test executions push their results directly into the TMS. Each automated scenario is linked to a corresponding test case, allowing execution results to be tracked without manual updates.</p> <p>This setup ensures that test case status reflects real execution outcomes.</p>"},{"location":"qa/reporting_tms_automation_integrations/#benefits-of-integrating-tms-with-automation","title":"Benefits of Integrating TMS with Automation","text":"<p>With proper integration, teams can:</p> <ul> <li>Automatically update test results after automation runs</li> <li>Track manual and automated coverage in one place</li> <li>Reduce repetitive manual reporting</li> <li>Maintain consistent traceability between test cases and executions</li> </ul> <p>This leads to a more efficient and reliable testing process.</p>"},{"location":"qa/reporting_tms_automation_integrations/#supporting-continuous-testing","title":"Supporting Continuous Testing","text":"<p>TMS and automation integrations play an important role in continuous testing strategies. Automated test results can be collected after each pipeline run, providing immediate feedback on system stability and regressions.</p> <p>This enables teams to detect issues earlier and respond faster.</p>"},{"location":"qa/reporting_tms_automation_integrations/#visibility-and-reporting","title":"Visibility and Reporting","text":"<p>When automation results are reflected in the TMS, reporting becomes more meaningful. Teams can easily answer questions such as:</p> <ul> <li>Which test cases are automated?</li> <li>What percentage of regression is covered by automation?</li> <li>How did automation results affect the latest release?</li> </ul> <p>This visibility supports better planning and more informed release decisions.</p>"},{"location":"qa/reporting_tms_automation_integrations/#why-tms-automation-integration-matters","title":"Why TMS + Automation Integration Matters","text":"<p>Integrating automation with a TMS helps align testing efforts across the team. It reduces manual overhead, improves transparency, and ensures that test documentation remains accurate as the product evolves.</p> <p>Over time, this integration becomes a key foundation for scalable and sustainable test practices.</p>"},{"location":"qa/test_approaches_in_quality_assurance/","title":"Test Approaches in Quality Assurance","text":"<p>A test approach is the backbone of any successful quality assurance (QA) strategy, defining how testing will be conducted based on project needs, constraints, and objectives. Unlike specific testing types, test approaches focus on high-level strategies for planning and execution. Below are some widely used test approaches, their characteristics, and when to apply them.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#1-preventive-test-approach","title":"1. Preventive Test Approach","text":"<p>The preventive approach emphasizes identifying and mitigating risks before they manifest in the development process. It relies heavily on early planning and documentation.</p> <p>Characteristics:</p> <p>Emphasis on early defect prevention.</p> <p>Strong focus on requirements and design reviews.</p> <p>Best Suited For:</p> <p>Waterfall or V-model projects with detailed upfront documentation.</p> <p>Critical systems where defects can be costly or catastrophic.</p> <p>Example Activities: Risk analysis, static code reviews, and requirement traceability matrix creation.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#2-reactive-test-approach","title":"2. Reactive Test Approach","text":"<p>In contrast to preventive testing, the reactive approach waits for testable components to be available and designs tests based on actual system behavior.</p> <p>Characteristics:</p> <p>Emphasis on defect detection over prevention.</p> <p>Heavily reliant on exploratory and ad-hoc testing.</p> <p>Best Suited For:</p> <p>Agile or fast-paced projects with dynamic requirements.</p> <p>Applications where frequent changes are expected.</p> <p>Example Activities: Exploratory testing, bug hunts, and session-based testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#3-risk-based-test-approach","title":"3. Risk-Based Test Approach","text":"<p>This approach prioritizes testing efforts based on the potential risk of failure and its impact on the business.</p> <p>Characteristics:</p> <p>Focuses on critical functionalities.</p> <p>Iterative and dynamic risk assessment throughout the project.</p> <p>Best Suited For:</p> <p>Projects with limited resources or tight deadlines.</p> <p>Complex systems with varying levels of criticality.</p> <p>Example Activities: Creating risk catalogs, prioritizing tests by risk level, and continuous risk reviews.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#4-context-driven-test-approach","title":"4. Context-Driven Test Approach","text":"<p>This adaptive approach recognizes that testing strategies must change based on the specific project context, constraints, and team dynamics.</p> <p>Characteristics:</p> <p>Rejects a one-size-fits-all methodology.</p> <p>Encourages creative, team-specific solutions.</p> <p>Best Suited For:</p> <p>Startups or projects with unique challenges.</p> <p>Teams that value collaboration and rapid iteration.</p> <p>Example Activities: Tailoring processes to stakeholder needs, balancing automation and manual testing based on context.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#5-analytical-test-approach","title":"5. Analytical Test Approach","text":"<p>The analytical approach uses data-driven techniques to design and prioritize tests. It is systematic and often based on quantitative metrics.</p> <p>Characteristics:</p> <p>Emphasis on coverage and traceability.</p> <p>Heavy use of models like decision tables or state-transition diagrams.</p> <p>Best Suited For:</p> <p>Regulatory or compliance-driven projects.</p> <p>Applications where high accuracy is essential.</p> <p>Example Activities: Test coverage analysis, metrics-based reporting, and decision-table-based testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#6-agile-test-approach","title":"6. Agile Test Approach","text":"<p>Aligned with Agile principles, this approach integrates testing into the development lifecycle, promoting continuous feedback and rapid adaptation.</p> <p>Characteristics:</p> <p>Testing is iterative and incremental.</p> <p>Collaborative approach involving QA, developers, and stakeholders.</p> <p>Best Suited For:</p> <p>Agile projects with short sprints and frequent deliverables.</p> <p>Teams practicing Test-Driven Development (TDD) or Behavior-Driven Development (BDD).</p> <p>Example Activities: Writing acceptance criteria as tests, automation in CI/CD pipelines, and sprint-based regression testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#7-model-based-test-approach","title":"7. Model-Based Test Approach","text":"<p>This approach relies on building abstract models (e.g., workflows, state diagrams) of the system to generate test cases systematically.</p> <p>Characteristics:</p> <p>Reduces manual effort in test design.</p> <p>Ensures high coverage of possible scenarios.</p> <p>Best Suited For:</p> <p>Complex systems with multiple workflows or states.</p> <p>Scenarios requiring repeatable and consistent testing.</p> <p>Example Activities: Building models using UML diagrams and auto-generating test cases from these models.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#8-process-oriented-test-approach","title":"8. Process-Oriented Test Approach","text":"<p>The process-oriented approach focuses on aligning testing activities with well-defined processes and standards.</p> <p>Characteristics:</p> <p>Based on industry standards like ISO or CMMI.</p> <p>Emphasizes compliance and documentation.</p> <p>Best Suited For:</p> <p>Organizations focused on audits and regulatory compliance.</p> <p>Projects requiring detailed process transparency.</p> <p>Example Activities: Creating process checklists, following test process improvement models, and conducting process audits.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#9-combined-test-approach","title":"9. Combined Test Approach","text":"<p>This approach integrates multiple strategies to address the diverse needs of a project, ensuring flexibility and adaptability.</p> <p>Characteristics:</p> <p>Uses a mix of preventive, reactive, and risk-based techniques.</p> <p>Promotes collaboration and cross-functional involvement.</p> <p>Best Suited For:</p> <p>Large, multidisciplinary projects.</p> <p>Scenarios requiring flexibility due to evolving requirements.</p> <p>Example Activities: Combining automated regression tests with exploratory manual sessions.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#conclusion","title":"Conclusion","text":"<p>Choosing the right test approach depends on project goals, team expertise, and resource availability. A thoughtful selection and combination of these approaches ensure not only the success of the testing phase but also the delivery of high-quality, reliable software.</p>"},{"location":"qa/test_processes_and_qa_practices/","title":"Test Processes and QA Practices","text":""},{"location":"qa/test_processes_and_qa_practices/#agile-and-qa","title":"Agile and QA","text":"<p>The Agile model is an approach in software development that emphasizes flexibility, collaboration, and adaptability. Its primary goal is to quickly respond to changing customer needs by delivering small, manageable increments of software frequently. In this model, QA processes play a critical role in ensuring software is developed to high-quality standards.</p>"},{"location":"qa/test_processes_and_qa_practices/#the-importance-of-qa-processes-in-agile","title":"The Importance of QA Processes in Agile","text":"<p>Agile QA adopts an agile and flexible approach to guarantee software quality. Unlike traditional models, QA processes are not a separate phase but an integral part of the Agile cycle. Throughout each sprint, QA teams contribute to developing fast and error-free software while maintaining quality.</p>"},{"location":"qa/test_processes_and_qa_practices/#continuous-involvement-and-collaboration","title":"Continuous Involvement and Collaboration","text":"<p>QA teams actively participate in all processes from the start to the end of sprints. They attend planning meetings to understand requirements, create test scenarios, and collaborate closely with developers. QA also communicates with product owners to clarify requirements and supports the creation of acceptance criteria.</p>"},{"location":"qa/test_processes_and_qa_practices/#early-testing","title":"Early Testing","text":"<p>In Agile, detecting defects as early as possible is crucial. Therefore, QA processes begin before coding starts. Practices like Test-Driven Development (TDD) and Acceptance Test-Driven Development (ATDD) are implemented to ensure robust testing from the beginning.</p>"},{"location":"qa/test_processes_and_qa_practices/#continuous-testing-and-feedback-loop","title":"Continuous Testing and Feedback Loop","text":"<p>QA ensures quick feedback by testing the current functionality of the software in every sprint. Regression Tests ensure new features do not break existing functionality, while Smoke Tests verify that core functionalities work smoothly.</p>"},{"location":"qa/test_processes_and_qa_practices/#test-automation","title":"Test Automation","text":"<p>To keep up with Agile's speed, a significant portion of testing processes is automated. QA teams reduce the manual testing burden by automating repetitive tests, enabling faster and more efficient testing.</p>"},{"location":"qa/test_processes_and_qa_practices/#devops-and-qa","title":"DevOps and QA","text":"<p>DevOps is an approach and culture that combines software development and IT operations to achieve faster, more reliable, and higher-quality software delivery. QA integrates into the entire software lifecycle, ensuring quality at every stage.</p>"},{"location":"qa/test_processes_and_qa_practices/#qas-role-in-cicd-processes","title":"QA's Role in CI/CD Processes","text":"<ul> <li>Continuous Integration (CI): Developers frequently merge their code into a shared repository and run automated tests to detect issues early.</li> <li>Continuous Delivery (CD): Code is delivered quickly and reliably to production after passing through CI processes.</li> </ul> <p>QA ensures that the software maintains functional and technical quality through practices like Shift-Left Testing, Test Automation, and Pre-Deployment Validation.</p>"},{"location":"qa/test_processes_and_qa_practices/#key-practices","title":"Key Practices","text":"<ol> <li>Shift-Left Testing: Early detection of errors by starting QA activities at the beginning of the development lifecycle.</li> <li>Test Automation: Utilizing automation tools for repetitive tasks, such as Unit, Integration, and Regression Tests.</li> <li>Pipeline Integration: Automated tests run after every code update in the CI/CD pipeline.</li> <li>Rapid Feedback: Sharing test results promptly for quick issue resolution.</li> <li>Pre-Deployment Validation: Conducting User Acceptance Testing (UAT) and Release Testing.</li> </ol>"},{"location":"qa/test_processes_and_qa_practices/#conclusion","title":"Conclusion","text":"<p>QA in Agile and DevOps ensures high-quality, reliable, and efficient software delivery. By integrating automated testing into CI/CD pipelines, teams can detect issues early, reduce manual effort, and accelerate delivery cycles.</p>"},{"location":"qa/tms_testrail_test_case_management/","title":"Test Case Management (TestRail)","text":"<p>Test Case Management is the process of documenting and maintaining test scenarios in a clear and structured way so they can be consistently reused throughout the project. In TestRail, this structure helps teams keep both manual and automated tests organized and accessible in a single place.</p> <p>Rather than being a static list of test steps, test cases act as living documentation of how the product is expected to behave.</p>"},{"location":"qa/tms_testrail_test_case_management/#what-we-aim-to-achieve","title":"What We Aim to Achieve","text":"<ul> <li>Maintain reliable and consistent test coverage</li> <li>Reduce the risk of missing critical scenarios during regression testing</li> <li>Keep testing practices aligned across the team</li> <li>Ensure test knowledge remains available even when team members change</li> </ul>"},{"location":"qa/tms_testrail_test_case_management/#test-case-management-in-practice","title":"Test Case Management in Practice","text":""},{"location":"qa/tms_testrail_test_case_management/#creating-test-cases","title":"Creating Test Cases","text":"<p>A typical test case in TestRail includes a short, descriptive title, preconditions, clear test steps and the expected test result. The priority or test type helps a team decide when and how often a test should be executed.</p>"},{"location":"qa/tms_testrail_test_case_management/#organizing-test-cases","title":"Organizing Test Cases","text":"<p>Test cases are usually grouped by features, modules, or user flows. This makes it easier to understand the scope of testing and quickly find relevant scenarios when preparing test runs or regression suites.</p>"},{"location":"qa/tms_testrail_test_case_management/#keeping-test-cases-maintainable","title":"Keeping Test Cases Maintainable","text":"<p>Well-written test cases are designed to be reusable. Common scenarios are maintained in one place and updated when product behavior changes, which helps avoid duplicated work and keeps regression suites up to date.</p>"},{"location":"qa/tms_testrail_test_case_management/#supporting-manual-and-automated-testing","title":"Supporting Manual and Automated Testing","text":"<p>TestRail allows teams to manage test cases that are executed manually as well as those covered by automation. Linking automated tests to existing test cases helps maintain traceability and provides a clear view of overall test coverage.</p>"},{"location":"qa/tms_testrail_test_case_management/#ensuring-traceability","title":"Ensuring Traceability","text":"<p>Test cases can be linked to requirements, user stories, or reported defects. This connection helps teams understand which parts of the product are validated and where potential gaps may exist.</p>"},{"location":"qa/tms_testrail_test_case_management/#why-its-important","title":"Why It\u2019s Important","text":"<p>A solid Test Case Management approach makes testing more predictable and easier to scale. It improves release confidence, supports smoother onboarding for new team members, and reduces dependency on individual knowledge. Over time, test cases become a reliable reference point for both product behavior and quality expectations.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/","title":"Test Runs &amp; Reporting (TestRail)","text":"<p>In TestRail, test runs are used to execute a defined group of test cases within a specific scope. They help teams monitor testing progress, capture execution results, and understand the overall quality of a feature or release.</p> <p>Instead of looking at test cases one by one, test runs provide a broader snapshot of where testing stands at a particular moment and whether the product is moving in the right direction.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/#how-test-runs-are-used","title":"How Test Runs Are Used","text":""},{"location":"qa/tms_testrail_test_runs_reporting/#creating-a-test-run","title":"Creating a Test Run","text":"<p>Test runs are usually created with a clear goal in mind, such as smoke testing, regression cycles, or release verification. The selected test cases depend on factors like recent changes, risk areas, and the current phase of development.</p> <p>Each run reflects the testing status for that defined scope and timeframe.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/#executing-tests-and-recording-results","title":"Executing Tests and Recording Results","text":"<p>As testing progresses, each test case in the run is updated with a result such as passed, failed, blocked, or retest. When a test fails, it is common to add supporting details like notes, screenshots, or links to related defect tickets.</p> <p>This additional context makes results easier to understand and helps teams act on issues more quickly.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/#reporting-in-testrail","title":"Reporting in TestRail","text":"<p>TestRail reporting turns raw test results into useful insights. Instead of manually checking individual executions, teams can rely on reports to see the bigger picture.</p> <p>Common reports include information such as:</p> <ul> <li>Pass and fail ratios</li> <li>Test execution progress</li> <li>Lists of failed or blocked tests</li> <li>Quality trends across different runs or releases</li> </ul> <p>These reports give teams a fast and clear overview of product quality.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/#supporting-release-decisions","title":"Supporting Release Decisions","text":"<p>Test run reports are often reviewed during release readiness discussions. They help teams answer important questions like:</p> <ul> <li>Has the planned testing been completed?</li> <li>Are there any critical issues still unresolved?</li> <li>How does the current release compare with previous ones?</li> </ul> <p>Having this data available in a structured format makes release decisions more objective and transparent.</p>"},{"location":"qa/tms_testrail_test_runs_reporting/#why-test-runs-reporting-are-important","title":"Why Test Runs &amp; Reporting Are Important","text":"<p>Using test runs and reports effectively brings visibility and structure to the testing process. Teams can track progress, spot risks early, and clearly communicate quality status to stakeholders. Over time, this approach leads to smoother releases and more consistent product quality.</p>"},{"location":"qa/types_of_tests/","title":"Types of Tests","text":"<p>Software testing is the process of evaluating software applications, systems, or components to ensure they meet specified requirements and function as expected. It is categorized into two main types: functional testing and non-functional testing.</p>"},{"location":"qa/types_of_tests/#1-functional-testing","title":"1. Functional Testing","text":"<p>Functional testing focuses on verifying that the software operates according to its intended functionality, based on specified requirements.</p>"},{"location":"qa/types_of_tests/#types-of-functional-testing","title":"Types of Functional Testing","text":"<p>Unit Testing Unit testing verifies the smallest testable parts of the software, such as functions or methods, in isolation. It is typically performed by developers during the coding phase and ensures that each unit of the software works as intended.</p> <p>Example: Testing the \"Add to Cart\" function to ensure the selected item is correctly added to the shopping cart.</p> <p>Technique: White-box testing.</p> <p>Integration Testing Integration testing examines how different modules or components of the software interact with each other. It ensures seamless communication between individual units.</p> <p>Example: In a banking application, verifying that the \"Deposit Funds\" function updates the balance correctly and the \"View Balance\" function reflects the updated value.</p> <p>System Testing System testing validates the complete and integrated system to ensure that it meets the defined requirements.</p> <p>Example: Testing an e-commerce website to ensure product searches, payment processing, and order tracking work together seamlessly.</p> <p>Sanity Testing It is performed after the project is built to ensure that the errors have been resolved and no new issues have been introduced due to code changes.</p> <p>Example: After fixing a payment error, testing only the payment process to ensure it works correctly.</p> <p>Smoke Testing Smoke testing ensures the basic functionalities of the software are operational. It acts as a preliminary check before moving into detailed testing.</p> <p>Example: Checking whether a website loads properly and the login feature works.</p> <p>Regression Testing Regression testing ensures that newly added features or bug fixes do not negatively impact the existing functionality.</p> <p>Example: After adding a new payment method, verifying that older payment options like credit cards still function correctly.</p> <p>Types:</p> <p>Unit Regression Testing (URT): Tests only the modified unit.</p> <p>Regional Regression Testing (RRT): Tests modules affected by the changes.</p> <p>Full Regression Testing (FRT): Tests the entire system.</p> <p>Exploratory Testing Exploratory testing is performed without predefined test cases or plans. The tester investigates the software to discover potential issues.</p> <p>Example: Clicking on various categories, attempting unusual login credentials, or initiating and canceling payments to observe system behavior.</p> <p>User Acceptance Testing (UAT) UAT validates whether the software meets the end-users' requirements and expectations. It is the final testing phase before the product is released.</p> <p>Example: A banking application is tested by real users to ensure smooth fund transfers and accurate balance displays.</p>"},{"location":"qa/types_of_tests/#2-non-functional-testing","title":"2. Non-Functional Testing","text":"<p>Non-functional testing evaluates the operational aspects of the software, such as performance, security, usability, and reliability. It focuses on \"how\" the software works rather than \"what\" it does.</p>"},{"location":"qa/types_of_tests/#types-of-non-functional-testing","title":"Types of Non-Functional Testing","text":"<p>Performance Testing This measures the system's speed, stability, and scalability under varying conditions.</p> <p>Example: Checking the response time of a website under normal user loads.</p> <p>Load Testing Load testing determines how the system performs under expected user traffic or load.</p> <p>Example: Simulating 5,000 concurrent users on an e-commerce website during peak hours.</p> <p>Stress Testing Stress testing evaluates the system's stability under extreme or unexpected conditions, such as high user loads or resource limitations.</p> <p>Example: Simulating 50,000 users during a Black Friday sale to observe if the system crashes.</p> <p>Security Testing Security testing identifies vulnerabilities and ensures the software is protected from unauthorized access or data breaches.</p> <p>Example: Testing a banking application to ensure sensitive data is encrypted and accessible only by authorized users.</p> <p>Usability Testing This ensures the software is user-friendly and provides a positive user experience.</p> <p>Example: Evaluating whether a mobile app's interface is intuitive and easy to navigate.</p> <p>Compatibility Testing Compatibility testing ensures the software performs consistently across various devices, browsers, operating systems, and networks.</p> <p>Example: Verifying that a website works seamlessly on Chrome, Safari, and Firefox.</p> <p>Recovery Testing Recovery testing examines the system's ability to recover from failures, such as power outages or hardware crashes.</p> <p>Example: Testing whether a database restores correctly after an unexpected shutdown.</p>"},{"location":"qa/ui_appium_android_ios_setup/","title":"Android &amp; iOS Setup (Overview)","text":""},{"location":"qa/ui_appium_android_ios_setup/#mobile-environment-setup","title":"Mobile Environment Setup","text":"<p>Environment setup is usually the most time-consuming part of mobile automation. A clean and stable configuration is critical for reliable tests.</p>"},{"location":"qa/ui_appium_android_ios_setup/#android-setup","title":"Android Setup","text":""},{"location":"qa/ui_appium_android_ios_setup/#required-tools","title":"Required Tools","text":"<ul> <li>Android Studio</li> <li>Android SDK</li> <li>Java JDK</li> <li><code>ANDROID_HOME</code> environment variable</li> <li>Emulator or real device with USB Debugging enabled</li> </ul>"},{"location":"qa/ui_appium_android_ios_setup/#basic-steps","title":"Basic Steps","text":"<ol> <li>Install Android Studio.</li> <li>Download:</li> <li>Platform Tools</li> <li>Build Tools</li> <li>Emulator images</li> <li>Set environment variables:</li> </ol> <pre><code>export ANDROID_HOME=/Users/username/Library/Android/sdk\nexport PATH=$PATH:$ANDROID_HOME/platform-tools\n</code></pre> <ol> <li>Install Appium driver:</li> </ol> <pre><code>appium driver install uiautomator2\n</code></pre>"},{"location":"qa/ui_appium_android_ios_setup/#ios-setup-macos-only","title":"iOS Setup (macOS only)","text":""},{"location":"qa/ui_appium_android_ios_setup/#required-tools_1","title":"Required Tools","text":"<ul> <li>macOS</li> <li>Xcode</li> <li>Xcode Command Line Tools</li> <li>iOS Simulator</li> <li>Provisioning profiles for real devices</li> </ul>"},{"location":"qa/ui_appium_android_ios_setup/#basic-steps_1","title":"Basic Steps","text":"<pre><code>xcode-select --install\nappium driver install xcuitest\n</code></pre> <p>Note: iOS setup is generally more complex due to signing and provisioning requirements.</p>"},{"location":"qa/ui_appium_best_practices/","title":"Mobile Test Best Practices","text":"<p>Locator strategy directly affects test stability and performance.</p> <ul> <li>Poor locator -&gt; Flaky tests</li> <li>Good locator -&gt; Stable tests</li> </ul>"},{"location":"qa/ui_appium_best_practices/#recommended-order","title":"Recommended Order","text":""},{"location":"qa/ui_appium_best_practices/#accessibility-id-best-practice","title":"Accessibility ID (Best Practice)","text":"<pre><code>driver.find_element(:accessibility_id, \"login_button\")\n</code></pre> <ul> <li>Fast</li> <li>Stable</li> <li>Cross-platform</li> </ul>"},{"location":"qa/ui_appium_best_practices/#id-resource-id","title":"ID / Resource ID","text":"<pre><code>driver.find_element(:id, \"com.app:id/loginBtn\")\n</code></pre> <ul> <li>Very reliable for Android</li> </ul>"},{"location":"qa/ui_appium_best_practices/#xpath-last-resort","title":"XPath (Last Resort)","text":"<pre><code>//android.widget.Button[@text='Login']\n</code></pre> <ul> <li>Slow</li> <li>Fragile</li> <li>Breaks easily after UI changes</li> </ul>"},{"location":"qa/ui_appium_best_practices/#best-practice","title":"Best Practice","text":"<ul> <li>Ask developers to add accessibility IDs</li> <li>Avoid XPath whenever possible</li> </ul> <p>This dramatically reduces flaky tests.</p>"},{"location":"qa/ui_appium_desired_capabilities/","title":"Desired Capabilities","text":"<p>Desired Capabilities are configuration parameters that tell Appium: \"Which device, which app, and how the test should run.\"</p> <p>They are sent before the test session starts.</p>"},{"location":"qa/ui_appium_desired_capabilities/#android-example","title":"Android Example","text":"<pre><code>{\n  \"platformName\": \"Android\",\n  \"deviceName\": \"Pixel_6\",\n  \"automationName\": \"UiAutomator2\",\n  \"app\": \"/apps/app.apk\"\n}\n</code></pre>"},{"location":"qa/ui_appium_desired_capabilities/#ios-example","title":"iOS Example","text":"<pre><code>{\n  \"platformName\": \"iOS\",\n  \"deviceName\": \"iPhone 15\",\n  \"automationName\": \"XCUITest\",\n  \"bundleId\": \"com.company.app\"\n}\n</code></pre>"},{"location":"qa/ui_appium_desired_capabilities/#common-capabilities","title":"Common Capabilities","text":"<ul> <li><code>platformName</code></li> <li><code>deviceName</code></li> <li><code>automationName</code></li> <li><code>app</code> / <code>bundleId</code></li> <li><code>noReset</code></li> <li><code>fullReset</code></li> <li><code>autoGrantPermissions</code></li> </ul> <p>Proper capability configuration equals more stable tests.</p>"},{"location":"qa/ui_appium_device_farm/","title":"Device Farm","text":"<p>A Device Farm allows you to run tests on multiple real devices simultaneously in the cloud.</p>"},{"location":"qa/ui_appium_device_farm/#why-is-it-important","title":"Why Is It Important?","text":"<p>Because:</p> <ul> <li>Users have different devices</li> <li>Android fragmentation is high</li> <li>Local device availability is limited</li> </ul>"},{"location":"qa/ui_appium_device_farm/#popular-solutions","title":"Popular Solutions","text":"<ul> <li>AWS Device Farm</li> <li>Sauce Labs</li> <li>BrowserStack</li> <li>Firebase Test Lab</li> </ul>"},{"location":"qa/ui_appium_device_farm/#advantages","title":"Advantages","text":"<ul> <li>Parallel execution</li> <li>Multiple OS versions</li> <li>Real devices</li> <li>CI/CD integration</li> </ul>"},{"location":"qa/ui_appium_mobile_locator_strategies/","title":"Mobile Locator Strategies","text":"<p>Locator strategy directly affects test stability and performance.</p> <ul> <li>Poor locator -&gt; Flaky tests</li> <li>Good locator -&gt; Stable tests</li> </ul>"},{"location":"qa/ui_appium_mobile_locator_strategies/#recommended-order","title":"Recommended Order","text":""},{"location":"qa/ui_appium_mobile_locator_strategies/#accessibility-id-best-practice","title":"Accessibility ID (Best Practice)","text":"<pre><code>driver.find_element(:accessibility_id, \"login_button\")\n</code></pre> <ul> <li>Fast</li> <li>Stable</li> <li>Cross-platform</li> </ul>"},{"location":"qa/ui_appium_mobile_locator_strategies/#id-resource-id","title":"ID / Resource ID","text":"<pre><code>driver.find_element(:id, \"com.app:id/loginBtn\")\n</code></pre> <ul> <li>Very reliable for Android</li> </ul>"},{"location":"qa/ui_appium_mobile_locator_strategies/#xpath-last-resort","title":"XPath (Last Resort)","text":"<pre><code>//android.widget.Button[@text='Login']\n</code></pre> <ul> <li>Slow</li> <li>Fragile</li> <li>Breaks easily after UI changes</li> </ul>"},{"location":"qa/ui_appium_mobile_locator_strategies/#best-practice","title":"Best Practice","text":"<ul> <li>Ask developers to add accessibility IDs</li> <li>Avoid XPath whenever possible</li> </ul> <p>This dramatically reduces flaky tests.</p>"},{"location":"qa/ui_appium_what_is/","title":"What Is Appium?","text":"<p>Appium is an open-source mobile test automation framework that enables testing of Android, iOS, and hybrid mobile applications using a single API.</p> <p>Its main goal is simple: automate real user interactions to ensure mobile application quality.</p>"},{"location":"qa/ui_appium_what_is/#why-appium","title":"Why Appium?","text":"<p>Appium provides several advantages:</p> <ul> <li>Cross-platform support (Android and iOS)</li> <li>Supports Native, Hybrid, and Mobile Web apps</li> <li>Multiple programming languages:</li> <li>Java</li> <li>JavaScript</li> <li>Python</li> <li>Ruby</li> <li>C#</li> </ul> <p>This flexibility allows teams to use their existing tech stack without learning a new language.</p>"},{"location":"qa/ui_appium_what_is/#how-appium-works","title":"How Appium Works","text":"<p>Appium follows the WebDriver protocol.</p> <p>Flow:</p> <p><code>Test Code -&gt; Appium Server -&gt; Native Drivers -&gt; Mobile Device</code></p> <p>Under the hood:</p> <ul> <li>Android -&gt; UiAutomator2</li> <li>iOS -&gt; XCUITest</li> </ul> <p>This architecture enables: \"Write once, run on multiple devices.\"</p> <p>You can reuse most of your test logic across platforms.</p>"},{"location":"qa/ui_playwright_agents_mcp/","title":"Playwright Agents &amp; Playwright MCP","text":"<p>One of the most exciting advancements in modern testing is AI-powered automation.</p>"},{"location":"qa/ui_playwright_agents_mcp/#what-is-mcp-model-context-protocol","title":"What Is MCP (Model Context Protocol)?","text":"<p>MCP allows AI models to interact directly with browsers and testing tools.</p> <p>With Playwright + MCP:</p> <ul> <li>Tests can be written using natural language</li> <li>UI elements can be discovered automatically</li> <li>Exploratory testing becomes automated</li> <li>Self-healing test structures are possible</li> <li>Intelligent failure analysis can be performed</li> </ul>"},{"location":"qa/ui_playwright_agents_mcp/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>AI test agent</li> <li>Test generation</li> <li>Exploratory testing</li> <li>Intelligent debugging</li> <li>Automated regression discovery</li> </ul> <p>This approach represents the future of test automation.</p>"},{"location":"qa/ui_playwright_cicd_integration/","title":"CI/CD Integration (Overview)","text":"<p>Playwright integrates seamlessly with modern CI/CD pipelines.</p>"},{"location":"qa/ui_playwright_cicd_integration/#common-ci-tools","title":"Common CI Tools","text":"<ul> <li>GitHub Actions</li> <li>GitLab CI</li> <li>Jenkins</li> <li>Azure DevOps</li> </ul>"},{"location":"qa/ui_playwright_cicd_integration/#sample-github-actions-step","title":"Sample GitHub Actions Step","text":"<pre><code>- name: Run Playwright Tests\n  run: npx playwright test\n</code></pre>"},{"location":"qa/ui_playwright_cicd_integration/#ci-advantages","title":"CI Advantages","text":"<ul> <li>Headless execution</li> <li>Parallel test runs</li> <li>Automatic reporting</li> <li>Failure artifacts (video, screenshots)</li> <li>Fast feedback loop</li> </ul>"},{"location":"qa/ui_playwright_installation/","title":"Installation (Brief)","text":"<p>One of Playwright\u2019s biggest advantages is its extremely simple installation process.</p>"},{"location":"qa/ui_playwright_installation/#1-installation","title":"1. Installation","text":"<pre><code>npm init playwright@latest\n</code></pre> <p>This command:</p> <ul> <li>Installs Playwright</li> <li>Downloads Chromium, Firefox, and WebKit</li> <li>Creates example test files</li> <li>Generates <code>playwright.config.ts</code></li> <li>Sets up the project structure automatically</li> </ul>"},{"location":"qa/ui_playwright_installation/#2-running-tests","title":"2. Running Tests","text":"<pre><code>npx playwright test\n</code></pre>"},{"location":"qa/ui_playwright_installation/#3-ui-mode-highly-recommended","title":"3. UI Mode (Highly Recommended)","text":"<pre><code>npx playwright test --ui\n</code></pre> <p>With UI Mode, you can:</p> <ul> <li>Visually observe test execution</li> <li>Debug step by step</li> <li>Inspect locators in real time</li> <li>Analyze failures more easily</li> </ul>"},{"location":"qa/ui_playwright_locator_strategies/","title":"Locator Strategies","text":"<p>Playwright offers one of the most powerful locator engines in the testing ecosystem.</p>"},{"location":"qa/ui_playwright_locator_strategies/#recommended-locator-priority","title":"Recommended Locator Priority","text":""},{"location":"qa/ui_playwright_locator_strategies/#getbyrole","title":"<code>getByRole()</code>","text":"<pre><code>page.getByRole('button', { name: 'Login' });\n</code></pre>"},{"location":"qa/ui_playwright_locator_strategies/#getbytestid","title":"<code>getByTestId()</code>","text":"<pre><code>page.getByTestId('login-btn');\n</code></pre>"},{"location":"qa/ui_playwright_locator_strategies/#getbytext","title":"<code>getByText()</code>","text":"<pre><code>page.getByText('Sign In');\n</code></pre>"},{"location":"qa/ui_playwright_locator_strategies/#css-xpath-last-resort","title":"CSS / XPath (last resort)","text":""},{"location":"qa/ui_playwright_locator_strategies/#why-xpath-is-discouraged","title":"Why XPath Is Discouraged","text":"<ul> <li>Easily breaks with UI changes</li> <li>Hard to maintain</li> <li>Low readability</li> <li>Causes flaky tests</li> </ul>"},{"location":"qa/ui_playwright_locator_strategies/#playwright-locators-are","title":"Playwright Locators Are","text":"<ul> <li>More stable</li> <li>Easier to read</li> <li>Auto-wait enabled</li> </ul>"},{"location":"qa/ui_playwright_pom/","title":"Page Object Model (POM)","text":"<p>Page Object Model is a fundamental design pattern in test automation.</p>"},{"location":"qa/ui_playwright_pom/#purpose","title":"Purpose","text":"<ul> <li>Reduce code duplication</li> <li>Improve readability</li> <li>Simplify maintenance</li> <li>Centralize UI changes</li> </ul>"},{"location":"qa/ui_playwright_pom/#example-page-class","title":"Example Page Class","text":"<pre><code>export class LoginPage {\n  constructor(page) {\n    this.page = page;\n    this.email = page.locator('#email');\n    this.password = page.locator('#password');\n    this.loginBtn = page.locator('#login');\n  }\n\n  async login(email, password) {\n    await this.email.fill(email);\n    await this.password.fill(password);\n    await this.loginBtn.click();\n  }\n}\n</code></pre>"},{"location":"qa/ui_playwright_pom/#test-file","title":"Test File","text":"<pre><code>test('Successful login', async ({ page }) =&gt; {\n  const loginPage = new LoginPage(page);\n  await loginPage.login('test@mail.com', '123456');\n});\n</code></pre>"},{"location":"qa/ui_playwright_project_structure_best_practices/","title":"Project Structure &amp; Best Practices","text":"<p>A clean and scalable project structure is essential for long-term maintainability.</p>"},{"location":"qa/ui_playwright_project_structure_best_practices/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<pre><code>\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 login.spec.ts\n\u2502   \u251c\u2500\u2500 register.spec.ts\n\u251c\u2500\u2500 pages/\n\u2502   \u251c\u2500\u2500 login.page.ts\n\u2502   \u251c\u2500\u2500 dashboard.page.ts\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 test-data.ts\n\u251c\u2500\u2500 fixtures/\n\u251c\u2500\u2500 playwright.config.ts\n</code></pre>"},{"location":"qa/ui_playwright_project_structure_best_practices/#best-practices","title":"Best Practices","text":""},{"location":"qa/ui_playwright_project_structure_best_practices/#1-use-page-object-model-pom","title":"1. Use Page Object Model (POM)","text":"<p>Keeps UI logic separated from test logic.</p>"},{"location":"qa/ui_playwright_project_structure_best_practices/#2-keep-tests-small-and-independent","title":"2. Keep tests small and independent","text":"<p>Each test should validate a single behavior.</p>"},{"location":"qa/ui_playwright_project_structure_best_practices/#3-avoid-hardcoded-data","title":"3. Avoid hardcoded data","text":"<p>Use configuration files or environment variables.</p>"},{"location":"qa/ui_playwright_project_structure_best_practices/#4-use-parallel-execution","title":"4. Use parallel execution","text":"<p>Playwright is optimized for parallel test runs.</p>"},{"location":"qa/ui_playwright_project_structure_best_practices/#5-enable-screenshots-and-videos","title":"5. Enable screenshots and videos","text":"<p>Helps significantly with debugging failures.</p>"},{"location":"qa/ui_playwright_test_scenarios_examples/","title":"Test Scenario Examples","text":""},{"location":"qa/ui_playwright_test_scenarios_examples/#positive-scenario","title":"Positive Scenario","text":"<pre><code>test('User logs in successfully', async ({ page }) =&gt; {\n  await page.goto('/login');\n  await page.fill('#email', 'test@mail.com');\n  await page.fill('#password', '123456');\n  await page.click('#login');\n\n  await expect(page).toHaveURL('/dashboard');\n});\n</code></pre>"},{"location":"qa/ui_playwright_test_scenarios_examples/#negative-scenarios","title":"Negative Scenarios","text":"<ul> <li>Invalid password</li> <li>Empty fields</li> <li>Incorrect email format</li> <li>Timeout handling</li> <li>Unauthorized access</li> </ul>"},{"location":"qa/ui_playwright_test_scenarios_examples/#playwright-handles-these-scenarios-with","title":"Playwright Handles These Scenarios With","text":"<ul> <li>Stability</li> <li>Speed</li> <li>Detailed reporting</li> <li>Screenshots and videos</li> </ul>"},{"location":"qa/ui_playwright_what_is/","title":"What is Playwright?","text":""},{"location":"qa/ui_playwright_what_is/#a-comprehensive-guide-to-modern-web-test-automation","title":"A Comprehensive Guide to Modern Web Test Automation","text":"<p>In today\u2019s software world, web applications have become increasingly complex due to rapid development cycles, microservice architectures, and continuous delivery pipelines. As a result, maintaining software quality has become more challenging than ever. This is where test automation plays a critical role.</p> <p>Playwright is a modern end-to-end (E2E) test automation framework developed by Microsoft. It is designed to simulate real user behavior as closely as possible and provides a powerful, reliable, and scalable solution for testing modern web applications.</p> <p>What makes Playwright stand out is its ability to handle modern UI complexity while remaining fast, stable, and developer-friendly.</p>"},{"location":"qa/ui_playwright_what_is/#why-was-playwright-created","title":"Why Was Playwright Created?","text":"<p>Traditional automation tools\u2014especially Selenium-based solutions\u2014started to show limitations over time:</p> <ul> <li>Flaky and unstable tests</li> <li>Complex waiting mechanisms</li> <li>Browser compatibility issues</li> <li>Slow execution times</li> <li>Difficult CI/CD integration</li> <li>High maintenance cost</li> </ul> <p>Playwright was built to solve these problems from the ground up.</p>"},{"location":"qa/ui_playwright_what_is/#key-advantages-of-playwright","title":"Key Advantages of Playwright","text":"<ul> <li>Automatic waiting mechanism</li> <li>Single API for all browsers</li> <li>Highly stable locator system</li> <li>Native parallel execution</li> <li>Powerful debugging tools</li> <li>Full compatibility with modern frontend frameworks</li> </ul>"},{"location":"qa/ui_selenium_framework_junit_testng/","title":"Framework Approach (JUnit / TestNG)","text":"<p>Selenium alone is not a testing framework. It must be combined with a testing framework such as JUnit or TestNG.</p>"},{"location":"qa/ui_selenium_framework_junit_testng/#junit","title":"JUnit","text":"<p>JUnit is one of the oldest and most widely used testing frameworks in Java.</p>"},{"location":"qa/ui_selenium_framework_junit_testng/#key-features","title":"Key Features","text":"<ul> <li>Simple structure</li> <li>Annotation-based</li> <li>Best suited for unit testing</li> <li>Lightweight and easy to learn</li> </ul>"},{"location":"qa/ui_selenium_framework_junit_testng/#example","title":"Example","text":"<pre><code>@Test\npublic void loginTest() {\n    driver.get(\"https://example.com\");\n    Assert.assertTrue(driver.getTitle().contains(\"Login\"));\n}\n</code></pre>"},{"location":"qa/ui_selenium_framework_junit_testng/#pros","title":"Pros","text":"<ul> <li>Easy to learn</li> <li>Good for small projects</li> </ul>"},{"location":"qa/ui_selenium_framework_junit_testng/#cons","title":"Cons","text":"<ul> <li>Limited parallel execution</li> <li>Basic reporting capabilities</li> </ul>"},{"location":"qa/ui_selenium_framework_junit_testng/#testng","title":"TestNG","text":"<p>TestNG is the most popular testing framework used with Selenium today.</p>"},{"location":"qa/ui_selenium_framework_junit_testng/#key-features_1","title":"Key Features","text":"<ul> <li>Parallel test execution</li> <li>Test grouping</li> <li>Test prioritization</li> <li>XML-based configuration</li> <li>Advanced reporting</li> </ul>"},{"location":"qa/ui_selenium_framework_junit_testng/#example_1","title":"Example","text":"<pre><code>@Test(priority = 1)\npublic void loginTest() {\n    // test steps\n}\n</code></pre>"},{"location":"qa/ui_selenium_framework_junit_testng/#advantages","title":"Advantages","text":"<ul> <li>Ideal for large projects</li> <li>CI/CD friendly</li> <li>Highly configurable</li> </ul>"},{"location":"qa/ui_selenium_vs_playwright_when_to_use/","title":"When to Use Selenium vs Playwright","text":"<p>This is one of the most frequently asked questions today.</p>"},{"location":"qa/ui_selenium_vs_playwright_when_to_use/#when-to-choose-selenium","title":"When to Choose Selenium?","text":"<ul> <li>When working with legacy systems</li> <li>In Java-heavy environments</li> <li>If an existing Selenium framework already exists</li> <li>When Selenium Grid is actively used</li> <li>In long-running enterprise projects</li> </ul>"},{"location":"qa/ui_selenium_vs_playwright_when_to_use/#when-to-choose-playwright","title":"When to Choose Playwright?","text":"<ul> <li>For new projects</li> <li>When fast test execution is required</li> <li>When CI/CD performance is critical</li> <li>For modern frontend applications</li> <li>When AI-powered testing is a goal</li> </ul>"},{"location":"qa/ui_selenium_vs_playwright_when_to_use/#selenium-vs-playwright-quick-comparison","title":"Selenium vs Playwright - Quick Comparison","text":"Feature Selenium Playwright Setup Complex Very easy Speed Medium Very fast Waiting Mechanism Manual Automatic Parallel Execution Needs setup Built-in AI Compatibility Limited Strong Learning Curve Steeper Easier"},{"location":"qa/ui_selenium_webdriver_grid/","title":"WebDriver &amp; Selenium Grid (Brief)","text":"<p>Selenium is composed of several components, the most important ones being WebDriver and Selenium Grid.</p>"},{"location":"qa/ui_selenium_webdriver_grid/#selenium-webdriver","title":"Selenium WebDriver","text":"<p>WebDriver is the core component of Selenium.</p> <ul> <li>It directly controls the browser</li> <li>Simulates real user behavior</li> <li>Interacts with the DOM</li> </ul>"},{"location":"qa/ui_selenium_webdriver_grid/#supported-browsers","title":"Supported Browsers","text":"<ul> <li>Chrome</li> <li>Firefox</li> <li>Edge</li> <li>Safari</li> </ul>"},{"location":"qa/ui_selenium_webdriver_grid/#example-java","title":"Example (Java)","text":"<pre><code>WebDriver driver = new ChromeDriver();\ndriver.get(\"https://example.com\");\ndriver.findElement(By.id(\"login\")).click();\n</code></pre>"},{"location":"qa/ui_selenium_webdriver_grid/#advantages","title":"Advantages","text":"<ul> <li>Flexible</li> <li>Powerful</li> <li>Supports many programming languages</li> </ul>"},{"location":"qa/ui_selenium_webdriver_grid/#limitations","title":"Limitations","text":"<ul> <li>Requires manual waits</li> <li>Setup can be complex</li> <li>Parallel execution requires extra configuration</li> </ul>"},{"location":"qa/ui_selenium_webdriver_grid/#selenium-grid","title":"Selenium Grid","text":"<p>Selenium Grid allows tests to be executed in parallel across multiple browsers and machines.</p>"},{"location":"qa/ui_selenium_webdriver_grid/#what-does-grid-provide","title":"What Does Grid Provide?","text":"<ul> <li>Cross-browser testing</li> <li>Cross-platform execution</li> <li>Faster test execution through parallelism</li> </ul>"},{"location":"qa/ui_selenium_webdriver_grid/#typical-usage-scenarios","title":"Typical Usage Scenarios","text":"<ul> <li>Chrome + Firefox + Edge at the same time</li> <li>Windows + macOS + Linux environments</li> <li>Large regression test suites</li> </ul> <p>Selenium Grid is commonly used with:</p> <ul> <li>Jenkins</li> <li>Docker</li> <li>Kubernetes</li> </ul>"},{"location":"qa/ui_selenium_what_is/","title":"What Is Selenium?","text":""},{"location":"qa/ui_selenium_what_is/#the-foundation-of-web-test-automation","title":"The Foundation of Web Test Automation","text":"<p>Selenium is one of the most widely used open-source test automation tools for web applications. Since its introduction in 2004, Selenium has become an industry standard, especially in large-scale and enterprise-level projects.</p> <p>The main purpose of Selenium is to automate user interactions in a browser just as a real user would. By doing so, it helps teams:</p> <ul> <li>Reduce manual testing effort</li> <li>Automate regression tests</li> <li>Detect bugs earlier in the development cycle</li> <li>Improve overall software quality</li> </ul> <p>One of Selenium\u2019s greatest strengths is that it is not a single tool, but rather an entire ecosystem.</p>"},{"location":"qa/ui_selenium_what_is/#why-is-selenium-so-widely-used","title":"Why Is Selenium So Widely Used?","text":"<p>Selenium\u2019s popularity comes from several key advantages:</p> <ul> <li>Open-source and free</li> <li>Backed by a large global community</li> <li>Supports multiple programming languages</li> <li>Works across different browsers</li> <li>Proven reliability in enterprise projects</li> <li>Highly customizable</li> </ul> <p>However, Selenium truly shines when it is used with the right architecture and best practices.</p>"}]}