{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Hepapi Knowledge Hub","text":""},{"location":"#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This repository was created with the singular goal of fostering collaboration, knowledge exchange, and continuous learning among our team members at Hepapi.</p> <p>We recognize that knowledge is power, and in our constantly evolving field, it's crucial to keep up with the latest technologies, methodologies, and best practices. </p> <p>This repository is a live document, and we encourage everyone to contribute. If you have something valuable to share, don't hesitate to make a contribution. Remember, what may be obvious to you could be new to someone else.</p> <p> Follow us on Linkedin   hepapi.com </p>"},{"location":"#help-us-improve","title":"Help us improve","text":"<p>We are always looking for ways to improve our documentation. If you have any suggestions, please feel free to open an issue or a pull request.</p> <p>Follow the docs on: How to contribute</p>"},{"location":"#compendium","title":"Compendium","text":"<p>nexus</p> <ul> <li>RKE2 Registry Configuration</li> <li>Copy Nexus Credentials into Kubernetes</li> <li>Docker Proxy Repository</li> <li>Nexus Installation - Docker Private Registry</li> <li>Principle of least privilege</li> <li>Docker Hosted Repository</li> </ul> <p>k8s-storage</p> <ul> <li>Ceph Installation Guide</li> <li>NFS Setup Requirements</li> <li>k8s-storage/longhorn.md</li> </ul> <p>sealed-secrets</p> <ul> <li>Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets</li> </ul> <p>sonarqube</p> <ul> <li>How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide</li> </ul> <p>monitoring</p> <ul> <li>\ud83d\udd10 Monitoring TLS Certificate Expiration in Kubernetes with Alerting</li> </ul> <p>azure</p> <ul> <li>Azure Self-Hosted Agent Installation</li> </ul> <p>jenkins</p> <ul> <li>Jenkins Install </li> <li>Jenkins Shared Library </li> <li>Jenkins</li> </ul> <p>terragrunt</p> <ul> <li>What is Terragrunt? </li> </ul> <p>vagrant</p> <ul> <li>Vagrant</li> </ul> <p>falcon-logscale</p> <ul> <li>Falcon LogScale Agent(Log Collector) Setup </li> <li>falcon-logscale/javainstallation.md</li> <li>Installation</li> <li>Apt package update ####</li> <li>Installation</li> <li>Installation</li> <li>Falcon LogScale Setup With Docker </li> <li>Humio Single Node Installation Guide </li> <li>Kafka Installation (kafka.sh)</li> <li>Apt package update ####</li> </ul> <p>service-mesh</p> <ul> <li>What is Istio?</li> </ul> <p>logging</p> <ul> <li>ELK Stack with FileBeat</li> <li>Elasticsearch Index Lifecycle Management</li> <li>Install Loki,Promtail,Grafana</li> <li>elastalert2</li> <li>Elasticsearch-Exporter</li> <li>EFK Stack (Elasticsearch, Fluentbit, Kibana) via Minikube</li> </ul> <p>docker</p> <ul> <li>Docker Management with Portainer</li> </ul> <p>rancher</p> <ul> <li>Rancher Installation</li> </ul> <p>devsecops</p> <ul> <li>External Secret Operator</li> <li>DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy</li> <li>Deep Dive into Security Monitoring in Kubernetes Environments: An Introduction to Falco </li> </ul> <p>sumo</p> <ul> <li>Local Configuration File Management</li> <li>Install a Sumo Logic Collector on Windows</li> <li>Install a Sumo Logic Collector on Linux</li> </ul> <p>postgres</p> <ul> <li>Backup</li> <li>Postgres Configuration</li> <li>psql CLI</li> <li>PoC</li> </ul> <p>aws</p> <ul> <li>AWS CLI</li> <li>AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?</li> </ul> <p>k8s-engine</p> <ul> <li>RKE2 Setup </li> <li>RKE2 Cluster Installation With Ansible</li> <li>K3S Setup </li> <li>Deploy RKE2 Highly Available Cluster</li> <li>System Upgrade Controller</li> <li>Restoring RKE2 Clusters</li> </ul> <p>git</p> <ul> <li>Version Control System (VCS)</li> <li>Downloading Git</li> <li>git/Commands.md</li> </ul> <p>ansible</p> <ul> <li>Install Ansible with pipx</li> <li>What is Ansible ?</li> <li>Ansible Roles</li> <li>Install Ansible with pipx</li> <li>Inventory and Variables</li> <li>Ansible Configuration File</li> <li>Ansible Playbooks</li> </ul> <p>sre</p> <ul> <li>k8sgpt</li> </ul> <p>gitlab</p> <ul> <li>GitLab Runner Installation Guide  </li> <li>GitLab Self-Hosted on Kubernetes - Installation Guide</li> </ul> <p>linux</p> <ul> <li>Netstat &amp; SS Command</li> <li>nohup and &amp; </li> <li>NSLOOKUP</li> <li>cht.sh Command Tool</li> <li>jobs, bg, and fg</li> <li><code>script</code> command</li> <li>SCP Command</li> <li>cat</li> <li>NMap Command</li> <li>Linux Tips</li> <li>Linux Tooling</li> </ul> <p>kubernetes</p> <ul> <li>KEDA (Kubernetes Event-driven Autoscaling) WITH CRON</li> <li>KEDA (Kubernetes Event-driven Autoscaling) </li> <li>Velero Cheat Sheet</li> <li>Simplify Cluster Backups with Velero</li> </ul>"},{"location":"repo-credit/","title":"Credits","text":"File Contributors .DS_Store deniz-icin,ersinsari13,oaltinoluk,turan.topcuoglu,turantopcuoglu .github/workflows/auto-index-generator.sh Oguzhan Yilmaz .github/workflows/deploy-mkdocs-website.yml Oguzhan Yilmaz,ersinsari13 .github/workflows/git-contributors-per-file.sh ersinsari13 .gitignore Oguzhan Yilmaz .idea/.gitignore turan.topcuoglu .idea/knowledge-hub.iml turan.topcuoglu .idea/misc.xml turan.topcuoglu .idea/modules.xml turan.topcuoglu .idea/vcs.xml turan.topcuoglu LICENSE Oguzhan Yilmaz README.md Oguzhan Yilmaz .DS_Store deniz-icin,ersinsari13,turan.topcuoglu devops/.DS_Store deniz-icin,ersinsari13 devops/ansible/ansible-installations.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible-roles.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible.md Oguzhan Yilmaz,ersinsari13 devops/ansible/config-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/inventory-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/playbook.md Oguzhan Yilmaz,ersinsari13 devops/ansible/what-is-ansible.md Oguzhan Yilmaz,ersinsari13 devops/aws/cli.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/aws/images/SSO-Architecture.png gokhanwell devops/aws/images/SSO-Attach-Account.png gokhanwell devops/aws/images/SSO-Enable.png gokhanwell devops/aws/images/SSO-Important.png gokhanwell devops/aws/images/SSO-Linked.png gokhanwell devops/aws/images/SSO-MFA.png gokhanwell devops/aws/images/SSO-Mail-Verify.png gokhanwell devops/aws/images/SSO-Permission-Set.png gokhanwell devops/aws/sso.md gokhanwell devops/azure/agent-installation.md Oguzhan Yilmaz,Utku Toraman devops/devsecops/.DS_Store deniz-icin,ersinsari13 devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy.md ersinsari13 devops/devsecops/What is Falco.md Necip Ulusoy devops/devsecops/external-secret-operator.md ersinsari13 devops/devsecops/image-eso/external-secret-1.png ersinsari13 devops/devsecops/image-eso/external-secret-operator-resources.png ersinsari13 devops/devsecops/image-eso/external-secret-operator.png ersinsari13 devops/devsecops/image-eso/iam-1.png ersinsari13 devops/devsecops/image-eso/iam-10.png ersinsari13 devops/devsecops/image-eso/iam-11.png ersinsari13 devops/devsecops/image-eso/iam-2.png ersinsari13 devops/devsecops/image-eso/iam-3.png ersinsari13 devops/devsecops/image-eso/iam-4.png ersinsari13 devops/devsecops/image-eso/iam-5.png ersinsari13 devops/devsecops/image-eso/iam-6.png ersinsari13 devops/devsecops/image-eso/iam-7.png ersinsari13 devops/devsecops/image-eso/iam-8.png ersinsari13 devops/devsecops/image-eso/iam-9.png ersinsari13 devops/devsecops/image-eso/iam.png ersinsari13 devops/devsecops/image-eso/kube-secret-1.png ersinsari13 devops/devsecops/image-eso/pod-shell.png ersinsari13 devops/devsecops/image-eso/secret-manager-1.png ersinsari13 devops/devsecops/image-eso/secret-manager-2.png ersinsari13 devops/devsecops/image-eso/secret-manager-3.png ersinsari13 devops/devsecops/image-eso/secret-manager-4.png ersinsari13 devops/devsecops/image-eso/secret-manager-5.png ersinsari13 devops/devsecops/image-eso/secret-manager-6.png ersinsari13 devops/devsecops/image-eso/secret-store-1.png ersinsari13 devops/devsecops/image-eso/secret-store.png ersinsari13 devops/devsecops/image/check-1.png ersinsari13 devops/devsecops/image/check-2.png ersinsari13 devops/devsecops/image/check-3.png ersinsari13 devops/devsecops/image/conftest-1.png ersinsari13 devops/devsecops/image/conftest-2.png ersinsari13 devops/devsecops/image/gate-3.png ersinsari13 devops/devsecops/image/gate-4.png ersinsari13 devops/devsecops/image/gates-1.png ersinsari13 devops/devsecops/image/gates-2.png ersinsari13 devops/devsecops/image/jdk.png ersinsari13 devops/devsecops/image/jenkins-passwd.png ersinsari13 devops/devsecops/image/jenkins-plug.png ersinsari13 devops/devsecops/image/jenkins-plugin.png ersinsari13 devops/devsecops/image/jenkins-user.png ersinsari13 devops/devsecops/image/maven-tool.png ersinsari13 devops/devsecops/image/pipe-1.png ersinsari13 devops/devsecops/image/pipeline-result.png ersinsari13 devops/devsecops/image/pipeline-script-2.png ersinsari13 devops/devsecops/image/pipeline-script.png ersinsari13 devops/devsecops/image/qality-1.png ersinsari13 devops/devsecops/image/qality-3.png ersinsari13 devops/devsecops/image/quality-2.png ersinsari13 devops/devsecops/image/sonar-dash.png ersinsari13 devops/devsecops/image/sonar-login.png ersinsari13 devops/devsecops/image/sonar-server.png ersinsari13 devops/devsecops/image/sonar-token-1.png ersinsari13 devops/devsecops/image/sonar-token-2.png ersinsari13 devops/devsecops/image/sonar-token-3.png ersinsari13 devops/devsecops/image/sonarqube-1.png ersinsari13 devops/devsecops/image/sonarqube-2.png ersinsari13 devops/devsecops/image/token-jenkins.png ersinsari13 devops/devsecops/image/trivy-1.png ersinsari13 devops/devsecops/image/trivy-2.png ersinsari13 devops/devsecops/image/trivy-3.png ersinsari13 devops/devsecops/image/webhook.png ersinsari13 devops/devsecops/images/image-1.png Necip Ulusoy devops/devsecops/images/image.png Necip Ulusoy devops/docker/portainer/README.md MehmetG171 devops/docker/portainer/docker-compose.yml MehmetG171 devops/docker/portainer/pics/dashboard.png MehmetG171 devops/docker/portainer/pics/env.png MehmetG171 devops/docker/portainer/pics/inspect.png MehmetG171 devops/docker/portainer/pics/login.png MehmetG171 devops/docker/portainer/pics/logs.png MehmetG171 devops/docker/portainer/pics/shell.png MehmetG171 devops/docker/portainer/pics/stats.png MehmetG171 devops/docker/portainer/pics/templates.png MehmetG171 devops/docker/portainer/user-data.sh MehmetG171 devops/falcon-logscale/agent-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/falcon-logscale-installation-docker.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humio.md FIRST_NAME LAST_NAME devops/falcon-logscale/humioinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humiosetup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/javainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/kafka.md FIRST_NAME LAST_NAME devops/falcon-logscale/kafkainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/readme.md FIRST_NAME LAST_NAME devops/falcon-logscale/zookeeperinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/git/Commands.md Oguzhan Yilmaz,Onur Ozcelik devops/git/Description.md Oguzhan Yilmaz,Onur Ozcelik devops/git/installation.md Oguzhan Yilmaz,Onur Ozcelik devops/gitlab/gitlab-runner-installation.md BoraKostem devops/gitlab/gitlab-self-hosted-installation.md BoraKostem devops/gitlab/images/image.png BoraKostem devops/gitlab/images/image1.png BoraKostem devops/gitlab/images/image2.png BoraKostem devops/gitlab/images/image3.png BoraKostem devops/index.md Oguzhan Yilmaz devops/jenkins/README.md Oguzhan Yilmaz devops/jenkins/jenkins-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/jenkins/shared-library.md Oguzhan Yilmaz,ersinsari13 devops/k8s-engine/k3s-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-highly-available-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-installation-ansible.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-engine/rke2-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-storage/images/ceph-status.png oaltinoluk devops/k8s-storage/images/rook-ceph-general.png oaltinoluk devops/k8s-storage/longhorn.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-storage/nfs-install.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-storage/rook-ceph.md oaltinoluk devops/kubernetes/Velero/Cheat-Sheet.md MehmetG171 devops/kubernetes/Velero/README.md MehmetG171 devops/kubernetes/Velero/credentials.txt MehmetG171 devops/kubernetes/Velero/pics/bucket-structure.png MehmetG171 devops/kubernetes/Velero/pics/s3-backup.png MehmetG171 devops/kubernetes/Velero/pics/s3-restore.png MehmetG171 devops/kubernetes/Velero/pics/velero-backup.png MehmetG171 devops/kubernetes/Velero/pics/velero-schedule.png MehmetG171 devops/kubernetes/Velero/pics/velero-status.png MehmetG171 devops/kubernetes/eks/pod-security-group.yaml Oguzhan Yilmaz,ersinsari13 devops/kubernetes/keda/keda-with-cron.md BERAT UYANIK,Berat Uyan\u0131k devops/kubernetes/keda/keda.md Oguzhan Yilmaz,ersinsari13 devops/linux/shell/ampersand-nohup.md Oguzhan Yilmaz,can devops/linux/shell/cat.md Oguzhan Yilmaz devops/linux/shell/chtsh.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/jobs-bg-fg.md Oguzhan Yilmaz,can devops/linux/shell/netstat.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nmap.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nslookup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/scp.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/script.md Oguzhan Yilmaz devops/linux/tips.md Oguzhan Yilmaz devops/linux/tooling.md Oguzhan Yilmaz devops/logging/.DS_Store deniz-icin,ersinsari13 devops/logging/ELK-stack-with-FileBeat.md ersinsari13 devops/logging/Index-Lifecycle-Management.md ersinsari13,ozihan devops/logging/efk-image/efk-0.png ersinsari13 devops/logging/efk-image/efk-1.png ersinsari13 devops/logging/efk.md ersinsari13 devops/logging/elastalert2.md oaltinoluk devops/logging/elasticsearch-exporter.md ersinsari13 devops/logging/image/image-1.png ersinsari13 devops/logging/image/image-10.png ersinsari13 devops/logging/image/image-11.png ersinsari13 devops/logging/image/image-12.png ersinsari13 devops/logging/image/image-13.png ersinsari13 devops/logging/image/image-2.png ersinsari13 devops/logging/image/image-3.png ersinsari13 devops/logging/image/image-4.png ersinsari13 devops/logging/image/image-5.png ersinsari13 devops/logging/image/image-6.png ersinsari13 devops/logging/image/image-7.png ersinsari13 devops/logging/image/image-8.png ersinsari13 devops/logging/image/image-9.png ersinsari13 devops/logging/images/1.png ersinsari13,ozihan devops/logging/images/2.png ersinsari13,ozihan devops/logging/images/3.png ersinsari13,ozihan devops/logging/images/4.png ersinsari13,ozihan devops/logging/images/5.png ersinsari13,ozihan devops/logging/images/6.png ersinsari13,ozihan devops/logging/images/7.png ersinsari13,ozihan devops/logging/images/dash-1.png ersinsari13 devops/logging/images/dash-2.png ersinsari13 devops/logging/images/dash-3.png ersinsari13 devops/logging/images/dash-4.png ersinsari13 devops/logging/images/dash-5.png ersinsari13 devops/logging/images/dash-6.png ersinsari13 devops/logging/images/elastalert1.png oaltinoluk devops/logging/loki.md ersinsari13 devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring.md oaltinoluk devops/nexus/docker-hosted-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/docker-proxy-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-installation.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-user-and-roles.md Oguzhan Yilmaz,deniz-icin devops/nexus/pull-to-kubernetes.md Oguzhan Yilmaz,deniz-icin devops/nexus/registry-configuration.md Oguzhan Yilmaz,deniz-icin devops/postgres/backup-restore.md Oguzhan Yilmaz devops/postgres/configuration.md Oguzhan Yilmaz devops/postgres/poc-backup-restore.md Oguzhan Yilmaz devops/postgres/psql.md Oguzhan Yilmaz devops/rancher/rancher-installation.md Oguzhan Yilmaz,Utku Toraman,deniz-icin devops/sealed-secrets/sealed-secrets.md Oguzhan Yilmaz,sametustaoglu devops/service-mesh/istio.md Barkin Atici devops/sonarqube/.DS_Store deniz-icin devops/sonarqube/advanced-installation.md deniz-icin devops/sonarqube/images/after_login.png deniz-icin devops/sonarqube/images/login_screen.png deniz-icin devops/sre/.DS_Store deniz-icin,ersinsari13 devops/sre/k8sgpt.md ersinsari13 devops/sre/k8sgpt/image-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-1.png ersinsari13 devops/sre/k8sgpt/k8sgpt-2.png ersinsari13 devops/sre/k8sgpt/k8sgpt-3.png ersinsari13 devops/sre/k8sgpt/k8sgpt-4.png ersinsari13 devops/sre/k8sgpt/k8sgpt-5.png ersinsari13 devops/sre/k8sgpt/k8sgpt-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-7.png ersinsari13 devops/sre/k8sgpt/k8sgpt-8.png ersinsari13 devops/sre/k8sgpt/k8sgpt-9.png ersinsari13 devops/sumo/sumo-linux-collector.md Oguzhan Yilmaz,can devops/sumo/sumo-local-file-management.md Oguzhan Yilmaz,can devops/sumo/sumo-windows-collector.md Oguzhan Yilmaz,can devops/terragrunt/Readme.md MehmetG171 devops/terragrunt/empty.txt MehmetG171 devops/terragrunt/environments/dev/env.hcl MehmetG171 devops/terragrunt/environments/dev/us-east-1/region.hcl MehmetG171 devops/terragrunt/environments/dev/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/dev/us-west-2/region.hcl MehmetG171 devops/terragrunt/environments/dev/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/prod/env.hcl MehmetG171 devops/terragrunt/environments/prod/us-east-1/region.hcl MehmetG171 devops/terragrunt/environments/prod/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/prod/us-west-2/region.hcl MehmetG171 devops/terragrunt/environments/prod/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/terragrunt/environments/terragrunt.hcl MehmetG171 devops/terragrunt/initial_configs/AWSTerraformInitialConfigs_Environment.yaml MehmetG171 devops/terragrunt/initial_configs/AWSTerraformInitialConfigs_Management.yaml MehmetG171 devops/terragrunt/modules/vpc/main.tf MehmetG171 devops/terragrunt/modules/vpc/outputs.tf MehmetG171 devops/terragrunt/modules/vpc/variables.tf MehmetG171 devops/terragrunt/modules/vpc/versions.tf MehmetG171 devops/vagrant/vagrant-quickstart.md Oguzhan Yilmaz,Utku Toraman images/hepapi-logo.png Oguzhan Yilmaz index.md Oguzhan Yilmaz,vfarukhepapi misc/how-to-contribute/about-markdown.md Oguzhan Yilmaz misc/how-to-contribute/about-mkdocs.md Oguzhan Yilmaz misc/how-to-contribute/mkdocs-features.md Oguzhan Yilmaz qa/Fundamentals_of_QA.md turan.topcuoglu qa/QA_Bug_Reporting.md turan.topcuoglu,turantopcuoglu qa/QA_Testing_Process.md turan.topcuoglu,turantopcuoglu qa/SSH_Config.md turan.topcuoglu qa/Software_Testing_Tools.md turan.topcuoglu,turantopcuoglu qa/automation_testing_and_best_practices.md turan.topcuoglu,turantopcuoglu qa/index.md Oguzhan Yilmaz,turantopcuoglu qa/test_approaches_in_quality_assurance.md turan.topcuoglu,turantopcuoglu qa/test_processes_and_qa_practices.md turan.topcuoglu,turantopcuoglu qa/types_of_tests.md turan.topcuoglu,turantopcuoglu repo-credit.md ersinsari13 mkdocs.yml Barkin Atici,Berat Uyan\u0131k,BoraKostem,Erdem Do\u011fanay,FIRST_NAME LAST_NAME,MehmetG171,Necip Ulusoy,Oguzhan Yilmaz,Onur Ozcelik,Utku Toraman,can,deniz-icin,ersinsari13,gokhanwell,oaltinoluk,ozihan,sametustaoglu,turan.topcuoglu,turantopcuoglu"},{"location":"devops/","title":"devops index","text":""},{"location":"devops/ansible/ansible-installations/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core==2.12.3\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre> <p>Lastly you can install the Ansible software with:</p> <pre><code>sudo apt install ansible -y\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible-roles/","title":"Roles","text":""},{"location":"devops/ansible/ansible-roles/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible-roles/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible-roles/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic</p> <p>my_role/ |-- defaults/    |   |-- main.yml |-- files/       |-- handlers/    |   |-- main.yml |-- meta/         |   |-- main.yml |-- tasks/       |   |-- main.yml |-- templates/   |-- tests/       |-- vars/        |   |-- main.yml |-- README.md     </p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible-roles/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/ansible/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre>"},{"location":"devops/ansible/ansible/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible/#what-is-ansible","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <p>1-Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators. 2-Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems. 3-Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times. 4-Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality. 5-Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures. 6-Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows. 7-Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</p>"},{"location":"devops/ansible/ansible/#inventory-file-and-building-an-inventory","title":"Inventory file and Building an inventory","text":"<p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/ansible/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role. </p>"},{"location":"devops/ansible/ansible/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/ansible/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/ansible/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/ansible/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic my_role/ |-- defaults/ |   |-- main.yml |-- files/ |-- handlers/ |   |-- main.yml |-- meta/ |   |-- main.yml |-- tasks/ |   |-- main.yml |-- templates/ |-- tests/ |-- vars/ |   |-- main.yml |-- README.md</p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/config-file/","title":"Configuration File","text":""},{"location":"devops/ansible/config-file/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/inventory-file/","title":"Inventory File","text":"<p>Inventory file and Building an inventory</p> <p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/inventory-file/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/inventory-file/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements</p> <p>The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml</p> <p>NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: </p> <p>my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role.</p>"},{"location":"devops/ansible/playbook/","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/playbook/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/playbook/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/what-is-ansible/","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <ul> <li>Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators.</li> <li>Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems.</li> <li>Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times.</li> <li>Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality.</li> <li>Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures.</li> <li>Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows.</li> <li>Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</li> </ul>"},{"location":"devops/aws/cli/","title":"AWS CLI","text":"<p>The AWS Command Line Interface (CLI) is a unified tool for managing AWS services from the command line. With just one tool, you can control multiple AWS services, including Amazon S3, Amazon EC2, and Amazon CloudFront.</p>"},{"location":"devops/aws/cli/#installation","title":"Installation","text":"<p>Install the dependencies:</p> <pre><code>apt install glibc groff less -y\n</code></pre> <p>Install the dependencies:</p> <pre><code>   apt install glibc groff less -y\n</code></pre> <p>Follow the official guide as the <code>curl</code>ed .zip link there updates frequently.</p> <p>AWS CLIv2 Official Installation Documentation</p>"},{"location":"devops/aws/sso/","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":""},{"location":"devops/aws/sso/#aws-iam-identity-center-successor-to-aws-single-sign-on-what-is-sso","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":"<p>IAM Identity Center (successor to AWS Single Sign-On)</p> <p>Single sign-on (SSO) is an authentication solution that allows users to log in to multiple applications and websites with one-time user authentication. Given that users today frequently access applications directly from their browsers, organizations are prioritizing access management strategies that improve both security and the user experience. SSO delivers both aspects, as users can access all password-protected resources without repeated logins once their identity is validated.</p>"},{"location":"devops/aws/sso/#why-is-sso-important","title":"Why is SSO important?","text":"<p>Using SSO to streamline user logins benefits users and organizations in several ways.</p> <p></p> <ol> <li> <p>Strengthen password security </p> <p>When people don\u2019t use SSO, they must remember multiple passwords for different websites. This might lead to non-recommended security practices, such as using simple or repetitive passwords for different accounts. Besides, users might forget or mistype their credentials when logging in to a service. SSO prevents password fatigue and encourages users to create a strong password that can be used for multiple websites.</p> </li> <li> <p>Improve productivity </p> <p>Employees often use more than one enterprise application that requires separate authentication. Manually entering the username and password for every application is time-consuming and unproductive. SSO streamlines the user validation process for enterprise applications and makes it easier to access protected resources.</p> </li> <li> <p>Reduce costs </p> <p>In their attempt to remember numerous passwords, enterprise users may forget their login credentials. This results in frequent requests to retrieve or reset their passwords, which increases workload for the in-house IT teams. Implementing SSO reduces occurrences of forgotten passwords and thus minimizes the support resources in handling requests for password resets.</p> </li> <li> <p>Improve security posture</p> <p>By minimizing the number of passwords per user, SSO facilitates user access auditing and provides robust access control to all types of data. This reduces the risk of security events that target passwords, while helping organizations comply with data security regulations.</p> </li> <li> <p>Provide a better customer experience </p> <p>Cloud application vendors use SSO to provide end-users with a seamless login experience and credential management. Users manage fewer passwords and can still securely access the information and apps they need to complete their day-to-day jobs.</p> </li> </ol>"},{"location":"devops/aws/sso/#is-sso-secure","title":"Is SSO secure?","text":"<p>Yes, SSO is an advanced and desirable identity access management solution. When deployed, a single sign-on solution helps organizations with user access management for enterprise applications and resources. An SSO solution makes setting and remembering strong passwords easier for application users. In addition, the IT team can use the SSO tool to monitor user behavior, improve system resilience, and reduce security risks. </p>"},{"location":"devops/aws/sso/#how-can-aws-help-with-sso","title":"How can AWS help with SSO?","text":"<p>AWS IAM Identity Center is a cloud authentication solution that allows organizations to securely create or connect their workforce identities and manage their access centrally across AWS accounts and applications. You can create user identities or import them from external identity providers such as Okta Universal Directory or Azure. Some benefits of AWS IAM Identity Center include:</p> <ol> <li> <p>A central dashboard to manage identities for your AWS account or business applications.</p> </li> <li> <p>Multi-factor authentication support to provide a highly secure authentication experience for users. </p> </li> <li> <p>Integration support with other AWS applications for zero-configuration authentication and authorization.</p> </li> </ol>"},{"location":"devops/aws/sso/#sso-setup-aws-console","title":"SSO Setup AWS Console","text":"<ol> <li> <p>In order to use SSO, you must first enable the AWS Single Sign-On service. AWS Organizations supports IAM Identity Center (Single Sign-On) in only one AWS Region at a time. AWS Single Sign-On service works as a single service only in one region. Once activated, accounts in AWS Organization appear in the SSO service and each account in the organization creates an SSO role. Every account that is included or leave in the organization automatically update on SSO.</p> <p> </p> </li> <li> <p>By creating users and groups within the SSO service, access can be given to all accounts within the organization or to a specific account. An e-mail is sent to the people whose users have been created, and the users are verified via e-mail and directed to the relevant link.</p> <p> </p> </li> <li> <p>After completing the password process, MFA authenticator must be installed and logged in with SSO.</p> <p></p> </li> <li> <p>Then, permission is set and the user or group is given authority on the account. For example, AdministratorAccess, Billing etc. Additionally, this permission can be limited to session duration, such as 1 hour, 2 hours.</p> <p></p> </li> <li> <p>Access permission for the desired account is attached to the user or group with the permission set by going to the AWS accounts section within the SSO service. Now, the desired authority has been given to the desired user or group to access the desired account.</p> <p></p> </li> <li> <p>Access is provided via the link on SSO or the invitation email sent to users. The authorized account within the organization is entered. While performing the redirection process, it allows both through the AWS manenmagent console and provides transaction permissions through the AWS CLI. It presents the relevant credentials for AWS CLI to the user during access.</p> <p></p> </li> <li> <p>For accounts to which access is not desired, no user or permission assignment should be made.</p> </li> </ol>"},{"location":"devops/aws/sso/#references","title":"References","text":"<p>What is SSO (Single-Sign-On)?</p>"},{"location":"devops/azure/agent-installation/","title":"Azure Self-Hosted Agent Installation","text":""},{"location":"devops/azure/agent-installation/#creating-a-personal-access-token","title":"Creating a Personal Access Token","text":"<ol> <li>Log into Azure DevOps.</li> <li>Under User settings select <code>Personal access tokens</code></li> <li>Click <code>New Token</code></li> <li>Fill the <code>Name</code> and <code>Expiration</code> fields.</li> <li>In scope select <code>Custom defined</code>, then click <code>Show all scopes</code> and tick <code>Read &amp; manage</code>under <code>Agent Pools</code></li> <li>Click <code>Create</code> and make sure to securely store the token because it will not be accessible later.</li> </ol>"},{"location":"devops/azure/agent-installation/#installing-and-configuring-the-agent","title":"Installing and configuring the agent","text":"<ol> <li> <ul> <li>If you want the agent to be usable by different projects in your organization go to <code>Organization Settings</code> on your Azure Devops Organization page.</li> <li>If you want the agent to be exclusive to a specific project, go to <code>Project Settings</code> on the Azure Devops Project page.</li> </ul> </li> <li> <p>Select <code>Agent pools</code> under the <code>Pipelines</code> section on the left</p> </li> <li>Select an existing <code>Agent Pool</code> or create a new one.</li> <li>After you've selected an <code>Agent Pool</code>, click <code>New Agent</code><ul> <li>Linux: <ul> <li>Select <code>Linux</code></li> <li>Press the Copy button next to the Download button to copy the URL.</li> </ul> </li> </ul> </li> </ol>"},{"location":"devops/azure/agent-installation/#linux","title":"Linux","text":"<p>In the Linux machine:</p> <ol> <li>Create a user for the Azure Agent:     <pre><code>sudo adduser azureagent\n</code></pre></li> <li>Add the user to sudoers:     <pre><code>sudo usermod -aG sudo azureagent\n</code></pre></li> <li>If needed add the user to other necessary groups like <code>docker</code>:     <pre><code>sudo usermod -aG docker azureagent\n</code></pre></li> <li>Create any necessary working directories and grant ownership to the user:     <pre><code>sudo chown -R azureagent:azureagent /home/app/foo\n</code></pre></li> <li>Switch to the azureagent user and navigate to the <code>/home/azureagent</code> directory:     <pre><code>su - azureagent\ncd /home/azureagent\n</code></pre></li> <li>Download the agent using the URL we copied earlier:     <pre><code>wget https://vstsagentpackage.azureedge.net/agent/3.220.2/vsts-agent-linux-x64-3.220.2.tar.gz\n</code></pre></li> <li>Create a new directory for the agent and extract the tar.gz inside and confirm with ls:     <pre><code>mkdir agent\ncd agent\ntar zxf ../vsts-agent-linux-x64-3.220.2.tar.gz\nls\n</code></pre></li> <li>Run the config script to start the agent configuration:     <pre><code>./config.sh\n</code></pre></li> <li> <p>Provide the info requested by the script</p> <ul> <li>Enter your Azure Devops server URL:     <pre><code>https://dev.azure.com/orgname\n</code></pre></li> <li>Press Enter to continue using PAT then paste the PAT we created earlier.</li> <li>Enter the agent pool name and a name for the agent we're creating.</li> <li>Press enter to use the default work folder (<code>_work</code>)</li> </ul> </li> <li> <p>Configure the agent to run as a service:     <pre><code>sudo ./svc.sh install azureagent\nsudo ./svc.sh start\n</code></pre></p> </li> <li> <p>Navigate to the <code>Agent pools</code> page on Azure Devops and select the relevant <code>Agent Pool</code>, then click on the <code>Agents</code> tab to verify that our new <code>Agent</code> is added as a self-hosted agent.</p> </li> </ol>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/","title":"DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy","text":"<p>Since DevOps entered our lives, it has been loved and widely adopted, and it seems it will continue to spread rapidly. While automating and speeding up delivery processes with DevOps, we cannot overlook the security aspect, which brings the DevSecOps methodology into focus. In this article, I discussed examples of how to fully implement DevSecOps in CI by checking code quality with SonarQube, scanning code dependencies with OWASP Dependency-Check, validating your Kubernetes, Terraform, and Dockerfile files with Conftest, and scanning Docker images with Trivy. Enjoy your learning!</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#sonarqube","title":"SonarQube","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-vulnerability","title":"What is Vulnerability ?","text":"<p>Vulnerabilities are basically the security weaknesses that one might use to undermine the availability, integrity, or security of information systems. Software, hardware, networks, or even human activities are among the several parts of a system that could have these flaws. A vulnerability could be as basic as a setting gone wrong or as sophisticated as a zero-day attack.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-sonarqube","title":"What is Sonarqube ?","text":"<p>SonarQube, previously named Sonar, is an open-source platform created by SonarSource. Its purpose is to consistently examine and evaluate the quality of code, identify security vulnerabilities, and assess technical debt across different programming languages. SonarQube delivers a single dashboard that provides real-time information into the health and security of software projects.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#how-is-sonarqube-used","title":"How is SonarQube Used?","text":"<p>SonarQube functions by looking at source code and finding possible problems and vulnerabilities. It uses static analysis, code smell recognition, and security vulnerability scanning all together to give complete results. SonarQube can be added to developers' work processes to help them find problems early in the development process. SonarQube works with many computer languages, such as Python, JavaScript, TypeScript, C#, and more. It has add-ons and plugins for well-known Integrated Development Environments (IDEs) like Eclipse, IntelliJ IDEA, and Visual Studio. This lets writers get feedback and suggestions while they're writing code.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#key-features-of-sonarqube","title":"Key Features of SonarQube","text":"<ul> <li>Code Quality Analysis</li> <li>Security Vulnerability Detection</li> <li>Technical Debt Management</li> <li>Continuous Integration/Continuous Deployment (CI/CD) Integration</li> <li>Customizable Rules and Quality Profiles</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#owasp-dependency-check","title":"OWASP Dependency-Check","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-owasp-dependency-check","title":"What is OWASP Dependency-Check?","text":"<p>Dependency-Check is a Software Composition Analysis (SCA) tool that attempts to detect publicly disclosed vulnerabilities contained within a project\u2019s dependencies. It does this by determining if there is a Common Platform Enumeration (CPE) identifier for a given dependency. If found, it will generate a report linking to the associated CVE entries. Dependency-check has a command line interface, a Maven plugin, an Ant task, and a Jenkins plugin. The core engine contains a series of analyzers that inspect the project dependencies, collect pieces of information about the dependencies (referred to as evidence within the tool). The evidence is then used to identify the Common Platform Enumeration (CPE) for the given dependency. If a CPE is identified, a listing of associated Common Vulnerability and Exposure (CVE) entries are listed in a report. Other 3rd party services and data sources such as the NPM Audit API, the OSS Index, RetireJS, and Bundler Audit are utilized for specific technologies.Dependency-check automatically updates itself using the NVD Data Feeds hosted by NIST.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#conftest","title":"Conftest","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-conftest","title":"What is Conftest?","text":"<p>Conftest leverages the Open Policy Agent (OPA) to evaluate policies written in Rego language against configuration files. It's commonly used for: - Kubernetes configurations: Ensuring that Kubernetes manifests meet security and compliance requirements. - Terraform files: Validating that Terraform plans and configurations adhere to organizational policies. - Dockerfiles: Checking that Docker images are built securely and according to best practices.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#trivy","title":"Trivy","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-trivy","title":"What is Trivy?","text":"<p>Trivy is a vulnerability scanner that is open-source and has been specifically developed for containers. This program is efficient and user-friendly, helping in the detection of vulnerabilities in container images and filesystems. Trivy's primary objective is to conduct scans on container images to identify any known vulnerabilities present in the installed packages and libraries.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#some-key-features-of-trivy-include","title":"Some key features of Trivy include:","text":"<ul> <li>Comprehensive vulnerability database</li> <li>Fast and efficient scanning</li> <li>Easy integration</li> <li>Multiple output formats</li> <li>Continuous updates</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#hands-on","title":"Hands-On","text":"<p>Let's include the devsecops tools we briefly mentioned above into the pipeline and do some hands-on. Let's get started.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-1-launch-ec2-instance","title":"Step-1 Launch EC2 Instance","text":"<p>Launch an AWS t2-large Instance. Use the image as Amazon Linux. You can create a new key pair or use an existing one.  - Enable 80, 443, 8080 and 9000 port settings in the Security Group. - You can add the userdata below for jenkins,docker,trivy installation.</p> <pre><code>#! /bin/bash\n# update os\ndnf update -y\n# set server hostname as jenkins-server\nhostnamectl set-hostname jenkins-server\n# install git\ndnf install git -y\n# install java 17\ndnf install java-17-amazon-corretto-devel -y\n# install jenkins\nwget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nrpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\ndnf upgrade\ndnf install jenkins -y\nsystemctl enable jenkins\nsystemctl start jenkins\n# install docker\ndnf install docker -y\nsystemctl start docker\nsystemctl enable docker\nusermod -a -G docker ec2-user\nusermod -a -G docker jenkins\n# configure docker as cloud agent for jenkins\ncp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.bak\nsed -i 's/^ExecStart=.*/ExecStart=\\/usr\\/bin\\/dockerd -H tcp:\\/\\/127.0.0.1:2376 -H unix:\\/\\/\\/var\\/run\\/docker.sock/g' /lib/systemd/system/docker.service\nsystemctl daemon-reload\nsystemctl restart jenkins\n# install trivy\nrpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.31.3/trivy_0.31.3_Linux-64bit.rpm\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-2-configure-jenkins-server","title":"Step-2 Configure Jenkins-Server","text":"<ul> <li>After instance state running, we can configure the jenkins server.Now, grab your Public IP Address</li> </ul> <pre><code>&lt;EC2 Public IP Address:8080&gt;\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <ul> <li>Unlock Jenkins using an administrative password and install the required plugins.</li> </ul> <ul> <li>Jenkins will now get installed and install all the libraries.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-3-install-sonarqube-as-a-docker-container","title":"Step-3 Install Sonarqube as a docker container","text":"<ul> <li>Go to Instance terminal and enter below code to install sonarqube</li> </ul> <pre><code>docker run -d --name sonar -p 9000:9000 sonarqube:lts-community\n</code></pre> <pre><code>&lt;EC2 Public IP Address:9000&gt;\nusername: admin\npassword: admin\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-4-install-plugins","title":"Step-4 Install Plugins","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Plugins --&gt; Available Plugins Install below plugins</li> </ul> <p>1-Eclipse Temurin Installer:  It allows you to automatically download and install different versions of the Temurin JDK on your Jenkins agents. This is useful for ensuring that your builds run with the correct version of Java without needing to manually manage JDK installations.</p> <p>2-SonarQube Scanner: You can configure Jenkins jobs to run SonarQube scans as part of your build process. The plugin sends the code analysis results to a SonarQube server, where you can view detailed reports and track quality metrics over time.</p> <p>3-OWASP Dependency-Check: You can use this plugin to scan your project dependencies for vulnerabilities as part of your Jenkins build process. The results include detailed reports on any vulnerabilities found, helping you to mitigate security risks by updating or replacing affected dependencies.</p> <p>4-Blue Ocean: With Blue Ocean, you can create, edit, and visualize pipelines using a graphical interface. It also provides enhanced visualization of pipeline stages and steps, making it easier to track the progress and status of builds and deployments.</p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-java-maven-in-global-tool-configuration","title":"Step-5 Configure Java, Maven in Global Tool Configuration","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Tools --&gt; Install JDK, Maven and SonarQube Scanner --&gt;Click on Apply and Save</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-sonarqube-in-manage-jenkins","title":"Step-5 Configure Sonarqube in Manage Jenkins","text":"<p><pre><code>&lt;EC2 Public IP Address:9000&gt;\n</code></pre> - Go to your Sonarqube Server. Click on Administration \u2192 Security \u2192 Users \u2192 Click on Tokens and Update Token \u2192 Give it a name \u2192 and click on Generate Token</p> <p></p> <p></p> <p></p> <ul> <li>Copy this Token</li> <li>Go to Jenkins WebUI --&gt; Manage Jenkins \u2192 Credentials \u2192 Add Secret Text.</li> </ul> <p></p> <ul> <li>Go to Jenkins Dashboard \u2192 Manage Jenkins \u2192 Configure System</li> <li>Give a name whatever you want</li> <li>Add Sonarqube url</li> <li>Select sonarqube credential token</li> </ul> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-6-webhook-configuration-on-sonarqube","title":"Step-6 WebHook Configuration on Sonarqube","text":"<ul> <li>Go to SonarQube WebUI --&gt; Administration \u2013&gt; Configuration \u2013&gt; webhooks</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-7-create-a-pipeline","title":"Step-7 Create a pipeline","text":"<ul> <li>Go to Jenkins WebUI --&gt;New item--&gt;Pipeline</li> </ul> <ul> <li>Add below jenkins code to pipeline section</li> </ul> <p><pre><code>pipeline {\n    agent any\n    tools {\n        jdk 'jdk'\n        maven 'maven'\n    }\n    stages {\n        stage(\"Git Checkout\") {\n            steps {\n                git branch: 'main', changelog: false, poll: false, url: 'https://github.com/ersinsari13/devsecops.git'\n            }\n        }\n        stage(\"Compile\") {\n            steps {\n                sh \"mvn clean compile\"\n            }\n        }\n        stage(\"Test Cases\") {\n            steps {\n                sh \"mvn test\"\n            }\n        }\n        stage(\"Sonarqube Analysis\") {\n            steps {\n                withSonarQubeEnv('sonar-server') {\n                    sh ''' \n                        mvn clean verify sonar:sonar \\\n                        -Dsonar.projectKey=Petclinic\n                    '''\n                }\n            }\n        }\n        stage(\"Quality Gate\") {\n            steps {\n                timeout(time: 2, unit: 'MINUTES') {\n                    script {\n                        waitForQualityGate abortPipeline: true\n                    }\n                }\n            }\n        }\n        stage(\"Build\") {\n            steps {\n                sh \"mvn clean install\"\n            }\n        }\n        stage('OWASP-Dependency-Check') {\n            steps {\n                sh \"mvn dependency-check:check\"\n            }\n            post {\n                always {\n                    dependencyCheckPublisher pattern: 'target/dependency-check-report.xml'\n                }\n            }\n        }\n\n        stage('Scan Dockerfile with conftest') {\n            steps {\n                echo 'Scanning Dockerfile'\n                sh \"docker run --rm -v $(pwd):/project openpolicyagent/conftest test --policy dockerfile-conftest.rego Dockerfile\"\n            }\n        }\n\n        stage('Prepare Tags for Docker Images') {\n            steps {\n                echo 'Preparing Tags for Docker Images'\n                script {\n                    MVN_VERSION=sh(script:'. ${WORKSPACE}/target/maven-archiver/pom.properties &amp;&amp; echo $version', returnStdout:true).trim()\n                    env.IMAGE_TAG_DEVSECOPS=\"ersinsari/devsecops:${MVN_VERSION}-b${BUILD_NUMBER}\"\n                }\n            }\n        }\n        stage('Build App Docker Images') {\n            steps {\n                echo 'Building App Dev Images'\n                sh \"docker build --force-rm -t ${IMAGE_TAG_DEVSECOPS} .\"\n                sh 'docker image ls'\n            }\n        }\n        stage('Scan Image with Trivy') {\n            steps {\n                script {\n                    def scanResult = sh(script: \"trivy image --severity CRITICAL --exit-code 1 ${IMAGE_TAG_DEVSECOPS}\", returnStatus: true)\n                    if (scanResult != 0) {\n                        error \"Critical vulnerabilities found in Docker image. Failing the pipeline.\"\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> - Tools: Specifies the tools needed for the pipeline, in this case, JDK and Maven.</p> <ul> <li> <p>Git Checkout: Check out the code from the specified Git repository.</p> </li> <li> <p>Compile: Runs Maven commands to clean the workspace and compile the code.</p> </li> <li> <p>Test Cases: Executes the Maven test phase to run the unit tests.</p> </li> <li> <p>SonarQube Analysis: Analyze the code quality using SonarQube.</p> </li> <li> <p>Quality Gate: Check the SonarQube quality gate status and abort the pipeline if it fails.</p> </li> <li> <p>Build: Runs Maven commands to clean the workspace and install the build artifacts.</p> </li> <li> <p>OWASP-Dependency-Check: Perform a security vulnerability check on project dependencies.</p> </li> <li> <p>Scan Dockerfile with conftest: Runs Conftest in a Docker container to test the Dockerfile against the specified policy.</p> </li> <li> <p>Prepare Tags for Docker Images: Extracts the Maven version from the build and sets the environment variable IMAGE_TAG_DEVSECOPS with the image tag.</p> </li> <li> <p>Build App Docker Images: Build the Docker image for the application.</p> </li> <li> <p>Scan Image with Trivy: Scans the Docker image for critical vulnerabilities and fails the pipeline if any are found.</p> </li> <li> <p>Click Build Now and Open Blue Ocean</p> </li> </ul> <p></p> <ul> <li>After the pipeline runs, you should receive a failure at the \"Scan Dockerfile with conftest\" step; this is a normal occurrence.</li> </ul> <p></p> <ul> <li>The reason for this is that if you check the GitHub repository we included in the pipeline, you will see a file named dockerfile-conftest.rego. Conftest performs the Dockerfile scan based on the conditions in this file. We received a failure because the Dockerfile we want to use does not meet the necessary requirements specified. We will correct this.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-8-sonarqube-inspection-and-add-custom-quality-gate","title":"Step-8 Sonarqube inspection and add Custom Quality Gate","text":"<ul> <li> <p>But first, let's discuss the pipeline output and then talk a bit about the SonarQube interface and quality gates.</p> </li> <li> <p>You can inspect your source code qality by clicking SonarQube section</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>You can add custom Quality-Gates depends on your company rules</p> </li> <li> <p>SonarQube UI click Qualiyy Gates --&gt; Create --&gt; give name and save --&gt; Unlock editing --&gt; Add Condition --&gt; On Overall Code</p> </li> </ul> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-9-dependency-check-inspection","title":"Step-9 Dependency-Check inspection","text":"<ul> <li>You can inspect your source code dependency-check score by clicking Dependency-Check section</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-10-improving-dockerfile-security","title":"Step-10 Improving Dockerfile security","text":"<p>Now it's time to improve the Dockerfile security based on the Conftest results.</p> <p></p> <ul> <li>Change your Dockerfile as below</li> </ul> <p><pre><code>FROM openjdk:8\nEXPOSE 8082\nRUN addgroup -S devops-security &amp;&amp; adduser -u 999 -S devsecops -G devops-security\nCOPY target/petclinic.war petclinic.war\nUSER 999\nENTRYPOINT [\"java\",\"-jar\",\"/home/devsecops/petclinic.war\"]\n</code></pre> After this change, you should be able to successfully pass the Dockerfile scanning stage with Conftest.</p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-11-docker-image-scan-via-trivy","title":"Step-11 Docker Image Scan via Trivy","text":"<p>Lastly, the pipeline will fail at the image scanning stage with Trivy. If we look at the Jenkinsfile, it is designed to fail if a critical vulnerability is found during the image scan with Trivy. At this stage, the critical vulnerabilities in the image need to be resolved before proceeding. The pipeline output includes recommendations on how to resolve the vulnerabilities.</p> <p></p> <p></p> <p>Once the image scan is successfully completed according to your requirements, the next step is to push the Docker image to the registry and then deploy your application. The key point here is to ensure maximum security before deploying the application, which is what we have aimed to achieve. Have a nice day.</p>"},{"location":"devops/devsecops/What%20is%20Falco/","title":"Deep Dive into Security Monitoring in Kubernetes Environments: An Introduction to Falco","text":""},{"location":"devops/devsecops/What%20is%20Falco/#introduction","title":"Introduction","text":"<p>With the rise of container-based and cloud-native applications, security risks have become increasingly complex. Falco, an open-source tool developed by Sysdig and supported by CNCF (Cloud Native Computing Foundation), offers real-time security monitoring for Kubernetes and container environments. Falco monitors system calls to detect suspicious activities and notifies users immediately about these events. By using Falco, you can respond quickly to security breaches.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#what-is-falco","title":"What is Falco?","text":"<p>Falco monitors security in container environments by tracking system calls at the kernel level. It analyzes these system calls against defined rules to detect anomalies. For example, if an unauthorized bash session is opened within a container or an unexpected process is executed, Falco logs this and alerts the user.</p> <p></p>"},{"location":"devops/devsecops/What%20is%20Falco/#key-use-cases","title":"Key Use Cases","text":"<ul> <li> <p>Real-time security monitoring</p> </li> <li> <p>Preventing potential security breaches in containers</p> </li> <li> <p>Monitoring network activities and detecting suspicious connections</p> </li> <li> <p>Logging configuration changes made inside containers</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#how-falco-works","title":"How Falco Works","text":"<p>Falco is a security monitoring solution that observes and analyzes system calls at the kernel level. Specifically, it tracks process activities in the system and compares them against predefined rules. These rules determine what is considered normal or abnormal based on security policies. Suspicious operations are then reported to users through specified channels.</p> <p></p> <p>Falco uses sysdig libraries to analyze system calls in the user space. These events are filtered through Falco's policy engine based on predefined rules. If an event is deemed suspicious, it is reported through various output channels, such as:</p> <ul> <li> <p>Log files</p> </li> <li> <p>Standard output</p> </li> <li> <p>Alert mechanisms like Slack or email notifications</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#core-features-of-falco","title":"Core Features of Falco","text":"<ol> <li> <p>Real-Time Monitoring: Falco detects security events in real-time that require immediate attention.</p> </li> <li> <p>Customizable Rule Sets: Falco offers default security rules, but users can add their own in the falco_rules.local.yaml file.</p> </li> <li> <p>Hierarchical Severity Levels: Falco classifies detected events into different levels of importance. Below is a brief explanation of the log levels:</p> </li> <li> <p>emergency: System is unusable. Used for very serious system errors.</p> </li> <li> <p>alert: Immediate action required. Suitable for critical system failures or security breaches.</p> </li> <li> <p>critical: Critical conditions. Used for errors that could cause system or application crashes.</p> </li> <li> <p>error: Error conditions. Logs system or application errors.</p> </li> <li> <p>warning: Warning conditions. Indicates potential issues.</p> </li> <li> <p>notice: Normal but significant conditions. Highlights situations that aren't errors or warnings but still noteworthy.</p> </li> <li> <p>info: Informational messages. Provides general information about system and application status (e.g., successful starts, configuration loads).</p> </li> <li> <p>debug: Debug-level messages. Includes detailed information for troubleshooting and debugging (e.g., variable values, workflow steps).</p> <p>These log levels allow Falco users to track events based on their severity.</p> </li> <li> <p>Kernel-Level System Call Monitoring: Tracks events at the kernel level, offering deep insights into container activities.</p> </li> </ol>"},{"location":"devops/devsecops/What%20is%20Falco/#installing-falco","title":"Installing Falco","text":""},{"location":"devops/devsecops/What%20is%20Falco/#running-falco-in-a-kubernetes-environment","title":"Running Falco in a Kubernetes Environment","text":"<p>There are two main ways to deploy Falco in a Kubernetes environment:</p> <ol> <li>As a Standalone Service: This method installs Falco as a system service independent of Kubernetes.</li> </ol> <p>\ud83d\udcc4 Standalone Falco Installation Guide</p> <ol> <li>As a DaemonSet: This option deploys Falco as a pod on each Kubernetes node.</li> </ol> <p>\ud83d\udcc4 Installing Falco on Kubernetes</p> <p>After installation, Falco-related files can be found in the /etc/falco directory.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#managing-rules-in-falco","title":"Managing Rules in Falco","text":"<p>One of Falco's strongest features is its configurable rule system, allowing users to define custom rules. Default rules are stored in the falco_rules.yaml file, which is updated during upgrades. User-defined rules go into the falco_rules.local.yaml file, which is not affected by updates. This setup enables users to tailor Falco to their specific security needs.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#example-rule-unauthorized-bash-access","title":"Example Rule: Unauthorized Bash Access","text":"<p>In this example, only <code>bash</code> processes running within a container environment are tracked. If a bash shell is opened, Falco detects this activity and generates an alert:</p> <pre><code>- rule: Unauthorized Shell in Container\n  desc: Detect when a bash shell is opened in a container\n  condition: container.id != host and proc.name = bash\n  output: \"Unauthorized shell detected in container (command=%proc.cmdline container_name=%container.name)\"\n  priority: warning\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#explanation-of-the-rule","title":"Explanation of the Rule","text":"<ul> <li> <p>condition: The <code>container.id != host</code> condition ensures that the rule applies only in the container environment. This means the rule will not trigger when <code>bash</code> is executed on the host. The <code>proc.name = bash</code> part specifies that the process being monitored must be <code>bash</code>.</p> </li> <li> <p>output: This defines the alert message that will be triggered when the event occurs. In this example, the alert includes the container name and the executed command.</p> </li> <li> <p>priority: Defines the severity of the event, which is set to <code>warning</code> in this case.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#using-references-when-creating-rules","title":"Using References When Creating Rules","text":"<p>This rule was created using Falco's extensive rule-writing reference. To see all the fields supported by Falco and how to use them, you can consult the official documentation:</p> <p>\ud83d\udcc4 Falco Supported Fields for Conditions and Outputs</p> <p>This page lists all the fields available for conditions and outputs. For instance, <code>proc.name</code> tracks the process name, while <code>container.id</code> identifies the container ID. When writing your own rules or customizing existing ones, this reference document helps you choose the correct fields and formats.</p> <p>This approach makes rule writing both more accurate and comprehensive, allowing you to easily detect any anomalies in your system.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#using-lists-in-falco","title":"Using Lists in Falco","text":"<p>If you want to monitor other shell types besides <code>bash</code>, you can specify them in a list. For instance, to track <code>bash</code>, <code>sh</code>, and <code>zsh</code>, you can write a rule like this:</p> <pre><code>- list: linux_shells\n  items: [bash, sh, zsh]\n\n- rule: Detect Shell inside a Container\n  desc: Alert if a shell such as bash is open inside the container\n  condition: container.id != host and proc.name in (linux_shells)\n  output: \"Shell Opened (user=%user.name container=%container.id)\"\n  priority: WARNING\n</code></pre> <p>By doing so, different types of shell processes in the container environment can be monitored, addressing potential security risks more comprehensively.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#advantages-of-using-lists","title":"Advantages of Using Lists","text":"<ul> <li> <p>Readability: Instead of writing long conditions to check multiple values, a list provides a cleaner structure.</p> </li> <li> <p>Reusability: The same list can be reused across multiple rules, simplifying management.</p> </li> <li> <p>Ease of Updates: Adding or modifying items only requires changes to the list.</p> </li> </ul> <p>This structure is particularly useful for monitoring different types of shell processes or other similar events.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#defining-rules-with-macros","title":"Defining Rules with Macros","text":"<p>In Falco, you can use macros to define frequently used conditions. Macros can be reused within rules, making the rules more readable and easier to manage.</p> <pre><code>- macro: unauthorized_shells\n  condition: proc.name in (bash, sh, zsh)\n\n- rule: Unauthorized Shell in Container\n  desc: Detect unauthorized shell in a container\n  condition: container.id != host and unauthorized_shells\n  output: \"Unauthorized shell detected in container (command=%proc.cmdline container_name=%container.name)\"\n  priority: warning\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#advantages-of-using-macros","title":"Advantages of Using Macros","text":"<ul> <li> <p>Reduced Code Duplication: Macros allow you to define a condition once and reuse it across multiple rules, reducing repetition.</p> </li> <li> <p>Improved Readability: Using macros makes rules more understandable and easier to manage.</p> </li> </ul> <p>These features make Falco\u2019s security monitoring rules more flexible and manageable in container environments.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#log-management-in-falco","title":"Log Management in Falco","text":"<p>Falco offers flexibility in storing and managing logs in various formats and destinations. Below, you will find several configuration options and how to use them.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#1-saving-falco-logs-to-a-file","title":"1. Saving Falco Logs to a File","text":"<p>To save Falco logs to a specific file, update the /etc/falco/falco.yaml file with the following settings:</p> <pre><code>file_output:\n  enabled: true\n  keep_alive: true\n  filename: ./falco_events.txt\n</code></pre> <ul> <li> <p>enabled: When set to <code>true</code>, file output is enabled, and logs are written to the specified file.</p> </li> <li> <p>keep_alive:</p> </li> <li><code>true</code>: Keeps the file open for continuous writing, which can improve performance since the file doesn\u2019t reopen for each event.</li> <li> <p><code>false</code>: Opens, writes, and closes the file for each event.</p> </li> <li> <p>filename: Specifies the path where logs will be saved. In this example, logs will be saved to <code>/var/log/falco_events.txt</code>.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#2-saving-falco-logs-in-json-format","title":"2. Saving Falco Logs in JSON Format","text":"<p>To save Falco events in JSON format, use the following configuration:</p> <pre><code>json_output:\n  enabled: true\n</code></pre> <p>By default, this setting is <code>enabled:false</code>, which saves logs in text format. To enable JSON logging, set the value to <code>enabled:true</code>.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#3-program-output-for-notifications","title":"3. Program Output for Notifications","text":"<p>To process logs through a program or send notifications, use the following configuration:</p> <pre><code>program_output:\n  enabled: true\n  program: mail -s \"Falco Alert\" admin@example.com\n</code></pre> <ul> <li>program: Falco sends detected events through this program. In this example, events are sent as an email.</li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#4-setting-the-log-level","title":"4. Setting the Log Level","text":"<p>You can define the minimum log level that Falco will record using <code>log_level</code>:</p> <pre><code>log_level: info\n</code></pre> <p>Events with a severity level of <code>info</code> and above will be recorded.</p> <ul> <li> <p>Log level descriptions:</p> </li> <li> <p><code>debug</code>: Records all log levels.</p> </li> <li> <p><code>info</code>: Provides general information about system status.</p> </li> <li> <p><code>warning</code>, <code>error</code>, <code>critical</code>, <code>alert</code>, and <code>emergency</code>: Indicate progressively higher levels of severity.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#5-rule-file-loading-order","title":"5. Rule File Loading Order","text":"<p>Falco loads rule files in the order specified in the configuration. Order matters, because:</p> <ul> <li> <p>If a rule defined in the falco_rules.yaml file is redefined in the falco_rules.local.yaml file, the rule in falco_rules.local.yaml will take precedence.</p> </li> <li> <p>This approach allows users to customize and override default rules.</p> </li> </ul>"},{"location":"devops/devsecops/What%20is%20Falco/#example-falcoyaml-configuration","title":"Example falco.yaml Configuration","text":"<pre><code>rules_file:\n  - /etc/falco/falco_rules.yaml\n  - /etc/falco/falco_rules.local.yaml\n</code></pre> <p>This configuration indicates that the default rules will be loaded first, followed by user-customized rules.</p> <p>In summary, the rule defined in the last file will always take precedence (overwrite).</p>"},{"location":"devops/devsecops/What%20is%20Falco/#important-note","title":"Important Note:","text":"<p>After modifying Falco's configuration file, you need to restart the Falco service for the changes to take effect:</p> <pre><code>systemctl restart falco\n</code></pre>"},{"location":"devops/devsecops/What%20is%20Falco/#conclusion","title":"Conclusion","text":"<p>Falco is a powerful and flexible tool for security monitoring in Kubernetes and container environments. With its easily customizable rule system and real-time monitoring capabilities, it helps you identify security vulnerabilities efficiently. If you\u2019re looking to enhance the security of your Kubernetes infrastructure, Falco is definitely worth considering.</p>"},{"location":"devops/devsecops/What%20is%20Falco/#references","title":"References","text":"<p>This document was prepared using the following resources:</p> <ul> <li>Official Falco Documentation</li> <li>Certified Kubernetes Security Specialist (CKS) Course - KodeKloud</li> </ul>"},{"location":"devops/devsecops/external-secret-operator/","title":"External Secret Operator","text":"<p>In the dynamic landscape of modern application development, managing secrets securely is crucial, especially within Kubernetes environments. Secrets such as API keys, passwords, and certificates are vital for the functionality and security of applications, but they also pose significant risks if not handled correctly. Kubernetes provides built-in mechanisms for secret management, but as applications grow in complexity, so does the challenge of managing these secrets effectively. This is where the External Secret Operator comes into play. By integrating with external secret management systems, the External Secret Operator enhances Kubernetes' native capabilities, allowing for seamless and secure management of sensitive data. In this blog, we'll explore why secrets are so critical in Kubernetes and how the External Secret Operator can streamline and fortify your secret management strategy.</p> <p></p> <p>External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, IBM Cloud Secrets Manager, CyberArk Conjur and many more. The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret.</p>"},{"location":"devops/devsecops/external-secret-operator/#secretstore","title":"SecretStore","text":"<p>The SecretStore is namespaced and specifies how to access the external API. The SecretStore maps to exactly one instance of an external API. By design, SecretStores are bound to a namespace and can not reference resources across namespaces. If you want to design cross-namespace SecretStores you must use ClusterSecretStores which do not have this limitation.</p> <p></p> <p>For a full list of supported fields see spec </p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: example\n  namespace: example-ns\nspec:\n\n  # Used to select the correct ESO controller (think: ingress.ingressClassName)\n  # The ESO controller is instantiated with a specific controller name\n  # and filters ES based on this property\n  # Optional\n  controller: dev\n\n  # You can specify retry settings for the http connection\n  # these fields allow you to set a maxRetries before failure, and\n  # an interval between the retries.\n  # Current supported providers: AWS, Hashicorp Vault, IBM\n  retrySettings:\n    maxRetries: 5\n    retryInterval: \"10s\"\n\n  # provider field contains the configuration to access the provider\n  # which contains the secret exactly one provider must be configured.\n  provider:\n\n    # (1): AWS Secrets Manager\n    # aws configures this store to sync secrets using AWS Secret Manager provider\n    aws:\n      service: SecretsManager\n      # Role is a Role ARN which the SecretManager provider will assume\n      role: iam-role\n      # AWS Region to be used for the provider\n      region: eu-central-1\n      # Auth defines the information necessary to authenticate against AWS by\n      # getting the accessKeyID and secretAccessKey from an already created Kubernetes Secret\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n\n    # (2) Hashicorp Vault\n    vault:\n      server: \"https://vault.acme.org\"\n      # Path is the mount path of the Vault KV backend endpoint\n      # Used as a path prefix for the external secret key\n      path: \"secret\"\n      # Version is the Vault KV secret engine version.\n      # This can be either \"v1\" or \"v2\", defaults to \"v2\"\n      version: \"v2\"\n      # vault enterprise namespace: https://www.vaultproject.io/docs/enterprise/namespaces\n      namespace: \"a-team\"\n      # base64 encoded string of certificate\n      caBundle: \"...\"\n      # Instead of caBundle you can also specify a caProvider\n      # this will retrieve the cert from a Secret or ConfigMap\n      caProvider:\n        # Can be Secret or ConfigMap\n        type: \"Secret\"\n        name: \"my-cert-secret\"\n        key: \"cert-key\"\n      # client side related TLS communication, when the Vault server requires mutual authentication\n      tls:\n        clientCert:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.crt\"\n        secretRef:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.key\"\n\n      auth:\n        # static token: https://www.vaultproject.io/docs/auth/token\n        tokenSecretRef:\n          name: \"my-secret\"\n          key: \"vault-token\"\n\n        # AppRole auth: https://www.vaultproject.io/docs/auth/approle\n        appRole:\n          path: \"approle\"\n          roleId: \"db02de05-fa39-4855-059b-67221c5c2f63\"\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault-token\"\n\n        # Kubernetes auth: https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"demo\"\n          # Optional service account reference\n          serviceAccountRef:\n            name: \"my-sa\"\n          # Optional secret field containing a Kubernetes ServiceAccount JWT\n          # used for authenticating with Vault\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault\"\n\n        # TLS certificates auth method: https://developer.hashicorp.com/vault/docs/auth/cert\n        cert:\n          clientCert:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.crt\"\n          secretRef:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.key\"\n\n    # (3): GCP Secret Manager\n    gcpsm:\n      # Auth defines the information necessary to authenticate against GCP by getting\n      # the credentials from an already created Kubernetes Secret.\n      auth:\n        secretRef:\n          secretAccessKeySecretRef:\n            name: gcpsm-secret\n            key: secret-access-credentials\n      projectID: myproject\n    # (TODO): add more provider examples here\n\nstatus:\n  # Standard condition schema\n  conditions:\n  # SecretStore ready condition indicates the given store is in ready\n  # state and able to referenced by ExternalSecrets\n  # If the `status` of this condition is `False`, ExternalSecret controllers\n  # should prevent attempts to fetch secrets\n  - type: Ready\n    status: \"False\"\n    reason: \"ConfigError\"\n    message: \"SecretStore validation failed\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#external-secret","title":"External Secret","text":"<p>The ExternalSecret describes what data should be fetched, how the data should be transformed and saved as a Kind=Secret:</p> <ul> <li>tells the operator what secrets should be synced by using spec.data to explicitly sync individual keys or use spec.dataFrom to get all values from the external API.</li> <li>you can specify how the secret should look like by specifying a spec.target.template</li> </ul> <p>Take a look at an annotated example to understand the design behind the ExternalSecret</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: \"hello-world\"\n\n  # labels and annotations are copied over to the\n  # secret that will be created\n  labels:\n    acme.org/owned-by: \"q-team\"\n  annotations:\n    acme.org/sha: 1234\n\nspec:\n\n  # Optional, SecretStoreRef defines the default SecretStore to use when fetching the secret data.\n  secretStoreRef:\n    name: aws-store\n    kind: SecretStore  # or ClusterSecretStore\n\n  # RefreshInterval is the amount of time before the values reading again from the SecretStore provider\n  # Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration)\n  # May be set to zero to fetch and create it once\n  refreshInterval: \"1h\"\n\n  # the target describes the secret that shall be created\n  # there can only be one target per ExternalSecret\n  target:\n\n    # The secret name of the resource\n    # Defaults to .metadata.name of the ExternalSecret\n    # It is immutable\n    name: application-config\n\n    # Specifies the ExternalSecret ownership details in the created Secret. Options:\n    # - Owner: (default) Creates the Secret and sets .metadata.ownerReferences. If the ExternalSecret is deleted, the Secret will also be deleted.\n    # - Merge: Does not create the Secret but merges data fields into the existing Secret (expects the Secret to already exist).\n    # - Orphan: Creates the Secret but does not set .metadata.ownerReferences. If the Secret already exists, it will be updated.\n    # - None: Does not create or update the Secret (reserved for future use with injector).\n    creationPolicy: Merge\n\n    # Specifies what happens to the Secret when data fields are deleted from the provider (e.g., Vault, AWS Parameter Store). Options:\n    # - Retain: (default) Retains the Secret if all Secret data fields have been deleted from the provider.\n    # - Delete: Removes the Secret if all Secret data fields from the provider are deleted.\n    # - Merge: Removes keys from the Secret but not the Secret itself.\n    deletionPolicy: Retain\n\n    # Specify a blueprint for the resulting Kind=Secret\n    template:\n      type: kubernetes.io/dockerconfigjson # or TLS...\n\n      metadata:\n        annotations: {}\n        labels: {}\n\n      # Use inline templates to construct your desired config file that contains your secret\n      data:\n        config.yml: |\n          database:\n            connection: postgres://{{ .username }}:{{ .password }}@{{ .database_host }}:5432/payments\n\n      # Uses an existing template from configmap\n      # Secret is fetched, merged and templated within the referenced configMap data\n      # It does not update the configmap, it creates a secret with: data[\"alertmanager.yml\"] = ...result...\n      templateFrom:\n      - configMap:\n          name: application-config-tmpl\n          items:\n          - key: config.yml\n\n  # Data defines the connection between the Kubernetes Secret keys and the Provider data\n  data:\n    - secretKey: username\n      remoteRef:\n        key: database-credentials\n        version: v1\n        property: username\n        decodingStrategy: None # can be None, Base64, Base64URL or Auto\n\n      # define the source of the secret. Can be a SecretStore or a Generator kind\n      sourceRef:\n        # point to a SecretStore that should be used to fetch a secret.\n        # must be defined if no spec.secretStoreRef is defined.\n        storeRef:\n          name: aws-secretstore\n          kind: ClusterSecretStore\n\n  # Used to fetch all properties from the Provider key\n  # If multiple dataFrom are specified, secrets are merged in the specified order\n  # Can be defined using sourceRef.generatorRef or extract / find\n  # Both use cases are exemplified below\n  dataFrom:\n  - sourceRef:\n      generatorRef:\n        apiVersion: generators.external-secrets.io/v1alpha1\n        kind: ECRAuthorizationToken\n        name: \"my-ecr\"\n  #Or\n  dataFrom:\n  - extract:\n      key: database-credentials\n      version: v1\n      property: data\n      conversionStrategy: Default\n      decodingStrategy: Auto\n    rewrite:\n    - regexp:\n        source: \"exp-(.*?)-ression\"\n        target: \"rewriting-${1}-with-groups\"\n  - find:\n      path: path-to-filter\n      name:\n        regexp: \".*foobar.*\"\n      tags:\n        foo: bar\n      conversionStrategy: Unicode\n      decodingStrategy: Base64\n    rewrite:\n    - regexp:\n        source: \"foo\"\n        target: \"bar\"\n\nstatus:\n  # refreshTime is the time and date the external secret was fetched and\n  # the target secret updated\n  refreshTime: \"2019-08-12T12:33:02Z\"\n  # Standard condition schema\n  conditions:\n  # ExternalSecret ready condition indicates the secret is ready for use.\n  # This is defined as:\n  # - The target secret exists\n  # - The target secret has been refreshed within the last refreshInterval\n  # - The target secret content is up-to-date based on any target templates\n  - type: Ready\n    status: \"True\" # False if last refresh was not successful\n    reason: \"SecretSynced\"\n    message: \"Secret was synced\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#hands-on","title":"Hands-On","text":"<p>Here\u2019s the step-by-step guide to using an external secret manager in a local Minikube cluster with AWS Secret Manager</p>"},{"location":"devops/devsecops/external-secret-operator/#set-up-minikube","title":"Set Up Minikube","text":"<pre><code>minikube start\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#install-external-secrets-operator","title":"Install External Secrets Operator","text":"<ul> <li>Add the Helm chart repository for the external secrets operator</li> </ul> <pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm repo update\n</code></pre> <ul> <li>Install the External Secrets Operator using Helm</li> </ul> <pre><code>helm install external-secrets \\\n   external-secrets/external-secrets \\\n    -n external-secrets \\\n    --create-namespace\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-in-aws-secrets-manager","title":"Create secret in AWS Secrets Manager","text":"<ul> <li>Go to the AWS Management Console</li> <li>In the AWS Management Console, search for and select Secrets Manager</li> <li>On the Secrets Manager dashboard, click the \"Store a new secret\" button.</li> <li>You can store a variety of secrets, such as database credentials, API keys, or custom key-value pairs.</li> <li>Select Other type of secret.</li> <li>Input the keys and values you want to store. DB_PASSWORD=mypassword DB_USER=ersin</li> <li>Secret name=DB-CREDENTIAL</li> <li>Leave all settings as default and save.</li> </ul>"},{"location":"devops/devsecops/external-secret-operator/#create-a-user-in-aws","title":"Create a user in AWS","text":"<ul> <li>Navigate to IAM (Identity and Access Management)</li> <li>In the IAM dashboard, click on Users in the left-hand menu</li> <li>Click the Add user button.</li> <li>Enter a unique username for the new user</li> <li>Select Programmatic access.</li> <li>Set User Permissions via below policy.</li> </ul> <p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\"\n            ],\n            \"Resource\": \"&lt;ENTER-SECRET-MANAGER-ARN&gt;\"\n        }\n    ]\n}\n</code></pre> - Review the user details and permissions - Click Create user to finish. - On the confirmation page, you\u2019ll see the user's access key ID and secret access key. -  Download the credentials or save them securely, as you won\u2019t be able to retrieve the secret access key again.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#create-a-secret-store-in-kubernetes","title":"Create a Secret Store in Kubernetes","text":"<p>Create a YAML file for the Secret Store configuration. This will define how the External Secrets Operator interacts with AWS Secrets Manager.</p> <p>secret-store.yaml <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: secretstore-sample\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: us-east-1\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n</code></pre></p> <pre><code>kubectl apply -f secret-store.yaml\n</code></pre> <p></p> <p>spec: Defines the specification for the SecretStore. provider: Specifies the external secrets provider and its configuration. aws: Indicates that AWS Secrets Manager is the provider. service: Should be SecretsManager to specify the AWS Secrets Manager service. region: AWS region where the secrets are stored. auth: Authentication configuration for accessing AWS Secrets Manager. secretRef: Refers to the Kubernetes secret containing AWS credentials. accessKeyIDSecretRef: Refers to the Kubernetes secret and key storing the AWS access key ID. secretAccessKeySecretRef: Refers to the Kubernetes secret and key storing the AWS secret access key.</p>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-for-access-key-and-secret-key","title":"Create Secret for access key and secret key.","text":"<p>The External Secrets Operator needs to authenticate with AWS Secrets Manager to fetch secrets. The access key and secret key provide the credentials for this authentication.</p> <pre><code>kubectl create secret generic awssm-secret --from-literal=access-key=&lt;ACCESS-KEY&gt; --from-literal=secret-access-key=&lt;SECRET_KEY&gt; -n external-secrets\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-the-externalsecret","title":"Create the ExternalSecret","text":"<p>ExternalSecret retrieves secrets from AWS Secrets Manager and makes them available in your Kubernetes cluster.</p> <p>external-secret.yaml</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: external-secret-example\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore-sample\n    kind: SecretStore\n  target:\n    name: kube-secret\n    creationPolicy: Owner\n  dataFrom:\n  - extract:\n      key: DB-CREDENTIAL\n</code></pre> <p><pre><code>kubectl apply -f external-secret.yaml\n</code></pre> You should see Status as SecretSynced.</p> <p></p> <p>spec: Defines the specification for the ExternalSecret. refreshInterval: 1h means the secret will be refreshed every hour from AWS Secrets Manager. secretStoreRef: References the SecretStore to use for accessing AWS Secrets Manager. name: The name of the SecretStore resource (e.g., secretstore-sample). kind: Specifies the type of reference, which should be SecretStore. target: Defines the Kubernetes secret that will be created or updated. name: The name of the Kubernetes secret (e.g., kube-secret). creationPolicy: Determines when the Kubernetes secret is created, with Owner meaning the External Secrets Operator will manage its lifecycle. dataFrom: Specifies that the ExternalSecret should pull data from AWS Secrets Manager. extract: Defines the secret to extract. key: The name of the secret in AWS Secrets Manager (e.g., DB-CREDENTIAL).</p> <ul> <li>After you've created the ExternalSecret resource, you'll be able to see the new Kubernetes Secret that has been synchronized with the Secrets Manager store. Execute the following command:</li> </ul> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#consuming-secret-in-pod","title":"Consuming Secret in Pod","text":"<p>By syncing your AWS Secrets Manager secret to a Kubernetes Secret, External Secrets allows you to use and consume the secret in your Pod specification.</p> <p>We will deploy a simple busybox pod in the external-secrets namespace and use the secret via pod environment variables.</p> <p>Create a manifest file external-secrets-demo-pod.yaml with the following specifications.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: external-secrets\nspec:\n  containers:\n  - image-eso: busybox:1.35.0\n    command:\n      - sleep\n      - \"3600\"\n    image-esoPullPolicy: IfNotPresent\n    name: busybox\n    env:\n      - name: password\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_PASSWORD\n      - name: username\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_USER\n  restartPolicy: Always\n</code></pre> <ul> <li>To deploy the pod, run the following command:</li> </ul> <pre><code>kubectl apply -f external-secrets-demo-pod.yaml\n</code></pre> <ul> <li>Once the pod is in a running state, run the following commands to get the container\u2019s shell.</li> </ul> <pre><code>kubectl exec -it busybox -- sh\n</code></pre> <p>From the container\u2019s shell, you can use echo to print the environment variables to view the secrets.</p> <p></p>"},{"location":"devops/docker/portainer/","title":"Docker Management with Portainer","text":""},{"location":"devops/docker/portainer/#overview","title":"Overview","text":"<p>Portainer simplifies containerized application management across Docker,  Docker Swarm, Kubernetes, and Podman, whether in the Cloud, Hybrid Cloud, On-Premise, or Data Centers.  As a universal container management platform, it provides an intuitive interface that streamlines deployment  and monitoring, making containerization accessible to all. By removing complexity, Portainer enables users  to focus on innovation and efficiency.</p>"},{"location":"devops/docker/portainer/#why-portainer","title":"Why Portainer?","text":""},{"location":"devops/docker/portainer/#simplicity","title":"Simplicity","text":"<p>Portainer\u2019s user-friendly interface is designed with simplicity at its core. It eliminates the need for  extensive training, allowing teams to get up and running quickly.</p>"},{"location":"devops/docker/portainer/#universality","title":"Universality","text":"<p>Portainer is agnostic, supporting a wide range of container technologies and environments. It provides a  single pane of glass to manage your entire container ecosystem, regardless of the underlying infrastructure.</p>"},{"location":"devops/docker/portainer/#empowerment","title":"Empowerment","text":"<p>Portainer democratizes container management. It empowers developers, DevOps teams, and IT operations to  take control of their environments without needing to become container experts.</p>"},{"location":"devops/docker/portainer/#community-support","title":"Community &amp; Support","text":"<p>Built on a foundation of open-source collaboration, Portainer is backed by a passionate community and a  robust support network, ensuring that you\u2019re never alone on your container journey.</p>"},{"location":"devops/docker/portainer/#demo-prerequisites","title":"Demo - Prerequisites","text":"<ul> <li> <p>AWS Account</p> </li> <li> <p>VS Code with Remote SSH (Optional)</p> </li> </ul>"},{"location":"devops/docker/portainer/#demo","title":"Demo","text":""},{"location":"devops/docker/portainer/#part-1-setting-up-the-environment","title":"Part 1: Setting up the environment","text":"<ul> <li> <p>Sign in to <code>AWS Management Console</code> and go to <code>EC2</code>.</p> </li> <li> <p>From the menu on the left, choose <code>Instances</code>. Then, choose <code>Launch instances</code>.</p> </li> <li> <p>Launch an EC2 with the following configuration:</p> </li> </ul> <pre><code>Number of instances        : 1\nName and tags              : portainer-demo-instance\nAmazon Machine Image (AMI) : Ubuntu Server 24.04 LTS (HVM)\nInstance type              : t3.medium\nKey pair                   : &lt;your-key-pair&gt;\nVPC                        : Default VPC\nSecurity Group             : 22, 80, 443, 8000, 8080, 9000, 9443\nStorage                    : 30 GiB, gp3\nUser data                  : &lt;content-of-user-data&gt;\n</code></pre> <ul> <li> <p>You can find the content of <code>user-data.sh</code> in the same directory with <code>README.md</code>.</p> </li> <li> <p>Choose <code>Launch instance</code>.</p> </li> <li> <p>After the launch, choose the instance you created from the EC2 instance list.</p> </li> <li> <p>Then, choose <code>Connect</code>. The <code>Connect to instance</code> page is opened.</p> </li> <li> <p>Under the <code>EC2 Instance Connect</code> section, choose <code>Connect</code>.</p> </li> <li> <p>Alternatively, you can connect to EC2 using its Public IP from VS Code.</p> </li> </ul>"},{"location":"devops/docker/portainer/#part-2-configuring-the-portainer","title":"Part 2: Configuring the Portainer","text":"<ul> <li>Ensure that Docker and Docker Compose are safely installed.</li> </ul> <pre><code>docker --version\ndocker-compose --version\n</code></pre> <ul> <li>Create a working directory.</li> </ul> <pre><code>mkdir demo\ncd demo\n</code></pre> <ul> <li>In the working directory, create <code>docker-compose.yml</code>.</li> </ul> <pre><code>vi docker-compose.yml\n</code></pre> <ul> <li> <p>You can find the content of <code>docker-compose.yml</code> in the same directory with <code>README.md</code>.</p> </li> <li> <p>Copy and paste the content to <code>docker-compose.yml</code>.</p> </li> <li> <p>Create a container for Portainer.</p> </li> </ul> <pre><code>sudo docker-compose up -d\n</code></pre> <ul> <li>Copy and paste the following in a new tab on browser.</li> </ul> <pre><code>http://&lt;public-ip-of-ec2&gt;:9000\n</code></pre> <ul> <li>The login page of Portainer is opened.</li> </ul> <p></p> <ul> <li>Enter a username and a password as required.</li> </ul> <pre><code>Username: admin\nPassword: password1234\n</code></pre>"},{"location":"devops/docker/portainer/#part-3-exploring-the-features","title":"Part 3: Exploring the features","text":"<ul> <li> <p>After successful login, choose <code>Home</code> from the menu on the left.</p> </li> <li> <p>Notice the local environment for Docker.</p> </li> </ul> <p></p> <ul> <li>Choose the local environment for Docker and analyze the dashboard.</li> </ul> <p></p> <ul> <li>From the menu on the left, choose <code>Templates -&gt; Application</code>.</li> </ul> <p></p> <ul> <li>Notice the commonly used templates. Search for <code>Nginx</code> and choose it.</li> </ul> <pre><code>Name: demo-container\nShow advanced options -&gt; Port mapping: Host (8080) -&gt; Container (80)\n</code></pre> <ul> <li> <p>Choose <code>Deploy the container</code>. Notice that two containers are running.</p> </li> <li> <p>Copy and paste the following in a new tab on browser.</p> </li> </ul> <pre><code>http://&lt;public-ip-of-ec2&gt;:8080\n</code></pre> <ul> <li> <p>The <code>Welcome to nginx!</code> page is opened.</p> </li> <li> <p>Alternatively, you can list the containers from the terminal of instance.</p> </li> </ul> <pre><code>sudo docker ps\n</code></pre> <ul> <li> <p>In the <code>Container list</code> page of Portainer, locate the <code>Quick Actions</code> column.</p> </li> <li> <p>The icons for <code>Logs</code>, <code>Inspect</code>, <code>Stats</code>, <code>Exec Console</code> and <code>Attach Console</code> are given from left to right for each container, respectively.</p> </li> <li> <p>Choose <code>Logs</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the logs for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Inspect</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the configurations for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Stats</code> icon for <code>demo-container</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Analyze the statistics for the container. Then, go back to the <code>Container list</code> page.</p> </li> <li> <p>Choose <code>Exec Console</code> icon for <code>demo-container</code>. Then, choose <code>Connect</code>.</p> </li> </ul> <p></p> <ul> <li> <p>The terminal for container is opened.</p> </li> <li> <p>Don't forget to destroy the resources you created from AWS Management Console.</p> </li> </ul>"},{"location":"devops/falcon-logscale/agent-installation/","title":"Falcon LogScale Agent(Log Collector) Setup","text":"<p>First of all, you need to create a new repo from <code>Repositories and View</code>.After creating the repo, you will go into the repo.You need to enter <code>Settings</code> from the upper menus.You need to enter <code>Ingest Tokens</code> under the <code>Ingest</code> category in the left menu.Then you will create a new token with <code>Add New Token</code>.</p> <p>Remember that this token will be useful later.</p>"},{"location":"devops/falcon-logscale/agent-installation/#download-collector","title":"Download Collector","text":"<p>You should follow the steps below to reach the download page, select the package suitable for your system and download it.</p> <p>Main Page \u27a1\ufe0f Fleet Management \u27a1\ufe0f LogScale Collector Download</p> <p>Important Installations Notes</p> <p>You may get an error while downloading.This is because the download URL is wrong or incorrect.</p> <p>For example: <code>http://10.40.140.2:8080/None/api/v1/log-collector/download/humio-log-collector_1.4.1_linux_amd64.deb</code>. </p> <p>The <code>NONE</code> here may cause you to download an incorrect file.Delete it !.After downloading the file, check its size with the <code>ls -alh</code> command.  </p> <p>Follow these steps after downloading the file.</p> <pre><code>dpkg -i humio-log-collector_x.x.x_linux_amd64.deb\n</code></pre> <p>By default, the humio-log-collector process will run as the humio-log-collector user, which is installed by the package and won't have access to logs in <code>/var/log</code>.</p> <p>This can be granted by adding the user to the adm group. <pre><code>sudo usermod -a -G adm humio-log-collector\n</code></pre></p>"},{"location":"devops/falcon-logscale/agent-installation/#you-can-run-the-logscale-collector-as-a-standalone-process-and-ignore-the-service-file-etc","title":"You can run the LogScale Collector as a standalone process and ignore the service file etc.","text":"<pre><code>/etc/humio-log-collector/config.yaml\n</code></pre> <p>Open the source field and enter the token and ip address you created in the relevant fields.</p> <p>Remember the source option specifies where you want to get the logs from</p> <pre><code>sources:\n  var_log:\n    type: file\n    include: /var/log/*\n    exclude: /var/log/*.gz\n    sink: humio\nsinks:\n  humio:\n    type: humio\n    token: &lt;Ingest Token&gt; # 210f4309-0d71-4d3d-b4e3-d503d46b93b9\n    url: &lt;host ip-address of the humio&gt; # http://52.91.72.78:8080/\n</code></pre> <p>Now you need to start and enable the services to apply the changes</p> <p><pre><code>sudo systemctl start humio-log-collector.service\n</code></pre> <pre><code>sudo systemctl enable humio-log-collector.service\n</code></pre></p> <p>Check the status of the service</p> <pre><code>sudo systemctl status humio-log-collector.service\n</code></pre> <p>Success</p> <p>If Active:active (running), you can go to the repository from the interface and check your logs</p> <p>Important Installations Notes</p> <p>If you get your logs as <code>Docker Container</code>, you can get your logs without installing an agent by using the following command with <code>Driver:Splunk</code></p> <pre><code>export YOUR_LOGSCALE_URL=\"&lt;10.40.140.2:8080&gt;\"\nexport INGEST_TOKEN=\"&lt;210f4309-0d71-4d3d-b4e3-d503d46b93b9&gt;\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\n</code></pre>"},{"location":"devops/falcon-logscale/agent-installation/#to-generate-logs-with-docker-for-testing-you-can-generate-test-logs-using-the-following-docker-compose-commands","title":"To generate logs with <code>docker</code> for testing, you can generate test logs using the following <code>docker-compose</code> commands.","text":"<pre><code>export YOUR_LOGSCALE_URL=\"\"\nexport INGEST_TOKEN=\"\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  chentex-random-logger:\n    image: chentex/random-logger:latest\n    command: [\"100\", \"300\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-apache-common:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"apache_common\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-rfc5424:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"rfc5424\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-json:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"json\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"]\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\ndocker-compose up -d\ndocker-compose logs\n</code></pre>"},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/","title":"Falcon LogScale Setup With Docker","text":""},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/#falcon-logscale-setup-requirements","title":"Falcon LogScale Setup Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>30GB</code>      STORAGE <p> The first step to install LogScale using <code>Docker</code> is to <code>install Docker</code> on the machine where you want to run <code>Docker</code> with LogScale. You can Download Docker from their site or by using a package installation program like yum or apt-get.</p> <p>Or You Can Use These Command For Ubuntu:</p> <pre><code>sudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt install docker.io\nsystemctl start docker\nsystemctl enable docker\ndocker --version\n</code></pre> <p>Now let's run a container on port 8080 using the following commands and watch it with <code>docker ps</code></p> <p>Important Installations Notes</p> <p>If your machine is not open to port 8080, make it open</p> <pre><code>export HOST_DATA_DIR=/home/ubuntu/mounts/data\nexport HOST_KAFKA_DATA_DIR=/home/ubuntu/mounts/kafka-data\nexport PATH_TO_READONLY_FILES=/home/ubuntu/mounts/readonly\nexport HOST_ENV_FILE=/home/ubuntu/mounts/\n\nmkdir -p $PATH_TO_READONLY_FILES\nmkdir -p $HOST_KAFKA_DATA_DIR\nmkdir -p $HOST_DATA_DIR\ntouch $HOST_ENV_FILE.env\n\ndocker run -v $HOST_DATA_DIR:/data  \\\n   -v $HOST_KAFKA_DATA_DIR:/data/kafka-data  \\\n   -v $PATH_TO_READONLY_FILES:/etc/humio:ro  \\\n   -e AUTHENTICATION_METHOD=single-user \\\n   -e SINGLE_USER_USERNAME=hepapi \\\n   -e SINGLE_USER_PASSWORD=123456 \\\n   --net=host \\\n   --name=humio \\\n   --ulimit=\"nofile=8192:8192\"  \\\n   --stop-timeout 300 \\\n   --env-file=$HOST_ENV_FILE  \\\n   -d \\\n   -p 8080:8080 \\\n   humio/humio\n</code></pre> 'HOST_DATA_DIR, HOST_KAFKA_DATA_DIR, HOST_ENV_FILE, PATH_TO_READONLY_FILES'<pre><code>We bind these exported variables to the downloaded HUMIO container.\n</code></pre> <p>Info</p> <p>SINGLE_USER_USERNAME= Your Username for login </p> <p>SINGLE_USER_PASSWORD= Your Password for login</p> <p>Success</p> <p>Let's go your http://ip-address:8080 and enter activation key and enter your <code>SINGLE_USER_USERNAME</code> and <code>SINGLE_USER_PASSWORD</code></p>"},{"location":"devops/falcon-logscale/humio/","title":"Humio Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"    # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"    # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"    # Change here with ip of kafka3 node\n\nEXTERNAL_IP=3.76.209.91         # Change here with ip of humio node\nINTERNAL_IP=3.76.209.91         # Change here with ip of humio node\n\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\nHUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\" # If you'll use here change here with path of humio\nHUMIO_PACKAGE_PATH=\"\"   # If you'll use here change here with path of humio package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\n\n#### Java version control ####\njava --version\nsleep 10\n\n\n#### Hunio user access ####\nadduser humio --shell=/bin/false --no-create-home --system --group\n\n#### Kafka folders create and access ####\nmkdir -p /opt/humio /etc/humio/filebeat /data/log/humio /data/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /data/log/humio /data/humio/data\nsleep 2\n\n#### Humio tar.gz package download or move ####\ncd /opt/humio/\nsleep 2\n\nif [ -n \"$HUMIO_DOWNLOAD_LINK\" ]; then\n\n    cd /opt/humio &amp;&amp; wget $HUMIO_DOWNLOAD_LINK\n    tar zxf \"$(basename $HUMIO_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\n\nelse \n\n    cd /opt/humio &amp;&amp; cp -R $HUMIO_PACKAGE_PATH .\n    tar zxf $HUMIO_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\nfi\nsleep 2\n\n\n#### Humio server.conf create ####\ncd /etc/humio/\ncat &lt;&lt;EOF &gt;&gt; server.conf\nAUTHENTICATION_METHOD=single-user\nSINGLE_USER_USERNAME=admin\nSINGLE_USER_PASSWORD=admin\nDIRECTORY=/data/humio/data\nHUMIO_AUDITLOG_DIR=/data/log/humio\nHUMIO_DEBUGLOG_DIR=/data/log/humio\nJVM_LOG_DIR=/data/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\n\nKAFKA_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092\nEXTERNAL_URL=http://$EXTERNAL_IP:8080\nPUBLIC_URL=http://$INTERNAL_IP:8080\nEOF\nsleep 2\n\n#### Humio system service create ####\ncd /etc/systemd/system/\ncat &lt;&lt;EOF &gt;&gt; humio.service\n[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/data/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\nTimeoutSec=900\n\n[Install]\nWantedBy=default.target\nEOF\nsleep 2\n\n#### User access ####\nchown -R humio:humio /opt/humio /etc/humio\n\nsleep 2\n\nchown -R humio:humio /data/log/humio /data/humio/data\n</code></pre>"},{"location":"devops/falcon-logscale/humioinstallation/","title":"LogScale Installation","text":"<p>First, you'll need to create a non-administrative user named, <code>humio</code> to run LogScale software in the background. You can do this by executing the following from the command-line:  <pre><code>adduser humio --shell=/bin/false --no-create-home --system --group\n</code></pre></p> <p>You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install Kafka.</p> <p>Next, create the LogScale system directories and give the <code>humio</code> user ownership of them: <pre><code>mkdir -p /opt/humio /etc/humio/filebeat /var/log/humio /var/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /var/log/humio /var/humio/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/humioinstallation/#installation","title":"Installation","text":"<p>You're now ready to download and install LogScale's software. You should go to the LogScale directory and use wget to download the LogScale Java Archive. You can do this from the command-line like so:  <pre><code>cd /opt/humio/\n\nwget https://repo.humio.com/repository/maven-releases/com/humio/server/1.112.0/server-1.112.0.tar.gz\n\ntar xzf /opt/humio/server-1.112.0.tar.gz\n</code></pre> The wget here is used to download the latest release from Download Humio Server. You'll have to adjust the lines for the correct directory and file name, based on the version at the time. After you've downloaded it, enter the last line here to create a symbolic link to it. </p>"},{"location":"devops/falcon-logscale/humioinstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, create the LogScale configuration file, server.conf in the <code>/etc/humio</code> directory. There are a few environment variables you will need to enter in this configuration file in order to run LogScale on a single server or instance. Below are those basic settings: <pre><code>AUTHENTICATION_METHOD=single-user\nSINGLE_USER_PASSWORD=&lt;Your-Password-Here&gt;\nSINGLE_USER_USERNAME=&lt;Your-Username-Here&gt;\nBOOTSTRAP_HOST_ID=1\nDIRECTORY=/var/humio/data\nHUMIO_AUDITLOG_DIR=/var/log/humio\nHUMIO_DEBUGLOG_DIR=/var/log/humio\nJVM_LOG_DIR=/var/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\nZOOKEEPER_URL=127.0.0.1:2181\nKAFKA_SERVERS=127.0.0.1:9092\nEXTERNAL_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;:8080\nPUBLIC_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;\nHUMIO_SOCKET_BIND=0.0.0.0\nHUMIO_HTTP_BIND=0.0.0.0\n</code></pre> Next you should set up a service file. Using a simple text editor, create a file named, humio.service in the <code>/etc/systemd/system/</code> sub-directory. Add these lines to that file: <pre><code>[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/var/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\n\n[Install]\nWantedBy=default.target\n</code></pre>  You will need to change the ownership of the LogScale files and start the LogScale service. To change the ownership, execute the following two lines from the command-line: <pre><code>chown -R humio:humio /opt/humio /etc/humio/filebeat\nchown -R humio:humio /var/log/humio /var/humio/data\n</code></pre> You're ready to start LogScale. <pre><code>systemctl start humio\n</code></pre> Just to be sure LogScale is running and everything is fine, check it with the journalctl tool. You can do this by entering the following from the command-line <pre><code>journalctl -fu humio\n</code></pre> If there are no errors, open a web browser and enter the domain name or IP address with port 8080. For example, you would enter something like http://example.com:8080 in the browser's address field.</p>"},{"location":"devops/falcon-logscale/humiosetup/","title":"Humio Single Node Installation Guide","text":""},{"location":"devops/falcon-logscale/humiosetup/#logscale-installers-are-available-for-several-linux-distributions","title":"LogScale installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Red Hat</li> </ul>"},{"location":"devops/falcon-logscale/humiosetup/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/falcon-logscale/humiosetup/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>16GB</code>  MEMORY <code>8CPU</code>  CPU <code>100GB</code>      STORAGE <p>Access Permissions</p> <p>The machine to be installed must have access to the following addresses.</p> <p>http://humio.com</p> <p>https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz </p> <p>https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz [downloads.apache.org]</p> <p>hkp://keyserver.ubuntu.com:80</p> <p>http://repos.azulsystems.com/ubuntu [repos.azulsystems.com]</p> <p>https://repo.humio.com/repository/maven-releases/com/humio/server/1.117.0/server-1.117.0.tar.gz</p> <p>The following ports must be open; <pre><code>80,443,8080,1514\n</code></pre></p> <p>You must follow the sequence below to install Falcon LogScale.</p> <ul> <li>Java Installation Page</li> <li>Zookeeper Installation Page</li> <li>Kafka Installation Page</li> <li>LogScale Installation Page</li> </ul>"},{"location":"devops/falcon-logscale/javainstallation/","title":"Java Installation","text":"<p>Install at least java version 17 in the following order</p> <p>Download java package <pre><code>https://cdn.azul.com/zulu/bin/zulu17.48.15-ca-jdk17.0.10-linux_amd64.deb\n</code></pre></p> <p>Import Azul\u2019s public key: <pre><code>sudo apt install gnupg ca-certificates curl\n</code></pre> <pre><code>curl -s https://repos.azul.com/azul-repo.key | sudo gpg --dearmor -o /usr/share/keyrings/azul.gpg\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/azul.gpg] https://repos.azul.com/zulu/deb stable main\" | sudo tee /etc/apt/sources.list.d/zulu.list\n</code></pre></p> <p>Install the required Azul Zulu package: <pre><code>sudo apt install zulu17-jdk\n</code></pre> Check java version <pre><code>java -version\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafka/","title":"Kafka Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"     # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"     # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"     # Change here with ip of kafka3 node\n\nKAFKA_NODE_NUMBER=1             # Change here with id of kafka1 node\n\nKAFKA_PACKAGE_PATH=\"\"           # If you'll use here change here with path of kafka package\nKAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"  # If you'll use here change here with path of kafka package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\njava --version\nsleep 10\n\n#### Add IP Addresses to hosts file\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\n\n#### Kafka user access ####\nadduser kafka --shell=/bin/false --no-create-home --system --group\nsleep 2\n\n#### Kafka tar.gz package download or move ####\nif [ -n \"$KAFKA_DOWNLOAD_LINK\" ]; then\n\n    cd /opt &amp;&amp; wget $KAFKA_DOWNLOAD_LINK\n    tar zxf \"$(basename $KAFKA_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nelse \n\n    cd /opt &amp;&amp; cp -R $KAFKA_PACKAGE_PATH .\n    tar zxf $KAFKA_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nfi\nsleep 2\n\n\n#### Kafka folders create and access ####\nmkdir -p /data/log/kafka\nmkdir -p /data/log/zookeeper\nmkdir -p /data/kafka/kafka\nmkdir -p /data/kafka/zookeeper\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka\nsleep 2\n\n\n#### Link for kafka ####\nln -s \"/opt/$KAFKA_PACKAGE_FILE\" /opt/kafka\nsleep 2\n\n\n#### Server properties config ####\ncd /opt/kafka/config\nFILE=\"server.properties\"\n\n# New variables\nBROKER_ID=\"broker.id=$KAFKA_NODE_NUMBER\"\nNEW_LOG_DIRS=\"log.dirs=/data/kafka/kafka\"\nsleep 2\n\n\n# Update specific values \nsed -i \"s/^broker.id=.*/$BROKER_ID/\" \"$FILE\"\nsed -i \"s|^log.dirs=.*|$NEW_LOG_DIRS|\" \"$FILE\"\necho \"delete.topic.enable = true\" &gt;&gt; \"$FILE\"\nsleep 2\n\n\n#### Kafka user chown ####\nchown -R kafka:kafka /opt/kafka\nsleep 2\n\n\n#### Zookeper Config ####\ncd /opt/kafka/config\nrm -rf zookeeper.properties\ncat &lt;&lt;EOF &gt;&gt; zookeeper.properties\ndataDir=/data/kafka/zookeeper\nclientPort=2181\nmaxClientCnxns=0\nadmin.enableServer=false\nserver.1=kafka1:2888:3888\nserver.2=kafka2:2888:3888\nserver.3=kafka3:2888:3888\n4lw.commands.whitelist=*\ntickTime=2000\ninitLimit=5\nsyncLimit=2\nEOF\nsleep 2\n\n\n#### Zookeper chown ####\necho $KAFKA_NODE_NUMBER &gt;/data/kafka/zookeeper/myid\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n\n#### Zookeeper service config ####\ncd /etc/systemd/system\nrm -rf zookeeper.service\ncat &lt;&lt;EOF &gt;&gt; zookeeper.service\n[Unit]\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/zookeeper\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\nsleep 2\n\n#### Zookeeper service start ####\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n#### Kafka service config ####\ncd /etc/systemd/system\nrm -rf kafka.service\ncat &lt;&lt;EOF &gt;&gt; kafka.service\n[Unit]\n\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsleep 2\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka /opt/kafka\n</code></pre>"},{"location":"devops/falcon-logscale/kafkainstallation/","title":"Kafka Installation","text":"<p>LogScale recommend that the latest Kafka version be used with your LogScale deployment. The latest version of Kafka is available at Kafka Downloads</p> <p><pre><code>apt-get update\napt-get upgrade\n</code></pre>  Next, create a non-administrative user named, <code>kafka</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser kafka --shell=/bin/false --no-create-home --system --group\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafkainstallation/#installation","title":"Installation","text":"<p>To install Kafka, you'll need to go to the /opt directory and download the latest release. You can do that like so with wget.  <pre><code>cd /opt\nwget https://www-us.apache.org/dist/kafka/x.x.x/kafka_x.x.x.x.tgz\n</code></pre> You would adjust this last line, change the Xs to the latest version number. Once it downloads, untar the file and then create the directories it needs like this:  <pre><code>tar zxf kafka_x.x.x.x.tgz\n\nmkdir /var/log/kafka\nmkdir /var/kafka/data\nchown kafka:kafka /var/log/kafka\nchown kafka:kafka /var/kafka/data\n\nln -s /opt/kafka_x.x.x.x /opt/kafka\n</code></pre></p> <p>The four lines in the middle here create the directories for Kafka's logs and data, and changes the ownership of those directories to the kafka user. The last line creates a symbolic to /opt/kafka. You would adjust that, though, replacing the Xs with the version number.</p>"},{"location":"devops/falcon-logscale/kafkainstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, open the Kafka properties file, server.properties, located in the kafka/config sub-directory. You'll need to set a few options \u2014 the lines below are not necessarily the order in which they'll be found in the configuration file: <pre><code>broker.id=1\nlog.dirs=/var/kafka/data\ndelete.topic.enable = true\n</code></pre> The first line sets the broker.id value to match the server number <code>(myid)</code> you set when configuring ZooKeeper. The second sets the data directory. The third line should be added to the end of the configuration file. When you're finished, save the file and change the owner to the kafka user:  <pre><code>chown -R kafka:kafka /opt/kafka_x.x.x.x\n</code></pre></p> <p>You'll have to adjust this to the version you installed. Note, changing the ownership of the link <code>/opt/kafka</code> doesn't change the ownership of the files in the directory.</p> <p>Now you'll need to create a service file for starting Kafka. Use a simple text editor to create a file named, kafka.service in the <code>/etc/systemd/system/</code> sub-directory. Then add the following lines to the service file: <pre><code>[Unit]\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/var/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Now you're ready to start the Kafka service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start kafka\nsystemctl status kafka\nsystemctl enable kafka\n</code></pre></p>"},{"location":"devops/falcon-logscale/readme/","title":"Kafka Installation (kafka.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Ex: KAFKA_HOST_1=\"172.296.22.10\"</p> <p>Usage of KAFKA_NODE_NUMBER</p> <p>If you are installing more than one kafka, you must specify a node number for each kafka. The value you provide indicates the rank of the kafka node.</p> <p><code>KAFKA_NODE_NUMBER=&lt;Kafka Node Number&gt;</code></p>"},{"location":"devops/falcon-logscale/readme/#3-cluster-kafka-example","title":"3 Cluster Kafka Example:","text":"<p><code>(Primary Node kafka.sh)</code>   : KAFKA_NODE_NUMBER=1</p> <p><code>(Secondary Node kafka.sh)</code> : KAFKA_NODE_NUMBER=2</p> <p><code>(Third Node kafka.sh)</code> : KAFKA_NODE_NUMBER=3</p> <p>Usage of KAFKA_DOWNLOAD_LINK / KAFKA_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"KAFKA_DOWNLOAD_LINK\"</code> variable and leave the <code>\"KAFKA_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"KAFKA_PACKAGE_PATH\"</code> variable and leave the <code>\"KAFKA_PACKAGE_URL\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>KAFKA_PACKAGE_PATH=\"\" KAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"</p> <p>Example Usage (We will use PATH):</p> <p>KAFKA_PACKAGE_PATH=\"/tmp/packages/kafka_2.13-3.7.0.tgz\" KAFKA_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#humio-setup-humiosh","title":"Humio Setup (humio.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code> KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code> KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Usage of EXTERNAL_IP / INTERNAL_IP</p> <p>To access the Humio interface:</p> <p>EXTERNAL_IP=<code>\"&lt;Humio External IP Address&gt;\"</code></p> <p>INTERNAL_IP=<code>\"&lt;Humio Internal IP Address&gt;\"</code></p> <p>Usage of HUMIO_DOWNLOAD_LINK / HUMIO_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable and leave the <code>\"HUMIO_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"HUMIO_PACKAGE_PATH\"</code> variable and leave the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>HUMIO_PACKAGE_PATH=\"\" HUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\"</p> <p>Example Usage (We will use PATH):</p> <p>HUMIO_PACKAGE_PATH=\"/tmp/packages/server-1.131.1.tar.gz\" HUMIO_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#run-the-services","title":"Run The Services","text":"<p>After the installation is completed, quickly run the following commands one by one.</p> <p>For each node (node1, node2, node3)</p> <p><code>systemctl enable zookeeper</code></p> <p><code>systemctl start zookeeper</code></p> <p><code>systemctl status zookeeper</code></p> <p><code>systemctl enable kafka</code></p> <p><code>systemctl start kafka</code></p> <p><code>systemctl status kafka</code></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/","title":"Zookeeper Installation","text":"<p><pre><code>apt-get update\napt-get upgrade\n</code></pre> Next, create a non-administrative user named, <code>zookeeper</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser zookeeper --shell=/bin/false --no-create-home --system --group\n</code></pre> You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install ZooKeeper.</p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#installation","title":"Installation","text":"<p>Navigate to opt directory and download a of ZooKeeper. The official release site is Apache Zookeeper Release <pre><code>cd /opt\nwget https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz\n</code></pre> After the file downloads, untar the ZooKeeper file and create a symbolic to <code>/opt/zookeeper</code> like so:  <pre><code>tar -zxf apache-zookeeper-x.x.x-bin.tar.gz\nln -s /opt/apache-zookeeper-x.x.x-bin /opt/zookeeper\n</code></pre> Navigate to zookeeper sub-directory and create a data directory for ZooKeeper:  <pre><code>cd /opt/zookeeper\nmkdir -p /var/zookeeper/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#configuration","title":"Configuration","text":"<p>Using a text editor, create the ZooKeeper configuration file in the conf sub-directory. Name the file, zoo.cfg. For example, <code>/opt/zookeeper/conf/zoo.cfg</code>. Copy the lines below into that file: <pre><code>tickTime = 2000\ndataDir = /var/zookeeper/data\nclientPort = 2181\ninitLimit = 5\nsyncLimit = 2\nmaxClientCnxns=60\nautopurge.purgeInterval=1\nadmin.enableServer=false\n4lw.commands.whitelist=*\nserver.1=127.0.0.1:2888:3888\nadmin.enableServer=false\n</code></pre> Create a myid file in the data sub-directory with just the number 1 as its contents. They you can start ZooKeeper to verify that the configuration is working:</p> <p><pre><code>bash -c 'echo 1 &gt; /var/zookeeper/data/myid'\n\n./bin/zkServer.sh start\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-x.x.x/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n</code></pre> Stop ZooKeeper and change the ownership of the zookeeper directory like so, adjusting for the version number you installed:  <pre><code>./bin/zkServer.sh stop\n\nchown -R zookeeper:zookeeper /opt/apache-zookeeper-x.x.x\nchown -R zookeeper:zookeeper /var/zookeeper/data\n</code></pre></p> <p>So that ZooKeeper will start when the server is rebooted, you'll need to create a ZooKeeper service file named zookeeper.service in the <code>/etc/systemd/system/</code> sub-directory. Use a text editor to create the file and copy the following lines into it. <pre><code>[Unit]\nDescription=ZooKeeper Daemon\nDocumentation=http://zookeeper.apache.org\nRequires=network.target\nAfter=network.target\n\n[Service]\nType=forking\nWorkingDirectory=/opt/zookeeper\nUser=zookeeper\nGroup=zookeeper\nExecStart=/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg\nExecStop=/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg\nExecReload=/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg\nTimeoutSec=30\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre> Start the ZooKeeper service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start zookeeper\nsystemctl status zookeeper\nsystemctl enable zookeeper\n</code></pre></p>"},{"location":"devops/git/Commands/","title":"Commands","text":"Command Explanation Usage push Pushing is the process of sending local commits to a remote repository. <code>git push origin branch_name</code> pull Pulling is the process of fetching and merging remote changes into the local repository. <code>git pull origin branch_name</code> add . Adds all modified and new files to the staging area. <code>git add .</code> add  Adds a specific file to the staging area. <code>git add file_name</code> commit A commit is a snapshot of changes made to the repository. <code>git commit -m \"Commit message\"</code> init Initializes a new Git repository in the current directory. <code>git init</code> log Log displays the commit history of the repository. <code>git log</code> status Status shows the current state of the repository. <code>git status</code> branch A branch is a separate line of development within a repository. <code>git branch branch_name</code> merge Merging combines changes from different branches into a single branch. <code>git merge branch_name</code> revert Revert creates a new commit that undoes changes from a previous commit. <code>git revert commit_hash</code> reset Reset moves the current branch pointer to a specific commit, potentially discarding commits. <code>git reset commit_hash</code> rm / remove Removes a file from the repository and the working directory. <code>git rm file_name</code> mv / move Renames or moves a file or directory within the repository. <code>git mv old_file_name new_file_name</code> clone Cloning creates a local copy of a remote repository. <code>git clone repository_url</code> echo Echo prints a message or value to the terminal or a file. <code>echo \"Hello, World!\"</code> touch Touch creates an empty file or updates the timestamp of an existing file. <code>touch file_name</code> ls / list List displays the files and directories in the current directory. <code>ls</code> or <code>list</code> cat Cat displays the contents of a file. <code>cat file_name</code> diff Diff shows the differences between different versions of files. <code>git diff</code> checkout Checkout allows you to switch between branches or restore files from previous commits. <code>git checkout branch_name</code> .gitignore A .gitignore file specifies files and patterns to be ignored by Git. Create a .gitignore file and list files/patterns to ignore repository A repository is a location where Git stores all the files, history, and changes for a project. <code>git init</code> fork Forking creates a copy of a repository under your GitHub account. Click on the \"Fork\" button in the GitHub UI pull request A pull request proposes changes from a forked repository to the original repository. Create a pull request through the GitHub UI PR (Pull Request) PR is an abbreviation for pull request. Use the term \"PR\" interchangeably with \"pull request\" master Master is the default branch in Git. The initial branch created in a repository (commonly used) config Config sets configuration options for Git. <code>git config --global user.name \"Your Name\"</code> remote Remote refers to a remote repository, typically on a server. <code>git remote add origin repository_url</code> stash Stash temporarily saves local modifications for later use. <code>git stash save \"Stash message\"</code> pop Pop applies the most recent stash and removes it from the stash list. <code>git stash pop</code> reflog Reflog shows a log of all reference updates in the repository. <code>git reflog</code> Action Explanation Usage Create and Switch to New Branch Creates a new branch and switches to it. <code>git checkout -b &lt;branch&gt;</code> Merge Branch into Main Branch Merges changes from a feature branch into the main branch. <code>git checkout &lt;main_branch&gt;</code><code>git merge &lt;feature_branch&gt;</code> Rebase Branch onto Main Branch Updates the feature branch with the latest changes from the main branch. <code>git checkout &lt;feature_branch&gt;</code><code>git rebase &lt;main_branch&gt;</code> Interactive Rebase Allows interactive modification, reordering, or squashing of commits. <code>git rebase -i HEAD~&lt;number_of_commits&gt;</code> Revert a Commit Creates a new commit that undoes the changes from a specific commit. <code>git revert &lt;commit_hash&gt;</code> Undo Last Commit (Keep Changes) Moves the branch pointer back one commit, keeping changes in the staging area. <code>git reset --soft HEAD~1</code> Discard Last Commit (Lose Changes) Moves the branch pointer back one commit, discarding changes in the working directory and staging area. <code>git reset --hard HEAD~1</code>"},{"location":"devops/git/Description/","title":"Description","text":""},{"location":"devops/git/Description/#version-control-system-vcs","title":"Version Control System (VCS)","text":"<p>A Version Control System (VCS) is a software tool that helps track changes made to files and directories over time. It allows multiple people to collaborate on a project, keeping a history of changes, and providing mechanisms to manage different versions of files. VCS enables teams to work concurrently, facilitating efficient collaboration and providing features like branching, merging, and conflict resolution.</p>"},{"location":"devops/git/Description/#git","title":"Git","text":"<p>Git is a widely used distributed version control system designed for speed, flexibility, and data integrity. It is free and open-source, offering powerful features that make it popular among individuals and large organizations. Git provides a decentralized approach, allowing users to have a local copy of the entire repository, including its history, branches, and tags.</p>"},{"location":"devops/git/installation/","title":"Installation","text":""},{"location":"devops/git/installation/#downloading-git","title":"Downloading Git","text":"<p>To download Git for different operating systems, follow these instructions:</p>"},{"location":"devops/git/installation/#windows","title":"Windows","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the latest Git version for Windows.</li> <li>Run the installer and follow the prompts.</li> <li>Choose the desired installation options and complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#macos","title":"macOS","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the macOS version of Git.</li> <li>Run the installer package and follow the prompts.</li> <li>Complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#linux-ubuntu","title":"Linux (Ubuntu)","text":"<ul> <li>Open the terminal.</li> <li>Install Git using the package manager:</li> <li>For Debian/Ubuntu-based systems: <code>sudo apt-get install git</code></li> <li>For Fedora: <code>sudo dnf install git</code></li> <li>For CentOS/RHEL: <code>sudo yum install git</code></li> </ul>"},{"location":"devops/git/installation/#linux-generic","title":"Linux (Generic)","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the source code archive for Linux.</li> <li>Extract the archive to a desired location.</li> <li>In the terminal, navigate to the extracted directory.</li> <li>Run the following commands:</li> <li><code>make prefix=/usr/local all</code></li> <li><code>sudo make prefix=/usr/local install</code></li> </ul>"},{"location":"devops/git/installation/#verification","title":"Verification","text":"<p>Once Git is installed on your system, you can verify the installation by opening a terminal and running <code>git --version</code> to display the installed version.</p>"},{"location":"devops/git/installation/#interfaces","title":"Interfaces","text":"<p>Git provides a command-line interface (CLI) and various graphical user interface (GUI) tools. You can choose the interface that suits your preference and start using Git for version control in your projects.</p>"},{"location":"devops/gitlab/gitlab-runner-installation/","title":"GitLab Runner Installation Guide","text":"<p>Hello everyone from Hepapi! Today, we will discuss how to install GitLab Runner on a self-hosted GitLab instance.  </p> <p>In modern DevOps workflows, GitLab Runner plays a critical role in executing CI/CD pipelines. Whether you need to run jobs on Kubernetes, Docker, or bare-metal environments, installing GitLab Runner correctly ensures that your builds, tests, and deployments are executed efficiently.  </p> <p>In this guide, we'll walk through the step-by-step installation process using Helm, making it easy to deploy, configure, and manage GitLab Runner in your Kubernetes cluster.  </p> <p>By the end of this tutorial, you'll have a fully functional GitLab Runner connected to your GitLab instance, ready to process CI/CD jobs seamlessly.  </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#1-add-the-gitlab-helm-repository","title":"1. Add the GitLab Helm Repository","text":"<p>Before installing GitLab Runner, ensure that the official GitLab Helm repository is added and updated in your system:  </p> <pre><code>helm repo add gitlab https://charts.gitlab.io\nhelm repo update\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#2-get-the-gitlab-runner-token","title":"2. Get the GitLab Runner Token","text":"<p>To register a new GitLab Runner, follow these steps:  </p> <ol> <li>Navigate to GitLab \u2192 Settings \u2192 CI/CD.  </li> <li>Scroll down to the Runners section.  </li> <li>Click on New instance runner.  </li> <li>Add a tag to the runner (e.g., <code>kubernetes-runner</code>).  </li> <li>Click Create Runner and save the generated token.  </li> </ol> <p>You'll need this runner token in the next step to connect GitLab Runner to your instance.  </p> <p> </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#3-install-gitlab-runner-with-helm","title":"3. Install GitLab Runner with Helm","text":"<p>Now, install GitLab Runner on your Kubernetes cluster using Helm. Replace <code>&lt;your-gitlab-url&gt;</code> with your GitLab instance URL and <code>&lt;your-runner-token&gt;</code> with the token you saved in the previous step.  </p> <pre><code>helm install --namespace gitlab gitlab-runner gitlab/gitlab-runner \\\n  --set gitlabUrl=&lt;your-gitlab-url&gt; \\\n  --set runnerToken=&lt;your-runner-token&gt;\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#example","title":"Example:","text":"<pre><code>helm install --namespace gitlab gitlab-runner gitlab/gitlab-runner \\\n  --set gitlabUrl=https://gitlab.example.com \\\n  --set runnerToken=your-unique-runner-token\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#4-verify-the-gitlab-runner-deployment","title":"4. Verify the GitLab Runner Deployment","text":"<p>Once the installation is complete, check if the GitLab Runner pods are running correctly:  </p> <pre><code>kubectl get pods -n gitlab\n</code></pre> <p>You should see an output similar to this:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\ngitlab-runner-56f7cbb6d8-tlqxm          1/1     Running   0          2m\n</code></pre> <p>If the runner is not running, check the logs for any issues:  </p> <pre><code>kubectl logs -n gitlab -l app=gitlab-runner\n</code></pre>"},{"location":"devops/gitlab/gitlab-runner-installation/#5-confirm-runner-registration-in-gitlab","title":"5. Confirm Runner Registration in GitLab","text":"<p>Go to GitLab \u2192 Settings \u2192 CI/CD \u2192 Runners, and you should see your new runner online with the tag you assigned earlier.  </p> <p>This means your GitLab Runner is successfully installed and ready to execute CI/CD jobs! \ud83c\udf89  </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#6-optional-customize-gitlab-runner-configuration","title":"6. Optional: Customize GitLab Runner Configuration","text":"<p>You can customize your GitLab Runner setup by modifying values in the Helm chart. For more details and how to enable docker in docker, refer to the GitLab Runner Helm Chart Configuration documentation. </p>"},{"location":"devops/gitlab/gitlab-runner-installation/#conclusion","title":"Conclusion","text":"<p>By following this guide, you have successfully installed GitLab Runner in your Kubernetes cluster using Helm. This setup enables you to leverage scalable and automated CI/CD pipelines while keeping infrastructure maintenance minimal.  </p> <p>Feel free to explore additional configurations based on your requirements, such as autoscaling, custom runner tags, and integration with Kubernetes-native workloads.  </p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/","title":"GitLab Self-Hosted on Kubernetes - Installation Guide","text":"<p>Hello everyone from Hepapi. Today, we will discuss GitLab Self-Hosted on Kubernetes. We are in an era where DevOps automation and containerized applications play a crucial role in software development. Deploying and managing a self-hosted GitLab instance on Kubernetes provides scalability, flexibility, and security for your CI/CD pipelines. Compared to traditional single-node installations, a Kubernetes-based deployment ensures high availability and resilience. However, managing such deployments manually can be complex and time-consuming. Thanks to GitLab Helm Charts, we can simplify the installation and configuration process while maintaining consistency and reusability across different environments.</p> <p>In our modern development landscape, where applications are increasingly built as microservices, deploying GitLab in a self-hosted environment provides a robust and scalable solution for your DevOps needs. Instead of wrestling with complex installation procedures, this guide simplifies the process into clear, modular steps\u2014from adding the GitLab Helm repository to installing GitLab with your custom configurations, and finally verifying the deployment. With this streamlined approach, you can quickly harness GitLab\u2019s powerful features for continuous integration and delivery while keeping your setup clean and maintainable.</p> <p></p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#1-add-the-gitlab-helm-repository","title":"1. Add the GitLab Helm Repository","text":"<p>First, add the official GitLab Helm repository and update your local Helm chart list:</p> <pre><code>helm repo add gitlab https://charts.gitlab.io/\nhelm repo update\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#2-install-gitlab-with-helm","title":"2. Install GitLab with Helm","text":"<p>To install GitLab on your Kubernetes cluster using Helm, run:</p> <pre><code>helm install gitlab gitlab/gitlab \\\n  --set global.hosts.domain=example.com \\\n  --set certmanager-issuer.email=me@example.com\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#using-an-existing-cert-manager","title":"Using an Existing Cert-Manager","text":"<p>If you already have Cert-Manager installed in your cluster, disable its installation by adding:</p> <pre><code>--set certmanager.install=false\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#disabling-runner-install","title":"Disabling runner install","text":"<p>If you want to install gitlab-runner seperately you can disable runner install with:</p> <pre><code>--set gitlab-runner.install=false\n</code></pre> <p>You can checkout all deployment options from here.</p>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#3-verify-the-deployment","title":"3. Verify the Deployment","text":"<p>Check if the GitLab pods are running with:</p> <pre><code>kubectl get pods -n gitlab\n</code></pre>"},{"location":"devops/gitlab/gitlab-self-hosted-installation/#4-retrieve-the-gitlab-root-password","title":"4. Retrieve the GitLab Root Password","text":"<p>To get the initial GitLab root password, use the following command:</p> <pre><code>kubectl get secret gitlab-gitlab-initial-root-password -ojsonpath='{.data.password}' | base64 --decode ; echo\n</code></pre> <p></p> <p></p>"},{"location":"devops/jenkins/","title":"Jenkins","text":""},{"location":"devops/jenkins/jenkins-installation/","title":"Jenkins Install","text":""},{"location":"devops/jenkins/jenkins-installation/#jenkins-installers-are-available-for-several-linux-distributions","title":"Jenkins installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Fedora</li> <li>Red Hat/Alma/Rocky</li> </ul>"},{"location":"devops/jenkins/jenkins-installation/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/jenkins/jenkins-installation/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>256MB</code>  MEMORY <code>512MB</code>  CPU <code>1GB</code>      STORAGE <p>1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container)</p>"},{"location":"devops/jenkins/jenkins-installation/#recommended-hardware-configuration-for-a-small-team","title":"Recommended hardware configuration for a small team:","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>50GB</code>      STORAGE"},{"location":"devops/jenkins/jenkins-installation/#installation-of-java","title":"Installation of Java","text":"<p><code>Jenkins requires Java in order to run, yet certain distributions don\u2019t include this by default and some Java versions are incompatible with Jenkins. There are multiple Java implementations which you can use. OpenJDK is the most popular one at the moment, we will use it in this guide. Update the Debian apt repositories, install OpenJDK 11, and check the installation with the commands:</code></p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jre\njava -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment (build 11.0.12+7-post-Debian-2)\nOpenJDK 64-Bit Server VM (build 11.0.12+7-post-Debian-2, mixed mode, sharing)\n</code></pre> <p>After installing Java without any problems, we will install jenkins</p>"},{"location":"devops/jenkins/jenkins-installation/#jenkins-install_1","title":"Jenkins Install","text":"<pre><code>curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install jenkins\n</code></pre> <p>Quote</p> <p>If Jenkins fails to start because a port is in use, run -- systemctl edit jenkins -- and add the following ; <pre><code>[Service]\nEnvironment=\"JENKINS_PORT=8081\"\n</code></pre></p>"},{"location":"devops/jenkins/jenkins-installation/#start-jenkins","title":"Start Jenkins","text":"<p>You can enable the Jenkins service to start at boot with the command: <pre><code>sudo systemctl enable jenkins\n</code></pre></p> <p>You can start the Jenkins service with the command: <pre><code>sudo systemctl start jenkins\n</code></pre></p> <p>You can check the status of the Jenkins service using the command: <pre><code>sudo systemctl status jenkins\n</code></pre></p> <p>If everything has been set up correctly, you should see an output like this: <pre><code>Loaded: loaded (/lib/systemd/system/jenkins.service; enabled; vendor preset: enabled)\nActive: active (running) since Tue 2018-11-13 16:19:01 +03; 4min 57s ago\n</code></pre></p> <p>If you have a firewall installed, you must add Jenkins as an exception. You must change YOURPORT in the script below to the port you want to use. Port 8080 is the most common. <pre><code>YOURPORT=8080\nPERM=\"--permanent\"\nSERV=\"$PERM --service=jenkins\"\n\nfirewall-cmd $PERM --new-service=jenkins\nfirewall-cmd $SERV --set-short=\"Jenkins ports\"\nfirewall-cmd $SERV --set-description=\"Jenkins port exceptions\"\nfirewall-cmd $SERV --add-port=$YOURPORT/tcp\nfirewall-cmd $PERM --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n</code></pre></p> <ul> <li> After installing Jenkins, we go to the instance ip address e.g : <code>&lt; ip-address &gt;:8080</code> </li> <li> <p> On the page that opens, it asks us to enter a token. We take this token from the server with the <code>cat /var/lib/jenkins/secrets/initialAdminPassword</code> code and paste it into the input and log in.</p> </li> <li> <p> Jenkins gives you two options.</p> <ul> <li> <code>Install Suggested Plugins</code></li> <li> <code>Select Plugins To Install</code></li> </ul> </li> </ul> <p>If you don't see jenkins when you go to your server's ip address, allow port 8080</p> <p> You can login to Jenkins by choosing the one you want and creating Username and Password.</p> <p>For more installation details</p>"},{"location":"devops/jenkins/jenkins-installation/#happy-jenkins","title":"Happy Jenkins","text":""},{"location":"devops/jenkins/shared-library/","title":"Jenkins Shared Library","text":"<p>Hello everybody from Hepapi.We will talk about Jenkins Shared Library.Lets start. We are in a period where modern applications are generally divided into small components and run on a microservice architecture. Compared to a monolithic application, there are many extra pipelines in an application with microservice architecture. Therefore, it is very important to ensure modularity and reusability of the created pipelines. Thanks to Jenkins Shared Library, we can get rid of the code complexity in the pipeline and comply with the DRY (Don't Repeat Yourself) principle.</p> <p>Example: You can perform the docker build process that you perform jointly for more than one service by simply sending the docker image information to a single function for all services.</p> <pre><code>// var/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String REPOSITORY) {\n  String REGISTRY = \"ersinsari\"\n  sh \"docker build -t ${REGISTRY}/${REPOSITORY}:latest .\"\n}\n// Jenkinsfile\n@Library('mylibrary') _\npipeline {\n  stages {\n    stage('Docker Build') {\n      dockerBuild \"nodejs-helloworld\"    \n    }\n  }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#folder-structure-of-shared-library","title":"Folder Structure of Shared Library","text":"<pre><code>\u2514\u2500\u2500 jenkins-shared\n    \u251c\u2500\u2500 src\n    \u2502   \u251c\u2500\u2500 main\n    \u2502       \u2514\u2500\u2500 groovy\n    |           \u2514\u2500\u2500 *.groovy\n    \u251c\u2500\u2500 vars           \n    |   \u251c\u2500\u2500 *.groovy\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 config.properties\n        \u2514\u2500\u2500 template.xml\n</code></pre> <ul> <li> <p>The src directory is structured like a standard Java project. This means that you can use the import statement to import classes from other directories in the src directory.</p> </li> <li> <p>The vars directory is a special directory that contains global variables that are defined in the shared library. These variables can be accessed from any Jenkins job that imports the shared library.</p> </li> <li> <p>The resources directory is a regular directory that can contain any type of file. However, it is typically used to store static resources that are used by the shared library.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#global-shared-libraries","title":"Global Shared Libraries","text":"<p>There are several places where Shared Libraries can be defined, depending on the use-case</p> <ul> <li>Manage Jenkins \u00bb System \u00bb Global Pipeline Libraries as many libraries as necessary can be configured</li> </ul> <p>These libraries are considered \"trusted:\" they can run any methods in Java, Groovy, Jenkins internal APIs, Jenkins plugins, or third-party libraries. This allows you to define libraries which encapsulate individually unsafe APIs in a higher-level wrapper safe for use from any Pipeline. Beware that anyone able to push commits to this SCM repository could obtain unlimited access to Jenkins. You need the Overall/RunScripts permission to configure these libraries (normally this will be granted to Jenkins administrators).</p>"},{"location":"devops/jenkins/shared-library/#folder-level-shared-libraries","title":"Folder-level Shared Libraries","text":"<p>Any Folder created can have Shared Libraries associated with it. This mechanism allows scoping of specific libraries to all the Pipelines inside of the folder or subfolder.Folder-based libraries are not considered \"trusted:\" they run in the Groovy sandbox just like typical Pipelines.</p>"},{"location":"devops/jenkins/shared-library/#automatic-shared-libraries","title":"Automatic Shared Libraries","text":"<p>Other plugins may add ways of defining libraries on the fly. For example, the Pipeline: GitHub Groovy Libraries plugin allows a script to use an untrusted library named like github.com/someorg/somerepo without any additional configuration. In this case, the specified GitHub repository would be loaded, from the master branch, using an anonymous checkout.</p>"},{"location":"devops/jenkins/shared-library/#using-libraries","title":"Using Libraries","text":"<p>Shared Libraries marked Load implicitly allows Pipelines to immediately use classes or global variables defined by any such libraries. To access other shared libraries, the Jenkinsfile needs to use the @Library annotation, specifying the library\u2019s name:</p> <pre><code>@Library('my-shared-library') _\n\n/* Using a version specifier, such as branch, tag, etc */\n@Library('my-shared-library@1.0') _\n\n/* Accessing multiple libraries with one statement */\n@Library(['my-shared-library', 'otherlib@abc1234']) _\n</code></pre>"},{"location":"devops/jenkins/shared-library/#loading-libraries-dynamically","title":"Loading libraries dynamically","text":"<p>As of version 2.7 of the Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time during the build.</p> <p>If you are only interested in using global variables/functions (from the vars/ directory), the syntax is quite simple:</p> <pre><code>library 'my-shared-library'\n</code></pre> <p>Thereafter, any global variables from that library will be accessible to the script.</p> <p>Using classes from the src/ directory is also possible, but trickier. Whereas the @Library annotation prepares the \u201cclasspath\u201d of the script prior to compilation, by the time a library step is encountered the script has already been compiled. Therefore you cannot import or otherwise \u201cstatically\u201d refer to types from the library.</p>"},{"location":"devops/jenkins/shared-library/#demo","title":"DEMO","text":"<p>First of all we need jenkins server for this demo.There are some options to deploy jenkins-server</p> <pre><code>https://www.jenkins.io/doc/book/installing/\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-1-creating-shared-library","title":"Step-1 Creating Shared library","text":"<p>Let's first create the groovy scripts of the Jenkins shared library and push them to git. We have two simple scripts for this example. We will perform docker build and docker push operations using these scripts.</p> <pre><code>// vars/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n    dir(\"${WORKSPACE}\") {\n        sh \"docker build -t ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER) .\"\n    }\n}\n</code></pre> <pre><code>// vars/dockerPush.groovy\n#!/usr/bin/env groovy\n\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n\n    dir(\"${WORKSPACE}\") {\n        sh \"echo $DOCKERHUB_CRED_PSW | docker login -u $DOCKERHUB_CRED_USR --password-stdin\"\n        sh \"docker push ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER)\"\n    }\n}\n</code></pre> <pre><code>// vars/sayHello.groovy\n#!/usr/bin/env groovy\n\ndef call(String name = 'human') {\n  echo \"Hello, ${name}.\"\n}\n</code></pre> <p>Go to the Global Pipeline Library to Jenkins. After accessing the Jenkins dashboard, navigate to Manage Jenkins &gt;  System to find the Global Pipeline Libraries section and click on the add button to add a new library. Manage Jenkins ---&gt; System ---&gt; Global Pipeline Libraries ---&gt; Click Add button</p> <ul> <li>Library Name: my-shared-library</li> <li>Default version: main</li> <li>Retrivial method: Modern SCM</li> <li>Source Code Management</li> <li> <p>Project Repository: https://github.com/ersinsari13/jenkins-shared-library.git  #enter your repo name</p> </li> <li> <p>We can leave other parameters as default and save the configuration.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-2-creating-jenkinsfile","title":"Step-2 Creating jenkinsfile","text":"<p>Let's first create github repo named docker-build-push then create the Jenkinsfile and push them to git.</p> <pre><code>@Library('my-shared-library') _\npipeline {\n    agent any\n\n    environment {\n        DOCKERHUB_CRED=credentials('docker-hub-credential')\n    }\n    stages {\n        stage('Set Environment') {\n            steps {\n                script {\n                    REGISTRY = \"ersinsari\"\n                    REPOSITORY = \"docker-build-push\"\n                }\n            }\n        }\n        stage('Hello') {\n            steps {\n                sayHello \"hepapi\"\n            }\n        }\n        stage('Build') {\n            steps {\n                dockerBuild \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n        stage('Push') {\n            steps {\n                dockerPush \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-3-creating-pipeline","title":"Step-3 Creating Pipeline","text":"<ul> <li>Go to Jenkins Server Dashboard</li> <li>Click New Item</li> <li>Enter \"docker-build-push\" as a name and select pipeline and click Ok</li> <li> <p>Under Pipeline Section:</p> </li> <li> <p>Definition: Pipeline script from SCM</p> </li> <li>SCM: Git</li> <li>Repository URL: https://github.com/ersinsari13/docker-build-push.git  #enter your repository name</li> <li>Branch: main</li> </ul> <p>Since the Jenkinsfile in the repo is not under any folder, it will be sufficient to just specify its name. For example, if my Jenkinsfile file was located under a folder named build; I should have written build/Jenkinsfile in the Script Path section.</p> <ul> <li>Save the pipeline configuration</li> </ul>"},{"location":"devops/jenkins/shared-library/#step-4-credential-for-dockerhub","title":"Step-4 Credential for Dockerhub","text":"<p>In order for the images we created in Pipeline to be pushed to the registry, we need to provide registry credential information to the Jenkins server.</p> <ul> <li>Go to Jenkins Dashboard</li> <li>Select Manage Jenkins</li> <li>Click Credentials</li> <li>Select Global</li> <li> <p>Click Add Credentials button:</p> </li> <li> <p>Kind: Username with password</p> </li> <li>username: enter your dockerhub registry username</li> <li>password: enter your dockerhub registry password</li> <li>id: dockerhub-registry-credentials</li> <li> <p>description: dockerhub-registry-credentials</p> </li> <li> <p>Save credentials configuration</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-5-build-pipeline","title":"Step-5 Build Pipeline","text":"<p>After all the steps, the pipeline we created is now ready to be tested.</p> <ul> <li>Go to Jenkins Server Dashboard</li> <li>Select Pipeline was created</li> <li>Click Build Now</li> <li>You should see the pipeline result success</li> <li>You can check your new image on the Dockerhub registry</li> </ul>"},{"location":"devops/k8s-engine/k3s-installation/","title":"K3S Setup","text":""},{"location":"devops/k8s-engine/k3s-installation/#k3s-requirements","title":"K3S Requirements :","text":"Resources Limits <code>512MB</code>  MEMORY (we recommend at least 1GB) <code>1CPU</code>  CPU (we recommend at least 2CPU) <code>20GB</code>      STORAGE"},{"location":"devops/k8s-engine/k3s-installation/#cpu-and-memory","title":"CPU and Memory","text":"<p>The following are the minimum CPU and memory requirements for nodes in a high-availability K3S server</p> Deployment Size Nodes VCPUS RAM Small Up to 10 2 4 GB Medium Up to 100 4 8 GB Large Up to 250 8 16 GB X-Large Up to 500 16 32 GB XX-Large 500+ 32 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#disks","title":"Disks","text":"<p> The cluster performance depends on database performance. To ensure optimal speed, we recommend always using SSD disks to back your K3S cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS.</p>"},{"location":"devops/k8s-engine/k3s-installation/#database","title":"Database","text":"<p>K3S supports different databases including MySQL, PostgreSQL, MariaDB, and etcd, the following is a sizing guide for the database resources you need to run large clusters:</p> Deployment Size Nodes VCPUS RAM Small Up to 10 1 2 GB Medium Up to 100 2 8 GB Large Up to 250 4 16 GB X-Large Up to 500 8 32 GB XX-Large 500+ 16 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#single-master-node-k3s-install","title":"Single Master Node K3S Install","text":"<p>A single-node server installation is a fully-functional Kubernetes cluster, including all the datastore, control-plane, kubelet, and container runtime components necessary to host workload pods. It is not necessary to add additional server or agents nodes, but you may want to do so to add additional capacity or redundancy to your cluster.</p> <p><code>Run this command :</code> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#high-available-k3s-install","title":"High Available K3S Install","text":"<p>To run K3S in this mode, you must have an odd number of server nodes. We recommend starting with three nodes.</p> <p>To get started, first launch a server node with the <code>cluster-init</code> flag to enable clustering and a token that will be used as a shared secret to join additional servers to the cluster.</p> <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s.service</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 13:54:17 UTC; 37min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>After launching the first server, join the second and third servers to the cluster using the shared secret: <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://&lt;ip or hostname of server1&gt;:6443\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#cluster-access","title":"Cluster Access","text":"<p>The kubeconfig file stored at <code>/etc/rancher/k3s/k3s.yaml</code> is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path.This can be done by either exporting the <code>KUBECONFIG</code> environment variable or by invoking the <code>--kubeconfig</code> command line flag. Refer to the examples below for details.</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Check to see that the second and third servers are now part of the cluster: <pre><code>$ kubectl get nodes\n\nNAME        STATUS   ROLES                       AGE   VERSION\nserver1     Ready    control-plane,etcd,master   28m   vX.Y.Z\nserver2     Ready    control-plane,etcd,master   13m   vX.Y.Z\nserver3     Ready    control-plane,etcd,master   10m   vX.Y.Z\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#k3s-agent-install","title":"K3S Agent Install","text":"<p>To install additional agent nodes and add them to the cluster, run the installation script with the <code>K3S_URL</code> and <code>K3S_TOKEN</code> environment variables. Here is an example showing how to join an agent: <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -\n</code></pre></p> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s-agent</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s-agent.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 14:24:53 UTC; 7min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>Setting the <code>K3S_URL</code> parameter causes the installer to configure K3S as an agent, instead of a server. The K3S agent will register with the K3S server listening at the supplied URL. if you have not set a stable token the value to use for <code>K3S_TOKEN</code> is stored at <code>/var/lib/rancher/k3s/server/node-token</code> on your server node.</p> <p>Example</p> Leverage the KUBECONFIG environment variable:Or specify the location of the kubeconfig file in the command: <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n</code></pre> <pre><code>kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces\nhelm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces\n</code></pre>"},{"location":"devops/k8s-engine/k3s-installation/#uninstalling-k3s","title":"Uninstalling K3S","text":"<p>Example</p> Uninstalling Servers:Uninstalling Agents: <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre> <pre><code>/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre> <p>Make sure all required port numbers are open</p> <p>For More Details</p>"},{"location":"devops/k8s-engine/rke2-installation-ansible/","title":"RKE2 Cluster Installation With Ansible","text":"<p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#set-up-passwordless-ssh","title":"Set up passwordless SSH","text":"<ol> <li> <p>Generate ssh key</p> <pre><code>ssh-keygen\n</code></pre> <p>Simply press enter when prompted for default name and no password, this will create <code>~/.ssh/id_rsa</code></p> </li> <li> <p>Copy ssh keys to master and worker nodes</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa root@10.40.140.4\n</code></pre> <p>Enter the ssh password when prompted and repeat for all master and worker nodes.</p> </li> <li> <p>Add the ssh key to the ssh-agent</p> <pre><code>ssh-agent bash\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>If you get an error run the following and try again</p> <pre><code>eval `ssh-agent`\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#install-ansible-and-rke2-deployment-role","title":"Install Ansible and RKE2 deployment role","text":"<ol> <li> <p>Install ansible</p> <pre><code>sudo apt update\nsudo apt install ansible -y\n</code></pre> </li> <li> <p>Install the lablabs/rke2 ansible role (https://galaxy.ansible.com/lablabs/rke2)</p> <pre><code>ansible-galaxy install lablabs.rke2\n</code></pre> <p>This will install the role under the <code>~/.ansible</code> directory</p> </li> <li> <p>Navigate to the <code>~/.ansible</code> directory</p> <pre><code>cd ~/.ansible\n</code></pre> </li> <li> <p>Create inventory file in <code>~/.ansible</code> (can be copied from local with scp)</p> <pre><code>vi inventory\n</code></pre> <p>Go into insert mode by pressing i</p> <p>Copy the following and paste with ctrl+shift+v</p> <pre><code>[masters]\nmaster-01 ansible_host=10.40.140.4 rke2_type=server\nmaster-02 ansible_host=10.40.140.5 rke2_type=server\nmaster-03 ansible_host=10.40.140.6 rke2_type=server\n\n[workers]\nworker-01 ansible_host=10.40.140.7 rke2_type=agent\nworker-02 ansible_host=10.40.140.8 rke2_type=agent\nworker-03 ansible_host=10.40.140.9 rke2_type=agent\n\n[k8s_cluster:children]\nmasters\nworkers\n</code></pre> <p>Host names (ex: master-01) can be changed, however they must be lowercase.</p> <p>Save and quit by pressing Esc, then :wq!</p> </li> <li> <p>Create playbook.yaml file in <code>~/.ansible</code> (check previous step for help with vi)</p> <pre><code>- name: Deploy RKE2\n  hosts: all\n  become: yes\n  vars:\n    rke2_ha_mode: true\n    rke2_api_ip : 10.40.140.4\n    rke2_download_kubeconf: true\n    rke2_ha_mode_keepalived: false\n    rke2_server_node_taints:\n      - 'CriticalAddonsOnly=true:NoExecute'\n  roles:\n    - role: lablabs.rke2\n</code></pre> <p><code>rke2_api_ip</code> should point to your load balancer for master nodes, this should be a TCP load balancer on port 6443. If you don't have one it can point to one of the master nodes.</p> <p>Alternatively you can use keepalived by removing the <code>rke2_ha_mode_keepalived</code> line, and pointing the <code>rke2_api_ip</code> to an unused static IP such as <code>10.40.140.100</code></p> </li> <li> <p>Confirm inventory is working</p> <pre><code>ansible all -i inventory -m ping\n</code></pre> <p>There should be no errors.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#run-the-playbook-and-confirm-the-rke2-cluster-is-up","title":"Run the playbook and confirm the RKE2 cluster is up","text":"<ol> <li> <p>Run the ansible playbook</p> <pre><code>ansible-playbook -i inventory playbook.yaml\n</code></pre> <p>If you're ssh'ing to the other machines as a non-root user, run the following instead:</p> <pre><code>ansible-playbook -i inventory playbook.yaml -K\n</code></pre> <p>Provide the password for the user you're logging in as when prompted.</p> <p>If it hangs at 'Wait for remaining nodes to be ready', check if rke2 is installed on all machines, if not the <code>rke2.sh</code> script may not be running. To fix this, edit the <code>~/.ansible/roles/lablabs.rke2/tasks/rke2.yml</code> by removing the <code>Check RKE2 version</code> task and replacing it with:</p> <pre><code>- name: Check rke2 bin exists\n  ansible.builtin.stat:\n    path: \"{{ rke2_bin_path }}\"\n  register: rke2_exists\n\n- name: Check RKE2 version\n  ansible.builtin.shell: |\n    set -o pipefail\n    {{ rke2_bin_path }} --version | grep -E \"rke2 version\" | awk '{print $3}'\n  args:\n    executable: /bin/bash\n  changed_when: false\n  register: installed_rke2_version\n  when: rke2_exists.stat.exists\n</code></pre> </li> <li> <p>Install kubectl</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Copy kubeconfig file to a better directory and export kubeconfig</p> <pre><code>cp /tmp/rke2.yaml ~/rke2.yaml\n</code></pre> <p>Now we can manage our cluster with <code>kubectl --kubeconfig ~/rke2.yaml</code>, or we can do the following to shorten our commands:</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm our cluster is running and with correct internal IP addresses</p> <pre><code>kubectl get nodes -o wide\n</code></pre> </li> <li> <p>Check the health of our pods</p> <pre><code>kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#cleanup","title":"Cleanup","text":""},{"location":"devops/k8s-engine/rke2-installation-ansible/#jump","title":"Jump","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Kubectl and Ansible</p> <pre><code>sudo snap remove kubectl\nsudo apt remove ansible\n</code></pre> </li> <li> <p>Remove remaining artifacts</p> <pre><code>rm -rf /root/.ansible\nrm /root/.ssh/id_rsa /root/.ssh/id_rsa.pub\nrm ~/rke2.yaml\nrm -rf /root/.kube\napt autoremove -y\n</code></pre> <p>If you followed the installation guide as a non-root user the directories might be different.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#master-and-worker-nodes","title":"Master and Worker nodes","text":"<p>Repeat the following on all master and worker nodes</p> <ol> <li> <p>ssh into the node</p> <pre><code>ssh root@10.40.140.4\n</code></pre> </li> <li> <p>Run the RKE2 uninstall script</p> <pre><code>sudo /usr/local/bin/rke2-uninstall.sh\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation/","title":"RKE2 Setup","text":""},{"location":"devops/k8s-engine/rke2-installation/#rke2-requirements","title":"RKE2 Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY (we recommend at least 8GB) <code>2</code>  CPU (we recommend at least 4CPU) <code>60GB</code>      STORAGE <p> We turn off the firewall to avoid problems in the future. We update the packages and clean up any files left over from previous installations.</p> <p>Ubuntu:</p> <pre><code># Ubuntu instructions \n# stop the software firewall\nsystemctl disable --now ufw\n# get updates, install nfs, and apply\napt update\napt install nfs-common -y  \napt upgrade -y\n# clean up\napt autoremove -y\n</code></pre> <p>Now that we have all the nodes up to date, let's focus on <code>rancher1</code>. While this might seem controversial, <code>curl | bash</code> does work nicely. The install script will use the tarball install for Ubuntu and the RPM install for Rocky/Centos. Please be patient, the start command can take a minute. Here are the rke2 docs and install options for reference.</p>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-server-install","title":"RKE2 Server Install","text":"<pre><code>#rancher1\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=server sh -\n\nmkdir -p /etc/rancher/rke2/\ncat &lt;&lt; EOF &gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\nEOF\n\n# enable and start\nsystemctl enable --now rke2-server.service\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status rke2-server</code> and make sure it is <code>active</code>.</p> <p>Quote</p> <p>If you want to install a specific version use the following command ;</p> <pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.24 INSTALL_RKE2_TYPE=server sh -\n</code></pre> <p>Perfect!  Now we can start talking Kubernetes. We need to symlink the <code>kubectl</code> cli on <code>rancher1</code> that gets installed from RKE2.</p> <pre><code># add kubectl conf\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# check node status\nkubectl get node\n</code></pre> <p>Quote</p> <p>In addition these commands can be used for KUBECONFIG</p> <pre><code># simlink all the things - kubectl\nln -s $(find /var/lib/rancher/rke2/data/ -name kubectl) /usr/local/bin/kubectl\n\n# add kubectl conf\nexport KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n</code></pre> <p>We will also need to get the token from <code>rancher1</code>.</p> <pre><code># save this for rancher2 and rancher3\ncat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-agent-install","title":"RKE2 Agent Install","text":"<p>The agent install is VERY similar to the server install. Except that we need an agent config file before starting. We will start with <code>rancher2</code>. We need to install the agent and setup the configuration file.</p> <pre><code># we add INSTALL_RKE2_TYPE=agent\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=agent sh -\n\n# create config file\nmkdir -p /etc/rancher/rke2/ \n\n# change the ip to reflect your rancher1 ip\necho \"server: https://$RANCHER1_IP:9345\" &gt; /etc/rancher/rke2/config.yaml\n\n# change the Token to the one from rancher1 /var/lib/rancher/rke2/server/node-token \necho \"token: $TOKEN\" &gt;&gt; /etc/rancher/rke2/config.yaml\n\n# enable and start\nsystemctl enable --now rke2-agent.service\n</code></pre> <p>If you want to add your Node as control plane, etcd replace here with below code</p> <p>Example</p> Orjinal CodeChange Code <pre><code>systemctl enable --now rke2-agent.service\n</code></pre> <pre><code>systemctl enable --now rke2-server.service\n</code></pre> <p>Rinse and repeat. Run the same install commands on <code>rancher2</code>, <code>rancher3</code>. Next we can validate all the nodes are playing nice by running kubectl get node -o wide on rancher1. </p>"},{"location":"devops/k8s-engine/rke2-installation/#run-this-code","title":"Run this code !","text":"'To check that Kubernetes is running'<pre><code>kubectl get node\n</code></pre> <p>Important Installations Notes</p> <p>If you are having problems with installations, make sure there are no problems with instances' accessing each other <code>(For Example --Ssh connection--: Permission Denied)</code></p> <p>Check below steps if RKE2-Server or RKE2-Agent is not working </p> <ul> <li> Rancher1 instances Node Token is correct ? </li> <li> Instances IP address is Correct ? </li> <li> Instance Ports is open (9345, 6443) ?</li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/","title":"Restoring RKE2 Clusters","text":"<p>During this process, the cluster will be unavailable. </p> <ul> <li>ETCD should be restored on the first master node that was created in the cluster.</li> <li>ETCD restoration process starts by disabling all rke2 services on all nodes.</li> <li>Don't forget to remove the <code>etcd database directory</code> on secondary master nodes before starting the <code>rke2-server</code>.</li> <li>ETCD restoration process starts with limitin etcd to single node. </li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-existing-nodes","title":"Restoring RKE2 Cluster ETCD on Existing Nodes","text":"<p>This process will restore the ETCD on the existing nodes. </p> <p>If you're adding new nodes to the cluster, you should follow the Restoring RKE2 Cluster ETCD on New Nodes section.</p> <p>1) On ALL nodes, disable the <code>rke2-server</code> service: <pre><code>systemctl disable rke2-server -now\n</code></pre></p> <p>2) On ALL nodes, kill all remaining processes: <pre><code>rke2-killall.sh\n</code></pre> 3 On FIRST NODE, restore etcd: <pre><code>rke2 server --cluster-reset \\\n    --cluster-reset-restore-path=$PATH_TO_SNAPSHOT_FILE\n</code></pre> 4) On FIRST NODE, start the rke2: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p> <p>5) On SECONDARY MASTER NODES, delete the etcd data directory: <pre><code># delete the old etcd data directory\n# make sure the directory is correct for your installation\nrm -rf /var/lib/rancher/rke2/server/db\n</code></pre> 6) On SECONDARY MASTER NODES, start the rke2 server: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-new-nodes","title":"Restoring RKE2 Cluster ETCD on New Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/","title":"Deploy RKE2 Highly Available Cluster","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prerequisites","title":"Prerequisites","text":"a b c 3 Linux Hosts RKE2 System Requirements"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prepare-the-hosts","title":"Prepare the hosts","text":"<p>[ATTENTION]: Do this on all of the master nodes</p> <p>1) Become root</p> <pre><code>sudo su\n</code></pre> <p>2) Install the OS dependencies</p> <pre><code>apt update -y &amp;&amp; apt upgrade -y\n\nsystemctl disable --now ufw\napt install nfs-common jq libselinux1 curl apparmor-profiles -y\napt autoremove -y\n\napparmor_status # check if apparmor is running\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-nodes","title":"MASTER NODES","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-1","title":"MASTER NODE 1","text":"<p>Notes</p> <p>[ATTENTION] Run this steps on the first master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation.</p> <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> <p>2) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\n\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n#etcd-s3: true\n#etcd-s3-endpoint: \"s3.amazonaws.com\"\n#etcd-s3-access-key: \"AKIAAAAAAAAAAAAAAAA\"\n#etcd-s3-secret-key: \"BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\"\n#etcd-s3-region: \"eu-central-1\"\n#etcd-s3-bucket: \"---my-s3-bucket-where-i-store-etcd-backups\"\n#etcd-s3-folder: \"---myclustername-etcd-backups-folder/\"\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>3) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>4) Start the <code>rke2-server</code></p> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> <p>5) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre> <p>6) [ATTENTION] Copy <code>node-token</code> to a safe place for joining other master nodes later on</p> <pre><code>cat /var/lib/rancher/rke2/server/node-token\n# output should look like this:\n# K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-2-3","title":"MASTER NODE 2 &amp; 3","text":"<p>Prerequisites</p> Required Name Description \u2705 <code>MASTER_IP</code> Static endpoint for the first master node \u2705 <code>MASTER_NODE_TOKEN</code> Should get this from first master node installation \u2705 <code>INSTALL_RKE2_VERSION</code> The RKE2 release version that you want to upgrade to <p>Notes</p> <p>[ATTENTION] Run this steps on the second and third master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Prerequisites. <pre><code>export MASTER_IP=\"172.31.15.113\" # TODO: change this to your master-1 IP\nexport MASTER_NODE_TOKEN=\"K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\"\n</code></pre> 2) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation. <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> 3) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\n# IMPORTANT\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nserver: https://$MASTER_IP:9345\ntoken: $MASTER_NODE_TOKEN\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>4) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>5) Start the <code>rke2-server</code> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> 6) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#adding-worker-nodes","title":"Adding Worker Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/","title":"System Upgrade Controller","text":"<p>rancher/system-upgrade-controller</p> <p>RKE2 Docs: Automatic Upgrades</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Highly Available RKE2 Cluster</li> <li>Install the <code>system-upgrade-controller</code> on the cluster</li> </ul> <p>Visit for the latest installation instructions: RKE2 Docs: Install the system-upgrade-controller</p> <p>For now, the command is: (the version might change, better to follow the docs) <pre><code>kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#upgrading-master-nodes-1-by-1","title":"Upgrading Master Nodes 1-by-1","text":"<p>1) [ATTENTION] Decide on which version of RKE2 to upgrade to. <pre><code># Kubernetes Label: remove '+' and replace with '-' or it doesnt work\n\n# TODO: SET THIS TO A RELEASE NUMBER or `latest` or `stable` works as well\nexport RKE2_UPGRADE_VERSION=\"v1.21.14+rke2r1\" \nexport RKE2_UPGRADE_VERSION_SAFE_STRING=$(echo -n $RKE2_UPGRADE_VERSION | tr '+' '-')\n# \"v1.21.14+rke2r1\" --becomes--&gt; \"v1.21.14-rke2r1\"\n</code></pre></p> <p>2) [ATTENTION] Create Master Node Upgrade Plan yaml file.</p> <p>This command will create a file named <code>rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml</code></p> <p>This plan can only be applied to master nodes that have the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p>You can also add the label <code>rke2-upgrade: disabled</code> to a master node to prevent it from being upgraded. <pre><code># create the upgrade-plan for it \ncat &lt;&lt; EOF &gt; rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: server-plan-for-$RKE2_UPGRADE_VERSION_SAFE_STRING\n  namespace: system-upgrade\n  labels:\n    rke2-upgrade: server\nspec:\n  concurrency: 1\n  nodeSelector:\n    matchExpressions:\n      - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]}\n      - {key: rke2-upgrade, operator: Exists}\n      - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]}\n      - {key: rke2-upgrade-to, operator: In, values: [\"$RKE2_UPGRADE_VERSION_SAFE_STRING\"]}\n  serviceAccountName: system-upgrade\n  cordon: true\n  drain:\n    force: true\n  upgrade:\n    image: rancher/rke2-upgrade\n  version: \"$RKE2_UPGRADE_VERSION\"\nEOF\n</code></pre></p> <p>3) Apply the <code>Plan</code> to the cluster</p> <p>This will not upgrade the nodes. That'll happen when we label the nodes correctly.</p> <p><code>Plan</code> will create <code>Job</code>s for each master node that has the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p><pre><code># apply the plan and check it\nkubectl apply -f rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL.yaml\n\n# check the plan and jobs\nkubectl -n system-upgrade get plans,jobs\n</code></pre> 4) [OPTIONAL] Disable a node from upgrading <pre><code>kubectl label nodes &lt;node-name&gt; rke2-upgrade=disabled --overwrite\n</code></pre></p> <p>5) [ATTENTION] Labeling &amp; Upgrading the master nodes 1-by-1</p> <p>This step should be done for each master node one after other.</p> <p>Labeling the master nodes will trigger an automatic upgrade.</p> <p>Do this step for all master nodes one after other.</p> <pre><code>echo \"RKE2_UPGRADE_VERSION=$RKE2_UPGRADE_VERSION\"\necho \"RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL=$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL\"\n\n# label the a master node to upgrade\nkubectl label nodes &lt;node-name&gt; rke2-upgrade=enabled --overwrite\nkubectl label nodes &lt;node-name&gt; rke2-upgrade-to=$RKE2_UPGRADE_VERSION_SAFE_STRING --overwrite\n\n# check if a job is created for the upgrade of the node\nkubectl -n system-upgrade get jobs,plans\n\n# watch as the node is upgraded\nkubectl get nodes --watch\n</code></pre>"},{"location":"devops/k8s-storage/longhorn/","title":"Longhorn Block Storage","text":"<p>Hello everybody! \ud83e\udee1</p> <p>Today I'm going to tell you about Longhorn Block Storage. First of all, what is Longhorn? What is it for? Why is it used? Let's find out... \ud83d\udcaa</p> <p>Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes.</p> <p>Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation.</p> <p>With Longhorn, you can:</p> <ul> <li>Use Longhorn volumes as persistent storage for the distributed stateful applications in your Kubernetes cluster</li> <li>Partition your block storage into Longhorn volumes so that you can use Kubernetes volumes with or without a cloud provider</li> <li>Replicate block storage across multiple nodes and data centers to increase availability</li> <li>Store backup data in external storage such as NFS or AWS S3</li> <li>Create cross-cluster disaster recovery volumes so that data from a primary Kubernetes cluster can be quickly recovered from backup in a second Kubernetes cluster</li> <li>Schedule recurring snapshots of a volume, and schedule recurring backups to NFS or S3-compatible secondary storage</li> <li>Restore volumes from backup</li> <li>Upgrade Longhorn without disrupting persistent volumes</li> </ul> <p>So what have we learned about Longhorn so far? Now let's see how we install K8s on it.</p> <p>\ud83d\udccc First of all, we need to set up the <code>helm</code> in our cluster.</p> <p>Okay, let's go on now.</p> <p>Now let's add <code>Longhorn</code> to the helm repository and update our repom. <pre><code>helm repo add longhorn https://charts.longhorn.io \nHelm repo update\n</code></pre></p> <p>If we get here smoothly, we can go on.</p> <p>Next, we have to set up the Longhorn. Let's create a new namespace and install Longhorn. \ud83d\udc47</p> <pre><code>kubectl create namespace longhorn-system helm install longhorn longhorn/longhorn --namespace longhorn-system\n</code></pre> <p>Installation finished \u2705</p> <p>How easy it was, wasn't it? It would be nice to see Longhorn as the UI at once,is it? Let's do that.</p> <pre><code>kubectl get service -n longhorn-system\n</code></pre> <p>Let's list <code>longhorn-frontend</code> with the command above. And with the command below, let's get this service <code>port-forward</code>.</p> <pre><code>kubectl port-forward -n longhorn-system service/longhorn-frontend 8080:80\n</code></pre> <p>Now visit <code>localhost:8080</code> and enjoy <code>longhorn</code>.</p> <p>We need to test the last <code>Longhorn</code> that we set up, right?</p> <pre><code>helm install my-release oci://registry-1.docker.io/bitnamicharts/mysql\n</code></pre> <p>Let's use the following command to display bitnamicharts/mysql <code>PersistentVolume, PersistentVolumeClaim</code> and the <code>Longhorn StorageClass</code> that we have installed.</p> <pre><code>kubectl get pv,pvc -n default\nkubectl get storageclass -n default\n</code></pre> <p>In fact, the <code>Longhorn</code> installation is so simple. See you on the next blog. \ud83c\udf88</p>"},{"location":"devops/k8s-storage/nfs-install/","title":"NFS Setup Requirements","text":"<ul> <li>1 Master Node</li> <li>1 Worker Node (NFS)</li> </ul> <p>First of all, we save the ip address of our NFS server in the <code>/etc/hosts</code> file on all our nodes.(<code>master - nfs</code>)</p> <pre><code>/etc/hosts\n\n172.31.18.194 master\n172.31.20.138 k8s-ankara-nfs01\n</code></pre> <p>Then we install the necessary applications for our NFS structure.(<code>nfs</code>)</p> <pre><code>sudo apt-get update\nsudo apt-get install -y nfs-kernel-server\n</code></pre> <p>Let's create a directory on our server to store files.(<code>nfs</code>) <pre><code>sudo mkdir /k8s-data &amp;&amp; sudo mkdir /k8s-data/ankara-data\nsudo chmod 1777 /k8s-data/ankara-data\ntouch /k8s-data/ankara-data/ankara-cluster.txt\n</code></pre> We need to edit the NFS server file for the directory we just created. At this stage, we will share the directory with all our nodes.(<code>nfs</code>) <pre><code>sudo nano /etc/exports\n</code></pre></p> <p>After opening, add the following values at the end.(<code>nfs</code>) <pre><code># /etc/exports: the access control list for filesystems which may be exported\n#               to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n\n/k8s-data/ankara-data *(rw,sync,no_root_squash,subtree_check)\n</code></pre></p> <p>Then run the following command to re-read exportfs and confirm the changes.(<code>nfs</code>) <pre><code>sudo exportfs -ra\n</code></pre></p> <p>Now we will do the following operations on all our Kubernetes nodes.(<code>master - nfs</code>) <pre><code>sudo apt-get -y install nfs-common\n</code></pre></p> <p>After the installation is finished, we check whether there is an access problem by running the following command on all our nodes.(<code>master</code>) <pre><code>showmount -e k8s-ankara-nfs01\n</code></pre></p> <p>There does not seem to be any problem. Now we can mount the folder we opened on our NFS server to our nodes.(<code>master</code>) <pre><code>sudo mount k8s-ankara-nfs01:/k8s-data/ankara-data /mnt\nls -l /mnt\n</code></pre></p> <p>We have made all the setup and adjustments for NFS. We can store the data of our pods without any problems.</p> <p>To make our NFS available to pods, the sample Persistent Volume, Persistent Volume Claim and Pod yaml file should be as follows. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ankara-cluster-test-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /k8s-data/ankara-data\n    server: k8s-ankara-nfs01\n    readOnly: false\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ankara-pv-claim\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: data-generator-pod\nspec:\n  containers:\n  - name: data-generator-container\n    image: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      while true; do\n        echo \"$(date) - Data content: $(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 10)\" &gt; /data/data-$(date +%s).txt\n        sleep 1\n      done\n    volumeMounts:\n    - name: data-volume\n      mountPath: /data\n  volumes:\n  - name: data-volume\n    persistentVolumeClaim:\n      claimName: ankara-pv-claim\n</code></pre></p>"},{"location":"devops/k8s-storage/rook-ceph/","title":"Ceph Installation Guide","text":"<p>Ceph is a highly scalable, open-source storage solution designed to provide unified storage for block, file, and object data. With its distributed architecture and strong data reliability features, Ceph eliminates single points of failure, making it a preferred choice for large-scale and dynamic storage environments.</p> <p></p> <p>This guide explains two approaches to setting up Ceph for Kubernetes:</p> <p>1- In-cluster Ceph using Rook: Integrating Ceph directly into a Kubernetes cluster by deploying Rook, a Kubernetes-native operator for managing Ceph. 2- External Ceph Cluster Setup: Installing Ceph on three Ubuntu nodes outside the Kubernetes cluster and integrating it as external storage.</p>"},{"location":"devops/k8s-storage/rook-ceph/#create-internal-ceph-cluster","title":"Create Internal Ceph Cluster","text":"<p>There are multiple ways to install Ceph. Rook deploys and manages Ceph clusters running in Kubernetes, while also enabling management of storage resources and provisioning via Kubernetes APIs. We recommend Rook as the way to run Ceph in Kubernetes or to connect an existing Ceph storage cluster to Kubernetes.</p> <p>Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for Ceph storage to natively integrate with cloud-native environments.</p>"},{"location":"devops/k8s-storage/rook-ceph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster with at least 3 worker nodes.(Kubernetes versions v1.27 through v1.32 are supported.)</li> <li>Helm installed on your local system.</li> <li>Ceph OSDs have a dependency on LVM in some scenario <pre><code># for ubuntu\nsudo apt-get install -y lvm2\n</code></pre></li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#steps","title":"Steps","text":"<p>A simple Rook cluster is created for Kubernetes with the following kubectl commands and example manifests.</p>"},{"location":"devops/k8s-storage/rook-ceph/#-deploy-the-rook-operator","title":"- Deploy the Rook Operator","text":"<p><pre><code>git clone --single-branch --branch v1.15.5 https://github.com/rook/rook.git\ncd rook/deploy/examples\n# create crds, common objects (ns, clusterroles..) and rook operator\nkubectl create -f crds.yaml -f common.yaml -f operator.yaml\n</code></pre> - verify the rook-ceph-operator is in the <code>Running</code> state before proceeding</p> <pre><code>kubectl config set-context --current --namespace rook-ceph\nkubectl -n rook-ceph get pod\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#-cluster-environments","title":"- Cluster Environments","text":"<p>The Rook documentation is focused around starting Rook in a variety of environments</p> <ul> <li>cluster.yaml: Cluster settings for a production cluster running on bare metal. Requires at least three worker nodes.</li> <li>cluster-on-pvc.yaml: Cluster settings for a production cluster running in a dynamic cloud environment.</li> <li>cluster-test.yaml: Cluster settings for a test environment such as minikube.</li> </ul> <p>We want to use production cluster with 3 worker nodes</p> <pre><code>kubectl create -f cluster.yaml\n\n#Verify the cluster is running by viewing the pods in the rook-ceph namespace.\nkubectl -n rook-ceph get pod\n\n# See how the cephcluster created successfully and HEALTH_OK\nkubectl get cephcluster -n rook-ceph\n</code></pre> <ul> <li>To verify that the cluster is in a healthy state, connect to the Rook toolbox and run the ceph status command.</li> </ul> <p><pre><code>kubectl create -f toolbox.yaml\n\n#Connect to the Rook toolbox\nkubectl get pod -n rook-ceph -l \"app=rook-ceph-tools\"\n\nkubectl exec -it rook-ceph-tools-7b67b65bd-kqjb6 -n rook-ceph -- bash\nceph status\nceph osd status\nceph osd ls\nceph osd crush rule dump\nceph df\nrados df\nceph mon stat    \n</code></pre> </p> <pre><code>ceph status: Overall health and operation of the cluster.\nceph osd status: Detailed status of the OSDs.\nceph df: High-level view of storage capacity.\nrados df: Object-level capacity and usage details.\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#-alternative-installment-of-rook","title":"- Alternative installment of rook","text":"<ul> <li> <p>The Rook Helm Chart is available to deploy the operator instead of example manifests</p> </li> <li> <p>Install Rook Helm Chart</p> </li> <li>Add the Rook Helm chart repository and install the operator:</li> </ul> <pre><code>helm repo add rook-release https://charts.rook.io/release  \nhelm repo update  \nhelm install rook-ceph rook-release/rook-ceph --namespace rook-ceph --create-namespace  \n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#login-ceph-dashoar-ui","title":"Login ceph-dashoar ui","text":"<p>https://rook.io/docs/rook/v1.16/Storage-Configuration/Monitoring/ceph-dashboard/?h=dashb</p> <ul> <li>Enable the Ceph Dashboard</li> </ul> <p><pre><code>[...]\nspec:\n  dashboard:\n    enabled: true\n</code></pre> - Viewing the Dashboard External to the Cluster (Ingress Controller) If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things:</p> <ul> <li>Exposes the dashboard on the Internet (using a reverse proxy)</li> <li>Issues a valid TLS Certificate for the specified domain name (using ACME)</li> <li>Tells the reverse proxy that the dashboard itself uses HTTPS</li> <li> <p>Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed)</p> </li> <li> <p>reach Ceph-dashboard over ssl</p> </li> <li> <p>install cert-managet to cluster</p> </li> </ul> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml\n\n#check cert pods is running\nkubectl get po --namespace cert-manager\n</code></pre> <ul> <li>create ClusterIssuer as issuer.yaml</li> </ul> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: test@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-prod-private-key\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <pre><code>kubectl apply -f issuer.yaml\nkubectl get ClusterIssuer\n\n**True**\n</code></pre> <ul> <li>then create ingress as ingress.yaml</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: \"nginx\"\n  tls:\n   - hosts:\n     - ceph.xxx.com\n     secretName: ceph.xxx.com\n  rules:\n  - host: ceph.xxx.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: rook-ceph-mgr-dashboard\n            port:\n              number: 8443\n</code></pre> <pre><code>kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre> <ul> <li> <p>create A record for ceph.xxx.com </p> </li> <li> <p>Login to https://ceph.xxx.com</p> </li> </ul> <pre><code>kubectl get secret\n\n* rook-ceph-dashboard-password\n</code></pre> <ul> <li>Get Credentials <pre><code>kubectl get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo\n</code></pre></li> <li>Overview of the status of Ceph cluster</li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-rbd-storage","title":"Create/build RBD storage","text":"<p>cd ~/rook/deploy/examples/csi/rbd</p> <pre><code>kubectl create -f storageclass.yaml \n## this also create replicapool (CephBlockPool) with 3 replicas\n\n# Verify created new storageclass for rbd\nkubectl get sc\n\n###\nNAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   xx\n###\n</code></pre> <ul> <li>Use this volume type in deployment. run wordpress app</li> </ul> <pre><code>cd ~/rook/deploy/examples/\nkubectl create -f mysql.yaml\nkubectl create -f wordpress.yaml\n\n#Verify the pod is up and running\nkubectl get pod\n\n#Verify the pvc and see bound \nkubectl get pvc,pv\n</code></pre> <ul> <li>Verify persistent volume</li> </ul> <p><pre><code>nodeport to service\n\nconnect to website\nhttp://ip:port\n\npublish new page\n\nkubectl rollout restart deploy wordpress\n</code></pre> <pre><code>kubectl exec -it &lt;wordpress-pod-name&gt; -- /bin/bash\n\ncd /var/www/html\n\necho \"hello\" &gt; test.txt\nls\n\nkubectl get pvc wp-pv-claim -o jsonpath='{.spec.volumeName}'\nor \nkubectl get pv\n#get pv name\n\nkubectl describe pv &lt;wordpress-pv-name&gt;\n#get volumename\n\n\nfindmnt | grep rbd\n#get mount path\n\ncd &lt;mount-path&gt;\nls\n\n#Verify exist test.txt\ncat test.txt\n</code></pre></p> <ul> <li>test exist persistent volume </li> </ul> <p><pre><code>kubectl get po \n\nkubectl delete &lt;pod-name&gt;\n#get new pod name\nkubectl get po \nkubectl exec -it &lt;pod-name&gt; -- cat /var/www/html/test.txt\n\n**hello**\n</code></pre> clean the sc <pre><code>kubectl delete -f mysql.yaml\nkubectl delete -f wordpress.yaml\n\n\ncd ~/rook/deploy/examples/csi/rbd\n\nkubectl delete -f storageclass.yaml\nk get sc\n ceph ui pools\n</code></pre></p>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-cephfs-storage","title":"Create/build CEPHFS storage","text":"<p>cd ~/rook/deploy/examples</p> <pre><code>kubectl create -f filesystem.yaml \n## this also create replicapool (ceph fs) with 3 replicas\n\n# crete cephfs storageclass\ncd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f storageclass.yaml\n\n# Verify created new storageclass for cephfile system\nkubectl get sc\n\n###\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   xx\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   xx\n###\n</code></pre> <ul> <li>Use this volume type in kube-registry deployment, replica:3 in kube-system namespace</li> </ul> <pre><code>cd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f kube-registry.yaml\n\n# pod mount path : /var/lib/registry\n\n#create new file on mounth path\nkubectl exec -it &lt;pod-replica-1&gt; -n &lt;namespace&gt; -- sh\necho \"test1\" &gt; /var/lib/registry/test.txt\n\n#verify the file exist on other pod \nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- ls /var/lib/registry/\n\nkubectl exec -it &lt;pod-replica-2&gt;  -- sh\necho \"test2\" &gt;&gt; /var/lib/registry/test.txt\n\nkubectl exec -it &lt;pod-replica-3&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n\n\n# veirfy volume is persistenet?\nkubectl rollout restart deployment kube-registry -n &lt;namespace&gt;\n\nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n# check volume path\n\nfindmnt | grep ceph\n\nsudo -i\ncd &lt;mount path&gt;\nls\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-object_storage","title":"Create/build OBJECT_STORAGE","text":"<pre><code>cd ~/rook/deploy/examples\n\nkubectl create -f object.yaml\n</code></pre> <ul> <li>After the CephObjectStore is created, the Rook operator will then create all the pools and other resources necessary to start the service. This may take a minute to complete.</li> </ul> <pre><code># can see ceph object storage\nkubectl get CephObjectStore\n\nNAME       PHASE   ENDPOINT                                         SECUREENDPOINT   AGE\nmy-store   Ready   http://rook-ceph-rgw-my-store.rook-ceph.svc:80                    xxx\n\n\n# To confirm the object store is configured, wait for the RGW pod(s) to start:\nkubectl -n rook-ceph get pod -l app=rook-ceph-rgw\n\nNAME                                        READY   STATUS    RESTARTS   AGE\nrook-ceph-rgw-my-store-a-xxxxxxx            2/2     Running   0          xxx\n</code></pre> <ul> <li>To consume the object store, Create a bucket:</li> </ul> <pre><code># Now that the object store is configured, next we need to create a bucket where a client can read and write objects\n# First create object storage class\n\nkubectl create -f storageclass-bucket-delete.yaml\n\nNAME                      PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block           rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   xxx\nrook-ceph-delete-bucket   rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  xxx\nrook-cephfs               rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   xxx\n\n\n# Based on this storage class, an object client can now request a bucket by creating an Object Bucket Claim (OBC). When the OBC is created, the Rook bucket provisioner will create a new bucket\n\nkubectl create -f object-bucket-claim-delete.yaml\n# Now that the claim is created, the operator will create the bucket as well as generate other artifacts to enable access to the bucket. also configure max object and max size inside yaml\n</code></pre> <ul> <li>Client Connections:</li> <li>The following commands extract key pieces of information from the secret and configmap:</li> </ul> <p>```bash</p>"},{"location":"devops/k8s-storage/rook-ceph/#config-map-secret-obc-will-part-of-default-if-no-specific-name-space-mentioned","title":"config-map, secret, OBC will part of default if no specific name space mentioned","text":"<p>export AWS_HOST=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_HOST}') export PORT=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_PORT}') export BUCKET_NAME=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_NAME}') export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode) export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode)  ```</p> <ul> <li>Consume the Object Storage</li> </ul> <p>This section will guide you through testing the connection to the CephObjectStore and uploading and downloading from it. Run the following commands after you have connected to the Rook toolbox.</p> <pre><code>#To test the CephObjectStore, set the object store credentials in the toolbox pod that contains the s5cmd tool\n#The default toolbox.yaml does not contain the s5cmd. The toolbox must be started with the rook operator image (toolbox-operator-image), which does contain s5cmd.\n\nkubectl create -f ~/rook/deploy/examples/toolbox-operator-image.yaml\nkubectl exec -it &lt;pod-name&gt; -n rook-ceph -- bash\n\nmkdir ~/.aws\ncat &gt; ~/.aws/credentials &lt;&lt; EOF\n[default]\naws_access_key_id = &lt;acceskey&gt;\naws_secret_access_key = &lt;secretkey&gt;\nEOF\n\n#Upload a file to the newly created bucket\necho \"Hello Rook\" &gt; /tmp/rookObj\n\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp /tmp/rookObj s3://$BUCKET_NAME\n\n#list bucket\ns5cmd --endpoint-url http://$AWS_HOST:$PORT  ls\n#print remote object content\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cat s3://$BUCKET_NAME/rookObj \n\n#Download and verify the file from the bucket\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp s3://$BUCKET_NAME/rookObj /tmp/rookObj-download\ncat /tmp/rookObj-download\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#create-external-ceph-cluster-and-use-it-exist-k8s-cluster","title":"Create External Ceph Cluster and use it exist K8S Cluster","text":""},{"location":"devops/k8s-storage/rook-ceph/#create-ceph-cluster-with-ceph-ansible","title":"Create ceph cluster with ceph-ansible","text":"<ul> <li> <p>pre-requirements</p> </li> <li> <p>all nodes</p> </li> </ul> <p><pre><code>sudo hostnamectl set-hostname ceph1\nsudo hostnamectl set-hostname ceph2\nsudo hostnamectl set-hostname ceph3\nsudo nano /etc/hosts\n&lt;node1-ip&gt;  ceph1\n&lt;node2-ip&gt;  ceph2\n&lt;node3-ip&gt;  ceph3\n</code></pre>   - on master ceph node <pre><code>sudo -i\nssh-keygen\ncat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys \ncat /root/.ssh/id_rsa.pub\n\nssh ceph2\nsudo -i\nsudo hostnamectl set-hostname ceph2\nnano .ssh/authorized_keys \n* paste id_rsa.pub\n\nssh ceph3\nsudo -i\nsudo hostnamectl set-hostname ceph3\nnano .ssh/authorized_keys\n* paste id_rsa.pub\n</code></pre></p>"},{"location":"devops/k8s-storage/rook-ceph/#install-ansible","title":"install ansible","text":"<pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository --yes --update ppa:ansible/ansible\nsudo apt install ansible -y\nansible --version\n</code></pre> <ul> <li>Clone cephadm-ansible repo</li> </ul> <pre><code>git clone https://github.com/ceph/cephadm-ansible.git\n\ncd cephadm-ansible\n\n# modified hosts\nnano hosts\n[ceph_servers]\nceph1 ansible_host=&lt;node1-ip&gt; \nceph2 ansible_host=&lt;node2-ip&gt; \nceph3 ansible_host=&lt;node3-ip&gt; \n\n[all:vars]\nansible_python_interpreter=/usr/bin/python3\nansible_ssh_private_key_file=/root/.ssh/id_rsa\nansible_user=root\n# verify all node is reachable\nansible all -i hosts -m ping\n\nansible-playbook -i hosts cephadm-preflight.yml\n\n\n\nnano initial_config.yaml\n\n---\nservice_type: host\naddr: &lt;ceph1-ip&gt;\nhostname: ceph1\n---\nservice_type: host\naddr: &lt;ceph2-ip&gt; \nhostname: ceph2\n---\nservice_type: host\naddr: &lt;ceph3-ip&gt;\nhostname: ceph3\n---\nservice_type: mon\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3    \n---\nservice_type: mgr\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3\n---\nservice_type: osd\nservice_id: default_drive_group\nplacement:\n  hosts:\n    - ceph1\n    - ceph2\n    - ceph3\ndata_devices:\n  paths:\n    - /dev/nvme1n1 # if need, change disk name \n---\n\ncephadm bootstrap --mon-ip= &lt;ceph1-ip&gt; --apply-spec=initial_config.yaml --initial-dashboard-password=&lt;change_me&gt; --dashboard-password-noupdate # change mon-ip eand dashboard password\n\n- check current infra\nceph -s\nceph orch ls\nceph osd pool ls\nceph osd pool ls detail\nceph osd tree \nceph orch host ls\n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-rbd-storage_1","title":"Create/build RBD storage","text":"<p>Ref: https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool</p> <ul> <li> <p>check current pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></p> </li> <li> <p>create new pool for rbd <pre><code>exm: ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]\n\nceph osd pool create rbd-pool replicated\n</code></pre></p> </li> <li>Associating a Pool with an Application Pools need to be associated with an application before they can be used</li> </ul> <pre><code>Exm: ceph osd pool application enable {pool-name} {application-name}\n\nceph osd pool application enable rbd-pool rbd\n\n## CephFS uses the application name cephfs, RBD uses the application name rbd, and RGW uses the application name rgw\n</code></pre> <ul> <li>check new pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></li> </ul> <pre><code>- some extra commands\nceph osd crush rule ls\nceph osd crush rule dump\n\n## Setting Pool Quotas\nceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]\n\n## Deleting a Pool\nceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]\n\n## Setting Pool Values\nceph osd pool set {pool-name} {key} {value}\nsize, pg_num, \n</code></pre>"},{"location":"devops/k8s-storage/rook-ceph/#createbuild-cephfs-storage_1","title":"Create/build CEPHFS storage","text":"<ul> <li> <p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph\u2019s distributed object store</p> </li> <li> <p>check current pools <pre><code>ceph osd pool ls\nceph osd pool ls detail\n</code></pre></p> </li> <li> <p>The Ceph Orchestrator will automatically create and configure MDS for your file system</p> </li> <li> <p>run on ceph master <pre><code>ceph fs volume create cephfs\n#create cephfs data and metadata pool automatically\n\n#or manual\n\nceph osd pool create cephfs_data 32\nceph osd pool create cephfs_metadata 1\n\n#check data pools\nceph osd pool ls\nceph osd pool ls detail\n\n#anable data pool\nceph osd pool application enable cephfs_data cephfs\nceph osd pool application enable cephfs_metadata cephfs\n\nceph osd pool ls detail\n\n#make file system from this data pool\nceph fs new cephfs-test cephfs_metadata cephfs_data\n\n#control file system\nceph fs ls\n\n#if need, delete file system: \"ceph fs rm  hepapi-cephfs --yes-i-really-mean-it\"\n\n#create mds\n\non ceph dashboard: services--&gt; create mds : 2 or cli: ceph fs volume create cephfs\n</code></pre></p> </li> </ul>"},{"location":"devops/k8s-storage/rook-ceph/#kubernetes-integration","title":"Kubernetes Integration","text":"<ul> <li> <p>Cluster export - import process</p> </li> <li> <p>Run on master ceph node</p> </li> </ul> <pre><code>git clone --single-branch --branch v1.15.5 https://github.com/rook/rook.git\n\ncd ~/rook/deploy/examples/external\n\npython3 create-external-cluster-resources.py --rbd-data-pool-name &lt;pool_name&gt; --cephfs-filesystem-name &lt;filesystem-name&gt; --rgw-endpoint  &lt;rgw-endpoint&gt; --namespace &lt;namespace&gt; --format bash\n\nsudo -i\ncd ~/home/ubuntu/rook/deploy/examples/external\nExm: python3 create-external-cluster-resources.py --cephfs-filesystem-name cephfs-hepapi --cephfs-data-pool-name cephfs_data --cephfs-metadata-pool-name cephfs_metadata --rbd-data-pool-name hepapi-ceph-replica --namespace rook-ceph --config-file config.ini --format bash\n````\n\n- Exmample output:\n\n```yaml\nexport ROOK_EXTERNAL_FSID=797f411a-aafe-11ec-a254-fa163e1539f5\nexport ROOK_EXTERNAL_USERNAME=client.healthchecker\nexport ROOK_EXTERNAL_CEPH_MON_DATA=ceph-rados-upstream-w4pdvq-node1-installer=10.0.210.83:6789\nexport ROOK_EXTERNAL_USER_SECRET=AQAdm0FilZDSJxAAMucfuu/j0ZYYP4Bia8Us+w==\nexport ROOK_EXTERNAL_DASHBOARD_LINK=https://10.0.210.83:8443/\nexport CSI_RBD_NODE_SECRET=AQC1iDxip45JDRAAVahaBhKz1z0WW98+ACLqMQ==\nexport CSI_RBD_PROVISIONER_SECRET=AQC1iDxiMM+LLhAA0PucjNZI8sG9Eh+pcvnWhQ==\nexport MONITORING_ENDPOINT=10.0.210.83\nexport MONITORING_ENDPOINT_PORT=9283\nexport RBD_POOL_NAME=replicated_2g\nexport RGW_POOL_PREFIX=default\n</code></pre> <ul> <li> <p>run on k8s cluster </p> </li> <li> <p>Prerequisites:   Rook operator is deployed</p> </li> </ul> <pre><code># Create common-external.yaml and cluster-external.yaml\ncd ~/rook/deploy/examples/\n# change namespace: rook-ceph-external in common-external.yaml and cluster-external.yaml then run\nkubectl create -f common-external.yaml\nkubectl create -f cluster-external.yaml\n\n# import external ceph cluster\nfirstly, Paste the above output from create-external-cluster-resources.py into your current shell to allow importing the source data.\n\n# then run\ncd ~/rook/deploy/examples/external\n# change namespace: rook-ceph-external in import-external-cluster.sh then run\n. import-external-cluster.sh\n\n#verify on k8s cluster cephcluster up and running, Health is OK\nkubectl -n rook-ceph  get CephCluster\n\nNAME                 DATADIRHOSTPATH   MONCOUNT   AGE    STATE       HEALTH\nrook-ceph-external   /var/lib/rook                xxx   Connected   HEALTH_OK\n\n#verify current storageclass\nkubectl get sc\n</code></pre> <ul> <li> <p>Verify created new cephfilesystem storageclass</p> </li> <li> <p>Use this volume type in kube-registry deployment, replica:3 in kube-system namespace</p> </li> </ul> <pre><code>cd ~/rook/deploy/examples/csi/cephfs\n\nkubectl create -f kube-registry.yaml\n\n#verify pvc and pvbound\nkubectl get pv,pvc\n\n# pod mount path : /var/lib/registry\n\n#create new file on mounth path\nkubectl exec -it &lt;pod-replica-1&gt; -n &lt;namespace&gt; -- sh\necho \"test1\" &gt; /var/lib/registry/test.txt\n\n#verify the file exist on other pod \nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- ls /var/lib/registry/\n\nkubectl exec -it &lt;pod-replica-2&gt;  -- sh\necho \"test2\" &gt;&gt; /var/lib/registry/test.txt\n\nkubectl exec -it &lt;pod-replica-3&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n\n\n# veirfy volume is persistenet?\nkubectl rollout restart deployment kube-registry -n &lt;namespace&gt;\n\nkubectl exec -it &lt;pod-replica-2&gt; -n &lt;namespace&gt; -- cat /var/lib/registry/test.txt\n# check volume path\n\nfindmnt | grep ceph\n\nsudo -i\ncd &lt;mount path&gt;\nls\n</code></pre>"},{"location":"devops/kubernetes/Velero/","title":"Simplify Cluster Backups with Velero","text":""},{"location":"devops/kubernetes/Velero/#overview","title":"Overview","text":"<p>Velero (formerly Heptio Ark) gives you tools to back up and restore your  Kubernetes cluster resources and persistent volumes. You can run Velero with a cloud provider  or on-premises. Velero lets you:</p> <ul> <li> <p>Take backups of your cluster and restore in case of loss.</p> </li> <li> <p>Migrate cluster resources to other clusters.</p> </li> <li> <p>Replicate your production cluster to development and testing clusters.</p> </li> </ul> <p>Velero consists of:</p> <ul> <li> <p>A server that runs on your cluster.</p> </li> <li> <p>A command-line client that runs locally.</p> </li> </ul>"},{"location":"devops/kubernetes/Velero/#how-velero-works","title":"How Velero Works?","text":"<p>Each Velero operation \u2013 on-demand backup, scheduled backup, restore \u2013 is a custom resource,  defined with a Kubernetes Custom Resource Definition (CRD) and stored in etcd. Velero also  includes controllers that process the custom resources to perform backups, restores, and all  related operations.</p> <p>You can back up or restore all objects in your cluster, or you can filter objects by type,  namespace, and/or label.</p> <p>Velero is ideal for the disaster recovery use case, as well as for snapshotting your application  state, prior to performing system operations on your cluster, like upgrades.</p>"},{"location":"devops/kubernetes/Velero/#demo-prerequisites","title":"Demo - Prerequisites","text":"<ol> <li> <p>Kubernetes Cluster</p> </li> <li> <p>AWS IAM User Account</p> </li> </ol>"},{"location":"devops/kubernetes/Velero/#demo","title":"Demo","text":"<ul> <li> <p>Go to Velero CLI Installation page to install <code>velero</code>.</p> </li> <li> <p>Go to <code>AWS Management Console</code> and sign in as <code>IAM-User</code>. </p> </li> <li> <p>Navigate to <code>S3</code>. From the menu on the left, choose <code>General purpose buckets</code>.</p> </li> <li> <p>Choose <code>Create Bucket</code> and follow these settings:</p> </li> </ul> <p><pre><code>Bucket name: velero-backup-&lt;YOUR_NAME&gt;\nLeave the rest as default.\n</code></pre> - Change <code>&lt;YOUR_NAME&gt;</code> with your name to make the bucket name unique.</p> <ul> <li> <p>Then, choose <code>Create bucket</code>. </p> </li> <li> <p>Choose the bucket you created. Then, choose <code>Create folder</code>.</p> </li> </ul> <pre><code>Folder name: backups\nLeave the rest as default.\n</code></pre> <ul> <li> <p>Choose <code>Create folder</code>. Then, go to main page of the bucket again.</p> </li> <li> <p>Choose <code>Create folder</code> again.</p> </li> </ul> <pre><code>Folder name: restores\nLeave the rest as default.\n</code></pre> <ul> <li>Choose <code>Create folder</code>. You should see the following structure:</li> </ul> <p></p> <ul> <li> <p>Now, go to <code>IAM</code>. From the menu on the left, choose <code>Users</code>.</p> </li> <li> <p>Choose your user and navigate to its page.</p> </li> <li> <p>Under <code>Permissions</code> section, choose <code>Add permissions -&gt; Create inline policy</code>.</p> </li> <li> <p>Choose <code>JSON</code> as <code>Policy editor</code> and enter the following:</p> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::velero-backup-mehmet\",\n                \"arn:aws:s3:::velero-backup-mehmet/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListAllMyBuckets\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <ul> <li> <p>Change <code>&lt;YOUR_BUCKET_NAME&gt;</code> with your bucket name.</p> </li> <li> <p>Choose <code>Next</code> ...</p> </li> </ul> <pre><code>Policy name: velero-role\nLeave the rest as default.\n</code></pre> <ul> <li> <p>Choose <code>Create policy</code>.</p> </li> <li> <p>Go to your main page of your IAM user account again.</p> </li> <li> <p>Under <code>Security credentials</code> section, choose <code>Create access key</code>.</p> </li> </ul> <pre><code>Use case: Command Line Interface (CLI)\n(Checked) Confirmation\n</code></pre> <ul> <li>Choose <code>Next</code> ...</li> </ul> <pre><code>Description tag value: Credentials for Velero\n</code></pre> <ul> <li> <p>Choose <code>Create access key</code> and choose <code>Download .csv file</code>.</p> </li> <li> <p>There is a <code>credentials.txt</code> file with the following content:</p> </li> </ul> <pre><code>[default]\naws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <ul> <li> <p>Enter the keys from the <code>.csv</code> file.</p> </li> <li> <p>Now, open the terminal. Check the status of cluster.</p> </li> </ul> <pre><code>kubectl version\n</code></pre> <ul> <li>Configure the <code>velero</code> in cluster.</li> </ul> <pre><code>velero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n    --bucket &lt;BUCKET_NAME&gt; \\\n    --backup-location-config region=&lt;REGION&gt; \\\n    --secret-file &lt;PATH_TO_AWS_CREDENTIALS_FILE&gt; \\\n    --pod-annotations iam.amazonaws.com/role=arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:user/&lt;AWS_IAM_USER&gt; \\\n    --use-volume-snapshots=false \n</code></pre> <ul> <li>Replace the related parts with your information. For example:</li> </ul> <pre><code>velero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n    --bucket velero-backup-mehmet-deneme \\\n    --backup-location-config region=us-east-1 \\\n    --secret-file ./credentials.txt \\\n    --pod-annotations iam.amazonaws.com/role=arn:aws:iam::123456789123:user/mehmet \\\n    --use-volume-snapshots=false\n</code></pre> <ul> <li>Run the following command to see the backup location.</li> </ul> <pre><code>velero backup-location get\n</code></pre> <ul> <li>The expected output will be: </li> </ul> <p></p> <ul> <li>After verifying that backup location is ready, take the backup.</li> </ul> <pre><code>velero create backup my-first-backup-demo\n</code></pre> <ul> <li>Open your bucket and check the backup.</li> </ul> <p></p> <ul> <li>Run the following command to see the backup status.</li> </ul> <pre><code>velero backup describe my-first-backup-demo\n</code></pre> <p></p> <ul> <li>Now, create a namespace.</li> </ul> <pre><code>kubectl create namespace velero-testing\n</code></pre> <ul> <li>Deploy a <code>nginx</code> pod in this namespace.</li> </ul> <pre><code>kubectl create deployment nginx --image=nginx --namespace=velero-testing\n</code></pre> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing \n</code></pre> <ul> <li>Take the backup of this cluster.</li> </ul> <pre><code>velero create backup testing-backup-do\n</code></pre> <ul> <li> <p>Check the bucket and notice the backup. </p> </li> <li> <p>Now, delete the pod.</p> </li> </ul> <pre><code>kubectl delete deploy nginx -n velero-testing\n</code></pre> <ul> <li>Then, delete the namespace.</li> </ul> <pre><code>kubectl delete namespace velero-testing\n</code></pre> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing\n</code></pre> <ul> <li>Now, restore the namespace and the pod.</li> </ul> <pre><code>velero restore create --from-backup testing-backup-do\n</code></pre> <ul> <li>Check the bucket for restoration files.</li> </ul> <p></p> <ul> <li>Check the status of the pod.</li> </ul> <pre><code>kubectl get po -n velero-testing\n</code></pre> <ul> <li>Also, you can create scheduled backups for every minute.</li> </ul> <pre><code>velero schedule create my-first-schedule --schedule=\"*/1 * * * *\"\n</code></pre> <ul> <li>Check the backup history.</li> </ul> <pre><code>velero get backup \n</code></pre> <p></p> <ul> <li>Delete the scheduled backups.</li> </ul> <pre><code>velero delete schedule my-first-schedule  \n</code></pre> <ul> <li>Delete the backups.</li> </ul> <pre><code>velero backup delete --all   \n</code></pre> <ul> <li>Don't forget to destroy the resources you created.</li> </ul>"},{"location":"devops/kubernetes/Velero/#references","title":"References","text":"<p>Mastering Kubernetes Backups with Velero</p>"},{"location":"devops/kubernetes/Velero/Cheat-Sheet/","title":"Velero Cheat Sheet","text":"<pre><code># Installation\nvelero install --provider &lt;provider&gt; --bucket &lt;bucket-name&gt; --secret-file ./credentials\n\n# Backup Management\nvelero backup create &lt;backup-name&gt;            # Create a Backup\nvelero backup get                             # List Backups\nvelero backup describe &lt;backup-name&gt;          # Describe Backup Details\nvelero backup delete &lt;backup-name&gt;            # Delete a Backup\nvelero backup logs &lt;backup-name&gt;              # View Backup Logs\n\n# Restore Management\nvelero restore create --from-backup &lt;backup-name&gt;  # Create a Restore\nvelero restore get                                 # List Restores\nvelero restore describe &lt;restore-name&gt;             # Describe Restore Details\nvelero restore delete &lt;restore-name&gt;               # Delete a Restore\nvelero restore logs &lt;restore-name&gt;                 # View Restore Logs\n\n# Schedule Management\nvelero schedule create &lt;schedule-name&gt; --schedule=\"*/5 * * * *\"  # Create a Backup Schedule\nvelero schedule get                                              # List Schedules\nvelero schedule describe &lt;schedule-name&gt;                         # Describe Schedule Details\nvelero schedule delete &lt;schedule-name&gt;                           # Delete a Schedule\n\n# Plugin Management\nvelero plugin get            # List Velero Plugins\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/","title":"KEDA (Kubernetes Event-driven Autoscaling) WITH CRON","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#what-is-keda","title":"What is KEDA ?","text":"<p>KEDA (Kubernetes Event-driven Autoscaling) is an open-source project that provides event-driven autoscaling capabilities in your Kubernetes environment. KEDA extends Kubernetes' HPA (Horizontal Pod Autoscaler) system, allowing you to scale your applications based on metrics beyond CPU and memory.</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Installation</li> <li>Installation with Helm</li> <li>Installation with YAML</li> <li>Scaling with Cron</li> <li>Creating a ScaledObject</li> <li>Troubleshooting</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"devops/kubernetes/keda/keda-with-cron/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.16+)</li> <li><code>kubectl</code> CLI</li> <li>(Optional) Helm 3</li> <li>Cluster admin privileges</li> </ul>"},{"location":"devops/kubernetes/keda/keda-with-cron/#installation","title":"Installation","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#installation-with-helm","title":"Installation with Helm","text":"<pre><code># Add Helm repository\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\n\n# Create KEDA namespace\nkubectl create namespace keda\n\n# Install KEDA\nhelm install keda kedacore/keda --namespace keda\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#installation-with-yaml","title":"Installation with YAML","text":"<pre><code># Install the latest version\nkubectl apply -f https://github.com/kedacore/keda/releases/download/v2.11.0/keda-2.11.0.yaml\n</code></pre> <p>Verify the installation:</p> <pre><code>kubectl get pods -n keda\n</code></pre> <p>Expected output: <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\nkeda-operator-7c8d65d96d-bzmqp            1/1     Running   0          30s\nkeda-operator-metrics-apiserver-7d9fd868b5-kvppj   1/1     Running   0          30s\n</code></pre></p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#scaling-with-cron","title":"Scaling with Cron","text":""},{"location":"devops/kubernetes/keda/keda-with-cron/#creating-a-scaledobject","title":"Creating a ScaledObject","text":"<p>ScaledObject is the core component KEDA uses to scale Kubernetes deployments.</p> <ol> <li>Create a sample deployment:</li> </ol> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cron-app\n  namespace: test\n  labels:\n    app: cron-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cron-app\n  template:\n    metadata:\n      labels:\n        app: cron-app\n    spec:\n      containers:\n      - name: cron-app\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cron-app-service\n  namespace: test\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: cron-app\n</code></pre> Apply the deployment:</p> <p><pre><code>kubectl apply -f deployment.yaml\n</code></pre> Verify the deployment:</p> <pre><code>kubectl get pods -n test\n</code></pre> <p>Expected output: <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\n\ncron-app-5df76bc88b-67krr   1/1     Running   0         33s\n</code></pre></p> <ol> <li>Define a ScaledObject:</li> </ol> <p><pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cron-app-scaledobject\n  namespace: test\nspec:\n  scaleTargetRef:\n    name: cron-app\n  minReplicaCount: 1\n  maxReplicaCount: 20\n  triggers:\n  - type: cron\n    metadata:\n      # Timezone\n      timezone: \"Europe/Amsterdam\"\n      # Scaling start time (e.g., starts every day at 08:45)\n      start: \"45 8 * * *\"\n      # Scaling end time (e.g., ends every day at 08:50)\n      end: \"50 8 * * *\"\n      # Desired number of pods during this time range\n      desiredReplicas: \"20\"\n</code></pre> Apply the ScaledObject:</p> <p><pre><code>kubectl apply -f scaleObject.yaml\n</code></pre> Verify the ScaledObject:</p> <pre><code>kubectl get so -n test\n</code></pre> <p>Expected output: <pre><code>NAME                    SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   READY   ACTIVE    FALLBACK   PAUSED    TRIGGERS   AUTHENTICATIONS   AGE\ncron-app-scaledobject   apps/v1.Deployment   cron-app          1     20    True    Unknown   False      Unknown                                18s\n</code></pre></p> <p>When the clock hits 08:45, the ScaledObject will activate and begin scaling the related service during the specified time range.</p> <p>Expected output when the ScaledObject is triggered: <pre><code>NAME                        READY   STATUS            RESTARTS    AGE\ncron-app-5df76bc88b-67krr   1/1     Running             0          7m8s\ncron-app-5df76bc88b-fqffn   0/1     Pending             0          0s\ncron-app-5df76bc88b-fqffn   0/1     Pending             0          0s\ncron-app-5df76bc88b-8llrz   0/1     Pending             0          0s\ncron-app-5df76bc88b-qk8qw   0/1     Pending             0          0s\ncron-app-5df76bc88b-8llrz   0/1     Pending             0          0s\ncron-app-5df76bc88b-qk8qw   0/1     Pending             0          0s\ncron-app-5df76bc88b-fqffn   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-qk8qw   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-8llrz   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-8llrz   1/1     Running             0          2s\ncron-app-5df76bc88b-fqffn   1/1     Running             0          3s\ncron-app-5df76bc88b-qk8qw   1/1     Running             0          5s\ncron-app-5df76bc88b-6ltbp   0/1     Pending             0          0s\ncron-app-5df76bc88b-6ltbp   0/1     Pending             0          0s\ncron-app-5df76bc88b-smfnl   0/1     Pending             0          0s\ncron-app-5df76bc88b-dbkzx   0/1     Pending             0          0s\ncron-app-5df76bc88b-29854   0/1     Pending             0          0s\ncron-app-5df76bc88b-smfnl   0/1     Pending             0          0s\ncron-app-5df76bc88b-dbkzx   0/1     Pending             0          0s\ncron-app-5df76bc88b-6ltbp   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-29854   0/1     Pending             0          0s\ncron-app-5df76bc88b-dbkzx   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-smfnl   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-29854   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-smfnl   1/1     Running             0          2s\ncron-app-5df76bc88b-6ltbp   1/1     Running             0          3s\ncron-app-5df76bc88b-dbkzx   1/1     Running             0          4s\ncron-app-5df76bc88b-29854   1/1     Running             0          7s\ncron-app-5df76bc88b-hvrc8   0/1     Pending             0          0s\ncron-app-5df76bc88b-mrws9   0/1     Pending             0          0s\ncron-app-5df76bc88b-hvrc8   0/1     Pending             0          0s\ncron-app-5df76bc88b-jnpm6   0/1     Pending             0          0s\ncron-app-5df76bc88b-mrws9   0/1     Pending             0          0s\ncron-app-5df76bc88b-jnpm6   0/1     Pending             0          0s\ncron-app-5df76bc88b-rgkcz   0/1     Pending             0          0s\ncron-app-5df76bc88b-xjl9c   0/1     Pending             0          0s\ncron-app-5df76bc88b-c9lrj   0/1     Pending             0          0s\ncron-app-5df76bc88b-9mxl8   0/1     Pending             0          0s\ncron-app-5df76bc88b-rgkcz   0/1     Pending             0          0s\ncron-app-5df76bc88b-xjl9c   0/1     Pending             0          0s\ncron-app-5df76bc88b-c9lrj   0/1     Pending             0          0s\ncron-app-5df76bc88b-hvrc8   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-9mxl8   0/1     Pending             0          0s\ncron-app-5df76bc88b-qcdfx   0/1     Pending             0          0s\ncron-app-5df76bc88b-qcdfx   0/1     Pending             0          0s\ncron-app-5df76bc88b-mrws9   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-jnpm6   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-9mxl8   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-xjl9c   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-qcdfx   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-rgkcz   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-c9lrj   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-mrws9   1/1     Running             0          4s\ncron-app-5df76bc88b-9mxl8   1/1     Running             0          5s\ncron-app-5df76bc88b-qcdfx   1/1     Running             0          6s\ncron-app-5df76bc88b-xjl9c   1/1     Running             0          7s\ncron-app-5df76bc88b-hvrc8   1/1     Running             0          8s\ncron-app-5df76bc88b-jnpm6   1/1     Running             0          9s\ncron-app-5df76bc88b-rgkcz   1/1     Running             0          10s\ncron-app-5df76bc88b-c9lrj   1/1     Running             0          12s\ncron-app-5df76bc88b-mk6d2   0/1     Pending             0          0s\ncron-app-5df76bc88b-xxhz9   0/1     Pending             0          0s\ncron-app-5df76bc88b-mk6d2   0/1     Pending             0          0s\ncron-app-5df76bc88b-pxwn4   0/1     Pending             0          0s\ncron-app-5df76bc88b-xxhz9   0/1     Pending             0          0s\ncron-app-5df76bc88b-pxwn4   0/1     Pending             0          0s\ncron-app-5df76bc88b-hzrg4   0/1     Pending             0          0s\ncron-app-5df76bc88b-hzrg4   0/1     Pending             0          0s\ncron-app-5df76bc88b-mk6d2   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-xxhz9   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-pxwn4   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-hzrg4   0/1     ContainerCreating   0          0s\ncron-app-5df76bc88b-mk6d2   1/1     Running             0          3s\ncron-app-5df76bc88b-xxhz9   1/1     Running             0          4s\ncron-app-5df76bc88b-pxwn4   1/1     Running             0          5s\ncron-app-5df76bc88b-hzrg4   1/1     Running             0          6s\n</code></pre> Current pod count after the ScaledObject runs within the specified time range: <pre><code>k get pod -n test\n\nNAME                        READY   STATUS    RESTARTS   AGE\ncron-app-5df76bc88b-29854   1/1     Running   0          69s\ncron-app-5df76bc88b-67krr   1/1     Running   0          9m20s\ncron-app-5df76bc88b-6ltbp   1/1     Running   0          69s\ncron-app-5df76bc88b-8llrz   1/1     Running   0          84s\ncron-app-5df76bc88b-9mxl8   1/1     Running   0          54s\ncron-app-5df76bc88b-c9lrj   1/1     Running   0          54s\ncron-app-5df76bc88b-dbkzx   1/1     Running   0          69s\ncron-app-5df76bc88b-fqffn   1/1     Running   0          84s\ncron-app-5df76bc88b-hvrc8   1/1     Running   0          54s\ncron-app-5df76bc88b-hzrg4   1/1     Running   0          39s\ncron-app-5df76bc88b-jnpm6   1/1     Running   0          54s\ncron-app-5df76bc88b-mk6d2   1/1     Running   0          39s\ncron-app-5df76bc88b-mrws9   1/1     Running   0          54s\ncron-app-5df76bc88b-pxwn4   1/1     Running   0          39s\ncron-app-5df76bc88b-qcdfx   1/1     Running   0          54s\ncron-app-5df76bc88b-qk8qw   1/1     Running   0          84s\ncron-app-5df76bc88b-rgkcz   1/1     Running   0          54s\ncron-app-5df76bc88b-smfnl   1/1     Running   0          69s\ncron-app-5df76bc88b-xjl9c   1/1     Running   0          54s\ncron-app-5df76bc88b-xxhz9   1/1     Running   0          39s```\n</code></pre></p> <p>Expected output after the specified time range ends:</p> <p><pre><code>cron-app-5df76bc88b-c9lrj   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-jnpm6   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-8llrz   1/1     Terminating         0          9m46s\ncron-app-5df76bc88b-fqffn   1/1     Terminating         0          9m46s\ncron-app-5df76bc88b-67krr   1/1     Terminating         0          17m\ncron-app-5df76bc88b-pxwn4   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-mk6d2   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-xxhz9   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-mrws9   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-6ltbp   1/1     Terminating         0          9m31s\ncron-app-5df76bc88b-dbkzx   1/1     Terminating         0          9m31s\ncron-app-5df76bc88b-rgkcz   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-qk8qw   1/1     Terminating         0          9m46s\ncron-app-5df76bc88b-hzrg4   1/1     Terminating         0          9m1s\ncron-app-5df76bc88b-9mxl8   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-29854   1/1     Terminating         0          9m31s\ncron-app-5df76bc88b-qcdfx   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-xjl9c   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-hvrc8   1/1     Terminating         0          9m16s\ncron-app-5df76bc88b-67krr   0/1     Completed           0          17m\ncron-app-5df76bc88b-pxwn4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-mk6d2   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-9mxl8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-jnpm6   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-xxhz9   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-c9lrj   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-6ltbp   0/1     Completed           0          9m33s\ncron-app-5df76bc88b-hvrc8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-hzrg4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-qk8qw   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-fqffn   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-mrws9   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-8llrz   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-xjl9c   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-29854   0/1     Completed           0          9m33s\ncron-app-5df76bc88b-rgkcz   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-qcdfx   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-jnpm6   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-jnpm6   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-67krr   0/1     Completed           0          17m\ncron-app-5df76bc88b-67krr   0/1     Completed           0          17m\ncron-app-5df76bc88b-dbkzx   0/1     Completed           0          9m33s\ncron-app-5df76bc88b-9mxl8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-9mxl8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-pxwn4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-pxwn4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-mk6d2   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-mk6d2   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-xxhz9   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-xxhz9   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-29854   0/1     Completed           0          9m33s\ncron-app-5df76bc88b-29854   0/1     Completed           0          9m33s\ncron-app-5df76bc88b-c9lrj   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-c9lrj   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-hvrc8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-hvrc8   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-qk8qw   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-qk8qw   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-8llrz   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-8llrz   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-qcdfx   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-qcdfx   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-mrws9   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-mrws9   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-rgkcz   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-rgkcz   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-xjl9c   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-xjl9c   0/1     Completed           0          9m18s\ncron-app-5df76bc88b-hzrg4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-hzrg4   0/1     Completed           0          9m3s\ncron-app-5df76bc88b-fqffn   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-fqffn   0/1     Completed           0          9m48s\ncron-app-5df76bc88b-6ltbp   0/1     Completed           0          9m34s\ncron-app-5df76bc88b-6ltbp   0/1     Completed           0          9m34s\ncron-app-5df76bc88b-dbkzx   0/1     Completed           0          9m34s\ncron-app-5df76bc88b-dbkzx   0/1     Completed           0          9m34s\n</code></pre> After the ScaledObject completes:</p> <pre><code>k get pod -n test\nNAME                        READY   STATUS    RESTARTS   AGE\ncron-app-5df76bc88b-smfnl   1/1     Running   0          10m\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#troubleshooting","title":"Troubleshooting","text":"<p>To troubleshoot KEDA issues:</p> <pre><code># Check KEDA operator logs\nkubectl logs -l app=keda-operator -n keda\n\n# Check metrics server logs\nkubectl logs -l app=keda-metrics-apiserver -n keda\n\n# Check ScaledObject status\nkubectl describe scaledobject cron-app-scaledobject -n test\n\n# Check HPA status\nkubectl get hpa -n test\nkubectl describe hpa keda-hpa-cron-app-scaledobject -n test\n</code></pre>"},{"location":"devops/kubernetes/keda/keda-with-cron/#contributing","title":"Contributing","text":"<p>To contribute to the KEDA project, you can submit a pull request via GitHub.</p>"},{"location":"devops/kubernetes/keda/keda-with-cron/#license","title":"License","text":"<p>KEDA is distributed under the Apache 2.0 licance.</p>"},{"location":"devops/kubernetes/keda/keda/","title":"KEDA (Kubernetes Event-driven Autoscaling)","text":"<p>What is KEDA ?</p> <p>KEDA is a lightweight, open-source Kubernetes event-driven autoscaler used by DevOps, SRE, and Ops teams to horizontally scale pods based on external events or triggers. KEDA helps to extend the capability of native Kubernetes autoscaling solutions, which rely on standard resource metrics such as CPU or memory. You can deploy KEDA into a Kubernetes cluster and manage the scaling of pods using custom resource definitions (CRDs). Built on top of Kubernetes HPA, KEDA scales pods based on information from event sources such as AWS SQS, Kafka, RabbitMQ, etc. These event sources are monitored using scalers, which activate or deactivate deployments based on the rules set for them. KEDA scalers can also feed custom metrics for a specific event source, helping DevOps teams observe metrics relevant to them</p> <p> KEDA scales down the number of pods to zero in case there are no events to process. This is harder to do using the standard HPA, and it helps ensure effective resource utilization and cost optimization, ultimately bringing down the cloud bills..</p> <p>Quote</p> <p>KEDA supports a lot of built-in scalers and external scalers. External scalers include Redis, MYSQL,Prometheus,Rabbit MQ etc. Using external events as triggers aids efficient autoscaling, especially for message-driven microservices like payment gateways or order systems. Since KEDA can be extended by developing integrations with any data source, it can easily fit into any DevOps toolchain. </p>"},{"location":"devops/kubernetes/keda/keda/#keda-components","title":"KEDA Components","text":"<p>Event Sources</p> <p>These are the external event/trigger sources by which KEDA changes the number of pods. Prometheus, RabbitMQ, and Apache Pulsar are some examples of event sources.</p> <p>Metric Adapter</p> <p>Metrics adapter takes metrics from scalers and translates or adapts them into a form that HPA/controller component can understand.</p> <p>Controller</p> <p>The controller/operator acts upon the metrics provided by the adapter and brings about the desired deployment state specified in the ScaledObject (refer below).</p> <p>ScaledObject and ScaledJob:</p> <p>ScaledObject represents the mapping between event sources and objects, and specifies the scaling rules for a Deployment, StatefulSet, Jobs or any Custom Resource in a K8s cluster. Similarly, ScaledJob is used to specify scaling rules for Kubernetes Jobs.</p> <p>Below is an example of a ScaledObject which configures KEDA autoscaling based on Prometheus metrics. Here, the deployment object \u2018keda-test\u2019 is scaled based on the trigger threshold (50) from Prometheus metrics. KEDA will scale the number of replicas between a minimum of 1 and a maximum of 10, and scale down to 0 replicas if the metric value drops below the threshold.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: demo3\nspec:\n  scaleTargetRef:\n    apiVersion: argoproj.io/v1alpha1\n    kind: Rollout\n    name: keda-test-demo3\n  triggers:\n    - type: prometheus\n      metadata:\n      serverAddress:  http://&lt;prometheus-host&gt;:9090\n      metricName: http_request_total\n      query: envoy_cluster_upstream_rq{appId=\"300\", cluster_name=\"300-0\", container=\"envoy\", namespace=\"test\", response_code=\"200\" }\n      threshold: \"50\"\n  idleReplicaCount: 0                       \n  minReplicaCount: 1\n  maxReplicaCount: 10\n</code></pre> <p>Deploying KEDA on any Kubernetes cluster is easy, as it doesn\u2019t need overwriting or duplication of existing functionalities. Once deployed and the components are ready, the event-based scaling starts with the external event source. The scaler will continuously monitor for events based on the source set in ScaledObject and pass the metrics to the metrics adapter in case of any trigger events. The metrics adapter then adapts the metrics and provides them to the controller component, which then scales up or down the deployment based on the scaling rules set in ScaledObject.</p> <p>Note that KEDA activates or deactivates a deployment by scaling the number of replicas to zero or one. It then triggers HPA to scale the number of workloads from one to n based on the cluster resources</p>"},{"location":"devops/kubernetes/keda/keda/#keda-deployment","title":"Keda Deployment","text":"<p>KEDA can be deployed in a Kubernetes cluster through Helm charts, operator hub, or YAML declarations</p> <p>Helm Chart</p> <ul> <li>Add Helm repo</li> </ul> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\n</code></pre> <ul> <li>Update Helm repo</li> </ul> <pre><code>helm repo update\n</code></pre> <ul> <li>Install keda Helm chart</li> </ul> <pre><code>helm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre> <p>Deploying with operator Hub</p> <p>On Operator Hub Marketplace locate and install KEDA operator to namespace keda Create KedaController resource named keda in namespace keda</p> <p>NOTE: Further information on Operator Hub installation method can be found in the following repository.</p> <p>https://github.com/kedacore/keda-olm-operator</p> <p>Deploying using the deployment YAML files</p> <p>Run the following command (if needed, replace the version, in this case 2.13.0, with the one you are using):</p> <pre><code># Including admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0.yaml\n# Without admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0-core.yaml\n</code></pre>"},{"location":"devops/linux/tips/","title":"Linux Tips","text":""},{"location":"devops/linux/tooling/","title":"Linux Tooling","text":""},{"location":"devops/linux/tooling/#fzf-fuzzy-finder","title":"FZF - Fuzzy Finder ().","text":"<ul> <li>Github: https://github.com/junegunn/fzf</li> <li>Description: A command-line fuzzy finder. Use <code>&lt;C-r&gt;</code> to search through your command history, <code>&lt;C-t&gt;</code> to search through your files.</li> </ul>"},{"location":"devops/linux/tooling/#tldr","title":"tldr ()","text":"<ul> <li>Website: https://tldr.sh/</li> <li>Description: Too Long Didn't Read. Use it to learn about a command and its most useful options. <code>tldr &lt;any command&gt;</code> (e.g. <code>tldr curl</code>).   </li> </ul>"},{"location":"devops/linux/shell/ampersand-nohup/","title":"nohup, &","text":""},{"location":"devops/linux/shell/ampersand-nohup/#nohup-and","title":"nohup and &amp;","text":"<p>Ampersand (&amp;)</p> <p>The ampersand (<code>&amp;</code>) is an operator that is used to send the execution of a command to the background.</p> <p>Syntax:</p> <p><code>[command] &amp;</code></p> <p>For example, the command <code>sleep 5 &amp;</code> will start the sleep command in the background. You can then run other commands without having to wait for the sleep command to finish.</p> <p>The nohup command</p> <p>The <code>nohup</code> command is used to run a command in the background without any interruption, even when the terminal session is closed.</p> <p>Syntax:</p> <p><code>nohup [command]</code></p> <p>For example, the command <code>nohup sleep 5 &amp;</code> will start the sleep command in the background and will continue to run even if you close the terminal session.</p>"},{"location":"devops/linux/shell/ampersand-nohup/#difference-between-nohup-and","title":"Difference between <code>nohup</code> and <code>&amp;</code>","text":"<p>The <code>nohup</code> and <code>&amp;</code> commands are both used to run commands in the background. However, there are some key differences between the two commands.</p> <ul> <li><code>nohup</code> prevents the command from being interrupted by the <code>HUP</code> signal, even if the terminal session is closed. The <code>HUP</code> signal is typically sent to a process when the terminal session is closed. This can cause the process to stop running. However, the <code>nohup</code> command catches the <code>HUP</code> signal and ignores it, so that the command continues to run even after the terminal session is closed.</li> <li><code>&amp;</code> does not prevent the command from being interrupted by the <code>HUP</code> signal. If you run a command with the <code>&amp;</code> operator and then close the terminal session, the command will stop running.</li> </ul> <p>Differences between the <code>nohup</code> and <code>&amp;</code> commands:</p> <code>nohup</code> <code>&amp;</code> Prevents <code>HUP</code> signal Yes No Redirects output to file Yes No Suitable for Running commands that need to continue running even after the terminal session is closed Running commands that don't need to continue running after the terminal session is closed"},{"location":"devops/linux/shell/cat/","title":"cat","text":"<ul> <li>Description: Concatenate files and print on the standard output. Can be used to create a file from the standard input.</li> </ul> <p>This would display the contents of the file in the terminal window.</p> <p>Options</p> <p>The <code>cat</code> command provides several options that can be used to modify its behavior. Here are some of the most useful options:</p> <ul> <li><code>-n</code>: Shows line numbers in the output.</li> <li><code>-e</code>: Shows end-of-line characters as \"$\".</li> <li><code>-t</code>: Shows tab characters as \"^I\".</li> <li><code>-v</code>: Shows non-printable characters as \"^M\".</li> <li><code>-s</code>: Squeezes consecutive empty lines into a single line.</li> <li><code>-E</code>: Prints a \"$\" character at the end of each line.</li> </ul> <p>Heredoc (Here Document)</p> <p>The <code>heredoc</code> (Here Document) is a type of redirection that allows you to pass multiple lines of input to a command.</p> <p>The basic syntax for <code>heredoc</code> looks like this:</p> <pre><code>cat &lt;&lt; LimitString\n  text...\nLimitString\n</code></pre> <p>Here, LimitString is any string you choose, and text... is the text you want to pass to the command.</p> <p>Inline file creation with redirection</p> <p>Same thing above applies here and it saves you from creating a file and then editing it.</p> <pre><code>cat &lt;&lt; EOF &gt; newfile.txt\nThis is line 1.\nThis is line 2.\nEOF\n</code></pre> <p>This command will create <code>newfile.txt</code> file with the two lines of text.</p>"},{"location":"devops/linux/shell/chtsh/","title":"cht.sh Command Tool","text":"<p>In Linux it can be hard to remember some commands or options and there is a great tool for that. <code>cht.sh</code></p> <p>For example :</p> <pre><code>curl cht.sh/cat\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/cat\n cheat.sheets:cat\n# POSIX way in which to cat(1); see cat(1posix).\ncat -u [FILE_1 [FILE_2] ...]\n\n# Output a file, expanding any escape sequences (default). Using this short\n# one-liner let's you view the boot log how it was show at boot-time.\ncat /var/log/boot.log\n\n# This is an ever-popular useless use of cat.\ncat /etc/passwd | grep '^root'\n# The sane way:\ngrep '^root' /etc/passwd\n\n# If in bash(1), this is often (but not always) a useless use of cat(1).\nBuffer=`cat /etc/passwd`\n# The sane way:\nBuffer=`&lt; /etc/passwd`\n\n cheat:cat\n# To display the contents of a file:\ncat &lt;file&gt;\n\n# To display file contents with line numbers\ncat -n &lt;file&gt;\n\n# To display file contents with line numbers (blank lines excluded)\ncat -b &lt;file&gt;\n\n tldr:cat\n# cat\n# Print and concatenate files.\n# More information: &lt;https://www.gnu.org/software/coreutils/cat&gt;.\n\n# Print the contents of a file to the standard output:\ncat path/to/file\n\n# Concatenate several files into an output file:\ncat path/to/file1 path/to/file2 ... &gt; path/to/output_file\n\n# Append several files to an output file:\ncat path/to/file1 path/to/file2 ... &gt;&gt; path/to/output_file\n\n# Copy the contents of a file into an output file without buffering:\ncat -u /dev/tty12 &gt; /dev/tty13\n\n# Write `stdin` to a file:\ncat - &gt; path/to/file\n</code></pre> <p>You may say, why do you need this when you already have the <code>man</code> command in Linux? The most important feature that makes <code>cht.sh</code> different is that it explains each option in the simplest and most simple way. It does not require installation.</p> <p>Here are a few more examples :</p> <pre><code>curl cht.sh/tail\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/tail\n cheat:tail\n# To show the last 10 lines of &lt;file&gt;:\ntail &lt;file&gt;\n\n# To show the last &lt;number&gt; lines of &lt;file&gt;:\ntail -n &lt;number&gt; &lt;file&gt;\n\n# To show the last lines of &lt;file&gt; starting with &lt;number&gt;:\ntail -n +&lt;number&gt; &lt;file&gt;\n\n# To show the last &lt;number&gt; bytes of &lt;file&gt;:\ntail -c &lt;number&gt; &lt;file&gt;\n\n# To show the last 10 lines of &lt;file&gt; and to wait for &lt;file&gt; to grow:\ntail -f &lt;file&gt;\n\n tldr:tail\n# tail\n# Display the last part of a file.\n# See also: `head`.\n# More information: &lt;https://www.gnu.org/software/coreutils/tail&gt;.\n\n# Show last 'count' lines in file:\ntail --lines count path/to/file\n\n# Print a file from a specific line number:\ntail --lines +count path/to/file\n\n# Print a specific count of bytes from the end of a given file:\ntail --bytes count path/to/file\n\n# Print the last lines of a given file and keep reading file until `Ctrl + C`:\ntail --follow path/to/file\n\n# Keep reading file until `Ctrl + C`, even if the file is inaccessible:\ntail --retry --follow path/to/file\n\n# Show last 'num' lines in 'file' and refresh every 'n' seconds:\ntail --lines count --sleep-interval seconds --follow path/to/file\n</code></pre> <pre><code>curl cht.sh/touch\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/touch\n cheat:touch\n# To change a file's modification time:\ntouch -d &lt;time&gt; &lt;file&gt;\ntouch -d 12am &lt;file&gt;\ntouch -d \"yesterday 6am\" &lt;file&gt;\ntouch -d \"2 days ago 10:00\" &lt;file&gt;\ntouch -d \"tomorrow 04:00\" &lt;file&gt;\n\n# To put the timestamp of a file on another:\ntouch -r &lt;refrence-file&gt; &lt;target-file&gt;\n\n tldr:touch\n# touch\n# Create files and set access/modification times.\n# More information: &lt;https://manned.org/man/freebsd-13.1/touch&gt;.\n\n# Create specific files:\ntouch path/to/file1 path/to/file2 ...\n\n# Set the file [a]ccess or [m]odification times to the current one and don't [c]reate file if it doesn't exist:\ntouch -c -a|m path/to/file1 path/to/file2 ...\n\n# Set the file [t]ime to a specific value and don't [c]reate file if it doesn't exist:\ntouch -c -t YYYYMMDDHHMM.SS path/to/file1 path/to/file2 ...\n\n# Set the file time of a specific file to the time of anothe[r] file and don't [c]reate file if it doesn't exist:\ntouch -c -r ~/.emacs path/to/file1 path/to/file2 ...\n</code></pre>"},{"location":"devops/linux/shell/jobs-bg-fg/","title":"jobs, bg, fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs-bg-and-fg","title":"jobs, bg, and fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs","title":"jobs","text":"<p>The <code>jobs</code> command will list all jobs on the system; active, stopped, or otherwise.</p>"},{"location":"devops/linux/shell/jobs-bg-fg/#example-usage","title":"Example usage:","text":"<p>1.Create a job with using</p> <p><code>sleep 500 &amp;</code></p> <p>and stop it with <code>ctrl + z</code>. </p> <p>2.List all the jobs with the command : <code>jobs</code></p> <p>You will see that you have a single stopped job identified by the job number [1].</p> <p>Other options to know for this command include:</p> <ul> <li><code>-l</code> - list PIDs in addition to default info</li> <li><code>-n</code> - list only processes that have changed since the last notification</li> <li><code>-p</code> - list PIDs only</li> <li><code>-r</code> - show only running jobs</li> <li><code>-s</code> - show only stopped jobs</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#background","title":"Background","text":"<p>The <code>bg</code> command restarts a suspended job, and runs it in the background.</p> <p><code>bg [JOB_SPEC]</code></p> <p>Where JOB_SPEC can be one of the following:</p> <ul> <li><code>%n</code>: where <code>n</code> is the job number.</li> <li><code>%abc</code>: refers to a job started by a command beginning with <code>abc</code>.</li> <li><code>%?abc</code>: refers to a job started by a command containing <code>abc</code>.</li> <li><code>%-</code>: specifies the previous job.</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#foreground","title":"Foreground","text":"<p>The <code>fg</code> command switches a job running in the background into the foreground.</p> <p><code>fg [JOB_SPEC]</code></p> <p>NOTE: If no <code>JOB_SPEC</code> is provided, <code>bg</code> and <code>fg</code> operate on the current job.</p> <p>For example, if you have two jobs running in the background, and you run the command <code>bg</code>, the job that was most recently started will be brought to the foreground.</p> <p>You can also use the <code>%</code> character to specify a job by its job number, or by a partial command name.</p>"},{"location":"devops/linux/shell/netstat/","title":"Netstat &amp; SS Command","text":"<p>You can check the listening ports and applications with netstat as follows.</p> <p>Prerequisite By default, netstat command may not be installed on your system. Hence, use the apk command on Alpine Linux, dnf command/yum command on RHEL &amp; co, apt command/apt-get command on Debian, Ubuntu &amp; co, zypper command on SUSE/OpenSUSE, pacman command on Arch Linux to install the netstat.</p> <pre><code>sudo apt update\nsudo apt install net-tools\n</code></pre> <p>Run the netstat command along with grep command to filter out port in LISTEN state:</p> <pre><code>netstat -tulpn | grep LISTEN\nnetstat -tulpn | more\n</code></pre> <p>Where netstat command options are:</p> <p><code>-t</code> : Select all TCP ports</p> <p><code>-u</code> : Select all UDP ports</p> <p><code>-l</code> : Show listening server sockets (open TCP and UDP ports in listing state)</p> <p><code>-p</code> : Display PID/Program name for sockets. In other words, this option tells who opened the TCP or UDP port. For example, on my system, Nginx opened TCP port 80/443, so I will /usr/sbin/nginx or its PID.</p> <p><code>-n</code> : Don\u2019t resolve name (avoid dns lookup, this speed up the netstat on busy Linux/Unix servers)</p> <p>The netstat command <code>deprecated</code> for some time on Linux. Therefore, you need to use the ss command as follows:</p> <pre><code>sudo ss -tulw\nsudo ss -tulwn\nsudo ss -tulwn | grep LISTEN\n</code></pre> <p><code>-t</code> : Show only TCP sockets on Linux</p> <p><code>-u</code> : Display only UDP sockets on Linux</p> <p><code>-l</code> : Show listening sockets. For example, TCP port 22 is opened by SSHD server.</p> <p><code>-p</code> : List process name that opened sockets</p> <p><code>-n</code> : Don\u2019t resolve service names i.e. don\u2019t use DNS</p>"},{"location":"devops/linux/shell/netstat/#ps-command","title":"PS Command","text":"<p>The ps command without any options displays information about processes that are bound by the controlling terminal. <pre><code>ps\n</code></pre> The command returns a similar output: <pre><code>PID TTY      TIME     CMD\n285 pts/2    00:00:00 zsh\n334 pts/2    00:00:00 ps\n</code></pre></p> <p>The default output of the ps command contains four columns that provide the following information:</p> <p><code>PID</code>: The process ID is your system\u2019s tracking number for the process. The PID is useful when you need to use a command like kill or nice, which take a PID as their input.</p> <p><code>TTY</code>: The controlling terminal associated with the process. Processes that do not originate from a controlling terminal and were initiated by the system at boot are displayed with a question mark.</p> <p><code>TIME</code>: The CPU usage of the process. Displays the amount of CPU time used by the process. This value is not the run time of the process.</p> <p><code>CMD</code>: The name of the command or executable that is running. The output only includes the name of the command or executable and does not display any options that were passed in.</p>"},{"location":"devops/linux/shell/netstat/#the-aux-shortcut","title":"The <code>aux</code> shortcut","text":"<p>Now that you understand the basics of the <code>ps</code> command, this section covers the benefits to the <code>ps</code> <code>aux</code> command. The <code>ps</code> <code>aux</code> displays the most amount of information a user usually needs to understand the current state of their system\u2019s running processes. Take a look at the following example: <pre><code>ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0    892   572 ?        Sl   Nov28   0:00 /init\nroot       227  0.0  0.0    900    80 ?        Ss   Nov28   0:00 /init\nroot       228  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nzaphod     229  0.0  0.1 749596 31000 pts/0    Ssl+ Nov28   0:15 docker\nroot       240  0.0  0.0      0     0 ?        Z    Nov28   0:00 [init] &lt;defunct&gt;\nroot       247  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nroot       248  0.0  0.1 1758276 31408 pts/1   Ssl+ Nov28   0:10 /mnt/wsl/docker-desktop/docker-desktop-proxy\nroot       283  0.0  0.0    892    80 ?        Ss   Dec01   0:00 /init\nroot       284  0.0  0.0    892    80 ?        R    Dec01   0:00 /init\nzaphod     285  0.0  0.0  11964  5764 pts/2    Ss   Dec01   0:00 -zsh\nzaphod     343  0.0  0.0  23764  9836 pts/2    T    17:44   0:00 vi foo\nroot       349  0.0  0.0    892    80 ?        Ss   17:45   0:00 /init\nroot       350  0.0  0.0    892    80 ?        S    17:45   0:00 /init\nzaphod     351  0.0  0.0  11964  5764 pts/3    Ss+  17:45   0:00 -zsh\nzaphod     601  0.0  0.0  10612  3236 pts/2    R+   18:24   0:00 ps aux\n</code></pre></p> <p>The <code>ps aux</code> command displays more useful information than other similar options. For example, the <code>UID</code> column is replaced with a human-readable <code>username</code> column. <code>ps aux</code> also displays statistics about your Linux system, like the percent of CPU and memory that the process is using. The <code>VSZ</code> column displays amount of virtual memory being consumed by the process. <code>RSS</code> is the actual physical wired-in memory that is being used. The <code>START</code> column shows the date or time for when the process was started. This is different from the CPU time reported by the <code>TIME</code> column.</p>"},{"location":"devops/linux/shell/nmap/","title":"NMap Command","text":"<p>Nmap (an acronym of Network Mapper) is an open-source command-line utility to securely manage the network. Nmap command has an extensive list of options to deal with security auditing and network exploration.</p> <p>Prerequisites To use the Nmap utility, the Nmap must be installed on your Ubuntu 22.04. Nmap is available on the official repository of Ubuntu 22.04. Before installation, it is a better practice to update the core libraries of Ubuntu 22.04 as follows:</p> <p><pre><code>sudo apt update\nsudo apt install nmap\n</code></pre> or <pre><code>sudo apt update\nsnap install nmap\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#syntax-of-nmap-command","title":"Syntax of Nmap command","text":"<p>The syntax of the Nmap command is given below: <pre><code>nmap [options] [IP-adress or web-address]\n</code></pre> The Nmap command can be used to scan through the open ports of the host. For instance, the following command will scan the \u201cxxx.xxx.xxx\u201d for open ports..</p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-scan-specific-ports","title":"How to use the Nmap command to scan specific port(s)","text":"<p>By default, the Nmap scans through only 1000 most used ports (these are not consecutive but important). However, there are a total of 65535 ports. The Nmap command can be used to scan a specific port or all the ports.</p> <p>To scan all ports: The -p- flag of the Nmap command helps to scan through all 65535 ports: <pre><code>nmap -p- 192.168.214.138\n</code></pre></p> <p>To scan a <code>specific port</code>: One can specify the port number as well. For instance, the following command will scan for port 88 only: <pre><code>nmap -p 88 88 192.168.214.138\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-get-the-os-information","title":"How to use the Nmap command to get the OS information","text":"<p>The Nmap command can be used to get the <code>Operating System\u2019s information</code>. For instance, the following command will get the information of the OS associated with the IP address.</p> <pre><code>sudo nmap -O 192.168.214.138\n</code></pre>"},{"location":"devops/linux/shell/nslookup/","title":"NSLOOKUP","text":""},{"location":"devops/linux/shell/nslookup/#what-is-the-nslookup","title":"What is the 'nslookup'","text":""},{"location":"devops/linux/shell/nslookup/#nslookup-stands-for-name-server-lookup-is-a-useful-command-for-getting-information-from-the-dns-server-it-is-a-network-administration-tool-for-querying-the-domain-name-system-dns-to-obtain-domain-name-or-ip-address-mapping-or-any-other-specific-dns-record-it-is-also-used-to-troubleshoot-dns-related-problems","title":"Nslookup (stands for \u201cName Server Lookup\u201d) is a useful command for getting information from the DNS server. It is a network administration tool for querying the Domain Name System (DNS) to obtain domain name or IP address mapping or any other specific DNS record. It is also used to troubleshoot DNS-related problems.","text":"<p>Syntax of the -<code>nslookup</code>- command in Linux System <pre><code>nslookup [option] [hosts]\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#options-of-nslookup-command","title":"Options of nslookup command:","text":"Options Description -domain=[domain-name] <code>allows you to change the default DNS name.</code> -debug <code>enables the display of debugging information.</code> -port=[port-number] <code>Use the -port option to specify the port number for queries. By default, nslookup uses port 53 for DNS queries</code> -timeout=[seconds] <code>you can specify the time allowed for the DNS server to respond. By default, the timeout is set to a few seconds</code> -type=a <code>Lookup for a record We can also view all the available DNS records for a particular record using the -type=a option</code> -type=any <code>Lookup for any record We can also view all the available DNS records using the -type=any option.</code> -type=hinfo <code>displays hardware-related information about the host. It provides details about the operating system and hardware platform</code> -type=mx <code>Lookup for an mx record MX (Mail Exchange) maps a domain name to a list of mail exchange servers for that domain. The MX record says that all the mails sent to \u201cgoogle.com\u201d should be routed to the Mail server in that domain.</code> -type=ns <code>Lookup for an ns record NS (Name Server) record maps a domain name to a list of DNS servers authoritative for that domain. It will output the name serves which are associated with the given domain.</code> -type=ptr <code>used in reverse DNS lookups. It retrieves the Pointer (PTR) records, which map IP addresses to domain names.</code> -type=soa <code>Lookup for a soa record SOA record (start of authority), provides the authoritative information about the domain, the e-mail address of the domain admin, the domain serial number, etc\u2026</code>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-service","title":"Examples For K8S Service","text":"<p><pre><code>kubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre> <code>kubectl exec busybox -- nslookup nginx-svc</code> <pre><code>Name:   nginx-svc.default.svc.cluster.local\nAddress: 10.100.245.19\n\nnslookup: can't resolve 'kubernetes.default'\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-pod","title":"Examples For K8S Pod","text":"<p><pre><code>pod-ip-address.my-namespace.pod.cluster-domain.example\n</code></pre> <code>kubectl exec busybox -- nslookup 10-244-1-2.default.pod.cluster.local</code> <pre><code>172-17-0-3.default.pod.cluster.local\n</code></pre></p>"},{"location":"devops/linux/shell/scp/","title":"SCP Command","text":"<p>SCP (secure copy) is a command-line utility that allows you to securely copy files and directories between two locations.</p> <p>From your local system to a remote system. <pre><code>scp -i \"pam.pem\" /home/kullanici/dizin/local_file.txt ubuntu@18.204.206.157:/home/ubuntu/\n</code></pre> From a remote system to your local system. <pre><code>scp -i \"pam.pem\" -r ubuntu@3.86.225.192:/home/ubuntu/  .\n</code></pre></p> <p>Attention  pam.pem is the password file of Ec2 instance</p> <p><code>scp</code> provides a number of options that control every aspect of its behavior. The most widely used options are:</p> <p><code>-P</code> - Specifies the remote host ssh port.</p> <p><code>-p</code> - Preserves files modification and access times.</p> <p><code>-q</code> - Use this option if you want to suppress the progress meter and non-error messages.</p> <p><code>-C</code> - This option forces scp to compresses the data as it is sent to the destination machine.</p> <p><code>-r</code> - This option tells scp to copy directories recursively.</p>"},{"location":"devops/linux/shell/script/","title":"<code>script</code> command","text":"<pre><code>man script\n</code></pre>"},{"location":"devops/linux/shell/script/#save-your-terminal-session","title":"Save your terminal session","text":"<pre><code>script -a -t 5 sav-my-session-name.log\n\n# do stuff\n\nexit\n# run cat sav-my-session-name.log to see the output\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/","title":"ELK Stack with FileBeat","text":"<p>The ELK Stack (Elasticsearch, Logstash, and Kibana) is the world\u2019s most popular open-source log analysis platform. ELK is quickly overtaking existing proprietary solutions and becoming companies\u2019 top choice for log analysis and management solutions. There is one more component \u2014 Beats \u2014 which collects the data and sends it to Logstash. This led Elastic to rename ELK as the Elastic Stack.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is a NoSQL database. It is based on Lucene search engine, and it is built with RESTful APIS. Elasticsearch offers simple deployment, maximum reliability, and easy management. It also offers advanced queries to perform detail analysis and stores all the data centrally. It is helpful for executing a quick search of the documents. Elasticsearch also allows you to store, search and analyze big volume of data. Modern web and mobile applications have adopted it in search engine platforms.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#why-use-elasticsearch","title":"Why use Elasticsearch?","text":"<ul> <li> <p>Search: The main advantage of using Elasticsearch is it\u2019s rapid and accurate search functionality. For large datasets, relational databases takes a lot more time for search queries because of the number of joins the query has to go through.</p> </li> <li> <p>Scaling: Distributed architecture of Elasticsearch allows you to scale a lot of servers and data. We can scale the clusters to hundreds of nodes and also we can replicate data to prevent data loss in case of a node failure.</p> </li> <li> <p>Analytical engine: Elasticsearch analytical use case has become more popular than the search use case. Elasticsearch is specifically used for log analysis</p> </li> </ul>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#logstash","title":"Logstash","text":"<p>Logstash is a  open-source powerful tool for obtaining, filtering, and normalizing log files. A wide range of plugins for input, output and filtering specifications gives the user a great opportunity to easily configure Logstash to collect, process and channel logs data in many different architectures.</p> <p>Working with log files is divided into one or more pipelines. In each configured pipeline, one or more input plugins retrieve or gather data that is then placed on an internal queue. Process handling threads read queued data in small data series and process these batches via specified filter plugins in sequence.</p> <p></p> <p>After finishing data processing, threads send the data to the related output plugins which in turn are responsible for formatting and sending data to Elasticsearch or any other corresponding engine.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#filebeat","title":"FileBeat","text":"<p>Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.</p> <p>How Filebeat works: When you start Filebeat, it starts one or more inputs that look in the locations you\u2019ve specified for log data. For each log that Filebeat locates, Filebeat starts a harvester. Each harvester reads a single log for new content and sends the new log data to libbeat, which aggregates the events and sends the aggregated data to the output that you\u2019ve configured for Filebeat.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#kibana","title":"Kibana","text":"<p>Kibana is a data visualization which completes the ELK stack. This tool is used for visualizing the Elasticsearch documents and helps developers to have a quick insight into it. Kibana dashboard offers various interactive diagrams, geospatial data, and graphs to visualize complex queries.</p> <p>It is used to search, view, and interact with data stored in Elasticsearch directories and helps you to perform advanced data analysis and visualize your data in a variety of tables, charts, and maps.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#demo","title":"Demo","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#create-aks-cluster","title":"Create AKS Cluster","text":"<p>To create an AKS cluster, use the az aks create command. The following example creates a cluster named myAKSCluster with one node and enables a system-assigned managed identity.</p> <pre><code>az group create -l westus -n MyResourceGroup\n</code></pre> <pre><code>az aks create --resource-group myResourceGroup --name myAKSCluster --enable-managed-identity --node-count 3 -l westus\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#connect-to-the-cluster","title":"Connect to the cluster","text":"<p>Configure kubectl to connect to your Kubernetes cluster using the az aks get-credentials command. This command downloads credentials and configures the Kubernetes CLI to use them.</p> <pre><code>az aks get-credentials --resource-group myResourceGroup --name myAKSCluster --overwrite-existing\n</code></pre> <p>Verify the connection to your cluster using the kubectl get command. This command returns a list of the cluster nodes.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#install-elk-stack-in-aks-cluster-using-helm","title":"Install ELK stack in AKS cluster using Helm:","text":"<ul> <li>Create  namespace with the name of elk.</li> </ul> <p><pre><code>kubectl create ns elk\nkubectl get ns\n</code></pre> - Install ELK Stack helm repo into your local repo with helm command.</p> <p><pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> - Update your repo after installation.</p> <p><pre><code>helm repo update\n</code></pre> - Liste your repo packages.</p> <pre><code>helm repo ls\n</code></pre> <ul> <li>List your helm chart and manifest files.</li> </ul> <pre><code>helm search repo elastic\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elk-stack-elasticsearch-filebeat-logstashkibana-installation-and-configuration-in-aks-cluster-with-helm","title":"ELK Stack (Elasticsearch, Filebeat, Logstash,Kibana) installation and configuration in AKS cluster with Helm:","text":"<ul> <li>Save your elastic/elasticsearch values and save it as elasticsearch.values file in order to make some configuration.</li> </ul> <p><pre><code>helm show values elastic/elasticsearch &gt;&gt; elasticsearch.values\n</code></pre> elasticsearch.values</p> <pre><code>---\nclusterName: \"elasticsearch\"\nnodeGroup: \"master\"\n\nroles:\n  - master\n  - data\n  - data_content\n  - data_hot\n  - data_warm\n  - data_cold\n  - ingest\n  - ml\n  - remote_cluster_client\n  - transform\n\nreplicas: 2\nminimumMasterNodes: 2\nimage: \"docker.elastic.co/elasticsearch/elasticsearch\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\nnetworkHost: \"0.0.0.0\"\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 30Gi\n\npersistence:\n  enabled: true\n  labels:\n    enabled: false\n  annotations: {}\n\nenableServiceLinks: true\n\nprotocol: https\nhttpPort: 9200\ntransportPort: 9300\n\nservice:\n  enabled: true\n  labels: {}\n  labelsHeadless: {}\n  type: ClusterIP\n  publishNotReadyAddresses: false\n  nodePort: \"\"\n  annotations: {}\n  httpPortName: http\n  transportPortName: transport\n  loadBalancerIP: \"\"\n  loadBalancerSourceRanges: []\n  externalTrafficPolicy: \"\"\n\nmaxUnavailable: 1\nsysctlVmMaxMapCount: 262144\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n\n\ntests:\n  enabled: true\n</code></pre> <pre><code>helm install elasticsearch elastic/elasticsearch -f elasticsearch.values -n elk\n</code></pre> <pre><code>helm ls -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-kibana-via-helm-into-aks-cluster","title":"Installation and configuration of Kibana via Helm into AKS Cluster:","text":"<p><pre><code>helm show values elastic/kibana &gt;&gt; kibana.values\n</code></pre> - Change the values configuration with LoadBalancer.</p> <pre><code>---\nelasticsearchHosts: \"https://elasticsearch-master:9200\"\n\nreplicas: 1\n\n\nimage: \"docker.elastic.co/kibana/kibana\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n\nprotocol: http\n\nserverHost: \"0.0.0.0\"\n\nhealthCheckPath: \"/app/kibana\"\n\nautomountToken: true\n\nhttpPort: 5601\n\nservice:\n  type: LoadBalancer\n  loadBalancerIP: \"\"\n  port: 5601\n  nodePort: \"\"\n  labels: {}\n  annotations: {}\n  loadBalancerSourceRanges: []\n  httpPortName: http\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n</code></pre> <pre><code>helm install kibana elastic/kibana -f kibana.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-logstash-via-helm-into-aks-cluster","title":"Installation and configuration of logstash via Helm into AKS Cluster:","text":"<ul> <li>Installation of logstash via Helm.</li> </ul> <pre><code>helm show values elastic/logstash &gt;&gt; logstash.values\n</code></pre> <ul> <li>logstash.values ---&gt; !!! update elasticsearch password you can access alestichsearcg password this command: </li> </ul> <pre><code>kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode\n</code></pre> <pre><code>---\nreplicas: 1\n\nlogstashConfig:\n  logstash.yml: |\n    http.host: 0.0.0.0\n    # xpack.monitoring.enabled: false\n\n\nlogstashPipeline:\n  logstash.conf: |\n    input {\n      beats {\n        port =&gt; 5044\n      }\n    }\n    output {\n      elasticsearch {\n        hosts =&gt; [ \"https://elasticsearch-master:9200\" ]\n        ssl =&gt; true\n        manage_template =&gt; false\n        ssl_certificate_verification =&gt; true\n        index =&gt; \"logstash-%{+YYYY.MM.dd}\"\n        document_type =&gt; \"%{[@metadata][type]}\"\n        cacert =&gt; \"/usr/share/logstash/certs/ca.crt\"\n        user =&gt; \"${ELASTICSEARCH_USERNAME}\"\n        password =&gt; \"${ELASTICSEARCH_PASSWORD}\"  #update password\n      }\n    }\n\n\nimage: \"docker.elastic.co/logstash/logstash\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nextraEnvs:\n  - name: \"ELASTICSEARCH_USERNAME\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: username\n  - name: \"ELASTICSEARCH_PASSWORD\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: password\n\n\nsecretMounts:\n  - name: elasticsearch-master-certs\n    secretName: elasticsearch-master-certs\n    path: /usr/share/logstash/certs/\n\n\nlogstashJavaOpts: \"-Xmx1g -Xms1g\"\n\nresources:\n  requests:\n    cpu: \"100m\"\n    memory: \"1536Mi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"1536Mi\"\n\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n\n\nhttpPort: 9600\n\nmaxUnavailable: 1\n\nlivenessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 300\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n\nreadinessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 3\n\nservice:\n  annotations: {}\n  type: ClusterIP\n  loadBalancerIP: \"\"\n  ports:\n    - name: beats\n      port: 5044\n      protocol: TCP\n      targetPort: 5044\n    - name: http\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n</code></pre> <pre><code>helm install logstash elastic/logstash -f logstash.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-filebeat-via-helm-into-aks-cluster","title":"Installation and configuration of Filebeat via Helm into AKS Cluster:","text":"<ul> <li>Installation of Filebeat via Helm.</li> </ul> <pre><code>helm show values elastic/filebeat &gt;&gt; filebeat.values\n</code></pre> <ul> <li>filebeat.values</li> </ul> <pre><code>---\ndaemonset:\n  enabled: true\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n      - type: container\n        paths:\n          - /var/log/containers/*.log\n        processors:\n        - add_kubernetes_metadata:\n            host: ${NODE_NAME}\n            matchers:\n            - logs_path:\n                logs_path: \"/var/log/containers/\"\n\n      output.logstash:\n        hosts: [\"logstash-logstash:5044\"]\n\n  maxUnavailable: 1\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n\ndeployment:\n\n  enabled: false\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n        - type: log\n          paths:\n            - /usr/share/filebeat/logs/filebeat\n\n      output.elasticsearch:\n        host: \"${NODE_NAME}\"\n        hosts: '[\"https://${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}\"]'\n        username: \"${ELASTICSEARCH_USERNAME}\"\n        password: \"${ELASTICSEARCH_PASSWORD}\"\n        protocol: https\n        ssl.certificate_authorities: [\"/usr/share/filebeat/certs/ca.crt\"]\n\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n  securityContext:\n    runAsUser: 0\n    privileged: false\n  resources:\n    requests:\n      cpu: \"100m\"\n      memory: \"100Mi\"\n    limits:\n      cpu: \"1000m\"\n      memory: \"200Mi\"\n\nreplicas: 1\n\nhostPathRoot: /var/lib\n\nimage: \"docker.elastic.co/beats/filebeat\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nimagePullSecrets: []\n\nlivenessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        curl --fail 127.0.0.1:5066\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nreadinessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        filebeat test output\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nmanagedServiceAccount: true\n\nclusterRoleRules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n      - nodes\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"apps\"\n    resources:\n      - replicasets\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre> <pre><code>helm install filebeat elastic/filebeat -f filebeat.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#deployment-of-sample-applications-into-aks-kubernetes-environment","title":"Deployment of sample applications into AKS kubernetes environment:","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: db-pv-vol\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/home/ubuntu/pv-data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: database-persistent-volume-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: manual\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mongo\n  template:\n    metadata:\n      labels:\n        name: mongo\n        app: todoapp\n    spec:\n      containers:\n      - image: mongo:5.0\n        name: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n          - name: mongo-storage\n            mountPath: /data/db\n      volumes:\n        #- name: mongo-storage\n        #  hostPath:\n        #    path: /home/ubuntu/pv-data\n        - name: mongo-storage\n          persistentVolumeClaim:\n            claimName: database-persistent-volume-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: db-service\n  labels:\n    name: mongo\n    app: todoapp\nspec:\n  selector:\n    name: mongo\n  type: ClusterIP\n  ports:\n    - name: db\n      port: 27017\n      targetPort: 27017\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: web\n  template:\n    metadata:\n      labels:\n        name: web\n        app: todoapp\n    spec:\n      containers: \n        - image: ersinsari/todo\n          imagePullPolicy: Always\n          name: myweb\n          ports: \n            - containerPort: 3000\n          env:\n            - name: \"DBHOST\"\n              value: db-service\n          resources:\n            limits:\n              memory: 500Mi\n              cpu: 100m\n            requests:\n              memory: 250Mi\n              cpu: 80m  \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\n  labels:\n    name: web\n    app: todoapp\nspec:\n  selector:\n    name: web \n  type: LoadBalancer\n  ports:\n   - name: http\n     port: 3000\n     targetPort: 3000\n     protocol: TCP\n</code></pre> <pre><code>kubectl apply -f to_do.yaml\n</code></pre> <pre><code>kubectl get all\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#part-5-kibana-dashboard-configuration-and-sample-app-log-monitoring","title":"Part-5 Kibana Dashboard configuration and sample app log monitoring:","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#dashboard-configuration-and-index-pattern-creation","title":"Dashboard configuration and index pattern creation:","text":"<ul> <li>Go to http://loadbalancer-ip:5601</li> </ul> <p>username: elastic password: $(kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode)</p> <ul> <li> <p>Discover: create an index pattern</p> </li> <li> <p>logstash-*</p> </li> <li> <p>select @timestamp</p> </li> <li> <p>create an index pattern.</p> </li> <li> <p>filter your data using KQL syntax : kubernetes.deployment.name web-deployment</p> </li> <li>You can explore logs info</li> </ul>"},{"location":"devops/logging/Index-Lifecycle-Management/","title":"Elasticsearch Index Lifecycle Management","text":""},{"location":"devops/logging/Index-Lifecycle-Management/#indexing","title":"Indexing:","text":"<p>Indices based on the indexname defined on Logstash need to be created to be visible on Kibana. To do this, the pattern specified in the left menu is written and created by clicking on the Create Index Pattern section under Stack management. </p> <p></p>"},{"location":"devops/logging/Index-Lifecycle-Management/#create-repository","title":"Create Repository:","text":"<p>However, a repo must be created and registered to keep snapshots.</p> <p>/Stack management/Snapshot and Restore/Repositories</p> <p>Here repo elasticsearch can be on a separate server as it could be on the built-in server. Importantly, this directory should be specified in the elasticsearch configuration file (elasticSearch.yaml, values.jaml). If it will be on another server, it should be mounted.</p> <p></p> <p>For example, commands executed on the Elasticsearch server for a separate server to be used as an NFS server:</p> <pre><code>sudo apt install nfs-common\n\nsudo apt install cifs-utils\n\nsudo mount.nfs &lt;path on nfs server&gt; &lt;path on elasticserach server&gt;\n\nsudo  mount.nfs :/mnt/disk2/elasticmount /mnt/elasticmount \n\nchown -R elasticsearch:elasticsearch elasticmount \n</code></pre>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-template","title":"Index Template:","text":"<p>A template should be created to manage the created indexes and define a lifecycle policy.</p> <p>/Stack management/ Index Management/Index Templates</p> <p>Here, click create template to create a template that belongs to a particular pattern.</p> <p></p> <p>Under Index settings section:</p> <p>Add:</p> <p>{ \"index\": {\"lifecycle\": { \"name\": \"kubernetes-pod-policy\" } } }</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-lifecycle-policy","title":"Index Lifecycle Policy:","text":"<p>A policy is created for what to do with the indexes of the specified pattern. </p> <p>For this;</p> <p>Click /Stack management/ Index Lifecycle Policies</p> <p>Here a new policy is created with create Policy.</p> <p></p> <p>This section specifies how long it will last in which phase and what to do during that time. </p> <p>For example, the policy of given pod-logs is to remove indexes with a one-hour lifetime in the warm phase after the replica numbers are drawn to 0 (to avoid holding space), and the policy is to delete indexes that have a seven-day lifecycle in the delete phase when the snapshot policy is applied.</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#snapshot-policy","title":"Snapshot Policy:","text":"<p>It can be deleted by taking a snapshot at certain intervals so that the specified indexes do not hold their place. If necessary, it can be restored from these snapshots. </p> <p>A policy is defined for taking these snapshots.</p> <p>/Stack management/Snapshot and Restore/Policies</p> <p></p> <p>For example, when creating a kubernetes-pod-daily-snapshot policy in the form;</p> <ul> <li> <p>The snapshot to be taken is created on a day-based basis, defined as , <li> <p>specified in which repository the snapshot to be taken will be held, defined by the schedule of the time of the day,</p> </li> <li> <p>specified which pattern index is to be taken,</p> </li> <li> <p>The validity period of this snapshot is specified (expiration - after which time deletion permission is given),</p> </li> <li> <p>This policy specifies the number of snapshots to hold min and max.</p> </li>"},{"location":"devops/logging/Index-Lifecycle-Management/#restore-snapshots","title":"Restore snapshots:","text":"<p>To restore snapshots taken on a specific date</p> <p>Stack management/Snapshot and Restore/snapshot</p> <p></p> <p>Here you click the snapshot of the day. The restore button will be clicked on the screen that opens. </p> <p>Here the snapshots will belong to more than a day. However, it should not be forgotten that it is taken incrementally.</p> <p>To restore a day's snapshot, untick Data streams and indices and click deselect all below. The restore is then done by clicking on the index of the desired day.</p>"},{"location":"devops/logging/efk/","title":"EFK Stack (Elasticsearch, Fluentbit, Kibana) via Minikube","text":"<p>EFK is a popular logging stack used to collect, store, and analyze logs in Kubernetes. \ud83d\udc49  Elasticsearch: Stores and indexes log data for easy retrieval. \ud83d\udc49  Fluentbit: A lightweight log forwarder that collects logs from different sources and sends them to Elasticsearch. \ud83d\udc49  Kibana: A visualization tool that allows users to explore and analyze logs stored in Elasticsearch.</p> <p></p>"},{"location":"devops/logging/efk/#step-by-step-setup-on-minikube","title":"Step-by-Step Setup on minikube","text":""},{"location":"devops/logging/efk/#create-cluster","title":"Create cluster","text":"<pre><code>minikube start\n</code></pre>"},{"location":"devops/logging/efk/#create-namespace-for-logging","title":"Create Namespace for Logging","text":"<pre><code>kubectl create namespace logging\n</code></pre>"},{"location":"devops/logging/efk/#install-elasticsearch-on-k8s","title":"Install Elasticsearch on K8s","text":"<pre><code>helm repo add elastic https://helm.elastic.co\n\nhelm install elasticsearch \\\n --set replicas=1 \\\n --set persistence.enabled=false elastic/elasticsearch -n logging\n</code></pre> <p>### Install Kibana</p> <pre><code>helm install kibana  elastic/kibana -n logging\nkubectl port-forward svc/kibana-kibana 5601:5601 -n logging\n</code></pre> <ul> <li>Go to localhost:5601 and login Kibana</li> </ul>"},{"location":"devops/logging/efk/#retrieve-elasticsearch-username-password","title":"Retrieve Elasticsearch Username &amp; Password","text":"<pre><code># for username\nkubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d\n# for password\nkubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"devops/logging/efk/#install-fluentbit-with-custom-valuesconfigurations","title":"Install Fluentbit with Custom Values/Configurations","text":"<p>\ud83d\udc49 Note: Please update the HTTP_Passwd field in the fluentbit-values.yml file with the password retrieved earlier in step 6: (i.e NJyO47UqeYBsoaEU)\"</p> <p>fluentbit-values.yaml</p> <pre><code>kind: DaemonSet\nreplicaCount: 1\n\nimage:\n  repository: cr.fluentbit.io/fluent/fluent-bit\n\nservice:\n  type: ClusterIP\n  port: 2020\n\n\n\nluaScripts: {}\n\n## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file\nconfig:\n  service: |\n    [SERVICE]\n        Daemon Off\n        Flush {{ .Values.flush }}\n        Log_Level info\n        Parsers_File /fluent-bit/etc/parsers.conf\n        Parsers_File /fluent-bit/etc/conf/custom_parsers.conf\n        HTTP_Server On\n        HTTP_Listen 0.0.0.0\n        HTTP_Port {{ .Values.metricsPort }}\n        Health_Check On\n\n  ## https://docs.fluentbit.io/manual/pipeline/inputs\n  inputs: |\n    [INPUT]\n        Name tail\n        Path /var/log/containers/webapp-deployment*.log\n        multiline.parser docker, cri\n        Tag kube.*\n        Mem_Buf_Limit 5MB\n        Skip_Long_Lines On\n\n\n\n  ## https://docs.fluentbit.io/manual/pipeline/filters\n  filters: |\n    [FILTER]\n        Name kubernetes\n        Match kube.*\n        Merge_Log On\n        Keep_Log Off\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n\n  # https://docs.fluentbit.io/manual/pipeline/outputs\n  outputs: |\n    [OUTPUT]\n        Name es\n        Match kube.*\n        Type  _doc\n        Host elasticsearch-master\n        Port 9200\n        HTTP_User elastic\n        HTTP_Passwd xB7UdQgmGkgRaCrH\n        tls On\n        tls.verify Off\n        Logstash_Format On\n        Logstash_Prefix ersin-fluntbit\n        Retry_Limit False\n        Suppress_Type_Name On\n\n\n  ## https://docs.fluentbit.io/manual/pipeline/parsers\n  customParsers: |\n    [PARSER]\n        Name docker_no_time\n        Format json\n        Time_Keep Off\n        Time_Key time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L\n\n\nvolumeMounts:\n  - name: config\n    mountPath: /fluent-bit/etc/conf\n\ndaemonSetVolumes:\n  - name: varlog\n    hostPath:\n      path: /var/log\n  - name: varlibdockercontainers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: etcmachineid\n    hostPath:\n      path: /etc/machine-id\n      type: File\n\ndaemonSetVolumeMounts:\n  - name: varlog\n    mountPath: /var/log\n  - name: varlibdockercontainers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: etcmachineid\n    mountPath: /etc/machine-id\n    readOnly: true\n\ncommand:\n  - /fluent-bit/bin/fluent-bit\n\nargs:\n  - --workdir=/fluent-bit/etc\n  - --config=/fluent-bit/etc/conf/fluent-bit.conf\n</code></pre> <pre><code>helm repo add fluent https://fluent.github.io/helm-charts\nhelm upgrade --install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging\n</code></pre> <p>Service Section Defines global configuration settings for the Fluent Bit service.</p> <p>Daemon Off: Runs Fluent Bit in the foreground. Flush {{ .Values.flush }}: Sets the flush interval for sending data, using a configurable Helm value. Log_Level info: Sets the logging level to info. Parsers_File: Specifies parser configuration files. HTTP_Server On: Enables the built-in HTTP server for metrics and health checks. HTTP_Listen 0.0.0.0: Sets the HTTP server to listen on all network interfaces. HTTP_Port {{ .Values.metricsPort }}: Sets the HTTP server port for metrics, using a Helm value.</p> <p>Inputs Section Defines where Fluent Bit collects logs from.</p> <p>[INPUT] Name tail: Specifies the input plugin tail to read log files. Path /var/log/containers/webapp-deployment.log: Specifies the log file path to monitor. multiline.parser docker, cri: Enables multi-line log parsing using Docker and CRI parsers. Tag kube.: Tags logs with a kube.* prefix for filtering. Mem_Buf_Limit 5MB: Sets the memory buffer limit to 5MB. Skip_Long_Lines On: Skips lines longer than the buffer limit.</p> <p>Filters Section Processes and enriches logs before output.</p> <p>[FILTER] Name kubernetes: Uses the kubernetes filter plugin for Kubernetes metadata enrichment. Match kube.: Filters logs with tags matching kube.. Merge_Log On: Combines partial log lines into a single entry. Keep_Log Off: Drops the original unparsed log after merging. K8S-Logging.Parser On: Uses parsers for logs based on Kubernetes metadata. K8S-Logging.Exclude On: Excludes logs that don't match certain Kubernetes metadata.</p> <p>Output Section</p> <p>[OUTPUT] Name es: Specifies the Elasticsearch output plugin. Match kube.: Sends logs with tags matching kube.. Type _doc: Specifies the document type (deprecated in modern Elasticsearch versions). Host elasticsearch-master, Port 9200: Sets the Elasticsearch host and port. HTTP_User, HTTP_Passwd: Provides authentication credentials for Elasticsearch. tls On: Enables TLS for secure communication. tls.verify Off: Disables certificate verification (not recommended in production). Logstash_Format On: Formats logs in Logstash-compatible JSON. Logstash_Prefix ersin-fluntbit: Sets the prefix for Elasticsearch index names. Retry_Limit False: Disables retrying on output failures.</p> <p>Custom Parsers Section Defines custom log parsing rules.</p> <p>[PARSER] Name docker_no_time: Names the parser docker_no_time. Format json: Specifies that the log format is JSON. Time_Keep Off: Ignores the timestamp from the original log. Time_Key time: Specifies the JSON key for extracting timestamps. Time_Format %Y-%m-%dT%H:%M:%S.%L: Defines the timestamp format with milliseconds.</p>"},{"location":"devops/logging/efk/#deploy-app-for-log","title":"Deploy App for log","text":"<p>python-app-service.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\n  namespace: app\n  labels:\n    app: webapp\nspec:\n  selector:\n    app: webapp\n  ports:\n  - port: 80\n    targetPort: 5005\n</code></pre> <p>python-app-deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-deployment\n  namespace: app\n  labels:\n    app: webapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: ersinsari/fluentbit-python:latest\n        volumeMounts:\n        - mountPath: /log\n          name: log-volume\n      volumes:\n      - name: log-volume\n        hostPath:\n          path: /var/log/webapp\n          type: DirectoryOrCreate\n</code></pre></p> <pre><code>kubectl create ns app\nkubectl apply -f python-app-service.yaml\nkubectl apply -f python-app-deployment.yaml\n</code></pre>"},{"location":"devops/logging/efk/#access-the-app-and-generate-log","title":"Access the App and generate log","text":"<pre><code>kubectl port-forward svc/webapp-service 5001:80 -n app\n</code></pre> <pre><code>username: ersin\npassword: password\n</code></pre> <p>If you enter username and credential right generate info log but wrong generate warn log</p> <p>Go to Kibana UI --&gt; Stack Management -- &gt; Kibana --&gt; Data View create new data view</p> <pre><code>name: ersin-fluntbit\nfilter: ersin-fluntbit*\n</code></pre> <pre><code>{\"level\": \"INFO\", \"message\": \"Response sent with status: 200\", \"time\": \"2025-01-20 18:59:49,375\", \"logger\": \"app\", \"pathname\": \"/app/app.py\", \"lineno\": 55, \"funcname\": \"log_response_info\", \"request\": {\"method\": \"GET\", \"url\": \"http://localhost:5001/second_level_auth\", \"remote_addr\": \"127.0.0.1\", \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}}\n</code></pre> <pre><code>{\"level\": \"WARNING\", \"message\": \"Login failed for user: ersin\", \"time\": \"2025-01-20 18:59:49,364\", \"logger\": \"app\", \"pathname\": \"/app/app.py\", \"lineno\": 75, \"funcname\": \"login\", \"request\": {\"method\": \"POST\", \"url\": \"http://localhost:5001/login\", \"remote_addr\": \"127.0.0.1\", \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}}\n</code></pre> <ul> <li>Go to Discover section and check logs</li> </ul> <p>The application logs were sent to the index in JSON format using a parser, which split the logs into fields. This allows for faster queries and makes it easier to find specific logs.</p> <p></p>"},{"location":"devops/logging/elastalert2/","title":"Elastalert2","text":""},{"location":"devops/logging/elastalert2/#elastalert2","title":"elastalert2","text":"<ul> <li>Introduction</li> </ul> <p>ElastAlert 2 is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch and OpenSearch.</p> <ul> <li>As a Kubernetes deployment</li> </ul> <p>The Docker container for ElastAlert 2 can be used directly as a Kubernetes deployment, but for convenience, a Helm chart is also available. See the Chart Readme for more information on how to install, configure, and run the chart.</p> <ul> <li>install and modified values.yaml</li> </ul> <pre><code>wget https://raw.githubusercontent.com/jertel/elastalert2/refs/heads/master/chart/elastalert2/values.yaml\n\nmv values.yaml elastalert2.yaml \n\nvi elastalert2.yaml \n````\n\n```yaml\nenabledRules: [\"deadman_slack\" ] # should match the name in the rule field\nelasticsearch:\n  host: elasticsearch-master # elasticsearch service name\n  port: 9200\n  useSsl: \"True\"\n  username: \"\"\n  password: \"\"\n  credentialsSecret: \"elasticsearch-master-credentials\" # elasticsearch secret name for user-password credential\n  credentialsSecretUsernameKey: \"username\" # username key in elasticsearch secret\n  credentialsSecretPasswordKey: \"password\" # password key in elasticsearch secret\n  verifyCerts: \"True\" # true for connection to elacticsearch with cert\n  caCerts: \"/certs/ca.crt\" # ca.crt --&gt; name in the elasticsearch cert secret\n  certsVolumes:\n    - name: es-certs\n      secret:\n        defaultMode: 420\n        secretName: elasticsearch-master-certs # elasticsearch cert secret name\n  certsVolumes:\n  certsVolumeMounts:\n    - name: es-certs\n      mountPath: /certs\n      readOnly: true\n\nrules:\n  deadman_slack: |-\n    ---\n    name: Deadman Switch Slack  \n    type: frequency\n    index: \"&lt;index_name&gt;\" \n    num_events: 2\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"&lt;message include string&gt;&gt;\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n</code></pre>"},{"location":"devops/logging/elastalert2/#rules","title":"Rules","text":"<ul> <li>Several rule types with common monitoring paradigms are included with ElastAlert 2</li> </ul> <p>frequency, spike, flatline, blacklist, whitelist etc..</p> <p>*Match when there are more than X events in Y time use frequency Exmp:</p> <pre><code>rules:\n  deadman_slack: |-\n    ---\n    name: deadman_slack more log then x event in y minutes\n    type: frequency\n    index: \"devops-dev\"\n    num_events: 2\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"log text\"\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n</code></pre> <p>*Match when there are less than X events in Y time use flatline Exmp:</p> <pre><code>  deadman_slack2: |-\n    ---\n    name: deadman_slack2 less log then 3 event\n    type: flatline\n    index: \"&lt;index_name&gt;\"\n    threshold: 3\n    timeframe:\n      minutes: 1\n    filter:\n    - match_phrase:\n        message: \"log text\"\n    alert:\n    - \"slack\"\n    include:\n    - \"@timestamp\"\n    - \"message\"\n    - \"kubernetes.pod.name\"\n    - \"kubernetes.namespace\"\n    - \"kubernetes.container.name\"\n    slack:\n    slack_webhook_url: &lt;url&gt;\n    slack_msg_color: good\n</code></pre>"},{"location":"devops/logging/elastalert2/#alerts","title":"Alerts","text":"<ul> <li>Each rule may have any number of alerts attached to it. Alerts are subclasses of Alerter and are passed a dictionary, or list of dictionaries, from ElastAlert 2 which contain relevant information. They are configured in the rule configuration file similarly to rule types.</li> </ul> <p>exmp:</p> <pre><code>alert:\n - email\nfrom_addr: \"no-reply@example.com\"\nemail: \"customer@example.com\"\n</code></pre> <ul> <li>Alert types</li> </ul> <pre><code>alert:\n  - datadog\n  - debug\n  - dingtalk\n  - discord\n  - email\n  - gitter\n  - googlechat\n  - jira\n  - ms_teams\n  - opsgenie\n  - slack\n  - sns\n  - stomp\n  - telegram\n  - twilio\n  - zabbix\n  ...\n  ...\n</code></pre> <ul> <li>install elastalert2 </li> </ul> <pre><code>helm repo add elastalert2 https://jertel.github.io/elastalert2/\n\nhelm upgrade --install elastalert2 elastalert2/elastalert2 --create-namespace -n logging -f elastalert2.yaml \n</code></pre> <ul> <li>Alert Notification exmp. (more log then x event in y minutes, type: frequency )</li> </ul> <p></p>"},{"location":"devops/logging/elasticsearch-exporter/","title":"Elasticsearch-Exporter","text":"<p>https://github.com/prometheus-community/elasticsearch_exporter</p> <p>The Elasticsearch Exporter is a tool that may be utilized to check the performance and health of Elasticsearch. This application gathers metrics and information from Elasticsearch and makes them available to Prometheus, a widely used open-source monitoring system. The Elasticsearch Exporter enables you to monitor a range of metrics, including cluster, node, and index-level information. These metrics encompass CPU utilization, memory usage, indexing rate, search rate, and other relevant data.</p>"},{"location":"devops/logging/elasticsearch-exporter/#step-by-step-guide-to-configure-elasticsearch-exporter","title":"Step-by-step guide to configure Elasticsearch Exporter","text":""},{"location":"devops/logging/elasticsearch-exporter/#create-a-system-user-for-elasticsearch","title":"Create a system user for Elasticsearch","text":"<pre><code>sudo useradd elastic_search\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#download-elasticsearch-exporter","title":"Download Elasticsearch Exporter","text":"<pre><code>sudo wget https://github.com/prometheus-community/elasticsearch_exporter/releases/download/v1.7.0/elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#extract-the-targz-file","title":"Extract the tar.gz file","text":"<pre><code>sudo tar -xvzf elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#move-to-the-extracted-directory","title":"Move to the extracted directory","text":"<pre><code>cd elasticsearch_exporter-1.7.0.linux-amd64/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#copy-the-exporter-binary-to-usrlocalbin","title":"Copy the exporter binary to /usr/local/bin/","text":"<pre><code>sudo cp elasticsearch_exporter /usr/local/bin/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#change-ownership-of-the-exporter-binary","title":"Change ownership of the exporter binary","text":"<pre><code>sudo chown elastic_search:elastic_search /usr/local/bin/elasticsearch_exporter\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#create-a-systemd-service-file","title":"Create a systemd service file","text":"<p>Copy the /etc/elasticsearch/certs/http_ca.crt file to the /home/elastic_search directory and set the necessary permissions with the chown elastic_search:elasticsearch /home/elastic_search/http_ca.crt command.</p> <p><pre><code>sudo vim /etc/systemd/system/elasticsearch_exporter.service\n-----------------------------------------------------------\n[Unit]\nDescription=Prometheus ES_exporter\nAfter=local-fs.target network-online.target network.target\nWants=local-fs.target network-online.target network.target\n[Service]\nUser=elastic_search\nNice=10\nExecStart=/usr/local/bin/elasticsearch_exporter --es.uri=https://elastic:password@localhost:9200 --es.ca /home/elastic_search/http_ca.crt  --es.all --es.indices --es.timeout 20s\nExecStop=/usr/bin/killall elasticsearch_exporter\n[Install]\nWantedBy=default.target\n</code></pre> ! Note: Update your user and password for elasticsearch</p>"},{"location":"devops/logging/elasticsearch-exporter/#start-the-elasticsearch-exporter-service-and-enable-the-service-to-start-on-boot","title":"Start the Elasticsearch Exporter service and enable the service to start on boot","text":"<p><pre><code>sudo systemctl start elasticsearch_exporter.service\nsudo systemctl enable elasticsearch_exporter.service\nsudo systemctl status elasticsearch_exporter.service\n</code></pre> !Note: Elastic search exporter uses port 9114, therefore expose it within the VPC in the security group.</p>"},{"location":"devops/logging/elasticsearch-exporter/#configure-prometheusyml","title":"Configure Prometheus.yml","text":"<p>To pull data from the elasticsearch-exporter into Prometheus and Grafana, you should update the prometheus.yml file with the following code, replacing the Elasticsearch node IP addresses as needed</p> <pre><code>    additionalScrapeConfigs:\n    - job_name: 'elasticsearch-exporter'\n      static_configs:\n        - targets: ['&lt;elasticsearch-node-ip-1&gt;:9114', '&lt;elasticsearch-node-ip-2&gt;:9114', '&lt;elasticsearch-node-ip-3&gt;:9114']\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#add-grafana-dashboard-for-elasticsearch","title":"Add Grafana Dashboard for Elasticsearch","text":"<p>Go to Grafana WebUI Click Dashboard --&gt; New --&gt; New Dashboard --&gt; Import --&gt; Add 266 --&gt; load</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/logging/elasticsearch-exporter/#add-alert","title":"Add alert","text":"<p>You can use Alertmanager or Grafana Alert section.</p> <p>elasticsearch.rules.yml</p> <pre><code>groups:\n  - name: elasticsearch\n    rules:\n      - record: elasticsearch_filesystem_data_used_percent\n        expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)\n          / elasticsearch_filesystem_data_size_bytes\n      - record: elasticsearch_filesystem_data_free_percent\n        expr: 100 - elasticsearch_filesystem_data_used_percent\n      - alert: ElasticsearchTooFewNodesRunning\n        expr: elasticsearch_cluster_health_number_of_nodes &lt; 3\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          description: There are only {{$value}} &lt; 3 Elasticsearch nodes running\n          summary: Elasticsearch running on less than 3 nodes\n      - alert: ElasticsearchHeapTooHigh\n        expr: elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n          &gt; 0.9\n        for: 15m\n        labels:\n          severity: critical\n        annotations:\n          description: The heap usage is over 90% for 15m\n          summary: Elasticsearch node {{$labels.node}} heap usage is high\n</code></pre> <p>elasticsearch.rules</p> <pre><code># calculate filesystem used and free percent\nelasticsearch_filesystem_data_used_percent = 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes\nelasticsearch_filesystem_data_free_percent = 100 - elasticsearch_filesystem_data_used_percent\n\n# alert if too few nodes are running\nALERT ElasticsearchTooFewNodesRunning\n  IF elasticsearch_cluster_health_number_of_nodes &lt; 3\n  FOR 5m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"There are only {{$value}} &lt; 3 Elasticsearch nodes running\", summary=\"Elasticsearch running on less than 3 nodes\"}\n\n# alert if heap usage is over 90%\nALERT ElasticsearchHeapTooHigh\n  IF elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} &gt; 0.9\n  FOR 15m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"The heap usage is over 90% for 15m\", summary=\"Elasticsearch node {{$labels.node}} heap usage is high\"}\n</code></pre> <p>You can add slack,email or another connector for alert mechanism.</p>"},{"location":"devops/logging/loki/","title":"Install Loki,Promtail,Grafana","text":""},{"location":"devops/logging/loki/#what-is-loki","title":"What is Loki?","text":"<p>Loki is a log aggregation system developed by Grafana Labs, designed specifically for storing and querying logs. Unlike traditional logging solutions, Loki is optimized for a \"cost-effective\" and \"lightweight\" approach by only indexing metadata (like labels) and not the full log content, making it more efficient and affordable to operate.</p>"},{"location":"devops/logging/loki/#what-is-promtail","title":"What is Promtail","text":"<p>Promtail is an agent that collects logs from various sources and sends them to Loki for storage and querying. It\u2019s part of the Grafana Loki logging stack, designed to simplify log collection and forwarding.</p>"},{"location":"devops/logging/loki/#loki-stack-helm-chart","title":"Loki-Stack Helm Chart","text":"<p>The Loki Stack Helm chart is a pre-configured set of resources that deploys the full Loki logging stack in Kubernetes. This Helm chart simplifies the deployment and management of Loki, Promtail, and other optional components like Grafana, making it easier to set up a complete logging solution within a Kubernetes cluster</p>"},{"location":"devops/logging/loki/#what-are-we-going-to-need","title":"What are we going to need?","text":"<p>Kubernetes cluster. Grafana installation. Grafana Loki installation. Promtail agent on every node of the Kubernetes cluster.</p>"},{"location":"devops/logging/loki/#create-kubernetes-cluster","title":"Create Kubernetes Cluster","text":"<p>Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node.</p> <ul> <li>First, install Docker Desktop on your Mac. The easiest way to do so is to get the .dmg file from Docker\u2019s website.</li> </ul> <pre><code>brew install minikube kubectl\nminikube start\n</code></pre>"},{"location":"devops/logging/loki/#loki-stack-helm-chart_1","title":"Loki-Stack Helm Chart","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We can check all repo </li> </ul> <p><pre><code>helm search repo loki\n</code></pre> </p> <ul> <li>We will use grafana/loki-stack helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values grafana/loki-stack &gt; loki-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>loki:\n  enabled: true\n  isDefault: true\n  url: http://{{(include \"loki.serviceName\" .)}}:{{ .Values.loki.service.port }}\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  datasource:\n    jsonData: \"{}\"\n    uid: \"\"\n\n\npromtail:\n  enabled: true\n  config:\n    logLevel: info\n    serverPort: 3101\n    clients:\n      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push\n\n\ngrafana:\n  enabled: true\n  sidecar:\n    datasources:\n      label: \"\"\n      labelValue: \"\"\n      enabled: true\n      maxLines: 1000\n  image:\n    tag: latest\n</code></pre> <ul> <li>Install loki stack include promtail,loki and grafana</li> </ul> <p><pre><code>helm upgrade --install loki -f loki-values.yaml -n logging --create-namespace grafana/loki-stack \n</code></pre> </p> <ul> <li>Port-forward grafana services and go to grafana ui</li> </ul> <pre><code>kubectl port-forward service/loki-grafana 3000:80\n</code></pre> <ul> <li>You can get grafana admin-password via below command</li> </ul> <pre><code>kubectl get secret loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre> <pre><code>http://localhost:3000\nusername: admin\npassword: admin-password # add your password\n</code></pre> <ul> <li>Thanks to grafana/loki-stack helm-chart you can see Loki has been configured In Data sources  as shown below</li> </ul> <p></p> <ul> <li>We can check all pods logs grafana explore section.</li> </ul> <p> </p>"},{"location":"devops/logging/loki/#deploy-app-to-collect-logs","title":"Deploy App to Collect logs","text":"<ul> <li>Lets deploy a app and collect logs by using grafana-loki</li> </ul> <p>deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-generator\n  labels:\n    app: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n        - name: log-generator\n          image: busybox\n          command:\n            - /bin/sh\n            - -c\n            - |\n              while true; do\n                echo \"$(date) - Test log message from log-generator\"\n                sleep 5\n              done\n</code></pre> <pre><code>kubectl create ns app\nkubectl apply -f deployment.yaml -n app\n</code></pre> <ul> <li>Go to grafana ui and select explore section  in left-hand-menu and select log generator pod</li> </ul> <p> </p> <ul> <li>We can add labels for spesific pod or app by manipulating promtail-config</li> </ul> <p><pre><code>kubectl get secret \nkubectl get secret loki-promtail -o jsonpath=\"{.data.promtail\\.yaml}\" | base64 --decode &gt; promtail-config.yaml\n</code></pre> promtail-config.yaml <pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}                \n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre></p> <ul> <li> <p>our logs format : <pre><code>2024-10-26 14:19:06.065 \n{\"log\":\"Sat Oct 26 11:19:06 UTC 2024 - Test log message from log-generator\\n\",\"stream\":\"stdout\",\"time\":\"2024-10-26T11:19:06.022721667Z\"}\n</code></pre></p> </li> <li> <p>We want to add 'time' and 'stream' section as a labels.</p> </li> <li> <p>Add below code-block into promtail-config.yaml</p> </li> </ul> <pre><code>      - match:\n          selector: '{app=\"log-generator\"}'\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:\n</code></pre> <ul> <li>new promtail-config.yaml as below.</li> </ul> <pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}\n\n      - match:                                         ##first\n          selector: '{app=\"log-generator\"}'            ## app label\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:                                  ### last \n\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre> <ul> <li>To apply the new Promtail configuration, the Loki secret needs to be deleted and recreated with the new configuration.</li> </ul> <p><pre><code>kubectl delete secret loki-promtail\n</code></pre> - Create secret by using new promtail-config</p> <p>loki-secret.yaml</p> <p><pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: loki-promtail\n  namespace: logging\n  labels:\n    app: promtail\ntype: Opaque\nstringData:\n  promtail.yaml: |\n    server:\n      log_level: info\n      log_format: logfmt\n      http_listen_port: 3101\n\n    clients:\n      - url: http://loki:3100/loki/api/v1/push\n\n    positions:\n      filename: /run/promtail/positions.yaml\n\n    scrape_configs:\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          - cri: {}\n          - match:\n              selector: '{app=\"log-generator\"}'\n              stages:\n                - json:\n                    expressions:\n                      stream: stream\n                      time: time\n                - labels:\n                    stream:\n                    time:\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_instance\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_node_name\n            target_label: node_name\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_namespace\n            target_label: namespace\n          - action: replace\n            replacement: $1\n            separator: /\n            source_labels:\n            - namespace\n            - app\n            target_label: job\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_name\n            target_label: pod\n          - action: replace\n            source_labels:\n            - __meta_kubernetes_pod_container_name\n            target_label: container\n          - action: replace\n            replacement: /var/log/pods/*$1/*.log\n            separator: /\n            source_labels:\n            - __meta_kubernetes_pod_uid\n            - __meta_kubernetes_pod_container_name\n            target_label: __path__\n          - action: replace\n            regex: true/(.*)\n            replacement: /var/log/pods/*$1/*.log\n            separator: /\n            source_labels:\n            - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n            - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n            - __meta_kubernetes_pod_container_name\n            target_label: __path__\n\n    limits_config:\n\n    tracing:\n      enabled: false\n</code></pre> <pre><code>kubectl apply -f loki-secret.yaml\n</code></pre> - To allow the Promtail pod to use the new configuration file, we need to restart the pod.</p> <p><pre><code>kubectl delete pod &lt;loki-promtail-pod-name&gt;\n</code></pre> - Wait until the new pod is up and running.</p> <ul> <li>Now we can see time and stream as a label in log </li> </ul> <p></p>"},{"location":"devops/logging/loki/#add-dashboard-grafana-to-loki-for-checking-logs","title":"Add Dashboard Grafana to loki for checking logs","text":"<ul> <li>Go to grafana webui and select dashboard left-hand-menu and click new and import</li> <li>Enter the template ID  --&gt; 15141  and click load</li> <li>Select Loki as a data source</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/","title":"\ud83d\udd10 Monitoring TLS Certificate Expiration in Kubernetes with Alerting","text":"<p>This document explains how to monitor TLS certificate expiration dates in a Kubernetes cluster and generate alerts when they approach critical thresholds.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#objective","title":"\ud83c\udfaf Objective","text":"<ul> <li>Monitor the expiration dates of TLS certificates used within Kubernetes components.</li> <li>Automatically generate alerts when certificates are about to expire or if any read errors occur.</li> <li>Ensure continuous and secure cluster operation by proactively addressing certificate issues.</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#components-used","title":"\ud83e\uddf0 Components Used","text":"<ul> <li><code>x509-certificate-exporter</code> (deployed via Helm)</li> <li>Prometheus + Alertmanager (from kube-prometheus-stack)</li> <li>ArgoCD (with ApplicationSet for GitOps-style deployment)</li> <li>Grafana (for visualizing certificate metrics)</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#deployment-steps","title":"\ud83d\ude80 Deployment Steps","text":""},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#1-deploy-x509-certificate-exporter-with-argocd-helm","title":"1\ufe0f\u20e3 Deploy <code>x509-certificate-exporter</code> with ArgoCD (Helm)","text":"<p>The following <code>ApplicationSet</code> definition deploys the <code>x509-certificate-exporter</code> Helm chart to the cluster:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: x509-certificate-exporter\n  namespace: argocd\nspec:\n  generators:\n    - list:\n        elements:\n          - cluster: dev\n  template:\n    metadata:\n      name: '{{cluster}}-x509-certificate-exporter'\n    spec:\n      syncPolicy:\n        automated:\n          selfHeal: true\n        syncOptions:\n          - CreateNamespace=true\n      destination:\n        namespace: rke2-cert-monitoring\n        name: '{{cluster}}'\n      project: default\n      source:\n        repoURL: \"https://charts.enix.io\"\n        targetRevision: 3.18.1 # if need change me\n        chart: x509-certificate-exporter\n        helm:\n          values: |\n            hostPathsExporter:\n              podAnnotations:\n                prometheus.io/port: \"9793\"\n                prometheus.io/scrape: \"true\"\n              daemonSets:\n                cp:\n                  nodeSelector:\n                    node-role.kubernetes.io/control-plane: \"true\"\n                    beta.kubernetes.io/os: \"linux\"\n                  tolerations:\n                    - effect: \"NoExecute\"\n                      key: \"CriticalAddonsOnly\"\n                      operator: \"Exists\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/server/tls/client-admin.crt\n                    - /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt\n                    - /var/lib/rancher/rke2/server/tls/server-ca.crt\n                    - /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt\n                    - /var/lib/rancher/rke2/server/tls/client-ca.crt\n                etcd:\n                  nodeSelector:\n                    node-role.kubernetes.io/etcd: \"true\"\n                    beta.kubernetes.io/os: \"linux\"\n                  tolerations:\n                    - effect: \"NoExecute\"\n                      key: \"CriticalAddonsOnly\"\n                      operator: \"Exists\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/server/tls/etcd/server-client.crt\n                    - /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt\n                worker:\n                  affinity:\n                    nodeAffinity:\n                      requiredDuringSchedulingIgnoredDuringExecution:\n                        nodeSelectorTerms:\n                          - matchExpressions:\n                              - key: \"node-role.kubernetes.io/worker\"\n                                operator: In\n                                values:\n                                  - \"true\"\n                          - matchExpressions:\n                              - key: \"node-role.kubernetes.io/worker\"\n                                operator: In\n                                values:\n                                  - \"worker\"\n                  watchFiles:\n                    - /var/lib/rancher/rke2/agent/client-ca.crt\n                    - /var/lib/rancher/rke2/agent/client-kubelet.crt\n                    - /var/lib/rancher/rke2/agent/client-kube-proxy.crt\n                    - /var/lib/rancher/rke2/agent/client-rke2-controller.crt\n                    - /var/lib/rancher/rke2/agent/server-ca.crt\n                    - /var/lib/rancher/rke2/agent/serving-kubelet.crt\n            prometheusPodMonitor:\n              create: true\n            secretsExporter:\n              enabled: false\n</code></pre>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#2-prometheus-alert-rules","title":"2\ufe0f\u20e3 Prometheus Alert Rules","text":"<p>Add the following alert rules to the Prometheus alerting configuration to monitor the exporter and certificate expiration:</p> <pre><code>groups:\n  - name: x509-certificate-exporter.rules\n    rules:\n      - alert: X509ExporterReadErrors\n        annotations:\n          summary: Increasing read errors for x509-certificate-exporter\n          description: &gt;\n            Over the last 15 minutes, this x509-certificate-exporter instance\n            has experienced errors reading certificate files or querying the Kubernetes API.\n            This could be caused by a misconfiguration if triggered when the exporter starts.\n        expr: delta(x509_read_errors[15m]) &gt; 0\n        for: 5m\n        labels:\n          severity: warning\n\n      - alert: CertificateRenewal\n        annotations:\n          summary: Certificate should be renewed\n          description: &gt;\n            Certificate for \"{{ $labels.subject_CN }}\" should be renewed\n            {{if $labels.secret_name }}in Kubernetes secret \"{{ $labels.secret_namespace }}/{{ $labels.secret_name }}\"\n            {{else}}at location \"{{ $labels.filepath }}\"{{end}}\n        expr: ((x509_cert_not_after - time()) / 86400) &lt; 28\n        for: 15m\n        labels:\n          severity: warning\n\n      - alert: CertificateExpiration\n        annotations:\n          summary: Certificate is about to expire\n          description: &gt;\n            Certificate for \"{{ $labels.subject_CN }}\" is about to expire\n            {{if $labels.secret_name }}in Kubernetes secret \"{{ $labels.secret_namespace }}/{{ $labels.secret_name }}\"\n            {{else}}at location \"{{ $labels.filepath }}\"{{end}}\n        expr: ((x509_cert_not_after - time()) / 86400) &lt; 14\n        for: 15m\n        labels:\n          severity: critical\n</code></pre>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#grafana-dashboard-integration","title":"\ud83d\udcca Grafana Dashboard Integration","text":"<p>To visualize certificate metrics, import the official Grafana dashboard created for the <code>x509-certificate-exporter</code>.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#importing-via-grafanacom-dashboard-id","title":"\ud83d\udd27 Importing via Grafana.com Dashboard ID","text":"<ol> <li>Log in to your Grafana instance.</li> <li>Go to the left menu and select + Create &gt; Import.</li> <li>In the Import via grafana.com field, enter the following dashboard ID:</li> </ol> <pre><code>13922\n</code></pre> <ol> <li>Click Load.</li> <li>Select your Prometheus data source.</li> <li>Click Import to complete the setup.</li> </ol> <p>This dashboard provides a visual representation of certificate expiration dates, issuers, subjects, and alert states.</p>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#summary","title":"\u2705 Summary","text":"<p>By completing the above steps, you will have:</p> <ul> <li>Deployed a certificate monitoring exporter to your Kubernetes cluster</li> <li>Configured Prometheus alert rules for expiring certificates</li> <li>Visualized certificate metrics in Grafana</li> <li>Ensured proactive alerting and observability for certificate expiration</li> </ul>"},{"location":"devops/monitoring/Rke2-cert-expiration/Rke2-cert-monitoring/#references","title":"\ud83d\udce6 References","text":"<ul> <li>Enix x509-certificate-exporter</li> <li>Helm Chart Repository</li> <li>Grafana Dashboard 13922</li> </ul>"},{"location":"devops/nexus/docker-hosted-repo/","title":"Docker Hosted Repository","text":""},{"location":"devops/nexus/docker-hosted-repo/#docker-hosted-repository","title":"Docker Hosted Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(hosted)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8082.</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul>"},{"location":"devops/nexus/docker-hosted-repo/#push-an-image-to-nexus","title":"Push an image to Nexus","text":"<p>Tag the image to repository url with HTTP connector port:</p> <pre><code>docker tag &lt;IMAGE&gt;:&lt;VERSION&gt; &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8082\" # hosted repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>If access to a repository requires the user to be authenticated, Docker will check for authentication access in the <code>.docker/config.json</code>file on your local machine. If authentication is not found, you will need to perform a <code>docker login</code> command.</p> <pre><code>docker login -u &lt;username&gt; &lt;EC2_PUBLIC_IP&gt;:8082\n</code></pre> <p>Then, push the image:</p> <pre><code>docker push &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre>"},{"location":"devops/nexus/docker-proxy-repo/","title":"Docker Proxy Repository","text":""},{"location":"devops/nexus/docker-proxy-repo/#docker-proxy-repository","title":"Docker Proxy Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(proxy)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8083.</li> <li>Enable Docker V1 API support if required by the remote repository.</li> <li>Add remote storage URL being proxied (e.g. https://registry-1.docker.io, https://gcr.io)</li> <li>If your remote repository is docker hub, select docker index as \"Use Docker Hub\". Otherwise, select \"Use proxy registry (specified above)\"</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8083\"  # proxy repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>From docker cli, pull an image but don't pull it from docker hub or gcr, pull it through the HTTP endpoint of your docker proxy repo that you have created above like so:</p> <pre><code>docker pull &lt;EC2_PUBLIC_IP&gt;:8083/example-image\n</code></pre> <p>This will create a pull request to your Nexus OSS, which will proxy the request to remote repository you specified before. The image from remote repository will be cached in your Nexus and will be delivered to you.</p>"},{"location":"devops/nexus/nexus-installation/","title":"Nexus Installation - Docker Private Registry","text":"<p>This guide shows how to install Nexus to an EC2 instance and run as a container.</p>"},{"location":"devops/nexus/nexus-installation/#requirements","title":"Requirements","text":"<ul> <li>EC2 Instance</li> <li>Docker</li> </ul>"},{"location":"devops/nexus/nexus-installation/#ec2-configurations","title":"EC2 Configurations","text":"<ul> <li>Min. 4 GB memory</li> <li>Edit security group rules to allow port range from 8080 to 8082.</li> <li>Enable Auto-assign public IP</li> </ul>"},{"location":"devops/nexus/nexus-installation/#installation","title":"Installation","text":"<p>Since docker volumes are persistent, a volume can be created specifically for this purpose. This is the recommended approach.</p> <pre><code>docker volume create --name nexus-data\n</code></pre> <p>The next step is to mount the volume with docker run command. We will use port 8081 to connect Nexus. Other port(s) are used for repository connections. Repository ports must be unique.</p> <p>In this guide, we open port 8082 as docker hosted repository connection and port 8083 as docker proxy connection.</p> <pre><code>docker run -d -p 8081:8081 -p 8082:8082 -p 8083:8083 --restart always --name nexus -v nexus-data:/nexus-data sonatype/nexus3\n</code></pre> <p>Browse following URL:</p> <pre><code>http://&lt;EC2_INSTANCE_PUBLIC_IP&gt;:8081\n</code></pre> <p>Default username is admin. To see the password, run the command:</p> <pre><code>sudo cat /var/lib/docker/volumes/nexus-data/_data/admin.password\n</code></pre> <p> After login, it is mandatory to set a new password.</p>"},{"location":"devops/nexus/nexus-user-and-roles/","title":"User And Roles","text":""},{"location":"devops/nexus/nexus-user-and-roles/#principle-of-least-privilege","title":"Principle of least privilege","text":"<p>For security purposes, we should use roles and users to grant permissions for specific tasks.</p>"},{"location":"devops/nexus/nexus-user-and-roles/#create-role-and-user","title":"Create Role and User","text":"<ul> <li>Type : Select Nexus role</li> <li>Privileges: Add <code>nx-repository-admin-*-*-*</code> This permission will allow all actions for all artifact and repository types. <ul> <li>First and second \"*\" represent recipe and repository type (docker hosted, docker proxy, apt hosted, apt proxy etc.) </li> <li>Last one represents actions (add,browse,read,edit,delete)</li> </ul> </li> <li>Create a new user using the role just created.</li> </ul> <p> In Nexus Repository, the <code>Docker Bearer Token Realm</code> is required in order to allow anonymous pulls from Docker repositories</p> <p>To allow anonymous pull:</p> <ul> <li>Go to <code>Realms</code> in Secutiry, add <code>Docker Bearer Token Realm</code> to active category.</li> <li>Edit the repo and click <code>Allow anonymous docker pull</code></li> </ul>"},{"location":"devops/nexus/pull-to-kubernetes/","title":"Pull Images to Kubernetes","text":""},{"location":"devops/nexus/pull-to-kubernetes/#copy-nexus-credentials-into-kubernetes","title":"Copy Nexus Credentials into Kubernetes","text":"<p>As we mentioned before, the login process creates or updates a config.json file that holds an authorization token.</p> <p>View the config.json file:</p> <pre><code>cat ~/.docker/config.json\n</code></pre> <p>The output contains a section similar to this:</p> <p><pre><code>{\n    \"auths\": {\n        \"&lt;EC2_PUBLIC_IP&gt;:8082\": {\n            \"auth\": \"c3R...zE2\"\n        }\n    }\n}\n</code></pre> A Kubernetes cluster uses the secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p> <p>If you already ran docker login, you can copy that credential into Kubernetes:</p> <pre><code>kubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=~/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then, add the secret to default service account.</p> <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"secret-name\"}]}'\n</code></pre> <p>Here is a manifest for an example Pod that needs access to your Docker credentials:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-private-pod\nspec:\n  containers:\n    - name: private\n      image: yourusername/privateimage:version\n  imagePullSecrets:\n    - name: secret-name\n</code></pre>"},{"location":"devops/nexus/registry-configuration/","title":"RKE2 Registry Configuration","text":"<p>Upon startup, RKE2 will check to see if a <code>registries.yaml</code> file exists at <code>/etc/rancher/rke2/</code> and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.</p> <p>Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the <code>registries.yaml</code> file and give different examples of using private registry configuration in RKE2.</p>"},{"location":"devops/nexus/registry-configuration/#configuration-file","title":"Configuration File","text":"<p>The file consists of two main sections:</p> <ul> <li>mirrors</li> <li>configs</li> </ul> <p>Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\n</code></pre> <p>When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one.</p> <p>The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define <code>auth</code> and/or <code>tls</code>. The credentials consist of either username/password or authentication token.</p>"},{"location":"devops/nexus/registry-configuration/#with-tls","title":"With TLS","text":"<p>Below are examples showing how you may configure <code>/etc/rancher/rke2/registries.yaml</code> on each node when using TLS.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: username # this is the registry username\n      password: password # this is the registry password\n    tls:\n      cert_file:            # path to the cert file used to authenticate to the registry\n      key_file:             # path to the key file for the certificate used to authenticate to the registry\n      ca_file:              # path to the ca file used to verify the registry's certificate\n      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate\n</code></pre> <p> If using a registry using plaintext HTTP without TLS, you need to specify <code>http://</code> as the endpoint URI scheme.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"http://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n</code></pre>"},{"location":"devops/postgres/backup-restore/","title":"Backup","text":"<p>https://www.postgresql.org/docs/current/app-pgdump.html</p>"},{"location":"devops/postgres/backup-restore/#opsec","title":"OPSEC","text":"<p>About the versions</p> <p>Make sure to use the same version on all postgres tooling. Do this OPSEC every time!</p> <ul> <li>Check the postgres config, including the version number on the last line:   <pre><code>pg_config\n</code></pre></li> <li>Check the <code>pg_dump</code> version:   <pre><code>pg_dump --version\npg_dumpall --version\n</code></pre></li> <li>Check the <code>psql</code> version:   <pre><code>psql --version\n</code></pre></li> <li>Check the <code>pg_restore</code> version:   <pre><code>pg_restore --version\n</code></pre></li> </ul> <p>Make sure all the versions match. Otherwise you might run into issues when restoring the backup.</p>"},{"location":"devops/postgres/backup-restore/#choosing-the-right-backup-tool","title":"Choosing the right backup tool","text":"tool right time to use <code>pg_basebackup</code> [Physical Copy] Typically used for disaster recovery scenarios or when you need a complete copy of the database cluster. It allows for easy and efficient restoration of the entire database cluster to a specific point in time. <code>pg_dumpall</code> [All DBs in Server] Routine backups, database migration, or replicating the database structure and data to another server. <code>pg_dump</code> [Single DB in Server] Commonly used for routine backups of individual databases, database migration, and selective restoration of specific databases or objects."},{"location":"devops/postgres/backup-restore/#pg_dump","title":"pg_dump","text":"<ul> <li><code>pg_dump</code> only dumps a single database.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_dumpall","title":"pg_dumpall","text":"<ul> <li> <p>Use <code>pg_dumpall</code> to back up an entire cluster, or to back up global objects that are common to all databases in a cluster (such as roles and tablespaces).</p> </li> <li> <p>Used for logical backups, creating a script that contains SQL commands to recreate the database objects and data.</p> </li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_basebackup","title":"pg_basebackup","text":"<ul> <li>Primarily used for creating physical backups of the entire PostgreSQL database cluster, including all databases, tablespaces, and configuration files.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_restore","title":"pg_restore","text":"<p>TODO: add pg_restore docs</p>"},{"location":"devops/postgres/configuration/","title":"Postgres Configuration","text":""},{"location":"devops/postgres/configuration/#helpers","title":"Helpers","text":"<ul> <li>postgresqlco.nf/ Documentation for explanations of all the configuration options.<ul> <li>You can upload your <code>postgresql.conf</code> file and it will give you recommendations on how to improve it.</li> </ul> </li> <li>pgtune.leopard.in.ua/#/ for generating a <code>postgresql.conf</code> file based on your hardware and database usage.</li> </ul>"},{"location":"devops/postgres/configuration/#notes","title":"Notes","text":"<ul> <li> <p>Always set the <code>PGDATA</code> environment variable to the path of the data directory. As the default directory used by the postgres image could change in the future.</p> </li> <li> <p>Always set the <code>POSTGRES_PASSWORD</code> environment variable. If not set, a random password will be generated and printed in the logs. </p> </li> <li>Always manage the password externally, for example by using a Kubernetes Secret.</li> </ul> <p>Status of postgres process</p> <ul> <li>Check the status     <pre><code>sudo systemctl restart postgresql.service\n</code></pre></li> <li>[If Applicable] Enable the service to start on boot      <pre><code>sudo systemctl enable postgresql.service\n</code></pre></li> </ul>"},{"location":"devops/postgres/configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li> <p>Located at: <code>/etc/postgresql/&lt;version&gt;/main/*</code></p> </li> <li> <p>After changing configuration, you must apply it with      <pre><code>sudo systemctl restart postgresql.service\n</code></pre></p> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_identconf-os-user-identification","title":"pg_ident.conf (OS User Identification)","text":"<ul> <li>This file is used for Operation System User -&gt; Database User mapping in PostgreSQL. </li> <li>It allows you to define mappings between a local operating system user and a PostgreSQL database user. </li> </ul>"},{"location":"devops/postgres/configuration/#postgresqlconf-configuration","title":"postgresql.conf (Configuration)","text":"<ul> <li>This is the primary configuration file for PostgreSQL. </li> <li>It contains a wide range of settings that control the behavior of the database server. </li> <li>These settings include parameters such as: <ul> <li>database connection settings, </li> <li>memory allocation, </li> <li>logging options, </li> <li>security configurations,</li> <li>and performance-related settings.</li> </ul> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_hbaconf-host-based-authentication","title":"pg_hba.conf (Host Based Authentication)","text":"<ul> <li>This file controls client authentication in PostgreSQL. </li> <li>It specifies the rules for allowing or denying client connections based on various authentication methods such as: <ul> <li>password authentication, </li> <li>certificate-based authentication, </li> <li>or IP address-based authentication. </li> </ul> </li> <li>It is responsible for defining who can connect to the database and how they are authenticated.</li> </ul>"},{"location":"devops/postgres/configuration/#pg_statconf-statistics","title":"pg_stat.conf (Statistics)","text":"<ul> <li>This file is related to the statistics collection in PostgreSQL. </li> <li>It defines which statistics are collected and how they are stored. </li> <li>By configuring this file, you can control the collection of various statistics such as: <ul> <li>the number of tuples read or written, </li> <li>index usage, </li> <li>query execution time, and more.  These statistics help in monitoring and performance tuning of the database.</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/","title":"PoC","text":""},{"location":"devops/postgres/poc-backup-restore/#setup","title":"Setup","text":"<ul> <li>create an Ubuntu VM</li> <li>install postgres</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#backup","title":"Backup","text":"<ul> <li>Assumes you have a shell access running postgres instance.<ul> <li>If you're trying to do this over a network, you'll need to figure out other options for the commands run (e.g. host, port, etc.)</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#steps","title":"Steps","text":""},{"location":"devops/postgres/poc-backup-restore/#1-go-to-a-directory-where-the-postgres-user-has-write-access","title":"1. Go to a directory where the <code>postgres</code> user has write access","text":"<pre><code>cd /tmp\n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#2-create-a-backup-file","title":"2. Create a Backup file","text":"<p>!!! Note If you're not certain on which tool to use, try to use the <code>pg_dumpall</code>.</p> <p>2.A. Using <code>pg_dump</code></p> <p>Only dumps a single database named <code>postgres</code>. </p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dump -U postgres -d postgres -f db_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dump -Fc -U postgres postgres &gt; db_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul> <p>2.B. Using <code>pg_dumpall</code></p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#upload-the-backup-file-to-a-remote-storage","title":"Upload the backup file to a remote storage","text":"<ul> <li>Create a script that will upload the backup file to a remote storage (e.g. AWS S3, Azure Blob Storage, etc.)</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#restore","title":"Restore","text":""},{"location":"devops/postgres/poc-backup-restore/#restore-a-single-db","title":"Restore a Single DB","text":"<p>We are now working on the host machine that'll be backed up to.</p> <p>[IA] Delete the existing DB from the server <pre><code>sudo -u postgres -- dropdb existing-db-name\n</code></pre></p> <p>[IA] Create a new DB to restore to</p> <p>Create a new DB named <code>new-db-name</code> from the <code>template0</code> template. <pre><code>sudo -u postgres -- createdb -T template0 new-db-name\n</code></pre></p> <p>Restore a single DB from .sql file</p> <p>SQL files are restored with <code>psql</code>.</p> <pre><code>sudo -u postgres -- psql -U postgres -f db_backup.sql new-db-name \n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#restore-a-entire-db-server","title":"Restore a Entire DB Server","text":""},{"location":"devops/postgres/poc-backup-restore/#check-the-restoration","title":"Check the Restoration","text":"<p>Enter the psql shell to verify the restore was successful. <pre><code>sudo -u postgres -- psql -U postgres\n</code></pre> Run psql commands: <pre><code>-- list databases \n\\l\n-- connect to the new db\n\\c new-db-name\n-- list tables\n\\dt\n-- list rows in a table\nSELECT * FROM users;\n-- list users\n\\du\n-- list groups\n\\dg\n</code></pre></p>"},{"location":"devops/postgres/psql/","title":"psql CLI","text":""},{"location":"devops/postgres/psql/#installation","title":"Installation","text":"<p>Make sure to use the appropriate version of the client for the server</p> <p><code>bash     sudo apt install postgresql-client-&lt;version-number&gt;</code></p> <p>Go to official Download Page</p>"},{"location":"devops/postgres/psql/#running-commands","title":"Running commands","text":"<p>Accessing with the OS user</p> <p>Run your commands as the <code>postgres</code> user. This is the default user created by the postgres image.</p> <pre><code># sudo -u postgres:: will run the command as the postgres user\nsudo -u postgres psql &lt;database-name&gt;\n</code></pre> <p>The <code>postgres</code> user is the only user that can connect to the database without a password, create other users and databases.</p> <p>Using the credentials</p> <pre><code>psql --host &lt;your-servers-dns-or-ip&gt; \\\n    --username postgres \\\n    --password \\\n    --dbname template1\n</code></pre>"},{"location":"devops/postgres/psql/#psql-commands","title":"psql commands","text":"Command Description <code>\\conninfo</code> Details about the current db connection <code>\\l</code> List all databases <code>\\c &lt;db-name&gt;</code> Connect/Select a database <code>\\dt+</code> Show tables in the current database <code>\\dg+</code> Show users in the current database"},{"location":"devops/rancher/rancher-installation/","title":"Rancher Installation","text":"<p>This is a follow up to \"RKE2 Ansible Installation\" and assumes you're working on an RKE2 cluster similar to the one set up in that document.</p> <p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> <li> <p>Install kubectl if it's not already installed</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm that our nodes and pods are correct and health</p> <pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#install-rancher-on-the-rke2-cluster","title":"Install Rancher on the RKE2 cluster","text":"<ol> <li> <p>Install Helm</p> <pre><code>sudo snap install helm --classic\n</code></pre> </li> <li> <p>Add Helm chart repository (used latest here, can be latest, stable or alpha)</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> </li> <li> <p>Create a namespace for Rancher</p> <pre><code>kubectl create namespace cattle-system\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-rancher-generated-tls-cert","title":"Using Rancher-Generated TLS Cert","text":"<ol> <li> <p>Install cert-manager (needed if using Rancher-generated TLS cert or Let\u2019s Encrypt)</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.11.0\n</code></pre> </li> <li> <p>Verify that cert-manager is deployed correctly</p> <pre><code>kubectl get pods --namespace cert-manager\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --set hostname=10.40.140.8.nip.io \\\n  --set bootstrapPassword=admin \\\n  --set global.cattle.psp.enabled=false\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> <li> <p>Wait for Rancher to be rolled out</p> <pre><code>watch kubectl -n cattle-system get pods\n</code></pre> </li> <li> <p>In a web browser navigate to the DNS name that points to your load balancer (ex: <code>10.40.140.8:nip.io</code>), you should see the login page</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-your-own-certs","title":"Using your own Certs","text":""},{"location":"devops/rancher/rancher-installation/#formatting-certs","title":"Formatting Certs","text":"<p>It can cause complications while editing files on a machine running some form of Windows and uploading them to a Linux server. Windows-based text editors put special characters at the end of lines to denote a line return or newline. There is a simple way to correct this problem.</p> <ul> <li> <p>Install <code>dos2unix</code> and execute the command to convert line endings from DOS to Unix</p> <pre><code>sudo apt install -y dos2unix\n\ndos2unix /path/to/file/&lt;file-name&gt;\n</code></pre> </li> <li> <p>Windows servers use .pfx files which contain the public and private key. However, this can also be converted to .pem files to be used on Linux server</p> <pre><code>openssl pkcs12 -in cert.pfx -nocerts -out tls.key -nodes\nopenssl pkcs12 -in cert.pfx -nokeys -out tls.crt\n</code></pre> </li> </ul>"},{"location":"devops/rancher/rancher-installation/#validating-certs","title":"Validating Certs","text":"<p>Before you set up your certificates, it's a good idea to test them to ensure that they are correct and will work together.</p> <ol> <li> <p>Check to see if the private key and main certificate are in PEM format. <code>openssl</code> must be installed</p> <pre><code>sudo apt install openssl -y\n\nopenssl rsa -inform PEM -in /path/to/key/tls.key\nopenssl x509 -inform PEM -in /path/to/cert/tls.crt\n</code></pre> </li> <li> <p>Verify that the private key and main certificate match</p> <pre><code>openssl x509 -noout -modulus -in tls.crt | openssl md5\nopenssl rsa -noout -modulus -in tls.key | openssl md5\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Verify that the public keys contained in the private key file and the main certificate are the same</p> <pre><code>openssl x509 -in tls.crt -noout -pubkey\nopenssl rsa -in tls.key -pubout\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Check the validty of certificate chain</p> <pre><code>openssl verify -CAfile cacerts.pem tls.crt\n\n# Response must be OK.\n</code></pre> </li> <li> <p>Check if <code>Subject Alternative Names</code> contains <code>Common Name</code></p> </li> </ol> <p>Subject Alternative Name must contains the same value of the CN. If it does not, the certificate is not valid because the industry moves away from CN</p> <pre><code>openssl x509 -noout -subject -in tls.crt\n# subject= /CN=&lt;example.domain.com&gt;\nopenssl x509 -noout -in tls.crt -text | grep DNS\n# DNS:&lt;example.domain.com&gt;\n</code></pre>"},{"location":"devops/rancher/rancher-installation/#create-secrets-and-install","title":"Create Secrets and Install","text":"<ol> <li> <p>Create tls-ca secret with your private CA's root certificate</p> <pre><code>kubectl -n cattle-system create secret generic tls-ca \\\n    --from-file=cacerts.pem=./cacerts.pem\n</code></pre> </li> <li> <p>Create cert and key secrets</p> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n    --cert=tls.crt \\\n    --key=tls.key\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n    --namespace cattle-system \\\n    --set hostname=10.40.140.8.nip.io \\\n    --set bootstrapPassword=admin \\\n    --set global.cattle.psp.enabled=false \\\n    --set ingress.tls.source=secret \\\n    --set privateCA=true\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#cleanup","title":"Cleanup","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Rancher using helm</p> <pre><code>helm uninstall rancher -n cattle-system\n</code></pre> </li> <li> <p>Remove Helm repositories</p> <pre><code>helm repo remove jetstack\nhelm repo remove rancher-latest\n</code></pre> <p>Change <code>rancher-latest</code> with the version you used while installing (ex: <code>stable</code>, <code>alpha</code>)</p> </li> <li> <p>Remove Helm</p> <pre><code>sudo snap remove helm\n</code></pre> </li> </ol>"},{"location":"devops/sealed-secrets/sealed-secrets/","title":"Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets","text":"<p>Let's start by defining Sealed Secrets and addressing the problem it aims to solve.</p> <p>When working with Kubernetes manifests, we often store them in Git version control systems alongside our application code. However, a challenge arises when dealing with Kubernetes Secret manifests, as they contain data that needs to be hidden but is stored in base64 format, which can be easily decrypted.</p> <p>This is where Sealed Secrets comes to the rescue. It allows us to encrypt our Secrets into SealedSecrets, making them safe to store, even in public repositories. The decryption of SealedSecrets is only possible by the controller running in the target cluster, ensuring that not even the original author can obtain the original Secret from the SealedSecret.</p> <p>To achieve this, two components are required: kubeseal, which we install locally, and the controller running in Kubernetes. With kubeseal, we encrypt our Secret manifests before pushing them to the Git repository. The Kubernetes controller is responsible for decrypting these encrypted Secret objects.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#sealed-secret-example","title":"Sealed Secret Example","text":"<p>These encrypted secrets are encoded in a SealedSecret resource, which you can see as a recipe for creating a secret. Here is how it looks:</p> <p><pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\nspec:\n  encryptedData:\n    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n</code></pre> Once unsealed, this produces a Secret equivalent to the following: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> Having explored how a Secret is securely stored in a Git repository, let's proceed to the usage.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#installation","title":"Installation","text":"<p>Firstly, let's set up the Sealed Secrets controller in Kubernetes. I'll perform this task using Helm.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#helm-chart","title":"Helm Chart","text":"<p>The Sealed Secrets helm chart is now officially supported and hosted in this GitHub repo.</p> <p><pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\n</code></pre> <pre><code>helm install sealed-secrets -n kube-system sealed-secrets/sealed-secrets\n</code></pre></p> <p>When the controller in the cluster up-and-running, it will generate a key. We will perform the encryption process with kubeseal, which we will install locally. You can find the kubeseal installation below for Mac and linux.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#kubeseal","title":"Kubeseal","text":""},{"location":"devops/sealed-secrets/sealed-secrets/#for-mac-homebrew","title":"For Mac (Homebrew)","text":"<p>The <code>kubeseal</code> client is also available on homebrew:</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"devops/sealed-secrets/sealed-secrets/#for-linux","title":"For Linux","text":"<p>The <code>kubeseal</code> client can be installed on Linux, using the below commands:</p> <p><pre><code>KUBESEAL_VERSION='' # Set this to, for example, KUBESEAL_VERSION='0.23.0'\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION:?}/kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> If you have <code>curl</code> and <code>jq</code> installed on your machine, you can get the version dynamically this way. This can be useful for environments used in automation and such.</p> <p><pre><code># Fetch the latest sealed-secrets version using GitHub API\nKUBESEAL_VERSION=$(curl -s https://api.github.com/repos/bitnami-labs/sealed-secrets/tags | jq -r '.[0].name' | cut -c 2-)\n\n# Check if the version was fetched successfully\nif [ -z \"$KUBESEAL_VERSION\" ]; then\n    echo \"Failed to fetch the latest KUBESEAL_VERSION\"\n    exit 1\nfi\n\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION}/kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> With the tools in place, both the cluster-side controller/operator and the client-side utility kubeseal are ready.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#usage","title":"Usage","text":"<p>Assuming we have a Kubernetes Secret manifest file named <code>mysecret.yaml</code> as follows: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> We can encrypt the Kubernetes Secret file using the following command: <pre><code>kubeseal --controller-name sealed-secrets -o yaml -n kube-system &lt; mysecrets.yaml &gt; encrypted-mysecret.yaml\n</code></pre> Subsequently, we can apply the <code>encrypted-mysecret.yaml</code> file: <pre><code>kubectl apply -f encrypted-mysecret.yaml\n</code></pre> To summarize the process:</p> <ul> <li>The Sealed Secrets controller, installed with Helm, generated public and private keys.</li> <li>We encrypted the decrypted mysecret.yaml file locally using the kubeseal command.</li> <li>When deploying the encrypted-mysecret.yaml file into Kubernetes, the controller decrypted it with the private key, converting it into a Kubernetes Secret.</li> </ul> <p>Now, with peace of mind, we can store our Secret manifests in Git repositories, especially if you are using GitOps, where you can automate your work by planning the file directory containing your encrypted manifests as an ArgoCD application.</p>"},{"location":"devops/service-mesh/istio/","title":"What is Istio?","text":"<p>Istio is an open-source service mesh solution that provides secure and observable networking between services running on Kubernetes. It helps manage service-to-service communication, enhances security, and controls traffic flow with minimal changes to application code. This document covers the following aspects of Istio:</p> <ul> <li>How Istiod (Istio Control Plane) works</li> <li>What mutual TLS (mTLS) is and how it secures service-to-service communication</li> <li>The differences between installing Istio using Helm and istioctl</li> <li>Different methods for enabling sidecar injection, including exclusions</li> <li>Demonstrating mTLS with two example pods</li> </ul>"},{"location":"devops/service-mesh/istio/#how-istiod-works","title":"How Istiod Works","text":"<p>Istiod is the control plane of Istio. It manages the configuration and behavior of the data plane, which consists of Envoy proxies deployed as sidecars in pods. The main responsibilities of Istiod include:</p> <ul> <li>Service Discovery: Detecting and managing services within the cluster.</li> <li>Configuration Management: Dynamically configuring Envoy proxies based on Istio policies.</li> <li>Traffic Control: Routing traffic based on rules defined in VirtualService and DestinationRule objects.</li> <li>Security Enforcement: Handling authentication, authorization, and mTLS policies.</li> <li>Observability: Collecting telemetry data such as logs, metrics, and traces for better monitoring.</li> </ul>"},{"location":"devops/service-mesh/istio/#what-is-mutual-tls-mtls","title":"What is Mutual TLS (mTLS)?","text":"<p>Mutual TLS (mTLS) is a security feature of Istio that ensures encrypted and authenticated communication between services. With mTLS enabled:</p> <ul> <li>Each service is issued a cryptographic certificate to verify its identity.</li> <li>Traffic between services is encrypted, preventing eavesdropping.</li> <li>Unauthorized services cannot communicate with protected services.</li> </ul> <p>Istio supports different mTLS modes:</p> <ul> <li>PERMISSIVE: Allows both plaintext and mTLS communication (useful for migration).</li> <li>STRICT: Enforces mTLS for all service-to-service communication.</li> <li>DISABLE: Turns off mTLS.</li> </ul>"},{"location":"devops/service-mesh/istio/#installing-istio-helm-vs-istioctl","title":"Installing Istio (Helm vs istioctl)","text":"<p>Istio can be installed using Helm or istioctl, with each having different advantages:</p> Method Pros Cons Helm More control over Kubernetes manifests, easier GitOps integration More manual configuration required istioctl Simplifies installation, automatic best-practice settings Less control over individual manifests"},{"location":"devops/service-mesh/istio/#install-istio-with-helm","title":"Install Istio with Helm","text":"<pre><code>helm repo add istio-official https://istio-release.storage.googleapis.com/charts\nhelm repo update\nhelm install my-istiod istio-official/istiod --version 1.25.0 -n istio-system --create-namespace\n</code></pre>"},{"location":"devops/service-mesh/istio/#enabling-sidecar-injection","title":"Enabling Sidecar Injection","text":"<p>Sidecar injection ensures that each pod in a namespace gets an Envoy proxy automatically. Istio supports different methods of enabling injection:</p>"},{"location":"devops/service-mesh/istio/#1-namespace-based-auto-injection","title":"1. Namespace-Based Auto Injection","text":"<pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> <ul> <li>This method enables automatic sidecar injection for all new pods in the <code>default</code> namespace.</li> <li>Existing pods must be restarted to get the sidecar.</li> </ul>"},{"location":"devops/service-mesh/istio/#2-manual-sidecar-injection","title":"2. Manual Sidecar Injection","text":"<pre><code>kubectl get deployment nginx -o yaml | istioctl kube-inject -f - | kubectl apply -f -\n</code></pre> <ul> <li>This injects the Envoy proxy manually into the <code>nginx</code> deployment.</li> <li>Useful for cases where you don't want automatic injection.</li> </ul>"},{"location":"devops/service-mesh/istio/#3-excluding-pods-from-injection","title":"3. Excluding Pods from Injection","text":"<p>To exclude specific pods from sidecar injection, use annotations:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: no-sidecar-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\nspec:\n  containers:\n  - name: app\n    image: nginx\n</code></pre> <p>This ensures that the pod does not get an Envoy sidecar, even if injection is enabled for the namespace.</p>"},{"location":"devops/service-mesh/istio/#verifying-sidecar-injection","title":"Verifying Sidecar Injection","text":"<p>To test if sidecar injection is working correctly, deploy an Nginx pod and check its status:</p> <pre><code>kubectl create deployment nginx --image=nginx\nkubectl get pods\n</code></pre> <p>Expected output (<code>READY</code> should be <code>2/2</code>):</p> <pre><code>NAME                     READY   STATUS    RESTARTS   AGE\nnginx-xxxxxxx-xxxxx      2/2     Running   0          1m\n</code></pre>"},{"location":"devops/service-mesh/istio/#enabling-and-testing-mtls","title":"Enabling and Testing mTLS","text":"<p>To enforce STRICT mTLS, apply the following <code>PeerAuthentication</code> policy:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\n</code></pre> <p>Apply with: <pre><code>kubectl apply -f mtls-policy.yaml\n</code></pre></p>"},{"location":"devops/service-mesh/istio/#testing-mtls-with-two-pods","title":"Testing mTLS with Two Pods","text":"<ol> <li>Deploy two pods (<code>client</code> and <code>server</code>) in the <code>default</code> namespace:</li> </ol> <pre><code>kubectl run client --image=curlimages/curl -i --tty -- sh\nkubectl run server --image=nginx\n</code></pre> <ol> <li>Try sending a request from <code>client</code> to <code>server</code>:</li> </ol> <pre><code>kubectl exec -it client -- curl http://server.default.svc.cluster.local\n</code></pre> <p>If mTLS is enabled (STRICT mode), the request should fail because direct HTTP (plaintext) communication is blocked. If mTLS were PERMISSIVE, the request would succeed.</p>"},{"location":"devops/service-mesh/istio/#conclusion","title":"Conclusion","text":"<ul> <li>Istiod manages service discovery, security, and traffic routing.</li> <li>mTLS ensures encrypted and authenticated service-to-service communication.</li> <li>Helm vs istioctl: Helm provides more control, istioctl is easier.</li> <li>Sidecar Injection can be enabled per namespace, manually, or excluded per pod.</li> <li>mTLS can be tested using two pods to verify encryption enforcement.</li> </ul> <p>This completes the basic Istio setup with security best practices.</p>"},{"location":"devops/service-mesh/istio/#kaynaklar","title":"Kaynaklar","text":"<ul> <li>Istio Resmi Dok\u00fcmantasyonu</li> <li>YouTube: Dok\u00fcmantasyona \u00f6\u011fretti\u011fi bilgilerle y\u00f6n veren Hintli abi \ud83e\udd1d</li> </ul>"},{"location":"devops/sonarqube/advanced-installation/","title":"How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide","text":"<p>SonarQube is a top-tier source code quality management application that provides comprehensive code analysis and support for 17 programming languages. It is the preferred solution for static code analysis and code coverage, and it is extensively used by both developers and organizations.</p> <p>This article will provide you with a step-by-step guide to establishing SonarQube using Docker Compose, which is integrated with a PostgreSQL database and a Nginx proxy to redirect traffic to port 80 on your domain.</p> <p>Finally, we will configure SonarQube authentication and authorization to an LDAP server by configuring the appropriate values in <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code>.</p>"},{"location":"devops/sonarqube/advanced-installation/#prerequisites","title":"Prerequisites","text":"<p>Before we start, ensure you have the following:</p> <ul> <li>An instance with a minimum of 2 vCPUs and 4 GB RAM.</li> <li>Docker and Docker Compose are installed on your machine.</li> <li>Open port 80</li> </ul> <p>We may move further with the system settings now.</p>"},{"location":"devops/sonarqube/advanced-installation/#system-configuration","title":"System Configuration","text":"<p>SonarQube has to make some system adjustments because it uses Elasticsearch to store its indices in an MMap FS directory. You must ensure that:</p> <ul> <li>The process is allowed to have a maximum of 524288 memory map areas, as specified by the <code>vm.max_map_count</code> parameter.</li> <li>The value of the maximum number of open file descriptors <code>fs.file-max</code> is set to a minimum of 131072.</li> <li>The SonarQube user has a minimum capacity to open 131072 file descriptors.</li> <li>The SonarQube user has the capability to initiate a minimum of 8192 threads.</li> </ul> <p>Use the steps provided below according to your operating system:</p>"},{"location":"devops/sonarqube/advanced-installation/#for-red-hat-centos-or-amazon-linux","title":"For Red Hat, CentOS, or Amazon Linux","text":"<pre><code>sudo yum update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#for-ubuntu-or-debian","title":"For Ubuntu or Debian","text":"<pre><code>sudo apt update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#installation","title":"Installation","text":"<p>In order to establish our services, we require a <code>docker-compose.yml</code> file.</p> <pre><code>version: \"3\"\n\nservices:\n  sonarqube:\n    image: sonarqube:latest\n    container_name: sonarqube\n    restart: unless-stopped\n    depends_on:\n      - db\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar\n      SONAR_JDBC_USERNAME: sonar\n      SONAR_JDBC_PASSWORD: sonar\n    volumes:\n      - sonarqube_data:/opt/sonarqube/data\n      - sonarqube_extensions:/opt/sonarqube/extensions\n      - sonarqube_logs:/opt/sonarqube/logs\n    ports:\n      - \"9000:9000\"\n\n  db:\n    image: postgres:15\n    container_name: postgresql\n    environment:\n      POSTGRES_USER: sonar\n      POSTGRES_PASSWORD: sonar\n      POSTGRES_DB: sonar\n    volumes:\n      - postgresql:/var/lib/postgresql\n      - postgresql_data:/var/lib/postgresql/data\n\n  nginx:\n    image: nginx:latest\n    container_name: nginx\n    restart: unless-stopped\n    depends_on:\n      - sonarqube\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n    ports:\n      - \"80:80\"\n\nvolumes:\n  sonarqube_data:\n  sonarqube_extensions:\n  sonarqube_logs:\n  postgresql:\n  postgresql_data:\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#what-we-have-here","title":"What We Have Here?","text":"<p>Three services (SonarQube, PostgreSQL, and Nginx) are included in this compose file.</p> <ul> <li>SonarQube Service uses the latest SonarQube image, connects to the PostgreSQL database, and exposes port 9000 for the web interface. It includes volumes for persistent data, extensions, and logs.</li> <li>PostgreSQL Service sets up a PostgreSQL database with environment variables for user credentials and includes volumes for data persistence.</li> <li>Nginx Service acts as a reverse proxy using the latest Nginx image, mapping port 80 to the host. It relies on a custom Nginx configuration file.</li> </ul> <p>In order to redirect the traffic from <code>localhost:9000</code> to your domain on port 80, it is necessary to create a <code>nginx.conf</code> file. Make sure that this file is located in the same directory as the docker-compose.yml file.</p> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n\n    location / {\n        proxy_pass http://sonarqube:9000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#running-the-setup","title":"Running the Setup","text":"<p>Run the following command to start the setup:</p> <p><pre><code>sudo docker-compose up -d\n</code></pre> Docker Compose orchestrates and executes your complete application. To access SonarQube, use the domain indicated in your <code>nginx.conf</code> file, which is <code>yourdomain.com</code></p>"},{"location":"devops/sonarqube/advanced-installation/#ldap-configuration","title":"LDAP Configuration","text":"<p>Integrating LDAP (Lightweight Directory Access Protocol) with SonarQube is an essential process for firms seeking to centrally manage user authentication and authorization. LDAP integration enables the utilization of an already established LDAP directory, such as Active Directory, for the purpose of managing users. This streamlines administration by ensuring that there is just one authoritative source for user data. </p> <p>The main setup for LDAP integration in SonarQube is performed via the <code>sonar.properties</code> file. This file is usually located in  <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code></p> <pre><code># LDAP Configuration\n# Enable the LDAP feature\nsonar.security.realm=LDAP\n\n# General Configuration\nldap.url=ldap://ldap_server_ip:389\nldap.bindDn=CN=your_sonar_user,OU=Service Accounts,DC=your_domain,DC=top_level_domain\nldap.bindPassword=password_here\n\n# User Configuration\nldap.user.baseDn=DC=your_domain,DC=top_level_domain\nldap.user.request=(&amp;(objectClass=user)(sAMAccountName={login}))\nldap.user.realNameAttribute=cn\nldap.user.emailAttribute=mail\n\n# Group Configuration\nldap.server1.group.baseDn=DC=your_domain,DC=top_level_domain\nldap.server1.group.request=(&amp;(objectClass=group)(memberUid={uid}))\n</code></pre> <p>To implement the new configuration, restart the SonarQube server after saving the changes to the sonar.properties file.</p> <pre><code>sudo systemctl restart sonarqube\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#accessing-sonarqube","title":"Accessing SonarQube","text":"<p>Verify the LDAP configuration by attempting to log in with an LDAP user account after SonarQube has restarted. Ensure that the user attributes, such as email and name, are accurately populated from the LDAP directory.</p> <p> </p>"},{"location":"devops/sre/k8sgpt/","title":"k8sgpt","text":"<p>As Kubernetes continues to grow in popularity, managing and monitoring your clusters can become quite complex. That's where k8sgpt comes in. k8sgpt is a tool that uses advanced AI to make Kubernetes management easier. It helps you with tasks like monitoring your clusters and solving problems by understanding and responding to natural language commands. In this blog, I\u2019ll look at what k8sgpt does, how it can help you, and why it might be a great addition to your Kubernetes setup.</p> <p>The project already supports multiple installation options and different AI backends such as openAI,localAI,azureAI etc. In this blog, I will show you how to install and get started with K8sGPT, both the CLI tools and the Operator as well as how K8sGPT supports additional integrations.</p>"},{"location":"devops/sre/k8sgpt/#installation","title":"Installation","text":"<p>You can choose from several installation methods based on your operating system and personal preferences. The installation section of the documentation provides a list of these options.</p> <pre><code>https://docs.k8sgpt.ai/getting-started/installation/\n</code></pre> <p>The installation of K8sGPT requires that Homebrew is installed on your Mac.</p> <p>Run the following commands:</p> <pre><code>brew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n</code></pre> <ul> <li>To verify that K8sGPT is install correctly, you can check the version installed:</li> </ul> <pre><code>k8sgpt version\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-cli","title":"K8sGPT CLI","text":"<p>The K8sGPT CLI is a tool that lets you manage your Kubernetes clusters using simple, natural language commands. It makes it easier to control and troubleshoot your clusters by letting you type commands.</p> <ul> <li>To view all the commands provided by K8sGPT:</li> </ul> <pre><code>k8sgpt --help\n</code></pre> <p></p> <p>You can see an overview of the different commands also below link:</p> <pre><code>https://docs.k8sgpt.ai/reference/cli/\n</code></pre>"},{"location":"devops/sre/k8sgpt/#authorise-an-ai-backend","title":"Authorise an AI Backend","text":"<p>K8sGPT supports multiple AI providers, allowing you to choose from a variety of options to fit your needs. For example, you can use providers like OpenAI, localAI and AzureAI. By default, OpenAI is the primary provider, but keep in mind that analyzing data with OpenAI requires a minimum balance of $5. To avoid this cost and still utilize powerful AI capabilities, I opted to use LocalAI for my setup. This choice allowed me to seamlessly integrate AI without incurring additional expenses.</p> <p>Ollama is an easy-to-use tool that helps you quickly start using open source large language models (LLMs). You can download Ollama from https://ollama.com/ and install it on your computer. Once installed, launch Ollama and use the command provided to download a LLM. Note that the mistral model requires ~ 4GB of disk space.</p> <pre><code>brew update\nbrew install ollama\nbrew services start ollama\nollama pull mistral\n</code></pre>"},{"location":"devops/sre/k8sgpt/#configure-k8sgpt-to-use-localai-using-ollama-backend","title":"Configure k8sgpt to use LocalAI using Ollama backend","text":"<pre><code>k8sgpt auth add --backend localai --model mistral --baseurl http://localhost:11434/v1\n</code></pre> <p>You can list your backends with the following command:</p> <pre><code>k8sgpt auth list\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#install-a-malicious-deployment","title":"Install a malicious Deployment","text":"<p>Next, we will install a malicious Deployment into our Kubernetes cluster.This manifest is deliberately malicious as it includes an incorrect command (slep instead of sleep), causing the pod to continuously fail and enter a CrashLoopBackOff state, which makes it an ideal example for analyzing and debugging issues with K8sGPT. Here is the YAML:</p> <p>deployment yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\n  labels:\n    app: example-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-container\n        image: busybox\n        command: [\"/bin/sh\", \"-c\", \"slep 3600\"]  # not slep\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vulnerable-service\n  labels:\n    app: vulnerable-app\nspec:\n  selector:\n    app: vulnerable-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> <ul> <li>Next, we will install the Deployment</li> </ul> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> <p>Now you will see the pods throwing errors:</p> <p></p>"},{"location":"devops/sre/k8sgpt/#analyze-your-cluster-by-using-k8s","title":"Analyze your cluster by using k8s","text":"<p>Run the command below to get GenAI-generated insights about problems in your cluster.</p> <pre><code>k8sgpt analyze --explain --backend localai --namespace default --filter Pod\nk8sgpt analyze --explain --backend localai --namespace default --filter Service\n</code></pre> <p></p> <p>The analysis provided by K8sGPT identifies a CrashLoopBackOff issue with the pod, indicating that the container within the pod encountered an error. The suggested steps to troubleshoot this include checking container logs, verifying the Docker image, ensuring correct configuration, scaling the deployment, and reviewing Kubernetes events for further insights.</p>"},{"location":"devops/sre/k8sgpt/#integration","title":"Integration","text":"<p>Integrating K8sGPT with tools like Trivy, Grafana, or Prometheus can enhance its capabilities for Kubernetes security, monitoring, and analysis.Trivy is a vulnerability scanner for container images and file systems. Integrating K8sGPT with Trivy can help analyze vulnerabilities in your Kubernetes clusters.</p> <p>You can list all available integration with the following command:</p> <pre><code>k8sgpt integration list\n</code></pre> <p>Next, we want to activate the Trivy integration:</p> <p><pre><code>k8sgpt integration activate trivy\n</code></pre> This will install the Trivy Operator inside of your cluster</p> <ul> <li>Once the integration is activated, we can use the Vulnerability Reports created by Trivy as part of our K8sGPT analysis through k8sgpt filters:</li> </ul> <p></p> <ul> <li>The filters in K8sGPT are mapped to specific analyzers in the code, which focus on extracting and examining only the most relevant information, such as the most critical vulnerabilities.</li> </ul> <p>To use the VulnerabilityReport filter </p> <pre><code>k8sgpt analyse --filter=VulnerabilityReport\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-operator","title":"K8sGPT Operator","text":"<p>The CLI tool lets cluster admins run on-the-spot scans on their infrastructure and workloads. Meanwhile, the K8sGPT operator works continuously in your cluster, running all the time. It is built specifically for Kubernetes, using Custom Resources to create reports that are stored in your cluster as YAML files.</p> <ul> <li>To install the Operator, follow the documentation link (https://docs.k8sgpt.ai/getting-started/in-cluster-operator/) or the commands provided below:</li> </ul> <p>You will  need to install the kube-prometheus-stack Helm Chart to use Grafana and Prometheus. You can do this with the following commands:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prom prometheus-community/kube-prometheus-stack -n monitoring --create-namespace\n</code></pre> <p>In this setup, we instruct K8sGPT to install a ServiceMonitor, which sends scan report metrics to Prometheus and sets up a Dashboard for K8sGPT.</p> <p>k8sgpt-values.yaml <pre><code>serviceAccount:\n  create: true\n  name: \"k8sgpt\"\n  # -- Annotations for the managed k8sgpt workload service account\n  annotations: {}\nserviceMonitor:\n  enabled: true\n  additionalLabels:\n    release: prom\n  namespace: monitoring\ngrafanaDashboard:\n  enabled: true\n  # The namespace where Grafana expects to find the dashboard\n  # namespace: \"\"\n  folder:\n    annotation: grafana_folder\n    name: ai\n  label:\n    key: grafana_dashboard\n    value: \"1\"\n  # create GrafanaDashboard custom resource referencing to the configMap.\n  # according to https://grafana-operator.github.io/grafana-operator/docs/examples/dashboard_from_configmap/readme/\n  grafanaOperator:\n    enabled: false\n    matchLabels:\n      dashboards: \"grafana\"\ncontrollerManager:\n  kubeRbacProxy:\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: gcr.io/kubebuilder/kube-rbac-proxy\n      tag: v0.16.0\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 5m\n        memory: 64Mi\n  manager:\n    sinkWebhookTimeout: 30s\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: ghcr.io/k8sgpt-ai/k8sgpt-operator\n      tag: v0.1.7  # x-release-please-version\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 10m\n        memory: 64Mi\n  replicas: 1\n  ## Node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  #\n  nodeSelector: {}\nkubernetesClusterDomain: cluster.local\nmetricsService:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: https\n  type: ClusterIP\n</code></pre></p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace -f k8sgpt-values.yaml\n</code></pre> <ul> <li>Then, we need to configure the K8sGPT Operator to know which version of K8sGPT to use and which AI backend:</li> </ul> <p>k8sgpt-backend.yaml <pre><code>apiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-ollama\nspec:\n  ai:\n    enabled: true\n    model: mistral\n    backend: localai\n    baseUrl: http://host.docker.internal:11434/v1\n  noCache: false\n  filters: [\"Pod\", \"Service\"]\n  repository: ghcr.io/k8sgpt-ai/k8sgpt\n  version: v0.3.17\n  # sink:\n  #   type: slack\n  #   webhook: &lt;url&gt;\n</code></pre> - We need to apply this file to our K8sGPT cluster namespace</p> <pre><code>kubectl apply -n k8sgpt-operator-system -f k8sgpt-backend.yaml\n</code></pre> <ul> <li>That\u2019s all with the operator setup. The operator will watch for problems in the cluster and generate analysis results that you can view using the command below</li> </ul> <pre><code>kubectl get results -n k8sgpt-operator-system -o json | jq .\n</code></pre> <ul> <li>During the installation of K8sGPT, we set serviceMonitor to true, which ensures that a ServiceMonitor is created to send metrics from K8sGPT to Prometheus. Once this is configured correctly, you should be able to see a new target for K8sGPT under the Prometheus targets section, specifically within the Kubernetes targets. This integration allows Prometheus to continuously monitor K8sGPT's performance and activity within the cluster, providing valuable insights through metrics.</li> </ul> <pre><code>kubectl port-forward service/prom-kube-prometheus-stack-prometheus -n monitoring 9090:9090\n</code></pre> <ul> <li>Open localhost:9090 and navigate to Status --&gt; Targets and then you should see the serviceMonitor with thes result:</li> </ul> <p></p> <ul> <li>Lastly Go to Grafana Dashboard.</li> </ul> <pre><code>kubectl port-forward service/prom-grafana -n monitoring 3000:3000\n</code></pre> <ul> <li>Open localhost:3000 and then navigate to Dashboards&gt;K8sGPT Overview and then you will see the Dashboard with your results:</li> </ul> <p></p> <p>This blog  explained the main features of K8sGPT. First, we looked at the K8sGPT CLI, then how it works better with integrations, and finally installed the operator for continuous analysis inside the cluster with grafana dashboard.</p>"},{"location":"devops/sumo/sumo-linux-collector/","title":"Install a Sumo Logic Collector on Linux","text":""},{"location":"devops/sumo/sumo-linux-collector/#download-a-sumo-logic-collector-from-a-static-url","title":"Download a Sumo Logic Collector from a Static URL","text":"<p>Invoke a web request utility such as wget. For Linux 64-bit host, you can wget the Collector from the command line:</p> <pre><code>wget \"https://collectors.sumologic.com/rest/download/linux/64\" -O SumoCollector.sh &amp;&amp; chmod +x SumoCollector.sh\n</code></pre> <p>For other hosts choose the related one above:</p> Platform Download URL Linux 64 https://collectors.au.sumologic.com/rest/download/linux/64 Linux Debian https://collectors.au.sumologic.com/rest/download/deb/64 Linux RPM https://collectors.au.sumologic.com/rest/download/rpm/64 Tarball https://collectors.au.sumologic.com/rest/download/tar Windows 32 https://collectors.au.sumologic.com/rest/download/windows Windows 64 https://collectors.au.sumologic.com/rest/download/win64 <p>Important Note: The latest release of the Sumo Collector targets the Java 8 runtime. Java 6 and Java 7 are no longer supported as the Collector runtime, and Solaris is no longer supported. When you upgrade Collectors, JRE 8 or later is required. The Sumo Collector with a bundled JRE now ships with JRE 8.</p>"},{"location":"devops/sumo/sumo-linux-collector/#system-requirements","title":"System Requirements","text":"<p>The Sumo Logic Collector has the following system requirements:</p> <ul> <li>Operating System: Linux, major distributions 64-bit, or any generic Unix capable of running Java 1.8</li> <li>CPU: Single core</li> <li>RAM: 512MB</li> <li>Disk Space: 8GB</li> <li>TLS: Package installers require TLS 1.2 or higher</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#install-using-the-command-line-installer","title":"Install using the command line installer","text":"<ol> <li>Add execution permissions to the downloaded Collector file (.sh):</li> </ol> <p><pre><code>chmod +x SumoCollector.sh\n</code></pre> 2. Run the script with the parameters that you want to configure </p>"},{"location":"devops/sumo/sumo-linux-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<p><pre><code>sudo ./SumoCollector.sh -q -Vsumo.token_and_url=&lt;installationToken&gt; -Vsources=&lt;absolute_filepath&gt;\n</code></pre> By default, the Collector will be installed in either <code>/opt/SumoCollector</code> or <code>/usr/local/SumoCollector</code>.</p>"},{"location":"devops/sumo/sumo-linux-collector/#other-parameters-for-the-command-line-installer","title":"Other parameters for the command line installer","text":"<p>The command line installer also supports a number of other parameters, including:</p> <ul> <li>-dir [directory] : Sets a different installation directory than the default.</li> <li>-Vsumo.accessid=[accessId] : The access ID is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.accesskey=[accessKey] : The access key is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.token_and_url=[token] : The token can be either an Installation Token or Setup Wizard Token.</li> </ul> <p>The command line installer can also use all of the parameters available in the user.properties file. To use parameters from user.properties just add a <code>-V</code> to the beginning of the parameter without a space character.</p> <p>The following parameters have a different format in the command line installer:</p> user.properties cli name <code>-Vcollector.name</code> url <code>-Vcollector.url</code> proxyHost <code>-Vproxy.host</code> proxyPort <code>-Vproxy.port</code> proxyUser <code>-Vproxy.user</code> proxyPassword <code>-Vproxy.password</code>"},{"location":"devops/sumo/sumo-linux-collector/#other-userproperties-parameters","title":"Other user.properties parameters","text":"<p>The user.properties file can also be used to configure the following parameters:</p> <ul> <li>-Vdescription : Description of the collector</li> <li>-VhostName : Name of the host machine that the collector is installed</li> <li>-Vsources : The contents of the file or files are read upon Collector registration only, it is not synchronized with the Collector's configuration on an on-going basis.</li> <li>-VsyncSources : The Source definitions will be continuously monitored and synchronized with the Collector's configuration.</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#start-or-stop-a-collector-using-scripts","title":"Start or Stop a Collector using Scripts","text":"<p>To start, stop, check the status of the Collector or restart it, run one of the following commands from the Collector installation directory: <pre><code>sudo ./collector start\n</code></pre> <pre><code>sudo ./collector stop\n</code></pre> <pre><code>sudo ./collector status\n</code></pre> <pre><code>sudo ./collector restart\n</code></pre></p>"},{"location":"devops/sumo/sumo-linux-collector/#uninstall-using-the-command-line","title":"Uninstall using the command line","text":"<ol> <li> <p>In a terminal prompt, change the directory to the collector installation directory:</p> </li> <li> <p>Run the uninstall binary with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts:</p> <pre><code>sudo ./uninstall -q\n</code></pre> </li> </ol>"},{"location":"devops/sumo/sumo-local-file-management/","title":"Local Configuration File Management","text":""},{"location":"devops/sumo/sumo-local-file-management/#local-configuration-file-management","title":"Local Configuration File Management","text":"<p>With local configuration file management, you can configure Sources for an Installed Collector in one or more UTF-8 encoded JSON files.</p> <p>IMPORTANT NOTE: After you switch over to local configuration file management, you can no longer manage Sources through the Sumo web application or the Collector Management API.</p> <p>Local configuration file management is available on Collector version v19.108 and later.</p> <p>Benefits of local configuration file management</p> <ul> <li>You don't need to log in to the Sumo web app or use API calls. Instead, you edit the JSON configuration file(s), and they are read almost immediately by the Collector.</li> <li>If you have a large scale deployment, it can be impractical to add or edit Sources one at a time. Using local configuration management allows you to manage Sources more easily.</li> <li>You can use deployment tools so that established policies for deployments are not interrupted.</li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#options-for-specifying-sources-in-local-configuration-files","title":"Options for specifying Sources in local configuration file(s)","text":"<p>There are two ways to implement local configuration file management:</p> <ol> <li>Specify all Sources in a single UTF-8 encoded JSON file.</li> <li>Use multiple UTF-8 encoded JSON files to specify your Sources, and put all of those files in a single folder.</li> </ol> <p>Note: Each JSON file must have a <code>.json</code> extension.</p>"},{"location":"devops/sumo/sumo-local-file-management/#define-multiple-sources-in-a-json-file","title":"Define multiple Sources in a JSON file","text":"<p>When you define multiple Sources in a JSON file, you can define each Source in a <code>sources</code> JSON array. For example:</p> <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"LocalFile\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    },\n    {\n      \"sourceType\": \"RemoteFile\",\n      \"name\": \"Example2\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre> <p>To define a single source in a JSON file, you just have one source definition under the <code>sources</code> array. <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"DockerLogs\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"devops/sumo/sumo-local-file-management/#configure-the-location-of-json-file-or-folder","title":"Configure the location of JSON file or folder","text":"<p>When using local file configuration management, you specify the location of the JSON file or the folder that contains multiple JSON files in the Collector's <code>config/user.properties</code> file. You need to use the <code>syncSources</code> parameter to point to your configuration file or folder.</p> <ul> <li> <p>To point to a JSON file that defines Sources for a Collector:</p> <p><code>syncSources=/path/to/sources.json</code></p> </li> <li> <p>To point to a folder that contains JSON files that define Sources for a Collector: <code>syncSources=/path/to/sources-folder</code></p> </li> <li> <p>On Windows (note the escaped backslashes): <code>syncSources=C:\\path\\to\\sources-folder\\</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#type-of-sources","title":"Type of Sources","text":"<p>In our case, we are using the <code>\"LocalFile\"</code> type value for Local File Type.</p> <p>Note: You should add the parameter <code>\"sourceType\":\"LocalFile\"</code> in the source file.</p> <p>JSON Parameters for Installed Sources</p> Parameter Description sourceType The type of the data that the collector will collect. description Type a description of the Source. category Type a category of the source. cutoffTimestamp If you have a file that contains logs with timestamps spanning an entire week and set the cutoffTimestamp to two days ago, all of the logs from the entire week will be ingested since the file itself was modified more recent than the cutoffTimestamp pathExpression A valid path expression (full path) of the file to collect denylist Comma-separated list of valid path expressions from which logs will not be collected. encoding Defines the encoding form. Default is \"UTF-8\"; options include \"UTF-16\"; \"UTF-16BE\"; \"UTF-16LE\"."},{"location":"devops/sumo/sumo-local-file-management/#important-tip","title":"Important Tip:","text":"<p>While giving the <code>pathExpression</code> use a single asterisk wildcard <code>[*]</code> for file or folder names. For example:</p> <p><code>pathExpression: \"/var/foo/*.log\"</code></p> <p>Use two asterisks <code>[**]</code> to recurse within directories and subdirectories. For example:</p> <p><code>pathExpression: \"var/**/*.log\"</code></p>"},{"location":"devops/sumo/sumo-local-file-management/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>You can edit the JSON configuration file at any time to edit Source attributes or add new Sources. When you delete Sources from the file, they are deleted from the Collector.</p> <p>To make the changes take effect, you need to restart the Collector.</p> <p>To restart the Collector, use these commands:</p> <ul> <li> <p>Linux: <code>sudo ./collector restart</code></p> </li> <li> <p>Windows: <code>net restart sumo-collector</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#full-example-of-local-source-file","title":"Full example of Local Source file :","text":"<pre><code>{\n   \"api.version\":\"v1\",\n   \"sources\":[\n      {\n         \"name\":\"Hepapi-Test-Logs\",\n         \"category\":\"DockerLogs\",\n         \"automaticDateParsing\":true,\n         \"multilineProcessingEnabled\":false,\n         \"useAutolineMatching\":false,\n         \"forceTimeZone\":false,\n         \"timeZone\":\"UTC\",\n         \"cutoffTimestamp\":0,\n         \"encoding\":\"UTF-8\",\n         \"pathExpression\":\"/var/lib/docker/containers/*/*-json.log\",\n         \"sourceType\":\"LocalFile\"\n      } \n   ]\n}\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/","title":"Install a Sumo Logic Collector on Windows","text":""},{"location":"devops/sumo/sumo-windows-collector/#system-requirements-for-windows","title":"System Requirements for Windows","text":"<ul> <li>Windows 7, 32 or 64 bit</li> <li>Windows 8, 32 or 64 bit</li> <li>Windows 8.1, 32 or 64 bit</li> <li>Windows 10, 32 or 64 bit</li> <li>Windows 11, 32 or 64 bit</li> <li>Windows Server 2012</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> <li>Single core, 512MB RAM</li> <li>8GB disk space</li> <li>Package installers require TLS 1.2 or higher.</li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#download-a-collector-from-a-static-url","title":"Download a Collector from a Static URL","text":"<ol> <li>Open a terminal window or command prompt.</li> <li>If you're using PowerShell on a 64-bit Windows host, you can use Invoke-WebRequest:</li> </ol>"},{"location":"devops/sumo/sumo-windows-collector/#configure-usage-of-tls","title":"Configure usage of TLS","text":"<pre><code>[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Tls,Tls11,Tls12'\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/#download-the-installer","title":"Download the installer","text":"<pre><code>Invoke-WebRequest 'https://collectors.sumologic.com/rest/download/win64' -outfile '&lt;download_path&gt;\\SumoCollector.exe'\n</code></pre> <p>Replace the  with the location where you want to download the Collector. For example, <code>C:\\user\\example\\path\\to\\SumoCollector.exe</code>"},{"location":"devops/sumo/sumo-windows-collector/#install-the-sumo-logic-collector-using-the-command-line-installer","title":"Install the Sumo Logic Collector using the command line installer","text":"<p>From the command prompt, run the downloaded EXE file with the parameters that you want to configure. For example:</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<pre><code>./SumoCollector.exe -console -q \"-Vsumo.token_and_url=&lt;installationToken&gt;\" \"-Vsources=&lt;filepath&gt;\"\n</code></pre> <p>Reminder: You can pass other <code>user.properties</code> parameters as well inside <code>\"\"</code>.</p> <p>When you see the <code>Finishing installation...</code> message, you can close the command prompt window. The installation is complete.</p> Parameter Description <code>-console</code> Only has an effect when used with <code>-q</code>. Causes the installer to send progress messages to the console. On Windows, for this option to take effect, you must run the installer with start /wait. For example: <code>start /wait installer.exe -q -console</code> <code>-q</code> Causes the installer to run silently, which means you won't be prompted to supply installation parameters. For any installation parameter that you do not define at the command line, Sumo will use a default value. No output is sent to the console during installation, unless you also use the <code>-console</code> parameter."},{"location":"devops/sumo/sumo-windows-collector/#configuring-sources-for-collectors","title":"Configuring Sources for Collectors","text":"<p>After installing Collectors, you can configure Sources directly in Sumo Logic or by providing the Source settings in a JSON file.</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-a-json-file","title":"Using a JSON file","text":"<p>If you're using a UTF-8 encoded JSON file, you must provide the file before starting the Collector. The JSON file needs to be UTF-8 encoded.</p> <p>Important Note:</p> <p>In Windows Host by using double backslashes, the JSON parser will interpret each backslash as a literal character rather than an escape character. For example:</p> <ul> <li>Backslashes are NOT treated correctly:  <code>\"-Vsources=&lt;C:\\some\\path\\to\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\path\\to\\source.json&gt;\"</code></li> <li>Backslashes ARE  treated correctly:    <code>\"-Vsources=&lt;C:\\\\some\\\\path\\\\to\\\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\\\path\\\\to\\\\source.json&gt;\"</code></li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#uninstalling-from-the-command-line","title":"Uninstalling from the command line","text":"<p>From the command prompt, run the <code>uninstall.exe</code> file with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts.</p> <pre><code>./uninstall.exe -q -console\n</code></pre>"},{"location":"devops/terragrunt/Readme/","title":"Readme","text":""},{"location":"devops/terragrunt/Readme/#what-is-terragrunt","title":"What is Terragrunt?","text":"<p>Terragrunt is a popular open-source tool or \u2018thin wrapper\u2019 developed by Gruntwork, that helps manage Terraform configurations by providing additional features and simplifying workflow. It is often used to address common challenges in Terraform, such as keeping configurations DRY (Don\u2019t Repeat Yourself), managing remote state, handling multiple environments, and executing custom code before or after running Terraform.</p> <p>See Terragrunt vs Terraform for further information.</p>"},{"location":"devops/terragrunt/Readme/#terragrunt-features","title":"Terragrunt features","text":"<ol> <li> <p>Remote state management Terragrunt simplifies remote state management for Terraform projects. It can automatically configure and store state files remotely in services like Amazon S3, Google Cloud Storage, or any other backend supported by Terraform.</p> </li> <li> <p>DRY (Don\u2019t Repeat Yourself) configurations Terragrunt promotes DRY principles by allowing you to define and reuse common configurations across multiple Terraform modules. This helps reduce duplication and makes configurations more maintainable.</p> </li> <li> <p>Dependency management Terragrunt supports dependency management between different Terraform modules and states, ensuring that dependent resources are deployed in the correct order.</p> </li> <li> <p>Configuration inheritance Terragrunt allows you to create modular configurations that can inherit parameters and settings from parent configurations, making it easier to manage and organize your infrastructure code.</p> </li> <li> <p>Environment-specific configurations Terragrunt supports the creation of environment-specific configurations (e.g., dev, staging, prod) using HCL (HashiCorp Configuration Language) interpolation, making it easier to maintain consistent environments.</p> </li> <li> <p>Remote backend configurations Terragrunt allows you to specify backend configurations (e.g., S3 bucket, DynamoDB table) for each environment, enabling a more dynamic and flexible approach to state storage.</p> </li> <li> <p>Locking mechanism Terragrunt provides a locking mechanism to prevent concurrent executions that could potentially cause conflicts when modifying shared infrastructure.</p> </li> <li> <p>Secrets management Terragrunt can integrate with external secrets management tools like AWS Secrets Manager or HashiCorp Vault to handle sensitive data securely.</p> </li> <li> <p>Integration with CI/CD pipelines Terragrunt can be integrated into continuous integration and continuous deployment (CI/CD) pipelines to automate infrastructure deployments.</p> </li> <li> <p>Configurable hooks Terragrunt supports pre- and post-terraform hooks, allowing you to run custom scripts or commands before or after running Terraform commands.</p> </li> </ol>"},{"location":"devops/terragrunt/Readme/#how-does-terragrunt-work","title":"How does Terragrunt work?","text":"<p>Terragrunt relies on a configuration file called <code>terragrunt.hcl</code>. This file is placed in the root directory of your Terraform project or in the directories of specific modules. It contains settings and parameters that customize Terragrunt\u2019s behavior for your project or module.</p>"},{"location":"devops/terragrunt/Readme/#how-to-install-terragrunt","title":"How to install Terragrunt?","text":"<p>STEP 1: Install Terraform As Terragrunt is a wrapper around Terraform, you\u2019ll need to have Terraform installed first. You can download the appropriate version of Terraform for your operating system here.</p> <p>STEP 2: Extract the binary and place it in a directory included in your system\u2019s PATH After downloading Terraform, extract the binary and place it in a directory included in your system\u2019s <code>PATH</code>. The PATH tells a system where it should look for executables, making them accessible via command-line interfaces or scripts. To add a new folder to PATH in Windows, navigate to Advanced System Settings &gt; Environment Variables, select PATH, click \u201cEdit\u201d and then \u201cNew.\u201d</p> <p>STEP 3: Download Terragrunt Next, head over to the Terragrunt GitHub page to download it.</p> <p>STEP 4: Place the Terragrunt binary in a directory included in your system\u2019s PATH Once you have downloaded the Terragrunt binary, place it in a directory included in your system\u2019s <code>PATH</code>. You may also rename the binary to simply <code>terragrunt</code> (without the platform-specific suffix) for convenience.</p> <p>STEP 5: Verify the installation Lastly, verify the installation by running <code>terragrunt --version</code> on your console command line. It should show the currently installed version.</p> <pre><code>terragrunt --version\n</code></pre>"},{"location":"devops/terragrunt/Readme/#terragrunt-basic-commands","title":"Terragrunt basic commands","text":"<p>Terragrunt command should be run from the project directory that contains your <code>terragrunt.hcl</code> configuration file. Terragrunt has many of the same commands available you will be familiar with the Terraform workflow, (you just need to replace <code>terraform</code> with <code>terragrunt</code>).</p> <p>These include:</p> <ul> <li>terragrunt init</li> <li>terragrunt validate</li> <li>terragrunt plan</li> <li>terragrunt apply</li> <li>terragrunt destroy</li> <li>terragrunt graph</li> <li>terragrunt state</li> <li>terragrunt version</li> <li>terragrunt output</li> </ul> <p>Also, check out this Terraform cheat sheet.</p>"},{"location":"devops/terragrunt/Readme/#how-to-set-up-terragrunt-configurations","title":"How to set up Terragrunt configurations?","text":"<p>First, create your <code>terragrunt.hcl</code> file in the directory you want to use Terragrunt in. The <code>terragrunt.hcl</code> file consists of configuration blocks that define various settings for Terragrunt.</p> <p>Note that the Terragrunt configuration file uses the same HCL syntax as Terraform itself in <code>terragrunt.hcl</code>. Terragrunt also supports JSON-serialized HCL in a <code>terragrunt.hcl.json</code> file: where <code>terragrunt.hcl</code> is mentioned, you can always use <code>terragrunt.hcl.json</code> instead.</p> <p>The <code>terraform</code> block is used to configure how Terragrunt will interact with Terraform. You can configure things like before and after hooks for indicating custom commands to run before and after each terraform call or what CLI args to pass in for each command.</p> <p>The source attribute specifies where to find Terraform configuration files and uses the same syntax as the Terraform module source attribute.</p> <p>For example, you can pull modules directly from a Github repo:</p> <pre><code>terraform { \n  source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"\n}\n</code></pre> <p>Or modules from the local file system (Terragrunt will make a copy of the source folder in the Terragrunt working directory, typically '.terragrunt-cache'):</p> <pre><code>terraform {  \n  source = \"../modules/networking/vpc\"\n}\n</code></pre> <p>Other blocks you can configure in your <code>terraform.hcl</code> file include:</p> <ul> <li>remote_state</li> <li>include</li> <li>locals</li> <li>dependency</li> <li>dependencies</li> <li>generate</li> </ul>"},{"location":"devops/terragrunt/Readme/#example","title":"Example","text":""},{"location":"devops/terragrunt/Readme/#step-0","title":"STEP 0","text":"<p>You will need <code>dev</code> and <code>prod</code> accounts. You can create them using your main account. Then, you should create an IAM user account for logging in multiple accounts, namely SSO. In our example, our account is the \"Management Account\" which controls the other accounts. Similarly, <code>dev</code> and <code>prod</code> accounts are the \"Environment Accounts\" on which the resources are created. The IAM user account should be able to login to all account using \"Access Portal\".</p> <p>Before starting, make sure that AWS CLI is installed and configured on your desktop. If not, refer to download page of AWS CLI. After successful download, configure the CLI with IAM credentials of created IAM user. If you don't have, you can get credentials under IAM service.</p> <pre><code>aws configure\n</code></pre> <p>You will be prompted to enter your AWS Access Key ID, AWS Secret Access Key, default region name and default output format (e.g., json). Now, you are ready to proceed. Below, you can see the folder structure of the example: </p> <pre><code>modules/\n\u2514\u2500\u2500 vpc/\n    \u251c\u2500\u2500 main.tf\n    \u251c\u2500\u2500 versions.tf\n    \u251c\u2500\u2500 variables.tf\n    \u2514\u2500\u2500 outputs.tf\n\nenvironments/\n\u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for dev\n\u251c\u2500\u2500 prod/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for prod\n\u2514\u2500\u2500 terragrunt.hcl               # Top-level configuration linking all environments\n\ninitial_configs/                  \n\u251c\u2500\u2500 AWSTerraformInitialConfigs_Management.yaml\n\u2514\u2500\u2500 AWSTerraformInitialConfigs_Environment.yaml\n</code></pre> <p>As you can see, there is a <code>vpc</code> module under <code>modules</code> folder. Refer to vpc module page to get the module. Then, change the static values as variables and define the variables in <code>variables.tf</code>. Also, you can add <code>outputs.tf</code> to check if the module is successfully created. Note that there is no <code>.tfvars</code> file and we will see handle this issue in the next steps.</p>"},{"location":"devops/terragrunt/Readme/#step-1","title":"STEP 1","text":"<p>Let's analyze the files under <code>initial_configs</code> folder. Starting with <code>AWSTerraformInitialConfigs_Management.yaml</code>, this CloudFormation template defines resources and configurations necessary for managing Terraform state using AWS S3 and DynamoDB, and it provisions an IAM user with the required permissions. </p> <p>PARAMETERS:</p> <ul> <li>Serial: A value used to notify CloudFormation to rotate access keys.</li> <li>IaCUserName: The IAM user name (default: terraform).</li> <li>TerraformStateBucketPrefix: Prefix for the S3 bucket storing Terraform state.</li> <li>TerraformStateLockTableName: Name of the DynamoDB table for state locking.</li> </ul> <p>RESOURCES:</p> <ul> <li> <p>IaCUser (IAM User): Creates an IAM user with tags indicating its provision through CloudFormation and its usage for management purposes.</p> </li> <li> <p>IaCUserPolicy (IAM Policy): Grants the IAM user permissions to: Manage the Terraform S3 bucket (create, access, and configure it). Lock Terraform state via DynamoDB (create, read, update, and delete items). Assume the TerraformExecutionRole for executing tasks.</p> </li> <li> <p>IaCUserAccessKey &amp; IaCUserSecret (IAM Access Keys &amp; Secret): Creates and stores the access keys in AWS Secrets Manager for secure access.</p> </li> <li> <p>TerraformStateS3Bucket (S3 Bucket): Creates an S3 bucket to store Terraform state files. It enforces security policies like blocking public access and enabling versioning.</p> </li> <li> <p>TerraformStateS3BucketBucketPolicy (S3 Bucket Policy): Adds a policy to the S3 bucket that denies the deletion of Terraform state files.</p> </li> <li> <p>TerraformStateLockDynamoDBTable (DynamoDB Table): Creates a DynamoDB table (LockID as the key) for Terraform state locking to prevent concurrent modifications of the state.</p> </li> </ul> <p>NOTE: You should create a stack in CloudFormation and upload this file on the \"Management Account\". Now, let's proceed with the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. </p> <p>PARAMETERS:</p> <ul> <li>IaCUserARN: A string parameter representing the ARN of the IAM user responsible for running Terraform. This user will be allowed to assume the role defined in the template. By default, this value needs to be provided (though a placeholder \"ARN of the IaC User\" is set).</li> </ul> <p>RESOURCES:</p> <ul> <li>TerraformExecutionRole (IAM Role): The TerraformExecutionRole is an IAM role that grants specific AWS permissions for Terraform operations, allowing a specified IAM user (via IaCUserARN) to assume it for a maximum of 4 hours. It is associated with the AdministratorAccess policy, providing full administrative privileges, and includes tags for tracking its provisioning through CloudFormation.</li> </ul> <p>NOTE: Look at the \"IaCUserARN\" from the \"Management Account\" and assign this value to the \"Parameters\" section of the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. You should create a stack in CloudFormation and upload this file on the \"Environment Accounts\".</p>"},{"location":"devops/terragrunt/Readme/#step-2","title":"STEP 2","text":"<p>Now, we are ready to analyze the <code>environments</code> folder. There is a top-level <code>terragrunt.hcl</code> under the folder which includes important configurations. This Terragrunt configuration file sets up local variables and configurations for managing Terraform modules and remote state.</p> <p>Local Variables: - base_source_url: Points to the local module source directory. - environment_vars and region_vars: Load environment-specific and region-specific variables from env.hcl and region.hcl files, respectively. - target_account, target_region, remote_state_account, and remote_state_bucket: Define the AWS account and region for the remote state and specify the bucket for storing Terraform state files.</p> <p>AWS Provider Configuration: Generates an aws provider block, setting the region for both the remote state and the target account, with an assume role for accessing the target account's resources.</p> <p>Remote State Configuration: Configures Terraform to use an S3 bucket for remote state storage, with encryption enabled and a DynamoDB table for state locking.</p> <p>Global Parameters: Merges global inputs from env.hcl and region.hcl, allowing all resources to inherit these configurations, which is useful for multi-account setups.</p> <p>Let's move with the environment folders. The structure is similar in both of the environments, so it will be enough to analyze one of them. Under the environments, there are regions in which the resources are created. Also, there is a <code>env.hcl</code> which includes local variables. The variables in the <code>env.hcl</code> are important because they determine the account. Diving into one of the regions, you can see the <code>region.hcl</code> which specifies the region. In the same directory, you can see the folders of the modules, which is <code>vpc</code> in our example. Under the <code>vpc</code> folder, there is a <code>terragrunt.hcl</code> file. This Terragrunt configuration file includes the root <code>terragrunt.hcl</code> configuration, which contains common settings for remote state management across all components and environments.</p> <p>Include Block: The configuration references the root settings using the include directive, allowing access to shared configurations and exposing them for use in the current module.</p> <p>Local Variables: It defines base_source_url from the root configuration, along with the module_name as \"vpc\" and module_version as \"v0.0.1.\"</p> <p>Terraform Source: The source for the Terraform module is set based on the base source URL, module name, and version.</p> <p>Inputs: Specifies the input variables for the VPC module, including availability zones, VPC CIDR block, NAT and VPN gateway settings, subnet configurations, and tags for environment identification.</p> <p>NOTE: Don't forget to change the necessary values of variables in this folder. For example, the value of \"aws_account_id\" in the <code>env.hcl</code>. Similarly, you should check the values in the both top and root level <code>terragrunt.hcl</code> files.</p>"},{"location":"devops/terragrunt/Readme/#step-3","title":"STEP 3","text":"<p>After compliting the necessary adjustments in the files and on the AWS console, you are ready to create the resources. You can open the folder with VSCode and using the terminal, proceed to the <code>vpc</code> folders in which the root level <code>terragrunt.hcl</code> is located. Finally, enter the following command to create VPC in desired region.</p> <pre><code>terragrunt apply\n</code></pre> <p>If you want to delete the resources, you should proceed to directory of related root level <code>terragrunt.hcl</code> file and enter the following command.</p> <pre><code>terragrunt destroy\n</code></pre> <p>For example, if you want to create a <code>dev-vpc</code> in <code>the us-east-1</code> region, go to <code>~/environments/dev/us-east-1/vpc</code> using the terminal and enter <code>terragrunt apply</code> command. Similarly, you can delete this using <code>terragrunt destroy</code>. Note that the <code>.tfstate</code> files are stored in S3 and they are locked in DynamoDB.</p> <p>The advantage of this configuration is that you can control various resources in different regions. In our example, we show this by creating a VPC  in two different regions. We created and configured multiple VPCs in different accounts and regions by only making minor changes in the structure. The same can be done using Terraform, but if the number of resources and regions increase, it would be faster and more practical to create with Terragrunt. Furthermore, you can control all of them from one point, namely \"Management Account\".</p>"},{"location":"devops/terragrunt/Readme/#terragrunt-benefits","title":"Terragrunt benefits","text":"<p>Where Terraform allows you the freedom to structure your code in multiple ways, Terragrunt places restraints on how you can organize your Terraform code and forces you to use directory structure hierarchies and shared variable definition files to organize your code. These restraints force your code to be more consistent and make it harder to make mistakes. The trade-off is that the amount of flexibility you have is reduced.</p> <p>The key to using Terragrunt effectively is to carefully plan your directory structure in order to keep your code base DRY. Organizing your infrastructure code into reusable modules that represent logical components of your infrastructure is one way to achieve this. </p>"},{"location":"devops/terragrunt/Readme/#key-points","title":"Key points","text":"<p>Terragrunt is a powerful tool that helps you manage Terraform configurations more efficiently. To make the most out of Terragrunt and maintain a clean, scalable, and organized infrastructure codebase, be sure to follow the best practices and plan your folder structure and use of Terragrunt carefully.</p>"},{"location":"devops/terragrunt/Readme/#references","title":"References","text":"<p>Terragrunt Tutorial \u2013 Getting Started &amp; Examples</p>"},{"location":"devops/vagrant/vagrant-quickstart/","title":"Vagrant","text":""},{"location":"devops/vagrant/vagrant-quickstart/#what-is-vagrant","title":"What is Vagrant?","text":"<p>CLI tool for managing the life-cycle of VMs</p> <ul> <li>Reproducible local dev environments  </li> <li>Vagrantfile, akin to Dockerfile for VMs</li> </ul>"},{"location":"devops/vagrant/vagrant-quickstart/#installation","title":"Installation","text":"<p>See the official Vagrant downloads page Ubuntu/Debian: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vagrant\n</code></pre></p> <p>You also need VirtualBox, VMware or Hyper-V on your machine.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#important-commands","title":"Important Commands","text":"<p>Initialize a new Vagrant environment by creating a Vagrantfile: <code>vagrant init</code> Starts and provisions the Vagrant environment: <code>vagrant up</code> Connects to machine via ssh: <code>vagrant ssh</code> Outputs status of the machine: <code>vagrant status</code> Suspends the machine: <code>vagrant suspend</code> Stops the machine: <code>vagrant halt</code> Stops and deletes all traces of the machine: <code>vagrant destroy</code> </p>"},{"location":"devops/vagrant/vagrant-quickstart/#boxes","title":"Boxes","text":"<p>Vagrant base images are called boxes. See the official Vagrant boxes page</p>"},{"location":"devops/vagrant/vagrant-quickstart/#synced-folders","title":"Synced Folders","text":"<p>By default, Vagrant will share your project directory (the directory with the Vagrantfile) to <code>/vagrant</code>.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#vagrantfile","title":"Vagrantfile","text":"<p>Example Vagrantfiles:</p> Vagrantfile for Jenkins   This Vagrantfile installs Jenkins, AWS CLI, unzip and zip tools.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/focal64\"\n\n  # Port forwarding\n  config.vm.network \"forwarded_port\", guest: 8080, host: 8080\n\n  config.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n    # Update repositories\n    sudo apt-get update\n\n    # Install CA certificates (optional but recommended)\n    sudo apt-get install -y ca-certificates\n\n    # Install Java (a requirement for Jenkins)\n    sudo apt-get install -y openjdk-11-jdk\n\n    # Add Jenkins repository\n    wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n    sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'\n\n    sudo apt-get update\n\n    # Install Jenkins\n    sudo apt-get install -y jenkins\n\n    # Start Jenkins\n    sudo systemctl start jenkins\n\n    # Install necessary utilities\n    sudo apt-get install -y unzip\n    sudo apt-get install -y zip\n\n    # Install AWS CLI version 2\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    unzip awscliv2.zip\n    sudo ./aws/install\n  SHELL\nend\n</code></pre> Vagrantfile using Ansible for provisioning   This Vagrantfile uses an Ansible playbook for provisioning.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.box = \"ubuntu/bionic64\"\n  config.vm.network :forwarded_port, guest: 80, host: 8080\n  config.vm.network :forwarded_port, guest: 443, host: 8081\n  config.vm.network :forwarded_port, guest: 8080, host: 8082\n  config.vm.provision \"ansible\" do |ansible|\n    ansible.playbook = \"main.yml\"\n  end\n\nend\n</code></pre> Vagrantfile with advanced networking   This Vagrantfile brings up 2 VMs, assigns them static hostnames and IPs, allows root login and password authentication, and installs Ansible on one of the VMs.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  # Define VMs\n  (1..2).each do |i|\n    config.vm.define \"vm#{i}\" do |vmconfig|\n\n      # Use CentOS 8\n      vmconfig.vm.box = \"generic/centos8\"\n\n      # Set hostname\n      vmconfig.vm.hostname = \"vm#{i}\"\n\n      # Set private network\n      vmconfig.vm.network \"private_network\", ip: \"192.168.56.1#{i}\"\n\n      # Sync project directory to /vagrant\n      vmconfig.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\"\n\n      # Enable provisioning with a shell script\n      vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n        echo 'vagrant:vagrant' | chpasswd\n        sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config\n        sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config\n        systemctl restart sshd\n      SHELL\n\n      if i == 1\n        vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n          sudo yum update -y\n          sudo yum install -y epel-release\n          sudo yum install -y python3-pip gcc openssl-devel libffi-devel python3-devel\n          sudo pip3 install --upgrade pip\n          sudo pip3 install setuptools_rust\n          pip3 install ansible\n        SHELL\n      end\n    end\n  end\n\n  # Enable ssh agent forwarding\n  config.ssh.forward_agent = true\n\nend\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/","title":"Markdown Syntax","text":""},{"location":"misc/how-to-contribute/about-markdown/#links","title":"Links","text":"<pre><code>[Link Text](https://www.example.com)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#images","title":"Images","text":"<p>Put a <code>!</code> in front of the link syntax.</p> <pre><code>![Alt Text](https://www.example.com/image.png)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#syntax-highlighting","title":"Syntax Highlighting","text":"<p>Insert the language name after the first set of backticks.</p> <pre><code># ```python\nimport os\nos.system(\"echo 'Hello World'\")\n</code></pre> <pre><code># ```bash\nexport HELLO=\"world\"\ncat some-file | grep \"hello\"\n</code></pre> <pre><code># ```yaml\nsome: key\nanother: key\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#tables","title":"Tables","text":"<pre><code>| name | value |\n| ---- | ----- |\n| a    | b     |\n</code></pre> <p>You can align headers to the left, center, or right by adding colons to the header syntax.</p> <pre><code>| --name-- | --value-- | description |\n| :------- | :-------: | ----------: |\n| a        |     b     |           c |\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#headers","title":"Headers","text":"<pre><code># H1 Header (biggest)\n\n## H2 Header\n\n### H3 Header\n\n#### H4 Header\n\n##### H5 Header\n\n###### H6 Header (smallest)\n</code></pre>"},{"location":"misc/how-to-contribute/about-mkdocs/","title":"creating new mkdocs pages","text":""},{"location":"misc/how-to-contribute/about-mkdocs/#how-to-add-a-new-page","title":"How to add a new Page","text":"<ol> <li>Create a new <code>.md</code> file under in the <code>docs/</code> folder</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section</li> <li>Do not put <code>docs/</code> prefix on the filepath</li> <li>Commit and push your changes to the <code>main</code> branch</li> <li>GitHub Action will automatically build and deploy the changes to the website.</li> </ol>"},{"location":"misc/how-to-contribute/about-mkdocs/#about-this-website","title":"About this website","text":"<p>Stack</p> Name Description mkDocs Docs static site generator Material for MkDocs Material theme for mkDocs"},{"location":"misc/how-to-contribute/about-mkdocs/#steps-to-run-it-locally","title":"Steps to run it locally","text":"<ol> <li>Make sure <code>python3</code> is installed</li> <li>Install python requirements    <pre><code>pip install mkdocstrings[python]\npip install mkdocs-material\n</code></pre></li> <li>Clone the repository &amp; <code>cd</code> into it</li> <li>Run the local server    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"misc/how-to-contribute/mkdocs-features/","title":"MkDocs and Material Theme Features","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#to-do-lists","title":"TO-DO Lists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> Aenean pretium efficitur erat</li> </ul>"},{"location":"misc/how-to-contribute/mkdocs-features/#admonitions-or-notes","title":"Admonitions or Notes","text":"<ul> <li>Supported Icon Types: note, abstract, info, tip, success, question, warning, failure, danger, error, example, quote</li> </ul> <p>Warning Note Header</p> <p>You can write notes like this to provide additional information or warnings.</p> <p>We've done it!</p> <p>Works for code blocks as well <pre><code>sudo apt install postgresql-client-&lt;version-number&gt;\n</code></pre></p> Collapsible Note (collapsed) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> Collapsible Note (expanded) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> <p>Tabbed Note</p> UbuntuRHEL/CentOS/AL2 <pre><code>apt install vim\n</code></pre> <pre><code>yum install vim\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#code-block-with-title","title":"Code block with title","text":"here is a codeblock title<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#buttons","title":"Buttons","text":"<p>Button Primary Button Emoji Button </p>"},{"location":"misc/how-to-contribute/mkdocs-features/#mermaid-diagrams","title":"Mermaid Diagrams","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#flow-chart","title":"Flow Chart","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#state-diagram","title":"State Diagram","text":"<pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre>"},{"location":"qa/","title":"Welcome to Hepapi QA Knowledge Hub","text":""},{"location":"qa/#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This section has been created to promote knowledge sharing, team collaboration, and continuous learning about Hepapi Teknoloji's QA (Quality Assurance) processes.</p> <p>QA, as one of the cornerstones of delivering high-quality software, does not only identify defects but also improves processes to ensure quality at every stage of the software development lifecycle. At Hepapi, our QA team works across different platforms with both manual and automation testing to deliver the best user experience.</p>"},{"location":"qa/#what-will-you-find-here","title":"What will you find here?","text":"<ul> <li>Fundamentals of QA: Explore every aspect of QA, from test planning to defect management.</li> <li>Automation Testing Tools: Guides on Selenium, Appium, and other testing tools.</li> <li>Best Practices: Tips for more effective and efficient testing processes. -FAQs and Guides: Solutions to frequently asked questions and helpful documentation.</li> </ul> <p>This knowledge hub is a living, evolving resource. As part of Hepapi\u2019s dynamic QA team, we continue to learn and share. If you have valuable insights or suggestions to share, don\u2019t hesitate to contribute! Remember, what might seem obvious to you could be a new discovery for someone else.</p> <p> Follow us on Linkedin   hepapi.com </p> <p>Follow the docs on: How to contribute</p>"},{"location":"qa/#compendium","title":"Compendium","text":""},{"location":"qa/Fundamentals_of_QA/","title":"Fundamentals of Quality Assurance (QA)","text":"<p>Quality Assurance (QA) is a vital process in software development that ensures the delivery of reliable, high-performing, and user-friendly products. By systematically verifying that a product meets its design specifications, QA minimizes bugs and increases customer satisfaction. Let\u2019s explore the fundamental components of QA to understand how it contributes to the success of any project.</p>"},{"location":"qa/Fundamentals_of_QA/#1-test-planning-and-defining-strategy","title":"1. Test Planning and Defining Strategy","text":"<p>The QA process begins with meticulous planning, aligning testing procedures with the project\u2019s objectives. This phase defines the structure and scope of testing, ensuring efficiency and focus.</p>"},{"location":"qa/Fundamentals_of_QA/#defining-scope","title":"Defining Scope:","text":"<ul> <li>Identify areas to be tested and those excluded.</li> <li>Prevent unnecessary tests, save time, and focus on critical features.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#resource-allocation","title":"Resource Allocation:","text":"<ul> <li>Organize tools, equipment, software, and team members for a smooth QA process.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#timeline-creation","title":"Timeline Creation:","text":"<ul> <li>Establish deadlines for each phase of testing to align with the overall project schedule.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#risk-management","title":"Risk Management:","text":"<ul> <li>Anticipate potential issues and develop mitigation strategies.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#defining-strategy","title":"Defining Strategy:","text":"<ul> <li>Choose appropriate test types (e.g., functional, performance, security).</li> <li>Select tools like Selenium or Postman.</li> <li>Establish clear success criteria, such as thresholds for critical errors.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#2-writing-test-scenarios-and-test-cases","title":"2. Writing Test Scenarios and Test Cases","text":"<p>QA professionals break down complex system operations into manageable components through test scenarios and test cases.</p>"},{"location":"qa/Fundamentals_of_QA/#test-scenarios","title":"Test Scenarios:","text":"<ul> <li>High-level descriptions of user flows.</li> <li>Example: \"The user should be able to access their profile page after logging in.\"</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#test-cases","title":"Test Cases:","text":"<ul> <li>Detailed steps to validate functionality.</li> <li>Example:</li> <li>Open the login screen.</li> <li>Enter the username and password.</li> <li>Click the \"Login\" button.</li> <li>Expected Result: The user is redirected to the homepage.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#risk-analysis","title":"Risk Analysis:","text":"<ul> <li>Prioritize scenarios critical to the business (e.g., payment processes).</li> <li>Assign lower priority to rarely used modules.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#3-bug-reporting","title":"3. Bug Reporting","text":"<p>Effective bug reporting accelerates debugging, enabling developers to resolve issues promptly.</p>"},{"location":"qa/Fundamentals_of_QA/#a-well-documented-bug-report-includes","title":"A Well-Documented Bug Report Includes:","text":"<ul> <li>Bug Category: Impact on performance, user interface, or security.</li> <li>Severity: Impact level (e.g., \"User cannot log in\" vs. \"A button is misaligned\").</li> <li>Reproduction Steps: Step-by-step actions to reproduce the issue.</li> <li>Supporting Materials: Screenshots, error logs, or video recordings.</li> </ul> <p>Clear and thorough reports reduce ambiguity and enhance developer efficiency.</p>"},{"location":"qa/Fundamentals_of_QA/#4-test-tracking-and-reporting","title":"4. Test Tracking and Reporting","text":"<p>Continuous monitoring and reporting are essential to keep stakeholders informed about the QA process.</p>"},{"location":"qa/Fundamentals_of_QA/#test-tracking-tools","title":"Test Tracking Tools:","text":"<ul> <li>Platforms like Jira or TestRail enable real-time updates on test statuses and bug progress.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#reporting","title":"Reporting:","text":"<ul> <li>Regular updates (daily, weekly, or sprint-based) on testing outcomes and areas of concern.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#key-metrics","title":"Key Metrics:","text":"<ul> <li>Bug resolution rate.</li> <li>Remaining workload.</li> <li>Ratio of passed vs. failed tests.</li> </ul> <p>These metrics provide insight into project health and testing efficiency.</p>"},{"location":"qa/Fundamentals_of_QA/#5-preparing-test-environments","title":"5. Preparing Test Environments","text":"<p>A realistic and reliable test environment is crucial for accurate testing results.</p>"},{"location":"qa/Fundamentals_of_QA/#key-considerations","title":"Key Considerations:","text":"<ul> <li>Accurate Data: Use dummy data that mimics real-world usage.</li> <li>Accessibility: Ensure seamless access for all team members to avoid delays.</li> <li>Backup and Recovery: Contingency plans for data loss ensure testing continuity.</li> </ul>"},{"location":"qa/Fundamentals_of_QA/#conclusion","title":"Conclusion","text":"<p>Quality Assurance is more than just identifying bugs; it's a comprehensive process that ensures a product meets its functional, performance, and user experience goals. By following a structured QA process\u2014test planning, scenario writing, bug reporting, tracking, and preparing environments\u2014teams can deliver software that meets and exceeds customer expectations.</p> <p>Investing in QA from the beginning is not merely a cost but a strategic move to build trust, ensure reliability, and secure long-term success.</p>"},{"location":"qa/QA_Bug_Reporting/","title":"Bug Management","text":""},{"location":"qa/QA_Bug_Reporting/#1what-is-a-bug","title":"1.What is a Bug?","text":"<p>A bug is a mismatch between the expected behavior and the actual behavior of a system. Bugs can be categorized as follows:</p> <ul> <li>Coding Bugs: Issues originating from developer errors.</li> <li>Requirement Bugs: Problems caused by incorrect or incomplete requirements.</li> <li>UI Bugs: Design and usability issues in the user interface.</li> <li>Performance Bugs: System slowdowns or crashes under high load.</li> </ul>"},{"location":"qa/QA_Bug_Reporting/#2types-of-bugs","title":"2.Types of Bugs","text":"<p>Proper classification of bugs is crucial for determining resolution priorities.</p> <ul> <li>Critical: Bugs that cause system crashes or prevent main functions from working. Example: Payment system not operational.</li> <li>Major: Bugs that affect significant functions but can be temporarily worked around. Example: Login fails for some users.</li> <li>Minor: Bugs that affect user experience but do not break the system. Example: Misaligned button.</li> <li>Trivial: Low-priority bugs, often cosmetic. Example: Typo in text.</li> </ul>"},{"location":"qa/QA_Bug_Reporting/#3bug-tracking-tools","title":"3.Bug Tracking Tools","text":"<p>Bug tracking tools like Jira allow teams to: - Create and assign bugs to specific members. - Track the bug resolution process (open, in progress, resolved). - Share bug reports easily.</p>"},{"location":"qa/QA_Bug_Reporting/#steps-in-bug-management","title":"Steps in Bug Management","text":"<ol> <li>Detection: Identify the bug during testing.</li> <li>Reporting: Log a detailed report in the system.</li> <li>Analysis: Investigate the root cause.</li> <li>Resolution: Fix and re-test the bug.</li> <li>Closure: Close the bug once resolved.</li> </ol> <p>Transparency is critical throughout the process.</p>"},{"location":"qa/QA_Testing_Process/","title":"Fundamental Testing Process","text":""},{"location":"qa/QA_Testing_Process/#1test-planning-and-defining-strategy","title":"1.Test Planning and Defining Strategy","text":"<p>Testing begins with planning, aligning procedures with project objectives.</p>"},{"location":"qa/QA_Testing_Process/#key-steps","title":"Key Steps:","text":"<ul> <li>Defining Scope: Identify areas to be tested and those excluded.</li> <li>Resource Allocation: Organize equipment, tools, and team members.</li> <li>Timeline Creation: Set deadlines for each phase.</li> <li>Risk Management: Assess potential risks and create mitigation strategies.</li> </ul>"},{"location":"qa/QA_Testing_Process/#defining-strategy","title":"Defining Strategy:","text":"<ul> <li>Choose test types (functional, performance, security).</li> <li>Specify tools (e.g., Selenium, Postman).</li> <li>Clarify test success criteria.</li> </ul>"},{"location":"qa/QA_Testing_Process/#2writing-test-scenarios-and-test-cases","title":"2.Writing Test Scenarios and Test Cases","text":"<ul> <li>Test Scenarios: High-level user flows.</li> <li>Example: \"The user should access their profile page after logging in.\"</li> <li>Test Cases: Detailed steps.</li> <li>Example:<ol> <li>Open login screen.</li> <li>Enter username/password.</li> <li>Click \"Login\" button.</li> <li>Expected Result: Redirect to homepage.</li> </ol> </li> </ul>"},{"location":"qa/QA_Testing_Process/#risk-analysis","title":"Risk Analysis:","text":"<ul> <li>High-risk areas (e.g., payment processes) need rigorous testing.</li> <li>Less used modules have lower priority.</li> </ul>"},{"location":"qa/QA_Testing_Process/#3bug-reporting","title":"3.Bug Reporting","text":"<p>A good bug report includes: - Category: Performance, UI, security. - Severity: Impact level (e.g., login failure vs. button misalignment). - Reproduction Steps: Detailed actions to replicate the bug. - Supporting Materials: Screenshots, error logs, etc.</p>"},{"location":"qa/QA_Testing_Process/#4test-tracking-and-reporting","title":"4.Test Tracking and Reporting","text":"<p>Use tools like Jira or TestRail to manage test statuses and open bugs.</p>"},{"location":"qa/QA_Testing_Process/#reporting","title":"Reporting:","text":"<ul> <li>Regular updates (daily, weekly, sprint-based).</li> </ul>"},{"location":"qa/QA_Testing_Process/#metrics","title":"Metrics:","text":"<ul> <li>Resolved bugs ratio.</li> <li>Remaining workload.</li> </ul>"},{"location":"qa/QA_Testing_Process/#5preparing-test-environments","title":"5.Preparing Test Environments","text":"<p>Ensure a realistic and reliable test environment. - Accurate Data: Use dummy data. - Accessibility: Seamless team access. - Backup/Recovery: Prepare for data loss.</p>"},{"location":"qa/SSH_Config/","title":"Managing Multiple SSH Keys for QA Consultants","text":"<p>As QA consultants, managing SSH keys efficiently across projects is essential. Here are two approaches:</p>"},{"location":"qa/SSH_Config/#option-1-one-ssh-key-for-everything","title":"Option 1: One SSH Key for Everything","text":""},{"location":"qa/SSH_Config/#steps","title":"Steps:","text":"<ol> <li>Generate an SSH Key: <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre></li> <li>Add Key to Agent: <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre></li> <li>Add Key to Repository Platform: Copy and paste the key into platform settings.</li> </ol>"},{"location":"qa/SSH_Config/#option-2-multiple-ssh-keys-with-config","title":"Option 2: Multiple SSH Keys with Config","text":""},{"location":"qa/SSH_Config/#steps_1","title":"Steps:","text":"<ol> <li>Generate Multiple Keys: <pre><code>ssh-keygen -t rsa -b 4096 -C \"company1@example.com\" -f ~/.ssh/id_rsa_company1\nssh-keygen -t rsa -b 4096 -C \"company2@example.com\" -f ~/.ssh/id_rsa_company2\n</code></pre></li> <li>Add Keys to Agent: <pre><code>ssh-add ~/.ssh/id_rsa_company1\nssh-add ~/.ssh/id_rsa_company2\n</code></pre></li> <li> <p>Create SSH Config File: <pre><code>nano ~/.ssh/config\n</code></pre>    Example Config:    <pre><code># Company 1\nHost company1\n    HostName github.com\n    User git\n    IdentityFile ~/.ssh/id_rsa_company1\n\n# Company 2\nHost company2\n    HostName gitlab.com\n    User git\n    IdentityFile ~/.ssh/id_rsa_company2\n</code></pre></p> </li> <li> <p>Use the Config: Clone using the defined Host:    <pre><code>git clone git@company1:username/repo.git\ngit clone git@company2:username/repo.git\n</code></pre></p> </li> </ol>"},{"location":"qa/SSH_Config/#which-one-to-use","title":"Which One to Use?","text":"<ul> <li>Single Key: Simple, but less secure.</li> <li>Multiple Keys: Secure and clean for multiple clients.</li> </ul> <p>The second method is recommended for better management.</p>"},{"location":"qa/Software_Testing_Tools/","title":"Software Testing Tools","text":"<p>Software testing is the process of checking whether the functionality, quality, and performance of a software meet user expectations. Testing tools and technologies offer software that makes this process faster, more efficient, and effective. They enhance quality and reduce error rates by supporting testing processes either manually or through automation.</p>"},{"location":"qa/Software_Testing_Tools/#1manual-testing-tools","title":"1.Manual Testing Tools","text":"<p>Manual testing tools simplify the testing process of software systems, playing a crucial role in identifying defects, ensuring functionality, and evaluating performance.</p>"},{"location":"qa/Software_Testing_Tools/#jira","title":"Jira","text":"<ul> <li>Developed by Atlassian, Jira is a bug tracking and project management tool.</li> </ul> <p>Features:   - Create detailed bug records, including steps to reproduce, expected and actual behaviors, and severity levels.   - Prioritize bugs based on impact for efficient resolution.   - Integrates seamlessly with other testing and development tools.</p>"},{"location":"qa/Software_Testing_Tools/#testrail","title":"TestRail","text":"<ul> <li>A software test management tool.</li> </ul> <p>Features:   - Organize test scenarios, track results, and generate reports.   - Define test scope and visualize progress.   - Integrates with Jira for aligned bug tracking and project management.</p>"},{"location":"qa/Software_Testing_Tools/#2automation-testing-tools","title":"2.Automation Testing Tools","text":"<p>Automation testing tools conduct tests automatically without manual intervention, improving efficiency for frequent or complex test scenarios.</p>"},{"location":"qa/Software_Testing_Tools/#selenium","title":"Selenium","text":"<ul> <li>An open-source automation testing tool for web applications. Features:</li> <li>Supports various programming languages.</li> <li>Simulates user behavior and automates test scenarios.</li> <li>WebDriver enables testing across multiple browsers.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#appium","title":"Appium","text":"<ul> <li>An open-source tool for mobile application testing. Features:</li> <li>Works on Android and iOS platforms.</li> <li>Adapts Selenium's features for mobile testing.</li> <li>Supports multiple programming languages.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#junit","title":"JUnit","text":"<ul> <li>An open-source testing framework for unit testing in Java. Features:</li> <li>Creates, executes, and reports test scenarios.</li> <li>Ensures software accuracy and defect identification.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#testng","title":"TestNG","text":"<ul> <li>A Java-based testing tool with advanced features. Features:</li> <li>Supports parallel test execution and dependency management.</li> <li>Handles data-driven and database-driven scenarios.</li> <li>Provides robust reporting and integration capabilities.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#3api-testing-tools","title":"3.API Testing Tools","text":"<p>API testing tools ensure proper communication between software systems by testing functions like validation, security, and performance.</p>"},{"location":"qa/Software_Testing_Tools/#postman","title":"Postman","text":"<ul> <li>A widely used API testing tool. Features:</li> <li>Sends API requests and reviews responses.</li> <li>Automates test scenarios and validates API functionality.</li> <li>Generates reports for efficient testing.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#soapui","title":"SoapUI","text":"<ul> <li>An open-source tool for SOAP and REST API testing. Features:</li> <li>Facilitates request sending, response validation, and performance analysis.</li> <li>Conducts security tests with advanced automation capabilities.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#4performance-and-security-testing-tools","title":"4.Performance and Security Testing Tools","text":"<p>These tools test software speed, resilience under load, and vulnerabilities.</p>"},{"location":"qa/Software_Testing_Tools/#jmeter","title":"JMeter","text":"<ul> <li>An open-source tool for performance and load testing. Features:</li> <li>Simulates multiple users to analyze system behavior under traffic.</li> <li>Supports protocols like HTTP, HTTPS, REST API, and database queries.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#loadrunner","title":"LoadRunner","text":"<ul> <li>A performance testing tool for large-scale applications. Features:</li> <li>Simulates high user loads to evaluate speed, stability, and scalability.</li> <li>Provides detailed reports for comprehensive analysis.</li> </ul>"},{"location":"qa/Software_Testing_Tools/#burp-suite","title":"Burp Suite","text":"<ul> <li>A tool for testing web application security. Features:</li> <li>Analyzes traffic to identify vulnerabilities like SQL injection and XSS.</li> <li>Offers automated scanning and manual testing.</li> <li>Provides detailed reports for enhanced security.</li> </ul>"},{"location":"qa/automation_testing_and_best_practices/","title":"Automation Testing and Best Practices","text":"<p>Automation testing is an integral part of modern software development processes. It is utilized to ensure software quality, deliver faster releases, and minimize errors that could arise in manual testing. Below is an overview of the key steps of automation testing and how these should be implemented.</p>"},{"location":"qa/automation_testing_and_best_practices/#1automation-test-planning","title":"1.Automation Test Planning","text":"<p>Effective planning is critical to the success of automation testing. Automation test planning should be carried out as follows: - Defining Test Objectives: Evaluating which areas are suitable for automation. - Selecting Test Environment and Tools: Choosing tools such as Java, Ruby, Python, or UiPath that fit the project requirements. - Determining Test Scope: Identifying test scenarios to be automated and defining areas integrated with manual testing.</p>"},{"location":"qa/automation_testing_and_best_practices/#2identifying-test-scenarios","title":"2.Identifying Test Scenarios","text":"<p>The following principles should be followed when detailing test scenarios: - Prioritizing Test Cases: Starting with functionalities of critical importance. - Reusability: Creating templates that can be reused in future projects. - Writing with Gherkin Language: Especially when using the BDD approach, scenarios should be written in a format understandable by business units.</p>"},{"location":"qa/automation_testing_and_best_practices/#3code-structure-and-modularity","title":"3.Code Structure and Modularity","text":"<p>Structuring code in modular forms is a crucial factor in testing processes. Each test step should be designed as independent functions or classes. Key considerations include: - Ease of Maintenance: Ensuring the code can adapt to changes. - Reducing Redundancy: Writing each test function only once and reusing it across multiple tests if necessary. - OOP Principles: Adhering to object-oriented programming structures in Java and Python projects.</p>"},{"location":"qa/automation_testing_and_best_practices/#4automation-frameworks","title":"4.Automation Frameworks","text":"<p>Different frameworks should be employed to create tailored solutions for specific projects. The frameworks and approaches include:</p>"},{"location":"qa/automation_testing_and_best_practices/#page-object-model-pom","title":"Page Object Model (POM)","text":"<p>POM enhances code readability and ease of maintenance. This model should be implemented as follows: - Each web page is defined as a separate class. - Element definitions and methods related to the page are contained within that class. - Classes should be created in adherence to the POM structure using Selenium or Playwright frameworks.</p>"},{"location":"qa/automation_testing_and_best_practices/#behavior-driven-development-bdd-and-gherkin-language","title":"Behavior Driven Development (BDD) and Gherkin Language","text":"<p>BDD facilitates better communication between business units and developers. BDD scenarios should be handled as follows: - Scenarios are written in the \u201cGiven-When-Then\u201d format. - Tools like Cucumber with Ruby or JBehave with Java are used to support the Gherkin language.</p>"},{"location":"qa/automation_testing_and_best_practices/#5best-practices-for-automation-testing","title":"5.Best Practices for Automation Testing","text":"<p>The following best practices should be observed in the testing process: - Parallel Test Execution: Running tests quickly using Playwright or Selenium Grid. - Using Dynamic Data: Simulating real scenarios more effectively by using dynamic datasets instead of static ones. - Logging and Reporting: Utilizing tools like Allure and Extent Reports to analyze test reports in detail. - CI/CD Integration: Integrating automation tests into pipelines using Jenkins and GitHub Actions.</p>"},{"location":"qa/automation_testing_and_best_practices/#conclusion","title":"Conclusion","text":"<p>Modern automation testing approaches and solutions should be leveraged to maximize software quality and delivery speed. By effectively using languages such as Java, Ruby, Python, and UiPath, and frameworks like Selenium and Playwright, unique test scenarios tailored to different projects can be developed.</p>"},{"location":"qa/test_approaches_in_quality_assurance/","title":"Test Approaches in Quality Assurance","text":"<p>A test approach is the backbone of any successful quality assurance (QA) strategy, defining how testing will be conducted based on project needs, constraints, and objectives. Unlike specific testing types, test approaches focus on high-level strategies for planning and execution. Below are some widely used test approaches, their characteristics, and when to apply them.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#1-preventive-test-approach","title":"1. Preventive Test Approach","text":"<p>The preventive approach emphasizes identifying and mitigating risks before they manifest in the development process. It relies heavily on early planning and documentation.</p> <p>Characteristics:</p> <p>Emphasis on early defect prevention.</p> <p>Strong focus on requirements and design reviews.</p> <p>Best Suited For:</p> <p>Waterfall or V-model projects with detailed upfront documentation.</p> <p>Critical systems where defects can be costly or catastrophic.</p> <p>Example Activities: Risk analysis, static code reviews, and requirement traceability matrix creation.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#2-reactive-test-approach","title":"2. Reactive Test Approach","text":"<p>In contrast to preventive testing, the reactive approach waits for testable components to be available and designs tests based on actual system behavior.</p> <p>Characteristics:</p> <p>Emphasis on defect detection over prevention.</p> <p>Heavily reliant on exploratory and ad-hoc testing.</p> <p>Best Suited For:</p> <p>Agile or fast-paced projects with dynamic requirements.</p> <p>Applications where frequent changes are expected.</p> <p>Example Activities: Exploratory testing, bug hunts, and session-based testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#3-risk-based-test-approach","title":"3. Risk-Based Test Approach","text":"<p>This approach prioritizes testing efforts based on the potential risk of failure and its impact on the business.</p> <p>Characteristics:</p> <p>Focuses on critical functionalities.</p> <p>Iterative and dynamic risk assessment throughout the project.</p> <p>Best Suited For:</p> <p>Projects with limited resources or tight deadlines.</p> <p>Complex systems with varying levels of criticality.</p> <p>Example Activities: Creating risk catalogs, prioritizing tests by risk level, and continuous risk reviews.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#4-context-driven-test-approach","title":"4. Context-Driven Test Approach","text":"<p>This adaptive approach recognizes that testing strategies must change based on the specific project context, constraints, and team dynamics.</p> <p>Characteristics:</p> <p>Rejects a one-size-fits-all methodology.</p> <p>Encourages creative, team-specific solutions.</p> <p>Best Suited For:</p> <p>Startups or projects with unique challenges.</p> <p>Teams that value collaboration and rapid iteration.</p> <p>Example Activities: Tailoring processes to stakeholder needs, balancing automation and manual testing based on context.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#5-analytical-test-approach","title":"5. Analytical Test Approach","text":"<p>The analytical approach uses data-driven techniques to design and prioritize tests. It is systematic and often based on quantitative metrics.</p> <p>Characteristics:</p> <p>Emphasis on coverage and traceability.</p> <p>Heavy use of models like decision tables or state-transition diagrams.</p> <p>Best Suited For:</p> <p>Regulatory or compliance-driven projects.</p> <p>Applications where high accuracy is essential.</p> <p>Example Activities: Test coverage analysis, metrics-based reporting, and decision-table-based testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#6-agile-test-approach","title":"6. Agile Test Approach","text":"<p>Aligned with Agile principles, this approach integrates testing into the development lifecycle, promoting continuous feedback and rapid adaptation.</p> <p>Characteristics:</p> <p>Testing is iterative and incremental.</p> <p>Collaborative approach involving QA, developers, and stakeholders.</p> <p>Best Suited For:</p> <p>Agile projects with short sprints and frequent deliverables.</p> <p>Teams practicing Test-Driven Development (TDD) or Behavior-Driven Development (BDD).</p> <p>Example Activities: Writing acceptance criteria as tests, automation in CI/CD pipelines, and sprint-based regression testing.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#7-model-based-test-approach","title":"7. Model-Based Test Approach","text":"<p>This approach relies on building abstract models (e.g., workflows, state diagrams) of the system to generate test cases systematically.</p> <p>Characteristics:</p> <p>Reduces manual effort in test design.</p> <p>Ensures high coverage of possible scenarios.</p> <p>Best Suited For:</p> <p>Complex systems with multiple workflows or states.</p> <p>Scenarios requiring repeatable and consistent testing.</p> <p>Example Activities: Building models using UML diagrams and auto-generating test cases from these models.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#8-process-oriented-test-approach","title":"8. Process-Oriented Test Approach","text":"<p>The process-oriented approach focuses on aligning testing activities with well-defined processes and standards.</p> <p>Characteristics:</p> <p>Based on industry standards like ISO or CMMI.</p> <p>Emphasizes compliance and documentation.</p> <p>Best Suited For:</p> <p>Organizations focused on audits and regulatory compliance.</p> <p>Projects requiring detailed process transparency.</p> <p>Example Activities: Creating process checklists, following test process improvement models, and conducting process audits.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#9-combined-test-approach","title":"9. Combined Test Approach","text":"<p>This approach integrates multiple strategies to address the diverse needs of a project, ensuring flexibility and adaptability.</p> <p>Characteristics:</p> <p>Uses a mix of preventive, reactive, and risk-based techniques.</p> <p>Promotes collaboration and cross-functional involvement.</p> <p>Best Suited For:</p> <p>Large, multidisciplinary projects.</p> <p>Scenarios requiring flexibility due to evolving requirements.</p> <p>Example Activities: Combining automated regression tests with exploratory manual sessions.</p>"},{"location":"qa/test_approaches_in_quality_assurance/#conclusion","title":"Conclusion","text":"<p>Choosing the right test approach depends on project goals, team expertise, and resource availability. A thoughtful selection and combination of these approaches ensure not only the success of the testing phase but also the delivery of high-quality, reliable software.</p>"},{"location":"qa/test_processes_and_qa_practices/","title":"Test Processes and QA Practices","text":""},{"location":"qa/test_processes_and_qa_practices/#agile-and-qa","title":"Agile and QA","text":"<p>The Agile model is an approach in software development that emphasizes flexibility, collaboration, and adaptability. Its primary goal is to quickly respond to changing customer needs by delivering small, manageable increments of software frequently. In this model, QA processes play a critical role in ensuring software is developed to high-quality standards.</p>"},{"location":"qa/test_processes_and_qa_practices/#the-importance-of-qa-processes-in-agile","title":"The Importance of QA Processes in Agile","text":"<p>Agile QA adopts an agile and flexible approach to guarantee software quality. Unlike traditional models, QA processes are not a separate phase but an integral part of the Agile cycle. Throughout each sprint, QA teams contribute to developing fast and error-free software while maintaining quality.</p>"},{"location":"qa/test_processes_and_qa_practices/#continuous-involvement-and-collaboration","title":"Continuous Involvement and Collaboration","text":"<p>QA teams actively participate in all processes from the start to the end of sprints. They attend planning meetings to understand requirements, create test scenarios, and collaborate closely with developers. QA also communicates with product owners to clarify requirements and supports the creation of acceptance criteria.</p>"},{"location":"qa/test_processes_and_qa_practices/#early-testing","title":"Early Testing","text":"<p>In Agile, detecting defects as early as possible is crucial. Therefore, QA processes begin before coding starts. Practices like Test-Driven Development (TDD) and Acceptance Test-Driven Development (ATDD) are implemented to ensure robust testing from the beginning.</p>"},{"location":"qa/test_processes_and_qa_practices/#continuous-testing-and-feedback-loop","title":"Continuous Testing and Feedback Loop","text":"<p>QA ensures quick feedback by testing the current functionality of the software in every sprint. Regression Tests ensure new features do not break existing functionality, while Smoke Tests verify that core functionalities work smoothly.</p>"},{"location":"qa/test_processes_and_qa_practices/#test-automation","title":"Test Automation","text":"<p>To keep up with Agile's speed, a significant portion of testing processes is automated. QA teams reduce the manual testing burden by automating repetitive tests, enabling faster and more efficient testing.</p>"},{"location":"qa/test_processes_and_qa_practices/#devops-and-qa","title":"DevOps and QA","text":"<p>DevOps is an approach and culture that combines software development and IT operations to achieve faster, more reliable, and higher-quality software delivery. QA integrates into the entire software lifecycle, ensuring quality at every stage.</p>"},{"location":"qa/test_processes_and_qa_practices/#qas-role-in-cicd-processes","title":"QA's Role in CI/CD Processes","text":"<ul> <li>Continuous Integration (CI): Developers frequently merge their code into a shared repository and run automated tests to detect issues early.</li> <li>Continuous Delivery (CD): Code is delivered quickly and reliably to production after passing through CI processes.</li> </ul> <p>QA ensures that the software maintains functional and technical quality through practices like Shift-Left Testing, Test Automation, and Pre-Deployment Validation.</p>"},{"location":"qa/test_processes_and_qa_practices/#key-practices","title":"Key Practices","text":"<ol> <li>Shift-Left Testing: Early detection of errors by starting QA activities at the beginning of the development lifecycle.</li> <li>Test Automation: Utilizing automation tools for repetitive tasks, such as Unit, Integration, and Regression Tests.</li> <li>Pipeline Integration: Automated tests run after every code update in the CI/CD pipeline.</li> <li>Rapid Feedback: Sharing test results promptly for quick issue resolution.</li> <li>Pre-Deployment Validation: Conducting User Acceptance Testing (UAT) and Release Testing.</li> </ol>"},{"location":"qa/test_processes_and_qa_practices/#conclusion","title":"Conclusion","text":"<p>QA in Agile and DevOps ensures high-quality, reliable, and efficient software delivery. By integrating automated testing into CI/CD pipelines, teams can detect issues early, reduce manual effort, and accelerate delivery cycles.</p>"},{"location":"qa/types_of_tests/","title":"Types of Tests","text":"<p>Software testing is the process of evaluating software applications, systems, or components to ensure they meet specified requirements and function as expected. It is categorized into two main types: functional testing and non-functional testing.</p>"},{"location":"qa/types_of_tests/#1-functional-testing","title":"1. Functional Testing","text":"<p>Functional testing focuses on verifying that the software operates according to its intended functionality, based on specified requirements.</p>"},{"location":"qa/types_of_tests/#types-of-functional-testing","title":"Types of Functional Testing","text":"<p>Unit Testing Unit testing verifies the smallest testable parts of the software, such as functions or methods, in isolation. It is typically performed by developers during the coding phase and ensures that each unit of the software works as intended.</p> <p>Example: Testing the \"Add to Cart\" function to ensure the selected item is correctly added to the shopping cart.</p> <p>Technique: White-box testing.</p> <p>Integration Testing Integration testing examines how different modules or components of the software interact with each other. It ensures seamless communication between individual units.</p> <p>Example: In a banking application, verifying that the \"Deposit Funds\" function updates the balance correctly and the \"View Balance\" function reflects the updated value.</p> <p>System Testing System testing validates the complete and integrated system to ensure that it meets the defined requirements.</p> <p>Example: Testing an e-commerce website to ensure product searches, payment processing, and order tracking work together seamlessly.</p> <p>Sanity Testing It is performed after the project is built to ensure that the errors have been resolved and no new issues have been introduced due to code changes.</p> <p>Example: After fixing a payment error, testing only the payment process to ensure it works correctly.</p> <p>Smoke Testing Smoke testing ensures the basic functionalities of the software are operational. It acts as a preliminary check before moving into detailed testing.</p> <p>Example: Checking whether a website loads properly and the login feature works.</p> <p>Regression Testing Regression testing ensures that newly added features or bug fixes do not negatively impact the existing functionality.</p> <p>Example: After adding a new payment method, verifying that older payment options like credit cards still function correctly.</p> <p>Types:</p> <p>Unit Regression Testing (URT): Tests only the modified unit.</p> <p>Regional Regression Testing (RRT): Tests modules affected by the changes.</p> <p>Full Regression Testing (FRT): Tests the entire system.</p> <p>Exploratory Testing Exploratory testing is performed without predefined test cases or plans. The tester investigates the software to discover potential issues.</p> <p>Example: Clicking on various categories, attempting unusual login credentials, or initiating and canceling payments to observe system behavior.</p> <p>User Acceptance Testing (UAT) UAT validates whether the software meets the end-users' requirements and expectations. It is the final testing phase before the product is released.</p> <p>Example: A banking application is tested by real users to ensure smooth fund transfers and accurate balance displays.</p>"},{"location":"qa/types_of_tests/#2-non-functional-testing","title":"2. Non-Functional Testing","text":"<p>Non-functional testing evaluates the operational aspects of the software, such as performance, security, usability, and reliability. It focuses on \"how\" the software works rather than \"what\" it does.</p>"},{"location":"qa/types_of_tests/#types-of-non-functional-testing","title":"Types of Non-Functional Testing","text":"<p>Performance Testing This measures the system's speed, stability, and scalability under varying conditions.</p> <p>Example: Checking the response time of a website under normal user loads.</p> <p>Load Testing Load testing determines how the system performs under expected user traffic or load.</p> <p>Example: Simulating 5,000 concurrent users on an e-commerce website during peak hours.</p> <p>Stress Testing Stress testing evaluates the system's stability under extreme or unexpected conditions, such as high user loads or resource limitations.</p> <p>Example: Simulating 50,000 users during a Black Friday sale to observe if the system crashes.</p> <p>Security Testing Security testing identifies vulnerabilities and ensures the software is protected from unauthorized access or data breaches.</p> <p>Example: Testing a banking application to ensure sensitive data is encrypted and accessible only by authorized users.</p> <p>Usability Testing This ensures the software is user-friendly and provides a positive user experience.</p> <p>Example: Evaluating whether a mobile app's interface is intuitive and easy to navigate.</p> <p>Compatibility Testing Compatibility testing ensures the software performs consistently across various devices, browsers, operating systems, and networks.</p> <p>Example: Verifying that a website works seamlessly on Chrome, Safari, and Firefox.</p> <p>Recovery Testing Recovery testing examines the system's ability to recover from failures, such as power outages or hardware crashes.</p> <p>Example: Testing whether a database restores correctly after an unexpected shutdown.</p>"}]}