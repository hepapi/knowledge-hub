{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Hepapi Knowledge Hub","text":""},{"location":"#hello-there","title":"Hello there! \ud83d\udc4b","text":"<p>This repository was created with the singular goal of fostering collaboration, knowledge exchange, and continuous learning among our team members at Hepapi.</p> <p>We recognize that knowledge is power, and in our constantly evolving field, it's crucial to keep up with the latest technologies, methodologies, and best practices. </p> <p>This repository is a live document, and we encourage everyone to contribute. If you have something valuable to share, don't hesitate to make a contribution. Remember, what may be obvious to you could be new to someone else.</p> <p> Follow us on Linkedin   hepapi.com </p>"},{"location":"#help-us-improve","title":"Help us improve","text":"<p>We are always looking for ways to improve our documentation. If you have any suggestions, please feel free to open an issue or a pull request.</p> <p>Follow the docs on: How to contribute</p>"},{"location":"#compendium","title":"Compendium","text":"<p>rancher</p> <ul> <li>Rancher Installation</li> </ul> <p>nexus</p> <ul> <li>Principle of least privilege</li> <li>Nexus Installation - Docker Private Registry</li> <li>Docker Proxy Repository</li> <li>Docker Hosted Repository</li> <li>RKE2 Registry Configuration</li> <li>Copy Nexus Credentials into Kubernetes</li> </ul> <p>vagrant</p> <ul> <li>Vagrant</li> </ul> <p>ansible</p> <ul> <li>Ansible Roles</li> <li>What is Ansible ?</li> <li>Install Ansible with pipx</li> <li>Inventory and Variables</li> <li>Ansible Playbooks</li> <li>Ansible Configuration File</li> <li>Install Ansible with pipx</li> </ul> <p>falcon-logscale</p> <ul> <li>Falcon LogScale Setup With Docker </li> <li>Installation</li> <li>Apt package update ####</li> <li>Installation</li> <li>Humio Single Node Installation Guide </li> <li>Kafka Installation (kafka.sh)</li> <li>falcon-logscale/javainstallation.md</li> <li>Apt package update ####</li> <li>Installation</li> <li>Falcon LogScale Agent(Log Collector) Setup </li> </ul> <p>k8s-engine</p> <ul> <li>System Upgrade Controller</li> <li>Deploy RKE2 Highly Available Cluster</li> <li>Restoring RKE2 Clusters</li> <li>RKE2 Setup </li> <li>RKE2 Cluster Installation With Ansible</li> <li>K3S Setup </li> </ul> <p>logging</p> <ul> <li>Elasticsearch Index Lifecycle Management</li> <li>ELK Stack with FileBeat</li> <li>Elasticsearch-Exporter</li> <li>Install Loki,Promtail,Grafana</li> </ul> <p>aws</p> <ul> <li>AWS CLI</li> <li>AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?</li> </ul> <p>Terragrunt</p> <ul> <li>What is Terragrunt? </li> </ul> <p>sealed-secrets</p> <ul> <li>Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets</li> </ul> <p>postgres</p> <ul> <li>Postgres Configuration</li> <li>Backup</li> <li>psql CLI</li> <li>PoC</li> </ul> <p>k8s-storage</p> <ul> <li>NFS Setup Requirements</li> <li>k8s-storage/longhorn.md</li> </ul> <p>kubernetes</p> <ul> <li>KEDA (Kubernetes Event-driven Autoscaling) </li> </ul> <p>sre</p> <ul> <li>k8sgpt</li> </ul> <p>jenkins</p> <ul> <li>Jenkins Install </li> <li>Jenkins</li> <li>Jenkins Shared Library </li> </ul> <p>sumo</p> <ul> <li>Local Configuration File Management</li> <li>Install a Sumo Logic Collector on Windows</li> <li>Install a Sumo Logic Collector on Linux</li> </ul> <p>devsecops</p> <ul> <li>DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy</li> <li>External Secret Operator</li> </ul> <p>linux</p> <ul> <li>Linux Tooling</li> <li>Linux Tips</li> <li>Netstat &amp; SS Command</li> <li>NMap Command</li> <li>nohup and &amp; </li> <li>cat</li> <li>cht.sh Command Tool</li> <li>jobs, bg, and fg</li> <li>SCP Command</li> <li><code>script</code> command</li> <li>NSLOOKUP</li> </ul> <p>git</p> <ul> <li>Downloading Git</li> <li>git/Commands.md</li> <li>Version Control System (VCS)</li> </ul> <p>azure</p> <ul> <li>Azure Self-Hosted Agent Installation</li> </ul> <p>sonarqube</p> <ul> <li>How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide</li> </ul>"},{"location":"repo-credit/","title":"Credits","text":"File Contributors .DS_Store deniz-icin,ersinsari13 .github/workflows/auto-index-generator.sh Oguzhan Yilmaz .github/workflows/deploy-mkdocs-website.yml Oguzhan Yilmaz,ersinsari13 .github/workflows/git-contributors-per-file.sh ersinsari13 .gitignore Oguzhan Yilmaz LICENSE Oguzhan Yilmaz README.md Oguzhan Yilmaz .DS_Store deniz-icin,ersinsari13 devops/.DS_Store deniz-icin,ersinsari13 devops/Terragrunt/README.md MehmetG171 devops/Terragrunt/environments/dev/env.hcl MehmetG171 devops/Terragrunt/environments/dev/us-east-1/region.hcl MehmetG171 devops/Terragrunt/environments/dev/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/Terragrunt/environments/dev/us-west-2/region.hcl MehmetG171 devops/Terragrunt/environments/dev/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/Terragrunt/environments/prod/env.hcl MehmetG171 devops/Terragrunt/environments/prod/us-east-1/region.hcl MehmetG171 devops/Terragrunt/environments/prod/us-east-1/vpc/terragrunt.hcl MehmetG171 devops/Terragrunt/environments/prod/us-west-2/region.hcl MehmetG171 devops/Terragrunt/environments/prod/us-west-2/vpc/terragrunt.hcl MehmetG171 devops/Terragrunt/environments/terragrunt.hcl MehmetG171 devops/Terragrunt/initial_configs/AWSTerraformInitialConfigs_Environment.yaml MehmetG171 devops/Terragrunt/initial_configs/AWSTerraformInitialConfigs_Management.yaml MehmetG171 devops/Terragrunt/modules/vpc/main.tf MehmetG171 devops/Terragrunt/modules/vpc/outputs.tf MehmetG171 devops/Terragrunt/modules/vpc/variables.tf MehmetG171 devops/Terragrunt/modules/vpc/versions.tf MehmetG171 devops/ansible/ansible-installations.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible-roles.md Oguzhan Yilmaz,ersinsari13 devops/ansible/ansible.md Oguzhan Yilmaz,ersinsari13 devops/ansible/config-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/inventory-file.md Oguzhan Yilmaz,ersinsari13 devops/ansible/playbook.md Oguzhan Yilmaz,ersinsari13 devops/ansible/what-is-ansible.md Oguzhan Yilmaz,ersinsari13 devops/aws/cli.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/aws/images/SSO-Architecture.png gokhanwell devops/aws/images/SSO-Attach-Account.png gokhanwell devops/aws/images/SSO-Enable.png gokhanwell devops/aws/images/SSO-Important.png gokhanwell devops/aws/images/SSO-Linked.png gokhanwell devops/aws/images/SSO-MFA.png gokhanwell devops/aws/images/SSO-Mail-Verify.png gokhanwell devops/aws/images/SSO-Permission-Set.png gokhanwell devops/aws/sso.md gokhanwell devops/azure/agent-installation.md Oguzhan Yilmaz,Utku Toraman devops/devsecops/.DS_Store deniz-icin,ersinsari13 devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy.md ersinsari13 devops/devsecops/external-secret-operator.md ersinsari13 devops/devsecops/image-eso/external-secret-1.png ersinsari13 devops/devsecops/image-eso/external-secret-operator-resources.png ersinsari13 devops/devsecops/image-eso/external-secret-operator.png ersinsari13 devops/devsecops/image-eso/iam-1.png ersinsari13 devops/devsecops/image-eso/iam-10.png ersinsari13 devops/devsecops/image-eso/iam-11.png ersinsari13 devops/devsecops/image-eso/iam-2.png ersinsari13 devops/devsecops/image-eso/iam-3.png ersinsari13 devops/devsecops/image-eso/iam-4.png ersinsari13 devops/devsecops/image-eso/iam-5.png ersinsari13 devops/devsecops/image-eso/iam-6.png ersinsari13 devops/devsecops/image-eso/iam-7.png ersinsari13 devops/devsecops/image-eso/iam-8.png ersinsari13 devops/devsecops/image-eso/iam-9.png ersinsari13 devops/devsecops/image-eso/iam.png ersinsari13 devops/devsecops/image-eso/kube-secret-1.png ersinsari13 devops/devsecops/image-eso/pod-shell.png ersinsari13 devops/devsecops/image-eso/secret-manager-1.png ersinsari13 devops/devsecops/image-eso/secret-manager-2.png ersinsari13 devops/devsecops/image-eso/secret-manager-3.png ersinsari13 devops/devsecops/image-eso/secret-manager-4.png ersinsari13 devops/devsecops/image-eso/secret-manager-5.png ersinsari13 devops/devsecops/image-eso/secret-manager-6.png ersinsari13 devops/devsecops/image-eso/secret-store-1.png ersinsari13 devops/devsecops/image-eso/secret-store.png ersinsari13 devops/devsecops/image/check-1.png ersinsari13 devops/devsecops/image/check-2.png ersinsari13 devops/devsecops/image/check-3.png ersinsari13 devops/devsecops/image/conftest-1.png ersinsari13 devops/devsecops/image/conftest-2.png ersinsari13 devops/devsecops/image/gate-3.png ersinsari13 devops/devsecops/image/gate-4.png ersinsari13 devops/devsecops/image/gates-1.png ersinsari13 devops/devsecops/image/gates-2.png ersinsari13 devops/devsecops/image/jdk.png ersinsari13 devops/devsecops/image/jenkins-passwd.png ersinsari13 devops/devsecops/image/jenkins-plug.png ersinsari13 devops/devsecops/image/jenkins-plugin.png ersinsari13 devops/devsecops/image/jenkins-user.png ersinsari13 devops/devsecops/image/maven-tool.png ersinsari13 devops/devsecops/image/pipe-1.png ersinsari13 devops/devsecops/image/pipeline-result.png ersinsari13 devops/devsecops/image/pipeline-script-2.png ersinsari13 devops/devsecops/image/pipeline-script.png ersinsari13 devops/devsecops/image/qality-1.png ersinsari13 devops/devsecops/image/qality-3.png ersinsari13 devops/devsecops/image/quality-2.png ersinsari13 devops/devsecops/image/sonar-dash.png ersinsari13 devops/devsecops/image/sonar-login.png ersinsari13 devops/devsecops/image/sonar-server.png ersinsari13 devops/devsecops/image/sonar-token-1.png ersinsari13 devops/devsecops/image/sonar-token-2.png ersinsari13 devops/devsecops/image/sonar-token-3.png ersinsari13 devops/devsecops/image/sonarqube-1.png ersinsari13 devops/devsecops/image/sonarqube-2.png ersinsari13 devops/devsecops/image/token-jenkins.png ersinsari13 devops/devsecops/image/trivy-1.png ersinsari13 devops/devsecops/image/trivy-2.png ersinsari13 devops/devsecops/image/trivy-3.png ersinsari13 devops/devsecops/image/webhook.png ersinsari13 devops/falcon-logscale/agent-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/falcon-logscale-installation-docker.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humio.md FIRST_NAME LAST_NAME devops/falcon-logscale/humioinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/humiosetup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/javainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/kafka.md FIRST_NAME LAST_NAME devops/falcon-logscale/kafkainstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/falcon-logscale/readme.md FIRST_NAME LAST_NAME devops/falcon-logscale/zookeeperinstallation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/git/Commands.md Oguzhan Yilmaz,Onur Ozcelik devops/git/Description.md Oguzhan Yilmaz,Onur Ozcelik devops/git/installation.md Oguzhan Yilmaz,Onur Ozcelik devops/index.md Oguzhan Yilmaz devops/jenkins/README.md Oguzhan Yilmaz devops/jenkins/jenkins-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/jenkins/shared-library.md Oguzhan Yilmaz,ersinsari13 devops/k8s-engine/k3s-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-highly-available-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-engine/rke2-installation-ansible.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-engine/rke2-installation.md Erdem Do\u011fanay,Oguzhan Yilmaz,Utku Toraman devops/k8s-storage/longhorn.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/k8s-storage/nfs-install.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/kubernetes/eks/pod-security-group.yaml Oguzhan Yilmaz,ersinsari13 devops/kubernetes/keda/keda.md Oguzhan Yilmaz,ersinsari13 devops/linux/shell/ampersand-nohup.md Oguzhan Yilmaz,can devops/linux/shell/cat.md Oguzhan Yilmaz devops/linux/shell/chtsh.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/jobs-bg-fg.md Oguzhan Yilmaz,can devops/linux/shell/netstat.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nmap.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/nslookup.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/scp.md Erdem Do\u011fanay,Oguzhan Yilmaz devops/linux/shell/script.md Oguzhan Yilmaz devops/linux/tips.md Oguzhan Yilmaz devops/linux/tooling.md Oguzhan Yilmaz devops/logging/ELK-stack-with-FileBeat.md ersinsari13 devops/logging/Index-Lifecycle-Management.md ersinsari13,ozihan devops/logging/elasticsearch-exporter.md ersinsari13 devops/logging/image/image-1.png ersinsari13 devops/logging/image/image-10.png ersinsari13 devops/logging/image/image-11.png ersinsari13 devops/logging/image/image-12.png ersinsari13 devops/logging/image/image-13.png ersinsari13 devops/logging/image/image-2.png ersinsari13 devops/logging/image/image-3.png ersinsari13 devops/logging/image/image-4.png ersinsari13 devops/logging/image/image-5.png ersinsari13 devops/logging/image/image-6.png ersinsari13 devops/logging/image/image-7.png ersinsari13 devops/logging/image/image-8.png ersinsari13 devops/logging/image/image-9.png ersinsari13 devops/logging/images/1.png ersinsari13,ozihan devops/logging/images/2.png ersinsari13,ozihan devops/logging/images/3.png ersinsari13,ozihan devops/logging/images/4.png ersinsari13,ozihan devops/logging/images/5.png ersinsari13,ozihan devops/logging/images/6.png ersinsari13,ozihan devops/logging/images/7.png ersinsari13,ozihan devops/logging/images/dash-1.png ersinsari13 devops/logging/images/dash-2.png ersinsari13 devops/logging/images/dash-3.png ersinsari13 devops/logging/images/dash-4.png ersinsari13 devops/logging/images/dash-5.png ersinsari13 devops/logging/images/dash-6.png ersinsari13 devops/logging/loki.md ersinsari13 devops/nexus/docker-hosted-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/docker-proxy-repo.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-installation.md Oguzhan Yilmaz,deniz-icin devops/nexus/nexus-user-and-roles.md Oguzhan Yilmaz,deniz-icin devops/nexus/pull-to-kubernetes.md Oguzhan Yilmaz,deniz-icin devops/nexus/registry-configuration.md Oguzhan Yilmaz,deniz-icin devops/postgres/backup-restore.md Oguzhan Yilmaz devops/postgres/configuration.md Oguzhan Yilmaz devops/postgres/poc-backup-restore.md Oguzhan Yilmaz devops/postgres/psql.md Oguzhan Yilmaz devops/rancher/rancher-installation.md Oguzhan Yilmaz,Utku Toraman,deniz-icin devops/sealed-secrets/sealed-secrets.md Oguzhan Yilmaz,sametustaoglu devops/sonarqube/.DS_Store deniz-icin devops/sonarqube/advanced-installation.md deniz-icin devops/sonarqube/images/after_login.png deniz-icin devops/sonarqube/images/login_screen.png deniz-icin devops/sre/.DS_Store deniz-icin,ersinsari13 devops/sre/k8sgpt.md ersinsari13 devops/sre/k8sgpt/image-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-1.png ersinsari13 devops/sre/k8sgpt/k8sgpt-2.png ersinsari13 devops/sre/k8sgpt/k8sgpt-3.png ersinsari13 devops/sre/k8sgpt/k8sgpt-4.png ersinsari13 devops/sre/k8sgpt/k8sgpt-5.png ersinsari13 devops/sre/k8sgpt/k8sgpt-6.png ersinsari13 devops/sre/k8sgpt/k8sgpt-7.png ersinsari13 devops/sre/k8sgpt/k8sgpt-8.png ersinsari13 devops/sre/k8sgpt/k8sgpt-9.png ersinsari13 devops/sumo/sumo-linux-collector.md Oguzhan Yilmaz,can devops/sumo/sumo-local-file-management.md Oguzhan Yilmaz,can devops/sumo/sumo-windows-collector.md Oguzhan Yilmaz,can devops/vagrant/vagrant-quickstart.md Oguzhan Yilmaz,Utku Toraman images/hepapi-logo.png Oguzhan Yilmaz index.md Oguzhan Yilmaz,vfarukhepapi misc/how-to-contribute/about-markdown.md Oguzhan Yilmaz misc/how-to-contribute/about-mkdocs.md Oguzhan Yilmaz misc/how-to-contribute/mkdocs-features.md Oguzhan Yilmaz qa/index.md Oguzhan Yilmaz qa/placeholder-subject/placeholder.md Oguzhan Yilmaz repo-credit.md ersinsari13 mkdocs.yml Erdem Do\u011fanay,FIRST_NAME LAST_NAME,MehmetG171,Oguzhan Yilmaz,Onur Ozcelik,Utku Toraman,can,deniz-icin,ersinsari13,gokhanwell,ozihan,sametustaoglu"},{"location":"devops/","title":"devops index","text":""},{"location":"devops/Terragrunt/","title":"What is Terragrunt?","text":""},{"location":"devops/Terragrunt/#what-is-terragrunt","title":"What is Terragrunt?","text":"<p>Terragrunt is a popular open-source tool or \u2018thin wrapper\u2019 developed by Gruntwork, that helps manage Terraform configurations by providing additional features and simplifying workflow. It is often used to address common challenges in Terraform, such as keeping configurations DRY (Don\u2019t Repeat Yourself), managing remote state, handling multiple environments, and executing custom code before or after running Terraform.</p> <p>See Terragrunt vs Terraform for further information.</p>"},{"location":"devops/Terragrunt/#terragrunt-features","title":"Terragrunt features","text":"<ol> <li> <p>Remote state management Terragrunt simplifies remote state management for Terraform projects. It can automatically configure and store state files remotely in services like Amazon S3, Google Cloud Storage, or any other backend supported by Terraform.</p> </li> <li> <p>DRY (Don\u2019t Repeat Yourself) configurations Terragrunt promotes DRY principles by allowing you to define and reuse common configurations across multiple Terraform modules. This helps reduce duplication and makes configurations more maintainable.</p> </li> <li> <p>Dependency management Terragrunt supports dependency management between different Terraform modules and states, ensuring that dependent resources are deployed in the correct order.</p> </li> <li> <p>Configuration inheritance Terragrunt allows you to create modular configurations that can inherit parameters and settings from parent configurations, making it easier to manage and organize your infrastructure code.</p> </li> <li> <p>Environment-specific configurations Terragrunt supports the creation of environment-specific configurations (e.g., dev, staging, prod) using HCL (HashiCorp Configuration Language) interpolation, making it easier to maintain consistent environments.</p> </li> <li> <p>Remote backend configurations Terragrunt allows you to specify backend configurations (e.g., S3 bucket, DynamoDB table) for each environment, enabling a more dynamic and flexible approach to state storage.</p> </li> <li> <p>Locking mechanism Terragrunt provides a locking mechanism to prevent concurrent executions that could potentially cause conflicts when modifying shared infrastructure.</p> </li> <li> <p>Secrets management Terragrunt can integrate with external secrets management tools like AWS Secrets Manager or HashiCorp Vault to handle sensitive data securely.</p> </li> <li> <p>Integration with CI/CD pipelines Terragrunt can be integrated into continuous integration and continuous deployment (CI/CD) pipelines to automate infrastructure deployments.</p> </li> <li> <p>Configurable hooks Terragrunt supports pre- and post-terraform hooks, allowing you to run custom scripts or commands before or after running Terraform commands.</p> </li> </ol>"},{"location":"devops/Terragrunt/#how-does-terragrunt-work","title":"How does Terragrunt work?","text":"<p>Terragrunt relies on a configuration file called <code>terragrunt.hcl</code>. This file is placed in the root directory of your Terraform project or in the directories of specific modules. It contains settings and parameters that customize Terragrunt\u2019s behavior for your project or module.</p>"},{"location":"devops/Terragrunt/#how-to-install-terragrunt","title":"How to install Terragrunt?","text":"<p>STEP 1: Install Terraform As Terragrunt is a wrapper around Terraform, you\u2019ll need to have Terraform installed first. You can download the appropriate version of Terraform for your operating system here.</p> <p>STEP 2: Extract the binary and place it in a directory included in your system\u2019s PATH After downloading Terraform, extract the binary and place it in a directory included in your system\u2019s <code>PATH</code>. The PATH tells a system where it should look for executables, making them accessible via command-line interfaces or scripts. To add a new folder to PATH in Windows, navigate to Advanced System Settings &gt; Environment Variables, select PATH, click \u201cEdit\u201d and then \u201cNew.\u201d</p> <p>STEP 3: Download Terragrunt Next, head over to the Terragrunt GitHub page to download it.</p> <p>STEP 4: Place the Terragrunt binary in a directory included in your system\u2019s PATH Once you have downloaded the Terragrunt binary, place it in a directory included in your system\u2019s <code>PATH</code>. You may also rename the binary to simply <code>terragrunt</code> (without the platform-specific suffix) for convenience.</p> <p>STEP 5: Verify the installation Lastly, verify the installation by running <code>terragrunt --version</code> on your console command line. It should show the currently installed version.</p> <pre><code>terragrunt --version\n</code></pre>"},{"location":"devops/Terragrunt/#terragrunt-basic-commands","title":"Terragrunt basic commands","text":"<p>Terragrunt command should be run from the project directory that contains your <code>terragrunt.hcl</code> configuration file. Terragrunt has many of the same commands available you will be familiar with the Terraform workflow, (you just need to replace <code>terraform</code> with <code>terragrunt</code>).</p> <p>These include:</p> <ul> <li>terragrunt init</li> <li>terragrunt validate</li> <li>terragrunt plan</li> <li>terragrunt apply</li> <li>terragrunt destroy</li> <li>terragrunt graph</li> <li>terragrunt state</li> <li>terragrunt version</li> <li>terragrunt output</li> </ul> <p>Also, check out this Terraform cheat sheet.</p>"},{"location":"devops/Terragrunt/#how-to-set-up-terragrunt-configurations","title":"How to set up Terragrunt configurations?","text":"<p>First, create your <code>terragrunt.hcl</code> file in the directory you want to use Terragrunt in. The <code>terragrunt.hcl</code> file consists of configuration blocks that define various settings for Terragrunt.</p> <p>Note that the Terragrunt configuration file uses the same HCL syntax as Terraform itself in <code>terragrunt.hcl</code>. Terragrunt also supports JSON-serialized HCL in a <code>terragrunt.hcl.json</code> file: where <code>terragrunt.hcl</code> is mentioned, you can always use <code>terragrunt.hcl.json</code> instead.</p> <p>The <code>terraform</code> block is used to configure how Terragrunt will interact with Terraform. You can configure things like before and after hooks for indicating custom commands to run before and after each terraform call or what CLI args to pass in for each command.</p> <p>The source attribute specifies where to find Terraform configuration files and uses the same syntax as the Terraform module source attribute.</p> <p>For example, you can pull modules directly from a Github repo:</p> <pre><code>terraform { \n  source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"\n}\n</code></pre> <p>Or modules from the local file system (Terragrunt will make a copy of the source folder in the Terragrunt working directory, typically '.terragrunt-cache'):</p> <pre><code>terraform {  \n  source = \"../modules/networking/vpc\"\n}\n</code></pre> <p>Other blocks you can configure in your <code>terraform.hcl</code> file include:</p> <ul> <li>remote_state</li> <li>include</li> <li>locals</li> <li>dependency</li> <li>dependencies</li> <li>generate</li> </ul>"},{"location":"devops/Terragrunt/#example","title":"Example","text":""},{"location":"devops/Terragrunt/#step-0","title":"STEP 0","text":"<p>You will need <code>dev</code> and <code>prod</code> accounts. You can create them using your main account. Then, you should create an IAM user account for logging in multiple accounts, namely SSO. In our example, our account is the \"Management Account\" which controls the other accounts. Similarly, <code>dev</code> and <code>prod</code> accounts are the \"Environment Accounts\" on which the resources are created. The IAM user account should be able to login to all account using \"Access Portal\".</p> <p>Before starting, make sure that AWS CLI is installed and configured on your desktop. If not, refer to download page of AWS CLI. After successful download, configure the CLI with IAM credentials of created IAM user. If you don't have, you can get credentials under IAM service.</p> <pre><code>aws configure\n</code></pre> <p>You will be prompted to enter your AWS Access Key ID, AWS Secret Access Key, default region name and default output format (e.g., json). Now, you are ready to proceed. Below, you can see the folder structure of the example: </p> <pre><code>modules/\n\u2514\u2500\u2500 vpc/\n    \u251c\u2500\u2500 main.tf\n    \u251c\u2500\u2500 versions.tf\n    \u251c\u2500\u2500 variables.tf\n    \u2514\u2500\u2500 outputs.tf\n\nenvironments/\n\u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for dev/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for dev/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for dev\n\u251c\u2500\u2500 prod/\n\u2502   \u251c\u2500\u2500 us-east-1/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-east-1\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-east-1\n\u2502   \u251c\u2500\u2500 us-west-2/\n\u2502   \u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl   # VPC module configuration for prod/us-west-2\n\u2502   \u2502   \u2514\u2500\u2500 region.hcl           # Region-specific configuration for prod/us-west-2\n\u2502   \u2514\u2500\u2500 env.hcl                  # General environment configuration for prod\n\u2514\u2500\u2500 terragrunt.hcl               # Top-level configuration linking all environments\n\ninitial_configs/                  \n\u251c\u2500\u2500 AWSTerraformInitialConfigs_Management.yaml\n\u2514\u2500\u2500 AWSTerraformInitialConfigs_Environment.yaml\n</code></pre> <p>As you can see, there is a <code>vpc</code> module under <code>modules</code> folder. Refer to vpc module page to get the module. Then, change the static values as variables and define the variables in <code>variables.tf</code>. Also, you can add <code>outputs.tf</code> to check if the module is successfully created. Note that there is no <code>.tfvars</code> file and we will see handle this issue in the next steps.</p>"},{"location":"devops/Terragrunt/#step-1","title":"STEP 1","text":"<p>Let's analyze the files under <code>initial_configs</code> folder. Starting with <code>AWSTerraformInitialConfigs_Management.yaml</code>, this CloudFormation template defines resources and configurations necessary for managing Terraform state using AWS S3 and DynamoDB, and it provisions an IAM user with the required permissions. </p> <p>PARAMETERS:</p> <ul> <li>Serial: A value used to notify CloudFormation to rotate access keys.</li> <li>IaCUserName: The IAM user name (default: terraform).</li> <li>TerraformStateBucketPrefix: Prefix for the S3 bucket storing Terraform state.</li> <li>TerraformStateLockTableName: Name of the DynamoDB table for state locking.</li> </ul> <p>RESOURCES:</p> <ul> <li> <p>IaCUser (IAM User): Creates an IAM user with tags indicating its provision through CloudFormation and its usage for management purposes.</p> </li> <li> <p>IaCUserPolicy (IAM Policy): Grants the IAM user permissions to: Manage the Terraform S3 bucket (create, access, and configure it). Lock Terraform state via DynamoDB (create, read, update, and delete items). Assume the TerraformExecutionRole for executing tasks.</p> </li> <li> <p>IaCUserAccessKey &amp; IaCUserSecret (IAM Access Keys &amp; Secret): Creates and stores the access keys in AWS Secrets Manager for secure access.</p> </li> <li> <p>TerraformStateS3Bucket (S3 Bucket): Creates an S3 bucket to store Terraform state files. It enforces security policies like blocking public access and enabling versioning.</p> </li> <li> <p>TerraformStateS3BucketBucketPolicy (S3 Bucket Policy): Adds a policy to the S3 bucket that denies the deletion of Terraform state files.</p> </li> <li> <p>TerraformStateLockDynamoDBTable (DynamoDB Table): Creates a DynamoDB table (LockID as the key) for Terraform state locking to prevent concurrent modifications of the state.</p> </li> </ul> <p>NOTE: You should create a stack in CloudFormation and upload this file on the \"Management Account\". Now, let's proceed with the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. </p> <p>PARAMETERS:</p> <ul> <li>IaCUserARN: A string parameter representing the ARN of the IAM user responsible for running Terraform. This user will be allowed to assume the role defined in the template. By default, this value needs to be provided (though a placeholder \"ARN of the IaC User\" is set).</li> </ul> <p>RESOURCES:</p> <ul> <li>TerraformExecutionRole (IAM Role): The TerraformExecutionRole is an IAM role that grants specific AWS permissions for Terraform operations, allowing a specified IAM user (via IaCUserARN) to assume it for a maximum of 4 hours. It is associated with the AdministratorAccess policy, providing full administrative privileges, and includes tags for tracking its provisioning through CloudFormation.</li> </ul> <p>NOTE: Look at the \"IaCUserARN\" from the \"Management Account\" and assign this value to the \"Parameters\" section of the <code>AWSTerraformInitialConfigs_Environment.yaml</code> file. You should create a stack in CloudFormation and upload this file on the \"Environment Accounts\".</p>"},{"location":"devops/Terragrunt/#step-2","title":"STEP 2","text":"<p>Now, we are ready to analyze the <code>environments</code> folder. There is a top-level <code>terragrunt.hcl</code> under the folder which includes important configurations. This Terragrunt configuration file sets up local variables and configurations for managing Terraform modules and remote state.</p> <p>Local Variables: - base_source_url: Points to the local module source directory. - environment_vars and region_vars: Load environment-specific and region-specific variables from env.hcl and region.hcl files, respectively. - target_account, target_region, remote_state_account, and remote_state_bucket: Define the AWS account and region for the remote state and specify the bucket for storing Terraform state files.</p> <p>AWS Provider Configuration: Generates an aws provider block, setting the region for both the remote state and the target account, with an assume role for accessing the target account's resources.</p> <p>Remote State Configuration: Configures Terraform to use an S3 bucket for remote state storage, with encryption enabled and a DynamoDB table for state locking.</p> <p>Global Parameters: Merges global inputs from env.hcl and region.hcl, allowing all resources to inherit these configurations, which is useful for multi-account setups.</p> <p>Let's move with the environment folders. The structure is similar in both of the environments, so it will be enough to analyze one of them. Under the environments, there are regions in which the resources are created. Also, there is a <code>env.hcl</code> which includes local variables. The variables in the <code>env.hcl</code> are important because they determine the account. Diving into one of the regions, you can see the <code>region.hcl</code> which specifies the region. In the same directory, you can see the folders of the modules, which is <code>vpc</code> in our example. Under the <code>vpc</code> folder, there is a <code>terragrunt.hcl</code> file. This Terragrunt configuration file includes the root <code>terragrunt.hcl</code> configuration, which contains common settings for remote state management across all components and environments.</p> <p>Include Block: The configuration references the root settings using the include directive, allowing access to shared configurations and exposing them for use in the current module.</p> <p>Local Variables: It defines base_source_url from the root configuration, along with the module_name as \"vpc\" and module_version as \"v0.0.1.\"</p> <p>Terraform Source: The source for the Terraform module is set based on the base source URL, module name, and version.</p> <p>Inputs: Specifies the input variables for the VPC module, including availability zones, VPC CIDR block, NAT and VPN gateway settings, subnet configurations, and tags for environment identification.</p> <p>NOTE: Don't forget to change the necessary values of variables in this folder. For example, the value of \"aws_account_id\" in the <code>env.hcl</code>. Similarly, you should check the values in the both top and root level <code>terragrunt.hcl</code> files.</p>"},{"location":"devops/Terragrunt/#step-3","title":"STEP 3","text":"<p>After compliting the necessary adjustments in the files and on the AWS console, you are ready to create the resources. You can open the folder with VSCode and using the terminal, proceed to the <code>vpc</code> folders in which the root level <code>terragrunt.hcl</code> is located. Finally, enter the following command to create VPC in desired region.</p> <pre><code>terragrunt apply\n</code></pre> <p>If you want to delete the resources, you should proceed to directory of related root level <code>terragrunt.hcl</code> file and enter the following command.</p> <pre><code>terragrunt destroy\n</code></pre> <p>For example, if you want to create a <code>dev-vpc</code> in <code>the us-east-1</code> region, go to <code>~/environments/dev/us-east-1/vpc</code> using the terminal and enter <code>terragrunt apply</code> command. Similarly, you can delete this using <code>terragrunt destroy</code>. Note that the <code>.tfstate</code> files are stored in S3 and they are locked in DynamoDB.</p> <p>The advantage of this configuration is that you can control various resources in different regions. In our example, we show this by creating a VPC  in two different regions. We created and configured multiple VPCs in different accounts and regions by only making minor changes in the structure. The same can be done using Terraform, but if the number of resources and regions increase, it would be faster and more practical to create with Terragrunt. Furthermore, you can control all of them from one point, namely \"Management Account\".</p>"},{"location":"devops/Terragrunt/#terragrunt-benefits","title":"Terragrunt benefits","text":"<p>Where Terraform allows you the freedom to structure your code in multiple ways, Terragrunt places restraints on how you can organize your Terraform code and forces you to use directory structure hierarchies and shared variable definition files to organize your code. These restraints force your code to be more consistent and make it harder to make mistakes. The trade-off is that the amount of flexibility you have is reduced.</p> <p>The key to using Terragrunt effectively is to carefully plan your directory structure in order to keep your code base DRY. Organizing your infrastructure code into reusable modules that represent logical components of your infrastructure is one way to achieve this. </p>"},{"location":"devops/Terragrunt/#key-points","title":"Key points","text":"<p>Terragrunt is a powerful tool that helps you manage Terraform configurations more efficiently. To make the most out of Terragrunt and maintain a clean, scalable, and organized infrastructure codebase, be sure to follow the best practices and plan your folder structure and use of Terragrunt carefully.</p>"},{"location":"devops/Terragrunt/#references","title":"References","text":"<p>Terragrunt Tutorial \u2013 Getting Started &amp; Examples</p>"},{"location":"devops/ansible/ansible-installations/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core==2.12.3\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre> <p>Lastly you can install the Ansible software with:</p> <pre><code>sudo apt install ansible -y\n</code></pre>"},{"location":"devops/ansible/ansible-installations/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible-roles/","title":"Roles","text":""},{"location":"devops/ansible/ansible-roles/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible-roles/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible-roles/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic</p> <p>my_role/ |-- defaults/    |   |-- main.yml |-- files/       |-- handlers/    |   |-- main.yml |-- meta/         |   |-- main.yml |-- tasks/       |   |-- main.yml |-- templates/   |-- tests/       |-- vars/        |   |-- main.yml |-- README.md     </p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible-roles/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/ansible/","title":"Install Ansible with pipx","text":"<p>Use pipx in your environment to install the full Ansible package:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>You can install the minimal ansible-core package:</p> <pre><code>pipx install ansible-core\n</code></pre> <p>Alternately, you can install a specific version of ansible-core</p> <pre><code>pipx install ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-with-pip","title":"Install Ansible with pip","text":"<p>Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3. For example, if you have determined that you want the Python at /usr/bin/python3.9 to be the one that you will install Ansible under, specify that instead of python3</p> <p>To verify whether pip is already installed for your preferred Python</p> <pre><code>python3 -m pip -V\n</code></pre> <p>If all is well, you should see something like the following:</p> <pre><code>pip 21.0.1 from /usr/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre> <p>If you see an error like No module named pip, you will need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip), or installing the latest pip directly from the Python Packaging Authority by running the following:</p> <p><pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py --user\n</code></pre> If so, pip is available, and you can move on to the install ansible</p> <p>Use pip in your selected Python environment to install the full Ansible package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>You can install the minimal ansible-core package for the current user:</p> <pre><code>python3 -m pip install --user ansible-core\n</code></pre>"},{"location":"devops/ansible/ansible/#install-ansible-on-ubuntu-2204","title":"Install Ansible on Ubuntu 22.04","text":"<p>The easiest way to install Ansible on ubuntu 22.04 is to use the apt package manager.</p> <p>Add a new Ansible repository to the list of software sources that your system uses to install and update software packages.</p> <pre><code>sudo apt-add-repository -y ppa:ansible/ansible\n</code></pre> <p>Update the package index using the following command</p> <pre><code>sudo apt-get update\n</code></pre> <p>If you get the following error You are probably missing the python-software-properties package.</p> <pre><code>sudo: add-apt-repository: command not found\n</code></pre> <p>Install it using the following command.</p> <pre><code>sudo apt-get install python-software-properties\n</code></pre>"},{"location":"devops/ansible/ansible/#confirm-your-installation","title":"Confirm your installation","text":"<p>You can test that Ansible is installed correctly by checking the version:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"devops/ansible/ansible/#what-is-ansible","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <p>1-Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators. 2-Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems. 3-Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times. 4-Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality. 5-Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures. 6-Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows. 7-Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</p>"},{"location":"devops/ansible/ansible/#inventory-file-and-building-an-inventory","title":"Inventory file and Building an inventory","text":"<p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.mycompany.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/ansible/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role. </p>"},{"location":"devops/ansible/ansible/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/ansible/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/ansible/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/ansible/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/ansible/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible Roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files. They enable us to reuse and share our Ansible code efficiently. This way, we can reference and call them in our playbooks with just a few lines of code while we can reuse the same roles over many projects without the need to duplicate our code.</p>"},{"location":"devops/ansible/ansible/#why-roles-are-useful-in-ansible","title":"Why Roles Are Useful in Ansible","text":"<p>When starting with Ansible, it\u2019s pretty common to focus on writing playbooks to automate repeating tasks quickly. As new users automate more and more tasks with playbooks and their Ansible skills mature, they reach a point where using just Ansible playbooks is limiting Since we have our code grouped and structured according to the Ansible standards, it is quite straightforward to share it with others. We will see an example of how we can accomplish that later with Ansible Galaxy. Organizing our Ansible content into roles provides us with a structure that is more manageable than just using playbooks. This might not be evident in minimal projects but as the number of playbooks grows, so does the complexity of our projects.</p>"},{"location":"devops/ansible/ansible/#ansible-role-structure","title":"Ansible Role Structure","text":"<p>Ansible checks for main.yml files, possible variations, and relevant content in each subdirectory. It\u2019s possible to include additional YAML files in some directories. For instance, you can group your tasks in separate YAML files according to some characteristic my_role/ |-- defaults/ |   |-- main.yml |-- files/ |-- handlers/ |   |-- main.yml |-- meta/ |   |-- main.yml |-- tasks/ |   |-- main.yml |-- templates/ |-- tests/ |-- vars/ |   |-- main.yml |-- README.md</p> <ul> <li>defaults:Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.</li> <li>files:Contains static and custom files that the role uses to perform various tasks.</li> <li>handlers: A set of handlers that are triggered by tasks of the role. </li> <li>meta:Includes metadata information for the role, its dependencies, the author, license, available platform, etc.</li> <li>tasks: A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.</li> <li>templates:Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)</li> <li>tests: Includes configuration files related to role testing.</li> <li>vars: Contains variables defined for the role. These have quite a high precedence in Ansible.</li> </ul>"},{"location":"devops/ansible/ansible/#sharing-roles-with-ansible-galaxy","title":"Sharing Roles with Ansible Galaxy","text":"<p>Ansible Galaxy is an online open-source, public repository of Ansible content. There, we can search, download and use any shared roles and leverage the power of its community. We have already used its client, ansible-galaxy, which comes bundled with Ansible and provides a framework for creating well-structured roles.You can use Ansible Galaxy to browse for roles that fit your use case and save time by using them instead of writing everything from scratch. For each role, you can see its code repository, documentation, and even a rating from other users. Before running any role, check its code repository to ensure it\u2019s safe and does what you expect.</p> <p>To download and install a role from Galaxy, use the ansible-galaxy install command. You can usually find the installation command necessary for the role on Galaxy</p>"},{"location":"devops/ansible/config-file/","title":"Configuration File","text":""},{"location":"devops/ansible/config-file/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>With a fresh installation of Ansible, like every other software, it ships with a default configuration file. This is the brain and the heart of Ansible, the file that governs the behavior of all interactions performed by the control node. In Ansible\u2019s case that default configuration file is (ansible.cfg) located in /etc/ansible/ansible.cfg.</p> <p>The default Ansible configuration file is very large and divided into ten different sections. Each section denoted within the square brackets gives you an idea about this massive configuration file.</p> <p>Ansible is so flexible, and it chooses its configuration file from one of several possible locations on the control node. One use case where this might be useful would be managing a web server and a database server. You might need to gather facts from one host and not on the other one. Having an ansible.cfg in the current project working directory can facilitate this behavior. If we\u2019re going to be working with multiple configuration files, it is important to understand the order of precedence on how it chooses its configuration file; we\u2019ll go through them below.</p> <p>By default Ansible reads its configuration file in /etc/ansible/ansible.cfg , however this behavior can be altered. The recommended practice is either to have an ansible.cfg in your current project working directory or to set it as an environment variable. One way to determine which configuration file ansible is using is to use the $ansible --version command, you can also run your ansible commands with the -v option. When it comes to the order of precedence, the ANSIBLE_CONFIG  environment variable has the highest precedence. If this environment variable is in your current shell, it will override all other configuration files. Here is one reason you might want to use the environment variable: let\u2019s say you have multiple projects and you want all of them to use one specific configuration file, besides the default one located in /etc/ansible. Setting the environment variable would be a good way to solve this problem. </p> <p>The second priority is  ansible.cfg in your current working directory. if Ansible doesn\u2019t find a configuration file in the current working directory, it will then look in for an .ansible.cfg file in the user\u2019s home directory, if there isn\u2019t one there either, it will finally grab the /etc/ansible/ansible.cfg.</p> <p>Use the ansible-config utility to view, list, or dump the various different settings available for Ansible. Running the $ansible-config view utility will print in your standard output your current ansible.cfg content, as you can see, this below outcome is the exact same as the earlier $cat ansible.cfg command</p> <p>sample of ansible.cfg</p> <pre><code># Location of inventory file\ninventory      = /path/to/your/inventory\n\n# Default user to use for playbooks if not specified\nremote_user    = your_remote_user\n\n# Path to private key file for authentication\nprivate_key_file = /path/to/your/private_key.pem\n\n# Disable host key checking (not recommended for production)\nhost_key_checking = False\n\n#Ansible may issue deprecation warnings when you use certain features that are slated for removal in future versions. Setting this parameter to False suppresses these deprecation warnings. Be cautious when using this option, as it might hide important information about upcoming changes in Ansible.\ndeprecation_warnings=False\n\n#This parameter sets the Python interpreter discovery mode. When set to auto_silent, Ansible will automatically discover the Python interpreter on the target hosts, and if not found, it will silently proceed. This can be useful in environments where Python may be installed in non-standard locations\ninterpreter_python=auto_silent\n</code></pre>"},{"location":"devops/ansible/inventory-file/","title":"Inventory File","text":"<p>Inventory file and Building an inventory</p> <p>An Ansible inventory is a collection of managed hosts we want to manage with Ansible for various automation and configuration management tasks. Typically, when starting with Ansible, we define a static list of hosts known as the inventory. These hosts can be grouped into different categories, and then we can leverage various patterns to run our playbooks selectively against a subset of hosts.  By default, the inventory is stored in /etc/ansible/hosts, but you can specify a different location with the -i flag or the ansible.cfg configuration file.</p> <p>The most common formats are either INI or YAML.</p> <p>In this example, we use the INI format, define four managed hosts, and we group them into two host groups; webservers and databases. The group names can be specified between brackets, as shown below.Inventory groups are one of the handiest ways to control Ansible execution. Hosts can also be part of multiple groups.</p> <p><pre><code>[webservers]\nhost01.hepapi.com\nhost02.hepapi.com\n\n[databases]\nhost03.hepapi.com\nhost04.hepapi.com\n\n[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n</code></pre> By default, we can also reference two groups without defining them. The all group targets all our hosts in the inventory, and the ungrouped contains any host that isn\u2019t part of any user-defined group.</p> <p>We can also create nested groups of hosts if necessary.</p> <p><pre><code>[londra]\nhost01.hepapi.com\nhost03.hepapi.com\n\n[istanbul]\nhost02.hepapi.com\nhost04.hepapi.com\n\n[hepapi:children]\nistanbul\nlondra\n</code></pre> Another useful functionality is the option to define aliases for hosts in the inventory. For example, we can run Ansible against the host alias host01 if we define it in the inventory as:</p> <pre><code>host01 ansible_host=host01.hepapi.com\n</code></pre>"},{"location":"devops/ansible/inventory-file/#inventory-and-variables","title":"Inventory and Variables","text":"<p>An important aspect of Ansible\u2019s project setup is variable\u2019s assignment and management. Ansible offers many different ways of setting variables, and defining them in the inventory is one of them.</p> <p>For example, let\u2019s define one variable for a different application version for every host in our dummy inventory from before.</p> <p><pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n</code></pre> Ansible-specific connection variables such as ansible_user or ansible_host are examples of host variables defined in the inventory.Similarly, variables can also be set at the group level in the inventory and offer a convenient way to apply variables to hosts with common characteristics.</p> <pre><code>[webservers]\nhost01.hepapi.com app_version=1.0.1\nhost02.hepapi.com app_version=1.0.2\n\n[databases]\nhost03.hepapi.com app_version=1.0.3\nhost04.hepapi.com app_version=1.0.4\n\n[webservers:vars]\napp_version=1.0.1\n\n[databases:vars]\napp_version=1.0.2\n</code></pre>"},{"location":"devops/ansible/inventory-file/#ansible-dynamic-inventories","title":"Ansible Dynamic Inventories","text":"<p>Many modern environments are dynamic, cloud-based, possibly spread across multiple providers, and constantly changing. In these cases, maintaining a static list of managed nodes is time-consuming, manual, and error-prone. </p> <p>Ansible has two methods to properly track and target a dynamic set of hosts: inventory plugins and inventory scripts. The official suggestion is to prefer inventory plugins that benefit from the recent updates to ansible core. </p> <p>To see a list of available inventory plugins you can leverage to build dynamic inventories, you can execute ansible-doc -t inventory -l. We will look at one of them, the amazon.aws.aws_ec2, to get hosts from Amazon Web Services EC2.</p> <p>Requirements</p> <p>The below requirements are needed on the local controller node that executes this inventory. - python &gt;= 3.6 - boto3 &gt;= 1.26.0 - botocore &gt;= 1.29.0</p> <p>dynamic_inventory_aws_ec2.yml</p> <p>NOTE: The inventory file is a YAML configuration file and must end with aws_ec2.{yml|yaml}. Example: </p> <p>my_inventory.aws_ec2.yml</p> <pre><code>plugin: amazon.aws.aws_ec2\nregions:\n  - us-east-1\n  - us-east-2\n  - us-west-2\n\nhostnames: tag:Name\nkeyed_groups:\n  - key: placement.region\n    prefix: aws_region\n  - key: tags['environment']\n    prefix: env\n  - key: tags['role']\n    prefix: role\ngroups:\n   # add hosts to the \"private_only\" group if the host doesn't have a public IP associated to it\n  private_only: \"public_ip_address is not defined\"\ncompose:\n  # use a private address where a public one isn't assigned\n  ansible_host: public_ip_address|default(private_ip_address)\n</code></pre> <p>We declare the plugin we want to use and other options, including regions to consider fetching data from, setting hostnames from the tag Name, and creating inventory groups based on region, environment, and role.</p>"},{"location":"devops/ansible/playbook/","title":"Ansible Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control.</p> <p>Playbooks can:</p> <ul> <li> <p>declare configurations</p> </li> <li> <p>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</p> </li> <li> <p>launch tasks synchronously or asynchronously</p> </li> </ul>"},{"location":"devops/ansible/playbook/#playbook-syntax","title":"Playbook syntax","text":"<p>Playbooks are expressed in YAML format with a minimum of syntax.A playbook is composed of one or more \u2018plays\u2019 in an ordered list. The terms \u2018playbook\u2019 and \u2018play\u2019 are sports analogies. Each play executes part of the overall goal of the playbook, running one or more tasks. Each task calls an Ansible module.A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple \u2018plays\u2019 can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on</p> <p><pre><code>---\n- name: Update web servers \n  hosts: webservers\n  remote_user: root\n\n  tasks:\n  - name: Ensure apache is at the latest version\n    ansible.builtin.yum:\n      name: httpd\n      state: latest\n\n  - name: Write the apache config file\n    ansible.builtin.template:\n      src: /srv/httpd.j2\n      dest: /etc/httpd.conf\n\n- name: Update db servers\n  hosts: databases\n  remote_user: root\n\n  tasks:\n  - name: Ensure postgresql is at the latest version\n    ansible.builtin.yum:\n      name: postgresql\n      state: latest\n\n  - name: Ensure that postgresql is started\n    ansible.builtin.service:\n      name: postgresql\n      state: started\n</code></pre> This Ansible playbook updates web servers by ensuring the Apache package is at the latest version and configuring Apache with a template. It also updates database servers by ensuring the PostgreSQL package is at the latest version and starting the PostgreSQL service. Each play is defined by a set of tasks to be executed on the specified hosts</p> <p>By default, Ansible executes each task in order, one at a time, against all machines matched by the host pattern. Each task executes a module with specific arguments. When a task has executed on all target machines, Ansible moves on to the next task. You can use strategies to change this default behavior. Within each play, Ansible applies the same task directives to all hosts. If a task fails on a host, Ansible takes that host out of the rotation for the rest of the playbook.</p> <p>When you run a playbook, Ansible returns information about connections, the name lines of all your plays and tasks, whether each task has succeeded or failed on each machine, and whether each task has made a change on each machine. At the bottom of the playbook execution, Ansible provides a summary of the nodes that were targeted and how they performed. General failures and fatal \u201cunreachable\u201d communication attempts are kept separate in the counts.</p>"},{"location":"devops/ansible/playbook/#desired-state-and-idempotency","title":"Desired state and idempotency","text":"<p>Most Ansible modules check whether the desired final state has already been achieved, and exit without performing any actions if that state has been achieved, so that repeating the task does not change the final state. Modules that behave this way are often called \u2018idempotent.\u2019 Whether you run a playbook once, or multiple times, the outcome should be the same. However, not all playbooks and not all modules behave this way. If you are unsure, test your playbooks in a sandbox environment before running them multiple times in production.</p>"},{"location":"devops/ansible/what-is-ansible/","title":"What is Ansible ?","text":"<p>Ansible is an open-source automation tool that allows you to automate various tasks, configurations, and deployments in a simple and efficient manner. It is designed to simplify complex IT infrastructure management and can be used for tasks such as application deployment, configuration management, orchestration, and provisioning.</p> <p>At its core, Ansible uses a declarative language called YAML (YAML Ain\u2019t Markup Language) to describe the desired state of the systems being managed. You define the desired configuration or tasks in simple, human-readable YAML files called \u201cplaybooks.\u201d Playbooks contain a series of instructions, known as \u201ctasks,\u201d that Ansible executes on the target systems.</p> <p>Ansible works by connecting to remote systems over SSH (Secure Shell) or other remote management protocols. It does not require any agents or additional software to be installed on the target systems, making it easy to get started with and maintain.</p> <p>Some key features and benefits of Ansible include:</p> <ul> <li>Simple and human-readable syntax: Ansible uses YAML syntax, which is easy to read and write, making it accessible to both developers and system administrators.</li> <li>Agentless architecture: Ansible communicates with remote systems using SSH or other protocols, eliminating the need for installing agents or daemons on the target systems.</li> <li>Idempotent nature: Ansible ensures that the desired state of the system is achieved regardless of the system\u2019s current state. It only makes necessary changes, which makes it safe to run playbooks multiple times.</li> <li>Extensibility and flexibility: Ansible has a large number of modules that provide the ability to manage a wide range of systems and services. You can also write your own modules to extend its functionality.</li> <li>Orchestration and automation: Ansible allows you to define complex workflows and orchestrate multiple systems simultaneously, making it suitable for automating tasks across large-scale infrastructures.</li> <li>Integration with existing tools and systems: Ansible can integrate with various external tools, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and configuration management databases (e.g., Ansible Tower), allowing you to incorporate it into your existing workflows.</li> <li>Ansible is widely adopted and used in various industries and organizations for managing infrastructure, automating deployments, and improving operational efficiency.</li> </ul>"},{"location":"devops/aws/cli/","title":"AWS CLI","text":"<p>The AWS Command Line Interface (CLI) is a unified tool for managing AWS services from the command line. With just one tool, you can control multiple AWS services, including Amazon S3, Amazon EC2, and Amazon CloudFront.</p>"},{"location":"devops/aws/cli/#installation","title":"Installation","text":"<p>Install the dependencies:</p> <pre><code>apt install glibc groff less -y\n</code></pre> <p>Install the dependencies:</p> <pre><code>   apt install glibc groff less -y\n</code></pre> <p>Follow the official guide as the <code>curl</code>ed .zip link there updates frequently.</p> <p>AWS CLIv2 Official Installation Documentation</p>"},{"location":"devops/aws/sso/","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":""},{"location":"devops/aws/sso/#aws-iam-identity-center-successor-to-aws-single-sign-on-what-is-sso","title":"AWS IAM Identity Center (successor to AWS Single Sign-On), What is SSO?","text":"<p>IAM Identity Center (successor to AWS Single Sign-On)</p> <p>Single sign-on (SSO) is an authentication solution that allows users to log in to multiple applications and websites with one-time user authentication. Given that users today frequently access applications directly from their browsers, organizations are prioritizing access management strategies that improve both security and the user experience. SSO delivers both aspects, as users can access all password-protected resources without repeated logins once their identity is validated.</p>"},{"location":"devops/aws/sso/#why-is-sso-important","title":"Why is SSO important?","text":"<p>Using SSO to streamline user logins benefits users and organizations in several ways.</p> <p></p> <ol> <li> <p>Strengthen password security </p> <p>When people don\u2019t use SSO, they must remember multiple passwords for different websites. This might lead to non-recommended security practices, such as using simple or repetitive passwords for different accounts. Besides, users might forget or mistype their credentials when logging in to a service. SSO prevents password fatigue and encourages users to create a strong password that can be used for multiple websites.</p> </li> <li> <p>Improve productivity </p> <p>Employees often use more than one enterprise application that requires separate authentication. Manually entering the username and password for every application is time-consuming and unproductive. SSO streamlines the user validation process for enterprise applications and makes it easier to access protected resources.</p> </li> <li> <p>Reduce costs </p> <p>In their attempt to remember numerous passwords, enterprise users may forget their login credentials. This results in frequent requests to retrieve or reset their passwords, which increases workload for the in-house IT teams. Implementing SSO reduces occurrences of forgotten passwords and thus minimizes the support resources in handling requests for password resets.</p> </li> <li> <p>Improve security posture</p> <p>By minimizing the number of passwords per user, SSO facilitates user access auditing and provides robust access control to all types of data. This reduces the risk of security events that target passwords, while helping organizations comply with data security regulations.</p> </li> <li> <p>Provide a better customer experience </p> <p>Cloud application vendors use SSO to provide end-users with a seamless login experience and credential management. Users manage fewer passwords and can still securely access the information and apps they need to complete their day-to-day jobs.</p> </li> </ol>"},{"location":"devops/aws/sso/#is-sso-secure","title":"Is SSO secure?","text":"<p>Yes, SSO is an advanced and desirable identity access management solution. When deployed, a single sign-on solution helps organizations with user access management for enterprise applications and resources. An SSO solution makes setting and remembering strong passwords easier for application users. In addition, the IT team can use the SSO tool to monitor user behavior, improve system resilience, and reduce security risks. </p>"},{"location":"devops/aws/sso/#how-can-aws-help-with-sso","title":"How can AWS help with SSO?","text":"<p>AWS IAM Identity Center is a cloud authentication solution that allows organizations to securely create or connect their workforce identities and manage their access centrally across AWS accounts and applications. You can create user identities or import them from external identity providers such as Okta Universal Directory or Azure. Some benefits of AWS IAM Identity Center include:</p> <ol> <li> <p>A central dashboard to manage identities for your AWS account or business applications.</p> </li> <li> <p>Multi-factor authentication support to provide a highly secure authentication experience for users. </p> </li> <li> <p>Integration support with other AWS applications for zero-configuration authentication and authorization.</p> </li> </ol>"},{"location":"devops/aws/sso/#sso-setup-aws-console","title":"SSO Setup AWS Console","text":"<ol> <li> <p>In order to use SSO, you must first enable the AWS Single Sign-On service. AWS Organizations supports IAM Identity Center (Single Sign-On) in only one AWS Region at a time. AWS Single Sign-On service works as a single service only in one region. Once activated, accounts in AWS Organization appear in the SSO service and each account in the organization creates an SSO role. Every account that is included or leave in the organization automatically update on SSO.</p> <p> </p> </li> <li> <p>By creating users and groups within the SSO service, access can be given to all accounts within the organization or to a specific account. An e-mail is sent to the people whose users have been created, and the users are verified via e-mail and directed to the relevant link.</p> <p> </p> </li> <li> <p>After completing the password process, MFA authenticator must be installed and logged in with SSO.</p> <p></p> </li> <li> <p>Then, permission is set and the user or group is given authority on the account. For example, AdministratorAccess, Billing etc. Additionally, this permission can be limited to session duration, such as 1 hour, 2 hours.</p> <p></p> </li> <li> <p>Access permission for the desired account is attached to the user or group with the permission set by going to the AWS accounts section within the SSO service. Now, the desired authority has been given to the desired user or group to access the desired account.</p> <p></p> </li> <li> <p>Access is provided via the link on SSO or the invitation email sent to users. The authorized account within the organization is entered. While performing the redirection process, it allows both through the AWS manenmagent console and provides transaction permissions through the AWS CLI. It presents the relevant credentials for AWS CLI to the user during access.</p> <p></p> </li> <li> <p>For accounts to which access is not desired, no user or permission assignment should be made.</p> </li> </ol>"},{"location":"devops/aws/sso/#references","title":"References","text":"<p>What is SSO (Single-Sign-On)?</p>"},{"location":"devops/azure/agent-installation/","title":"Azure Self-Hosted Agent Installation","text":""},{"location":"devops/azure/agent-installation/#creating-a-personal-access-token","title":"Creating a Personal Access Token","text":"<ol> <li>Log into Azure DevOps.</li> <li>Under User settings select <code>Personal access tokens</code></li> <li>Click <code>New Token</code></li> <li>Fill the <code>Name</code> and <code>Expiration</code> fields.</li> <li>In scope select <code>Custom defined</code>, then click <code>Show all scopes</code> and tick <code>Read &amp; manage</code>under <code>Agent Pools</code></li> <li>Click <code>Create</code> and make sure to securely store the token because it will not be accessible later.</li> </ol>"},{"location":"devops/azure/agent-installation/#installing-and-configuring-the-agent","title":"Installing and configuring the agent","text":"<ol> <li> <ul> <li>If you want the agent to be usable by different projects in your organization go to <code>Organization Settings</code> on your Azure Devops Organization page.</li> <li>If you want the agent to be exclusive to a specific project, go to <code>Project Settings</code> on the Azure Devops Project page.</li> </ul> </li> <li> <p>Select <code>Agent pools</code> under the <code>Pipelines</code> section on the left</p> </li> <li>Select an existing <code>Agent Pool</code> or create a new one.</li> <li>After you've selected an <code>Agent Pool</code>, click <code>New Agent</code><ul> <li>Linux: <ul> <li>Select <code>Linux</code></li> <li>Press the Copy button next to the Download button to copy the URL.</li> </ul> </li> </ul> </li> </ol>"},{"location":"devops/azure/agent-installation/#linux","title":"Linux","text":"<p>In the Linux machine:</p> <ol> <li>Create a user for the Azure Agent:     <pre><code>sudo adduser azureagent\n</code></pre></li> <li>Add the user to sudoers:     <pre><code>sudo usermod -aG sudo azureagent\n</code></pre></li> <li>If needed add the user to other necessary groups like <code>docker</code>:     <pre><code>sudo usermod -aG docker azureagent\n</code></pre></li> <li>Create any necessary working directories and grant ownership to the user:     <pre><code>sudo chown -R azureagent:azureagent /home/app/foo\n</code></pre></li> <li>Switch to the azureagent user and navigate to the <code>/home/azureagent</code> directory:     <pre><code>su - azureagent\ncd /home/azureagent\n</code></pre></li> <li>Download the agent using the URL we copied earlier:     <pre><code>wget https://vstsagentpackage.azureedge.net/agent/3.220.2/vsts-agent-linux-x64-3.220.2.tar.gz\n</code></pre></li> <li>Create a new directory for the agent and extract the tar.gz inside and confirm with ls:     <pre><code>mkdir agent\ncd agent\ntar zxf ../vsts-agent-linux-x64-3.220.2.tar.gz\nls\n</code></pre></li> <li>Run the config script to start the agent configuration:     <pre><code>./config.sh\n</code></pre></li> <li> <p>Provide the info requested by the script</p> <ul> <li>Enter your Azure Devops server URL:     <pre><code>https://dev.azure.com/orgname\n</code></pre></li> <li>Press Enter to continue using PAT then paste the PAT we created earlier.</li> <li>Enter the agent pool name and a name for the agent we're creating.</li> <li>Press enter to use the default work folder (<code>_work</code>)</li> </ul> </li> <li> <p>Configure the agent to run as a service:     <pre><code>sudo ./svc.sh install azureagent\nsudo ./svc.sh start\n</code></pre></p> </li> <li> <p>Navigate to the <code>Agent pools</code> page on Azure Devops and select the relevant <code>Agent Pool</code>, then click on the <code>Agents</code> tab to verify that our new <code>Agent</code> is added as a self-hosted agent.</p> </li> </ol>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/","title":"DevSecOps End to End Pipeline with SonarQube,OWASP Dependency-Check,Conftest and Trivy","text":"<p>Since DevOps entered our lives, it has been loved and widely adopted, and it seems it will continue to spread rapidly. While automating and speeding up delivery processes with DevOps, we cannot overlook the security aspect, which brings the DevSecOps methodology into focus. In this article, I discussed examples of how to fully implement DevSecOps in CI by checking code quality with SonarQube, scanning code dependencies with OWASP Dependency-Check, validating your Kubernetes, Terraform, and Dockerfile files with Conftest, and scanning Docker images with Trivy. Enjoy your learning!</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#sonarqube","title":"SonarQube","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-vulnerability","title":"What is Vulnerability ?","text":"<p>Vulnerabilities are basically the security weaknesses that one might use to undermine the availability, integrity, or security of information systems. Software, hardware, networks, or even human activities are among the several parts of a system that could have these flaws. A vulnerability could be as basic as a setting gone wrong or as sophisticated as a zero-day attack.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-sonarqube","title":"What is Sonarqube ?","text":"<p>SonarQube, previously named Sonar, is an open-source platform created by SonarSource. Its purpose is to consistently examine and evaluate the quality of code, identify security vulnerabilities, and assess technical debt across different programming languages. SonarQube delivers a single dashboard that provides real-time information into the health and security of software projects.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#how-is-sonarqube-used","title":"How is SonarQube Used?","text":"<p>SonarQube functions by looking at source code and finding possible problems and vulnerabilities. It uses static analysis, code smell recognition, and security vulnerability scanning all together to give complete results. SonarQube can be added to developers' work processes to help them find problems early in the development process. SonarQube works with many computer languages, such as Python, JavaScript, TypeScript, C#, and more. It has add-ons and plugins for well-known Integrated Development Environments (IDEs) like Eclipse, IntelliJ IDEA, and Visual Studio. This lets writers get feedback and suggestions while they're writing code.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#key-features-of-sonarqube","title":"Key Features of SonarQube","text":"<ul> <li>Code Quality Analysis</li> <li>Security Vulnerability Detection</li> <li>Technical Debt Management</li> <li>Continuous Integration/Continuous Deployment (CI/CD) Integration</li> <li>Customizable Rules and Quality Profiles</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#owasp-dependency-check","title":"OWASP Dependency-Check","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-owasp-dependency-check","title":"What is OWASP Dependency-Check?","text":"<p>Dependency-Check is a Software Composition Analysis (SCA) tool that attempts to detect publicly disclosed vulnerabilities contained within a project\u2019s dependencies. It does this by determining if there is a Common Platform Enumeration (CPE) identifier for a given dependency. If found, it will generate a report linking to the associated CVE entries. Dependency-check has a command line interface, a Maven plugin, an Ant task, and a Jenkins plugin. The core engine contains a series of analyzers that inspect the project dependencies, collect pieces of information about the dependencies (referred to as evidence within the tool). The evidence is then used to identify the Common Platform Enumeration (CPE) for the given dependency. If a CPE is identified, a listing of associated Common Vulnerability and Exposure (CVE) entries are listed in a report. Other 3rd party services and data sources such as the NPM Audit API, the OSS Index, RetireJS, and Bundler Audit are utilized for specific technologies.Dependency-check automatically updates itself using the NVD Data Feeds hosted by NIST.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#conftest","title":"Conftest","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-conftest","title":"What is Conftest?","text":"<p>Conftest leverages the Open Policy Agent (OPA) to evaluate policies written in Rego language against configuration files. It's commonly used for: - Kubernetes configurations: Ensuring that Kubernetes manifests meet security and compliance requirements. - Terraform files: Validating that Terraform plans and configurations adhere to organizational policies. - Dockerfiles: Checking that Docker images are built securely and according to best practices.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#trivy","title":"Trivy","text":""},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#what-is-trivy","title":"What is Trivy?","text":"<p>Trivy is a vulnerability scanner that is open-source and has been specifically developed for containers. This program is efficient and user-friendly, helping in the detection of vulnerabilities in container images and filesystems. Trivy's primary objective is to conduct scans on container images to identify any known vulnerabilities present in the installed packages and libraries.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#some-key-features-of-trivy-include","title":"Some key features of Trivy include:","text":"<ul> <li>Comprehensive vulnerability database</li> <li>Fast and efficient scanning</li> <li>Easy integration</li> <li>Multiple output formats</li> <li>Continuous updates</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#hands-on","title":"Hands-On","text":"<p>Let's include the devsecops tools we briefly mentioned above into the pipeline and do some hands-on. Let's get started.</p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-1-launch-ec2-instance","title":"Step-1 Launch EC2 Instance","text":"<p>Launch an AWS t2-large Instance. Use the image as Amazon Linux. You can create a new key pair or use an existing one.  - Enable 80, 443, 8080 and 9000 port settings in the Security Group. - You can add the userdata below for jenkins,docker,trivy installation.</p> <pre><code>#! /bin/bash\n# update os\ndnf update -y\n# set server hostname as jenkins-server\nhostnamectl set-hostname jenkins-server\n# install git\ndnf install git -y\n# install java 17\ndnf install java-17-amazon-corretto-devel -y\n# install jenkins\nwget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nrpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\ndnf upgrade\ndnf install jenkins -y\nsystemctl enable jenkins\nsystemctl start jenkins\n# install docker\ndnf install docker -y\nsystemctl start docker\nsystemctl enable docker\nusermod -a -G docker ec2-user\nusermod -a -G docker jenkins\n# configure docker as cloud agent for jenkins\ncp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.bak\nsed -i 's/^ExecStart=.*/ExecStart=\\/usr\\/bin\\/dockerd -H tcp:\\/\\/127.0.0.1:2376 -H unix:\\/\\/\\/var\\/run\\/docker.sock/g' /lib/systemd/system/docker.service\nsystemctl daemon-reload\nsystemctl restart jenkins\n# install trivy\nrpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.31.3/trivy_0.31.3_Linux-64bit.rpm\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-2-configure-jenkins-server","title":"Step-2 Configure Jenkins-Server","text":"<ul> <li>After instance state running, we can configure the jenkins server.Now, grab your Public IP Address</li> </ul> <pre><code>&lt;EC2 Public IP Address:8080&gt;\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <ul> <li>Unlock Jenkins using an administrative password and install the required plugins.</li> </ul> <ul> <li>Jenkins will now get installed and install all the libraries.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-3-install-sonarqube-as-a-docker-container","title":"Step-3 Install Sonarqube as a docker container","text":"<ul> <li>Go to Instance terminal and enter below code to install sonarqube</li> </ul> <pre><code>docker run -d --name sonar -p 9000:9000 sonarqube:lts-community\n</code></pre> <pre><code>&lt;EC2 Public IP Address:9000&gt;\nusername: admin\npassword: admin\n</code></pre>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-4-install-plugins","title":"Step-4 Install Plugins","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Plugins --&gt; Available Plugins Install below plugins</li> </ul> <p>1-Eclipse Temurin Installer:  It allows you to automatically download and install different versions of the Temurin JDK on your Jenkins agents. This is useful for ensuring that your builds run with the correct version of Java without needing to manually manage JDK installations.</p> <p>2-SonarQube Scanner: You can configure Jenkins jobs to run SonarQube scans as part of your build process. The plugin sends the code analysis results to a SonarQube server, where you can view detailed reports and track quality metrics over time.</p> <p>3-OWASP Dependency-Check: You can use this plugin to scan your project dependencies for vulnerabilities as part of your Jenkins build process. The results include detailed reports on any vulnerabilities found, helping you to mitigate security risks by updating or replacing affected dependencies.</p> <p>4-Blue Ocean: With Blue Ocean, you can create, edit, and visualize pipelines using a graphical interface. It also provides enhanced visualization of pipeline stages and steps, making it easier to track the progress and status of builds and deployments.</p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-java-maven-in-global-tool-configuration","title":"Step-5 Configure Java, Maven in Global Tool Configuration","text":"<ul> <li>Go to Jenkins WebUI Manage Jenkins --&gt; Tools --&gt; Install JDK, Maven and SonarQube Scanner --&gt;Click on Apply and Save</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-5-configure-sonarqube-in-manage-jenkins","title":"Step-5 Configure Sonarqube in Manage Jenkins","text":"<p><pre><code>&lt;EC2 Public IP Address:9000&gt;\n</code></pre> - Go to your Sonarqube Server. Click on Administration \u2192 Security \u2192 Users \u2192 Click on Tokens and Update Token \u2192 Give it a name \u2192 and click on Generate Token</p> <p></p> <p></p> <p></p> <ul> <li>Copy this Token</li> <li>Go to Jenkins WebUI --&gt; Manage Jenkins \u2192 Credentials \u2192 Add Secret Text.</li> </ul> <p></p> <ul> <li>Go to Jenkins Dashboard \u2192 Manage Jenkins \u2192 Configure System</li> <li>Give a name whatever you want</li> <li>Add Sonarqube url</li> <li>Select sonarqube credential token</li> </ul> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-6-webhook-configuration-on-sonarqube","title":"Step-6 WebHook Configuration on Sonarqube","text":"<ul> <li>Go to SonarQube WebUI --&gt; Administration \u2013&gt; Configuration \u2013&gt; webhooks</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-7-create-a-pipeline","title":"Step-7 Create a pipeline","text":"<ul> <li>Go to Jenkins WebUI --&gt;New item--&gt;Pipeline</li> </ul> <ul> <li>Add below jenkins code to pipeline section</li> </ul> <p><pre><code>pipeline {\n    agent any\n    tools {\n        jdk 'jdk'\n        maven 'maven'\n    }\n    stages {\n        stage(\"Git Checkout\") {\n            steps {\n                git branch: 'main', changelog: false, poll: false, url: 'https://github.com/ersinsari13/devsecops.git'\n            }\n        }\n        stage(\"Compile\") {\n            steps {\n                sh \"mvn clean compile\"\n            }\n        }\n        stage(\"Test Cases\") {\n            steps {\n                sh \"mvn test\"\n            }\n        }\n        stage(\"Sonarqube Analysis\") {\n            steps {\n                withSonarQubeEnv('sonar-server') {\n                    sh ''' \n                        mvn clean verify sonar:sonar \\\n                        -Dsonar.projectKey=Petclinic\n                    '''\n                }\n            }\n        }\n        stage(\"Quality Gate\") {\n            steps {\n                timeout(time: 2, unit: 'MINUTES') {\n                    script {\n                        waitForQualityGate abortPipeline: true\n                    }\n                }\n            }\n        }\n        stage(\"Build\") {\n            steps {\n                sh \"mvn clean install\"\n            }\n        }\n        stage('OWASP-Dependency-Check') {\n            steps {\n                sh \"mvn dependency-check:check\"\n            }\n            post {\n                always {\n                    dependencyCheckPublisher pattern: 'target/dependency-check-report.xml'\n                }\n            }\n        }\n\n        stage('Scan Dockerfile with conftest') {\n            steps {\n                echo 'Scanning Dockerfile'\n                sh \"docker run --rm -v $(pwd):/project openpolicyagent/conftest test --policy dockerfile-conftest.rego Dockerfile\"\n            }\n        }\n\n        stage('Prepare Tags for Docker Images') {\n            steps {\n                echo 'Preparing Tags for Docker Images'\n                script {\n                    MVN_VERSION=sh(script:'. ${WORKSPACE}/target/maven-archiver/pom.properties &amp;&amp; echo $version', returnStdout:true).trim()\n                    env.IMAGE_TAG_DEVSECOPS=\"ersinsari/devsecops:${MVN_VERSION}-b${BUILD_NUMBER}\"\n                }\n            }\n        }\n        stage('Build App Docker Images') {\n            steps {\n                echo 'Building App Dev Images'\n                sh \"docker build --force-rm -t ${IMAGE_TAG_DEVSECOPS} .\"\n                sh 'docker image ls'\n            }\n        }\n        stage('Scan Image with Trivy') {\n            steps {\n                script {\n                    def scanResult = sh(script: \"trivy image --severity CRITICAL --exit-code 1 ${IMAGE_TAG_DEVSECOPS}\", returnStatus: true)\n                    if (scanResult != 0) {\n                        error \"Critical vulnerabilities found in Docker image. Failing the pipeline.\"\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> - Tools: Specifies the tools needed for the pipeline, in this case, JDK and Maven.</p> <ul> <li> <p>Git Checkout: Check out the code from the specified Git repository.</p> </li> <li> <p>Compile: Runs Maven commands to clean the workspace and compile the code.</p> </li> <li> <p>Test Cases: Executes the Maven test phase to run the unit tests.</p> </li> <li> <p>SonarQube Analysis: Analyze the code quality using SonarQube.</p> </li> <li> <p>Quality Gate: Check the SonarQube quality gate status and abort the pipeline if it fails.</p> </li> <li> <p>Build: Runs Maven commands to clean the workspace and install the build artifacts.</p> </li> <li> <p>OWASP-Dependency-Check: Perform a security vulnerability check on project dependencies.</p> </li> <li> <p>Scan Dockerfile with conftest: Runs Conftest in a Docker container to test the Dockerfile against the specified policy.</p> </li> <li> <p>Prepare Tags for Docker Images: Extracts the Maven version from the build and sets the environment variable IMAGE_TAG_DEVSECOPS with the image tag.</p> </li> <li> <p>Build App Docker Images: Build the Docker image for the application.</p> </li> <li> <p>Scan Image with Trivy: Scans the Docker image for critical vulnerabilities and fails the pipeline if any are found.</p> </li> <li> <p>Click Build Now and Open Blue Ocean</p> </li> </ul> <p></p> <ul> <li>After the pipeline runs, you should receive a failure at the \"Scan Dockerfile with conftest\" step; this is a normal occurrence.</li> </ul> <p></p> <ul> <li>The reason for this is that if you check the GitHub repository we included in the pipeline, you will see a file named dockerfile-conftest.rego. Conftest performs the Dockerfile scan based on the conditions in this file. We received a failure because the Dockerfile we want to use does not meet the necessary requirements specified. We will correct this.</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-8-sonarqube-inspection-and-add-custom-quality-gate","title":"Step-8 Sonarqube inspection and add Custom Quality Gate","text":"<ul> <li> <p>But first, let's discuss the pipeline output and then talk a bit about the SonarQube interface and quality gates.</p> </li> <li> <p>You can inspect your source code qality by clicking SonarQube section</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>You can add custom Quality-Gates depends on your company rules</p> </li> <li> <p>SonarQube UI click Qualiyy Gates --&gt; Create --&gt; give name and save --&gt; Unlock editing --&gt; Add Condition --&gt; On Overall Code</p> </li> </ul> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-9-dependency-check-inspection","title":"Step-9 Dependency-Check inspection","text":"<ul> <li>You can inspect your source code dependency-check score by clicking Dependency-Check section</li> </ul>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-10-improving-dockerfile-security","title":"Step-10 Improving Dockerfile security","text":"<p>Now it's time to improve the Dockerfile security based on the Conftest results.</p> <p></p> <ul> <li>Change your Dockerfile as below</li> </ul> <p><pre><code>FROM openjdk:8\nEXPOSE 8082\nRUN addgroup -S devops-security &amp;&amp; adduser -u 999 -S devsecops -G devops-security\nCOPY target/petclinic.war petclinic.war\nUSER 999\nENTRYPOINT [\"java\",\"-jar\",\"/home/devsecops/petclinic.war\"]\n</code></pre> After this change, you should be able to successfully pass the Dockerfile scanning stage with Conftest.</p> <p></p> <p></p>"},{"location":"devops/devsecops/DevSecOps-SonarQube-OWASP-Conftest-Trvy/#step-11-docker-image-scan-via-trivy","title":"Step-11 Docker Image Scan via Trivy","text":"<p>Lastly, the pipeline will fail at the image scanning stage with Trivy. If we look at the Jenkinsfile, it is designed to fail if a critical vulnerability is found during the image scan with Trivy. At this stage, the critical vulnerabilities in the image need to be resolved before proceeding. The pipeline output includes recommendations on how to resolve the vulnerabilities.</p> <p></p> <p></p> <p>Once the image scan is successfully completed according to your requirements, the next step is to push the Docker image to the registry and then deploy your application. The key point here is to ensure maximum security before deploying the application, which is what we have aimed to achieve. Have a nice day.</p>"},{"location":"devops/devsecops/external-secret-operator/","title":"External Secret Operator","text":"<p>In the dynamic landscape of modern application development, managing secrets securely is crucial, especially within Kubernetes environments. Secrets such as API keys, passwords, and certificates are vital for the functionality and security of applications, but they also pose significant risks if not handled correctly. Kubernetes provides built-in mechanisms for secret management, but as applications grow in complexity, so does the challenge of managing these secrets effectively. This is where the External Secret Operator comes into play. By integrating with external secret management systems, the External Secret Operator enhances Kubernetes' native capabilities, allowing for seamless and secure management of sensitive data. In this blog, we'll explore why secrets are so critical in Kubernetes and how the External Secret Operator can streamline and fortify your secret management strategy.</p> <p></p> <p>External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, IBM Cloud Secrets Manager, CyberArk Conjur and many more. The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret.</p>"},{"location":"devops/devsecops/external-secret-operator/#secretstore","title":"SecretStore","text":"<p>The SecretStore is namespaced and specifies how to access the external API. The SecretStore maps to exactly one instance of an external API. By design, SecretStores are bound to a namespace and can not reference resources across namespaces. If you want to design cross-namespace SecretStores you must use ClusterSecretStores which do not have this limitation.</p> <p></p> <p>For a full list of supported fields see spec </p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: example\n  namespace: example-ns\nspec:\n\n  # Used to select the correct ESO controller (think: ingress.ingressClassName)\n  # The ESO controller is instantiated with a specific controller name\n  # and filters ES based on this property\n  # Optional\n  controller: dev\n\n  # You can specify retry settings for the http connection\n  # these fields allow you to set a maxRetries before failure, and\n  # an interval between the retries.\n  # Current supported providers: AWS, Hashicorp Vault, IBM\n  retrySettings:\n    maxRetries: 5\n    retryInterval: \"10s\"\n\n  # provider field contains the configuration to access the provider\n  # which contains the secret exactly one provider must be configured.\n  provider:\n\n    # (1): AWS Secrets Manager\n    # aws configures this store to sync secrets using AWS Secret Manager provider\n    aws:\n      service: SecretsManager\n      # Role is a Role ARN which the SecretManager provider will assume\n      role: iam-role\n      # AWS Region to be used for the provider\n      region: eu-central-1\n      # Auth defines the information necessary to authenticate against AWS by\n      # getting the accessKeyID and secretAccessKey from an already created Kubernetes Secret\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n\n    # (2) Hashicorp Vault\n    vault:\n      server: \"https://vault.acme.org\"\n      # Path is the mount path of the Vault KV backend endpoint\n      # Used as a path prefix for the external secret key\n      path: \"secret\"\n      # Version is the Vault KV secret engine version.\n      # This can be either \"v1\" or \"v2\", defaults to \"v2\"\n      version: \"v2\"\n      # vault enterprise namespace: https://www.vaultproject.io/docs/enterprise/namespaces\n      namespace: \"a-team\"\n      # base64 encoded string of certificate\n      caBundle: \"...\"\n      # Instead of caBundle you can also specify a caProvider\n      # this will retrieve the cert from a Secret or ConfigMap\n      caProvider:\n        # Can be Secret or ConfigMap\n        type: \"Secret\"\n        name: \"my-cert-secret\"\n        key: \"cert-key\"\n      # client side related TLS communication, when the Vault server requires mutual authentication\n      tls:\n        clientCert:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.crt\"\n        secretRef:\n          namespace: ...\n          name: \"my-cert-secret\"\n          key: \"tls.key\"\n\n      auth:\n        # static token: https://www.vaultproject.io/docs/auth/token\n        tokenSecretRef:\n          name: \"my-secret\"\n          key: \"vault-token\"\n\n        # AppRole auth: https://www.vaultproject.io/docs/auth/approle\n        appRole:\n          path: \"approle\"\n          roleId: \"db02de05-fa39-4855-059b-67221c5c2f63\"\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault-token\"\n\n        # Kubernetes auth: https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"demo\"\n          # Optional service account reference\n          serviceAccountRef:\n            name: \"my-sa\"\n          # Optional secret field containing a Kubernetes ServiceAccount JWT\n          # used for authenticating with Vault\n          secretRef:\n            name: \"my-secret\"\n            key: \"vault\"\n\n        # TLS certificates auth method: https://developer.hashicorp.com/vault/docs/auth/cert\n        cert:\n          clientCert:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.crt\"\n          secretRef:\n            namespace: ...\n            name: \"my-cert-secret\"\n            key: \"tls.key\"\n\n    # (3): GCP Secret Manager\n    gcpsm:\n      # Auth defines the information necessary to authenticate against GCP by getting\n      # the credentials from an already created Kubernetes Secret.\n      auth:\n        secretRef:\n          secretAccessKeySecretRef:\n            name: gcpsm-secret\n            key: secret-access-credentials\n      projectID: myproject\n    # (TODO): add more provider examples here\n\nstatus:\n  # Standard condition schema\n  conditions:\n  # SecretStore ready condition indicates the given store is in ready\n  # state and able to referenced by ExternalSecrets\n  # If the `status` of this condition is `False`, ExternalSecret controllers\n  # should prevent attempts to fetch secrets\n  - type: Ready\n    status: \"False\"\n    reason: \"ConfigError\"\n    message: \"SecretStore validation failed\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#external-secret","title":"External Secret","text":"<p>The ExternalSecret describes what data should be fetched, how the data should be transformed and saved as a Kind=Secret:</p> <ul> <li>tells the operator what secrets should be synced by using spec.data to explicitly sync individual keys or use spec.dataFrom to get all values from the external API.</li> <li>you can specify how the secret should look like by specifying a spec.target.template</li> </ul> <p>Take a look at an annotated example to understand the design behind the ExternalSecret</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: \"hello-world\"\n\n  # labels and annotations are copied over to the\n  # secret that will be created\n  labels:\n    acme.org/owned-by: \"q-team\"\n  annotations:\n    acme.org/sha: 1234\n\nspec:\n\n  # Optional, SecretStoreRef defines the default SecretStore to use when fetching the secret data.\n  secretStoreRef:\n    name: aws-store\n    kind: SecretStore  # or ClusterSecretStore\n\n  # RefreshInterval is the amount of time before the values reading again from the SecretStore provider\n  # Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration)\n  # May be set to zero to fetch and create it once\n  refreshInterval: \"1h\"\n\n  # the target describes the secret that shall be created\n  # there can only be one target per ExternalSecret\n  target:\n\n    # The secret name of the resource\n    # Defaults to .metadata.name of the ExternalSecret\n    # It is immutable\n    name: application-config\n\n    # Specifies the ExternalSecret ownership details in the created Secret. Options:\n    # - Owner: (default) Creates the Secret and sets .metadata.ownerReferences. If the ExternalSecret is deleted, the Secret will also be deleted.\n    # - Merge: Does not create the Secret but merges data fields into the existing Secret (expects the Secret to already exist).\n    # - Orphan: Creates the Secret but does not set .metadata.ownerReferences. If the Secret already exists, it will be updated.\n    # - None: Does not create or update the Secret (reserved for future use with injector).\n    creationPolicy: Merge\n\n    # Specifies what happens to the Secret when data fields are deleted from the provider (e.g., Vault, AWS Parameter Store). Options:\n    # - Retain: (default) Retains the Secret if all Secret data fields have been deleted from the provider.\n    # - Delete: Removes the Secret if all Secret data fields from the provider are deleted.\n    # - Merge: Removes keys from the Secret but not the Secret itself.\n    deletionPolicy: Retain\n\n    # Specify a blueprint for the resulting Kind=Secret\n    template:\n      type: kubernetes.io/dockerconfigjson # or TLS...\n\n      metadata:\n        annotations: {}\n        labels: {}\n\n      # Use inline templates to construct your desired config file that contains your secret\n      data:\n        config.yml: |\n          database:\n            connection: postgres://{{ .username }}:{{ .password }}@{{ .database_host }}:5432/payments\n\n      # Uses an existing template from configmap\n      # Secret is fetched, merged and templated within the referenced configMap data\n      # It does not update the configmap, it creates a secret with: data[\"alertmanager.yml\"] = ...result...\n      templateFrom:\n      - configMap:\n          name: application-config-tmpl\n          items:\n          - key: config.yml\n\n  # Data defines the connection between the Kubernetes Secret keys and the Provider data\n  data:\n    - secretKey: username\n      remoteRef:\n        key: database-credentials\n        version: v1\n        property: username\n        decodingStrategy: None # can be None, Base64, Base64URL or Auto\n\n      # define the source of the secret. Can be a SecretStore or a Generator kind\n      sourceRef:\n        # point to a SecretStore that should be used to fetch a secret.\n        # must be defined if no spec.secretStoreRef is defined.\n        storeRef:\n          name: aws-secretstore\n          kind: ClusterSecretStore\n\n  # Used to fetch all properties from the Provider key\n  # If multiple dataFrom are specified, secrets are merged in the specified order\n  # Can be defined using sourceRef.generatorRef or extract / find\n  # Both use cases are exemplified below\n  dataFrom:\n  - sourceRef:\n      generatorRef:\n        apiVersion: generators.external-secrets.io/v1alpha1\n        kind: ECRAuthorizationToken\n        name: \"my-ecr\"\n  #Or\n  dataFrom:\n  - extract:\n      key: database-credentials\n      version: v1\n      property: data\n      conversionStrategy: Default\n      decodingStrategy: Auto\n    rewrite:\n    - regexp:\n        source: \"exp-(.*?)-ression\"\n        target: \"rewriting-${1}-with-groups\"\n  - find:\n      path: path-to-filter\n      name:\n        regexp: \".*foobar.*\"\n      tags:\n        foo: bar\n      conversionStrategy: Unicode\n      decodingStrategy: Base64\n    rewrite:\n    - regexp:\n        source: \"foo\"\n        target: \"bar\"\n\nstatus:\n  # refreshTime is the time and date the external secret was fetched and\n  # the target secret updated\n  refreshTime: \"2019-08-12T12:33:02Z\"\n  # Standard condition schema\n  conditions:\n  # ExternalSecret ready condition indicates the secret is ready for use.\n  # This is defined as:\n  # - The target secret exists\n  # - The target secret has been refreshed within the last refreshInterval\n  # - The target secret content is up-to-date based on any target templates\n  - type: Ready\n    status: \"True\" # False if last refresh was not successful\n    reason: \"SecretSynced\"\n    message: \"Secret was synced\"\n    lastTransitionTime: \"2019-08-12T12:33:02Z\"\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#hands-on","title":"Hands-On","text":"<p>Here\u2019s the step-by-step guide to using an external secret manager in a local Minikube cluster with AWS Secret Manager</p>"},{"location":"devops/devsecops/external-secret-operator/#set-up-minikube","title":"Set Up Minikube","text":"<pre><code>minikube start\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#install-external-secrets-operator","title":"Install External Secrets Operator","text":"<ul> <li>Add the Helm chart repository for the external secrets operator</li> </ul> <pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm repo update\n</code></pre> <ul> <li>Install the External Secrets Operator using Helm</li> </ul> <pre><code>helm install external-secrets \\\n   external-secrets/external-secrets \\\n    -n external-secrets \\\n    --create-namespace\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-in-aws-secrets-manager","title":"Create secret in AWS Secrets Manager","text":"<ul> <li>Go to the AWS Management Console</li> <li>In the AWS Management Console, search for and select Secrets Manager</li> <li>On the Secrets Manager dashboard, click the \"Store a new secret\" button.</li> <li>You can store a variety of secrets, such as database credentials, API keys, or custom key-value pairs.</li> <li>Select Other type of secret.</li> <li>Input the keys and values you want to store. DB_PASSWORD=mypassword DB_USER=ersin</li> <li>Secret name=DB-CREDENTIAL</li> <li>Leave all settings as default and save.</li> </ul>"},{"location":"devops/devsecops/external-secret-operator/#create-a-user-in-aws","title":"Create a user in AWS","text":"<ul> <li>Navigate to IAM (Identity and Access Management)</li> <li>In the IAM dashboard, click on Users in the left-hand menu</li> <li>Click the Add user button.</li> <li>Enter a unique username for the new user</li> <li>Select Programmatic access.</li> <li>Set User Permissions via below policy.</li> </ul> <p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\"\n            ],\n            \"Resource\": \"&lt;ENTER-SECRET-MANAGER-ARN&gt;\"\n        }\n    ]\n}\n</code></pre> - Review the user details and permissions - Click Create user to finish. - On the confirmation page, you\u2019ll see the user's access key ID and secret access key. -  Download the credentials or save them securely, as you won\u2019t be able to retrieve the secret access key again.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#create-a-secret-store-in-kubernetes","title":"Create a Secret Store in Kubernetes","text":"<p>Create a YAML file for the Secret Store configuration. This will define how the External Secrets Operator interacts with AWS Secrets Manager.</p> <p>secret-store.yaml <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: secretstore-sample\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: us-east-1\n      auth:\n        secretRef:\n          accessKeyIDSecretRef:\n            name: awssm-secret\n            key: access-key\n          secretAccessKeySecretRef:\n            name: awssm-secret\n            key: secret-access-key\n</code></pre></p> <pre><code>kubectl apply -f secret-store.yaml\n</code></pre> <p></p> <p>spec: Defines the specification for the SecretStore. provider: Specifies the external secrets provider and its configuration. aws: Indicates that AWS Secrets Manager is the provider. service: Should be SecretsManager to specify the AWS Secrets Manager service. region: AWS region where the secrets are stored. auth: Authentication configuration for accessing AWS Secrets Manager. secretRef: Refers to the Kubernetes secret containing AWS credentials. accessKeyIDSecretRef: Refers to the Kubernetes secret and key storing the AWS access key ID. secretAccessKeySecretRef: Refers to the Kubernetes secret and key storing the AWS secret access key.</p>"},{"location":"devops/devsecops/external-secret-operator/#create-secret-for-access-key-and-secret-key","title":"Create Secret for access key and secret key.","text":"<p>The External Secrets Operator needs to authenticate with AWS Secrets Manager to fetch secrets. The access key and secret key provide the credentials for this authentication.</p> <pre><code>kubectl create secret generic awssm-secret --from-literal=access-key=&lt;ACCESS-KEY&gt; --from-literal=secret-access-key=&lt;SECRET_KEY&gt; -n external-secrets\n</code></pre>"},{"location":"devops/devsecops/external-secret-operator/#create-the-externalsecret","title":"Create the ExternalSecret","text":"<p>ExternalSecret retrieves secrets from AWS Secrets Manager and makes them available in your Kubernetes cluster.</p> <p>external-secret.yaml</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: external-secret-example\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore-sample\n    kind: SecretStore\n  target:\n    name: kube-secret\n    creationPolicy: Owner\n  dataFrom:\n  - extract:\n      key: DB-CREDENTIAL\n</code></pre> <p><pre><code>kubectl apply -f external-secret.yaml\n</code></pre> You should see Status as SecretSynced.</p> <p></p> <p>spec: Defines the specification for the ExternalSecret. refreshInterval: 1h means the secret will be refreshed every hour from AWS Secrets Manager. secretStoreRef: References the SecretStore to use for accessing AWS Secrets Manager. name: The name of the SecretStore resource (e.g., secretstore-sample). kind: Specifies the type of reference, which should be SecretStore. target: Defines the Kubernetes secret that will be created or updated. name: The name of the Kubernetes secret (e.g., kube-secret). creationPolicy: Determines when the Kubernetes secret is created, with Owner meaning the External Secrets Operator will manage its lifecycle. dataFrom: Specifies that the ExternalSecret should pull data from AWS Secrets Manager. extract: Defines the secret to extract. key: The name of the secret in AWS Secrets Manager (e.g., DB-CREDENTIAL).</p> <ul> <li>After you've created the ExternalSecret resource, you'll be able to see the new Kubernetes Secret that has been synchronized with the Secrets Manager store. Execute the following command:</li> </ul> <p></p>"},{"location":"devops/devsecops/external-secret-operator/#consuming-secret-in-pod","title":"Consuming Secret in Pod","text":"<p>By syncing your AWS Secrets Manager secret to a Kubernetes Secret, External Secrets allows you to use and consume the secret in your Pod specification.</p> <p>We will deploy a simple busybox pod in the external-secrets namespace and use the secret via pod environment variables.</p> <p>Create a manifest file external-secrets-demo-pod.yaml with the following specifications.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: external-secrets\nspec:\n  containers:\n  - image-eso: busybox:1.35.0\n    command:\n      - sleep\n      - \"3600\"\n    image-esoPullPolicy: IfNotPresent\n    name: busybox\n    env:\n      - name: password\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_PASSWORD\n      - name: username\n        valueFrom:\n          secretKeyRef:\n            name: kube-secret\n            key: DB_USER\n  restartPolicy: Always\n</code></pre> <ul> <li>To deploy the pod, run the following command:</li> </ul> <pre><code>kubectl apply -f external-secrets-demo-pod.yaml\n</code></pre> <ul> <li>Once the pod is in a running state, run the following commands to get the container\u2019s shell.</li> </ul> <pre><code>kubectl exec -it busybox -- sh\n</code></pre> <p>From the container\u2019s shell, you can use echo to print the environment variables to view the secrets.</p> <p></p>"},{"location":"devops/falcon-logscale/agent-installation/","title":"Falcon LogScale Agent(Log Collector) Setup","text":"<p>First of all, you need to create a new repo from <code>Repositories and View</code>.After creating the repo, you will go into the repo.You need to enter <code>Settings</code> from the upper menus.You need to enter <code>Ingest Tokens</code> under the <code>Ingest</code> category in the left menu.Then you will create a new token with <code>Add New Token</code>.</p> <p>Remember that this token will be useful later.</p>"},{"location":"devops/falcon-logscale/agent-installation/#download-collector","title":"Download Collector","text":"<p>You should follow the steps below to reach the download page, select the package suitable for your system and download it.</p> <p>Main Page \u27a1\ufe0f Fleet Management \u27a1\ufe0f LogScale Collector Download</p> <p>Important Installations Notes</p> <p>You may get an error while downloading.This is because the download URL is wrong or incorrect.</p> <p>For example: <code>http://10.40.140.2:8080/None/api/v1/log-collector/download/humio-log-collector_1.4.1_linux_amd64.deb</code>. </p> <p>The <code>NONE</code> here may cause you to download an incorrect file.Delete it !.After downloading the file, check its size with the <code>ls -alh</code> command.  </p> <p>Follow these steps after downloading the file.</p> <pre><code>dpkg -i humio-log-collector_x.x.x_linux_amd64.deb\n</code></pre> <p>By default, the humio-log-collector process will run as the humio-log-collector user, which is installed by the package and won't have access to logs in <code>/var/log</code>.</p> <p>This can be granted by adding the user to the adm group. <pre><code>sudo usermod -a -G adm humio-log-collector\n</code></pre></p>"},{"location":"devops/falcon-logscale/agent-installation/#you-can-run-the-logscale-collector-as-a-standalone-process-and-ignore-the-service-file-etc","title":"You can run the LogScale Collector as a standalone process and ignore the service file etc.","text":"<pre><code>/etc/humio-log-collector/config.yaml\n</code></pre> <p>Open the source field and enter the token and ip address you created in the relevant fields.</p> <p>Remember the source option specifies where you want to get the logs from</p> <pre><code>sources:\n  var_log:\n    type: file\n    include: /var/log/*\n    exclude: /var/log/*.gz\n    sink: humio\nsinks:\n  humio:\n    type: humio\n    token: &lt;Ingest Token&gt; # 210f4309-0d71-4d3d-b4e3-d503d46b93b9\n    url: &lt;host ip-address of the humio&gt; # http://52.91.72.78:8080/\n</code></pre> <p>Now you need to start and enable the services to apply the changes</p> <p><pre><code>sudo systemctl start humio-log-collector.service\n</code></pre> <pre><code>sudo systemctl enable humio-log-collector.service\n</code></pre></p> <p>Check the status of the service</p> <pre><code>sudo systemctl status humio-log-collector.service\n</code></pre> <p>Success</p> <p>If Active:active (running), you can go to the repository from the interface and check your logs</p> <p>Important Installations Notes</p> <p>If you get your logs as <code>Docker Container</code>, you can get your logs without installing an agent by using the following command with <code>Driver:Splunk</code></p> <pre><code>export YOUR_LOGSCALE_URL=\"&lt;10.40.140.2:8080&gt;\"\nexport INGEST_TOKEN=\"&lt;210f4309-0d71-4d3d-b4e3-d503d46b93b9&gt;\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\n</code></pre>"},{"location":"devops/falcon-logscale/agent-installation/#to-generate-logs-with-docker-for-testing-you-can-generate-test-logs-using-the-following-docker-compose-commands","title":"To generate logs with <code>docker</code> for testing, you can generate test logs using the following <code>docker-compose</code> commands.","text":"<pre><code>export YOUR_LOGSCALE_URL=\"\"\nexport INGEST_TOKEN=\"\"\ncat &lt;&lt; EOF &gt; docker-compose.yaml\nversion: '3'\nservices:\n  nginx-log-generator:\n    image: kscarlett/nginx-log-generator\n    environment:\n      - RATE=10\n      # other environment variables\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  chentex-random-logger:\n    image: chentex/random-logger:latest\n    command: [\"100\", \"300\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-apache-common:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"apache_common\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-rfc5424:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"rfc5424\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"] \n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\n\n  flog-format-json:\n    image: mingrammer/flog\n    command: [\"--loop\", \"--format\",  \"json\", \"--number\", \"1\", \"--delay\", \"250ms\", \"--type\", \"stdout\"]\n    logging:\n      driver: \"splunk\"\n      options:\n        splunk-url: $YOUR_LOGSCALE_URL\n        splunk-token: $INGEST_TOKEN\nEOF\ndocker-compose up -d\ndocker-compose logs\n</code></pre>"},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/","title":"Falcon LogScale Setup With Docker","text":""},{"location":"devops/falcon-logscale/falcon-logscale-installation-docker/#falcon-logscale-setup-requirements","title":"Falcon LogScale Setup Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>30GB</code>      STORAGE <p> The first step to install LogScale using <code>Docker</code> is to <code>install Docker</code> on the machine where you want to run <code>Docker</code> with LogScale. You can Download Docker from their site or by using a package installation program like yum or apt-get.</p> <p>Or You Can Use These Command For Ubuntu:</p> <pre><code>sudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt install docker.io\nsystemctl start docker\nsystemctl enable docker\ndocker --version\n</code></pre> <p>Now let's run a container on port 8080 using the following commands and watch it with <code>docker ps</code></p> <p>Important Installations Notes</p> <p>If your machine is not open to port 8080, make it open</p> <pre><code>export HOST_DATA_DIR=/home/ubuntu/mounts/data\nexport HOST_KAFKA_DATA_DIR=/home/ubuntu/mounts/kafka-data\nexport PATH_TO_READONLY_FILES=/home/ubuntu/mounts/readonly\nexport HOST_ENV_FILE=/home/ubuntu/mounts/\n\nmkdir -p $PATH_TO_READONLY_FILES\nmkdir -p $HOST_KAFKA_DATA_DIR\nmkdir -p $HOST_DATA_DIR\ntouch $HOST_ENV_FILE.env\n\ndocker run -v $HOST_DATA_DIR:/data  \\\n   -v $HOST_KAFKA_DATA_DIR:/data/kafka-data  \\\n   -v $PATH_TO_READONLY_FILES:/etc/humio:ro  \\\n   -e AUTHENTICATION_METHOD=single-user \\\n   -e SINGLE_USER_USERNAME=hepapi \\\n   -e SINGLE_USER_PASSWORD=123456 \\\n   --net=host \\\n   --name=humio \\\n   --ulimit=\"nofile=8192:8192\"  \\\n   --stop-timeout 300 \\\n   --env-file=$HOST_ENV_FILE  \\\n   -d \\\n   -p 8080:8080 \\\n   humio/humio\n</code></pre> 'HOST_DATA_DIR, HOST_KAFKA_DATA_DIR, HOST_ENV_FILE, PATH_TO_READONLY_FILES'<pre><code>We bind these exported variables to the downloaded HUMIO container.\n</code></pre> <p>Info</p> <p>SINGLE_USER_USERNAME= Your Username for login </p> <p>SINGLE_USER_PASSWORD= Your Password for login</p> <p>Success</p> <p>Let's go your http://ip-address:8080 and enter activation key and enter your <code>SINGLE_USER_USERNAME</code> and <code>SINGLE_USER_PASSWORD</code></p>"},{"location":"devops/falcon-logscale/humio/","title":"Humio Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"    # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"    # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"    # Change here with ip of kafka3 node\n\nEXTERNAL_IP=3.76.209.91         # Change here with ip of humio node\nINTERNAL_IP=3.76.209.91         # Change here with ip of humio node\n\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\nHUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\" # If you'll use here change here with path of humio\nHUMIO_PACKAGE_PATH=\"\"   # If you'll use here change here with path of humio package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\n\n#### Java version control ####\njava --version\nsleep 10\n\n\n#### Hunio user access ####\nadduser humio --shell=/bin/false --no-create-home --system --group\n\n#### Kafka folders create and access ####\nmkdir -p /opt/humio /etc/humio/filebeat /data/log/humio /data/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /data/log/humio /data/humio/data\nsleep 2\n\n#### Humio tar.gz package download or move ####\ncd /opt/humio/\nsleep 2\n\nif [ -n \"$HUMIO_DOWNLOAD_LINK\" ]; then\n\n    cd /opt/humio &amp;&amp; wget $HUMIO_DOWNLOAD_LINK\n    tar zxf \"$(basename $HUMIO_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\n\nelse \n\n    cd /opt/humio &amp;&amp; cp -R $HUMIO_PACKAGE_PATH .\n    tar zxf $HUMIO_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    HUMIO_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n\nfi\nsleep 2\n\n\n#### Humio server.conf create ####\ncd /etc/humio/\ncat &lt;&lt;EOF &gt;&gt; server.conf\nAUTHENTICATION_METHOD=single-user\nSINGLE_USER_USERNAME=admin\nSINGLE_USER_PASSWORD=admin\nDIRECTORY=/data/humio/data\nHUMIO_AUDITLOG_DIR=/data/log/humio\nHUMIO_DEBUGLOG_DIR=/data/log/humio\nJVM_LOG_DIR=/data/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\n\nKAFKA_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092\nEXTERNAL_URL=http://$EXTERNAL_IP:8080\nPUBLIC_URL=http://$INTERNAL_IP:8080\nEOF\nsleep 2\n\n#### Humio system service create ####\ncd /etc/systemd/system/\ncat &lt;&lt;EOF &gt;&gt; humio.service\n[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/data/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\nTimeoutSec=900\n\n[Install]\nWantedBy=default.target\nEOF\nsleep 2\n\n#### User access ####\nchown -R humio:humio /opt/humio /etc/humio\n\nsleep 2\n\nchown -R humio:humio /data/log/humio /data/humio/data\n</code></pre>"},{"location":"devops/falcon-logscale/humioinstallation/","title":"LogScale Installation","text":"<p>First, you'll need to create a non-administrative user named, <code>humio</code> to run LogScale software in the background. You can do this by executing the following from the command-line:  <pre><code>adduser humio --shell=/bin/false --no-create-home --system --group\n</code></pre></p> <p>You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install Kafka.</p> <p>Next, create the LogScale system directories and give the <code>humio</code> user ownership of them: <pre><code>mkdir -p /opt/humio /etc/humio/filebeat /var/log/humio /var/humio/data\nchown humio:humio /opt/humio /etc/humio/filebeat\nchown humio:humio /var/log/humio /var/humio/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/humioinstallation/#installation","title":"Installation","text":"<p>You're now ready to download and install LogScale's software. You should go to the LogScale directory and use wget to download the LogScale Java Archive. You can do this from the command-line like so:  <pre><code>cd /opt/humio/\n\nwget https://repo.humio.com/repository/maven-releases/com/humio/server/1.112.0/server-1.112.0.tar.gz\n\ntar xzf /opt/humio/server-1.112.0.tar.gz\n</code></pre> The wget here is used to download the latest release from Download Humio Server. You'll have to adjust the lines for the correct directory and file name, based on the version at the time. After you've downloaded it, enter the last line here to create a symbolic link to it. </p>"},{"location":"devops/falcon-logscale/humioinstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, create the LogScale configuration file, server.conf in the <code>/etc/humio</code> directory. There are a few environment variables you will need to enter in this configuration file in order to run LogScale on a single server or instance. Below are those basic settings: <pre><code>AUTHENTICATION_METHOD=single-user\nSINGLE_USER_PASSWORD=&lt;Your-Password-Here&gt;\nSINGLE_USER_USERNAME=&lt;Your-Username-Here&gt;\nBOOTSTRAP_HOST_ID=1\nDIRECTORY=/var/humio/data\nHUMIO_AUDITLOG_DIR=/var/log/humio\nHUMIO_DEBUGLOG_DIR=/var/log/humio\nJVM_LOG_DIR=/var/log/humio\nHUMIO_PORT=8080\nELASTIC_PORT=9200\nZOOKEEPER_URL=127.0.0.1:2181\nKAFKA_SERVERS=127.0.0.1:9092\nEXTERNAL_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;:8080\nPUBLIC_URL=http://&lt;Your-LogScale-Machine-Ip-Address&gt;\nHUMIO_SOCKET_BIND=0.0.0.0\nHUMIO_HTTP_BIND=0.0.0.0\n</code></pre> Next you should set up a service file. Using a simple text editor, create a file named, humio.service in the <code>/etc/systemd/system/</code> sub-directory. Add these lines to that file: <pre><code>[Unit]\nDescription=LogScale service\nAfter=network.service\n\n[Service]\nType=notify\nRestart=on-abnormal\nUser=humio\nGroup=humio\nLimitNOFILE=250000:250000\nEnvironmentFile=/etc/humio/server.conf\nWorkingDirectory=/var/humio\nExecStart=/opt/humio/humio/bin/humio-server-start.sh\n\n[Install]\nWantedBy=default.target\n</code></pre>  You will need to change the ownership of the LogScale files and start the LogScale service. To change the ownership, execute the following two lines from the command-line: <pre><code>chown -R humio:humio /opt/humio /etc/humio/filebeat\nchown -R humio:humio /var/log/humio /var/humio/data\n</code></pre> You're ready to start LogScale. <pre><code>systemctl start humio\n</code></pre> Just to be sure LogScale is running and everything is fine, check it with the journalctl tool. You can do this by entering the following from the command-line <pre><code>journalctl -fu humio\n</code></pre> If there are no errors, open a web browser and enter the domain name or IP address with port 8080. For example, you would enter something like http://example.com:8080 in the browser's address field.</p>"},{"location":"devops/falcon-logscale/humiosetup/","title":"Humio Single Node Installation Guide","text":""},{"location":"devops/falcon-logscale/humiosetup/#logscale-installers-are-available-for-several-linux-distributions","title":"LogScale installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Red Hat</li> </ul>"},{"location":"devops/falcon-logscale/humiosetup/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/falcon-logscale/humiosetup/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>16GB</code>  MEMORY <code>8CPU</code>  CPU <code>100GB</code>      STORAGE <p>Access Permissions</p> <p>The machine to be installed must have access to the following addresses.</p> <p>http://humio.com</p> <p>https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz </p> <p>https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz [downloads.apache.org]</p> <p>hkp://keyserver.ubuntu.com:80</p> <p>http://repos.azulsystems.com/ubuntu [repos.azulsystems.com]</p> <p>https://repo.humio.com/repository/maven-releases/com/humio/server/1.117.0/server-1.117.0.tar.gz</p> <p>The following ports must be open; <pre><code>80,443,8080,1514\n</code></pre></p> <p>You must follow the sequence below to install Falcon LogScale.</p> <ul> <li>Java Installation Page</li> <li>Zookeeper Installation Page</li> <li>Kafka Installation Page</li> <li>LogScale Installation Page</li> </ul>"},{"location":"devops/falcon-logscale/javainstallation/","title":"Java Installation","text":"<p>Install at least java version 17 in the following order</p> <p>Download java package <pre><code>https://cdn.azul.com/zulu/bin/zulu17.48.15-ca-jdk17.0.10-linux_amd64.deb\n</code></pre></p> <p>Import Azul\u2019s public key: <pre><code>sudo apt install gnupg ca-certificates curl\n</code></pre> <pre><code>curl -s https://repos.azul.com/azul-repo.key | sudo gpg --dearmor -o /usr/share/keyrings/azul.gpg\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/azul.gpg] https://repos.azul.com/zulu/deb stable main\" | sudo tee /etc/apt/sources.list.d/zulu.list\n</code></pre></p> <p>Install the required Azul Zulu package: <pre><code>sudo apt install zulu17-jdk\n</code></pre> Check java version <pre><code>java -version\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafka/","title":"Kafka Install","text":"<pre><code>#!/bin/bash\n\nKAFKA_HOST_1=\"172.31.18.173\"     # Change here with ip of kafka1 node\nKAFKA_HOST_2=\"172.31.23.248\"     # Change here with ip of kafka2 node\nKAFKA_HOST_3=\"172.31.25.184\"     # Change here with ip of kafka3 node\n\nKAFKA_NODE_NUMBER=1             # Change here with id of kafka1 node\n\nKAFKA_PACKAGE_PATH=\"\"           # If you'll use here change here with path of kafka package\nKAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"  # If you'll use here change here with path of kafka package\n\n#### Apt package update ####\napt-get update -y &amp;&amp; apt-get upgrade -y\nsleep 10\n\n\n#### Java Install ####\napt-get install openjdk-21-jdk -y\njava --version\nsleep 10\n\n#### Add IP Addresses to hosts file\necho \"$KAFKA_HOST_1 kafka1\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_2 kafka2\" &gt;&gt; /etc/hosts\necho \"$KAFKA_HOST_3 kafka3\" &gt;&gt; /etc/hosts\n\n\n#### Kafka user access ####\nadduser kafka --shell=/bin/false --no-create-home --system --group\nsleep 2\n\n#### Kafka tar.gz package download or move ####\nif [ -n \"$KAFKA_DOWNLOAD_LINK\" ]; then\n\n    cd /opt &amp;&amp; wget $KAFKA_DOWNLOAD_LINK\n    tar zxf \"$(basename $KAFKA_DOWNLOAD_LINK)\"\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nelse \n\n    cd /opt &amp;&amp; cp -R $KAFKA_PACKAGE_PATH .\n    tar zxf $KAFKA_PACKAGE_PATH\n\n    # Get list of output files\n    EXTRACTED_FILES=$(ls)\n\n    # Print the resulting files to the screen\n    echo \"Get list of output files:\"\n    echo \"$EXTRACTED_FILES\"\n\n    # Assign the name of the first file to a variable\n    KAFKA_PACKAGE_FILE=$(echo \"$EXTRACTED_FILES\" | head -n 1)\n    ln -s /opt/$KAFKA_PACKAGE_FILE /opt/kafka\n\nfi\nsleep 2\n\n\n#### Kafka folders create and access ####\nmkdir -p /data/log/kafka\nmkdir -p /data/log/zookeeper\nmkdir -p /data/kafka/kafka\nmkdir -p /data/kafka/zookeeper\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka\nsleep 2\n\n\n#### Link for kafka ####\nln -s \"/opt/$KAFKA_PACKAGE_FILE\" /opt/kafka\nsleep 2\n\n\n#### Server properties config ####\ncd /opt/kafka/config\nFILE=\"server.properties\"\n\n# New variables\nBROKER_ID=\"broker.id=$KAFKA_NODE_NUMBER\"\nNEW_LOG_DIRS=\"log.dirs=/data/kafka/kafka\"\nsleep 2\n\n\n# Update specific values \nsed -i \"s/^broker.id=.*/$BROKER_ID/\" \"$FILE\"\nsed -i \"s|^log.dirs=.*|$NEW_LOG_DIRS|\" \"$FILE\"\necho \"delete.topic.enable = true\" &gt;&gt; \"$FILE\"\nsleep 2\n\n\n#### Kafka user chown ####\nchown -R kafka:kafka /opt/kafka\nsleep 2\n\n\n#### Zookeper Config ####\ncd /opt/kafka/config\nrm -rf zookeeper.properties\ncat &lt;&lt;EOF &gt;&gt; zookeeper.properties\ndataDir=/data/kafka/zookeeper\nclientPort=2181\nmaxClientCnxns=0\nadmin.enableServer=false\nserver.1=kafka1:2888:3888\nserver.2=kafka2:2888:3888\nserver.3=kafka3:2888:3888\n4lw.commands.whitelist=*\ntickTime=2000\ninitLimit=5\nsyncLimit=2\nEOF\nsleep 2\n\n\n#### Zookeper chown ####\necho $KAFKA_NODE_NUMBER &gt;/data/kafka/zookeeper/myid\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n\n#### Zookeeper service config ####\ncd /etc/systemd/system\nrm -rf zookeeper.service\ncat &lt;&lt;EOF &gt;&gt; zookeeper.service\n[Unit]\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/zookeeper\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\nsleep 2\n\n#### Zookeeper service start ####\nchown -R kafka:kafka /data/kafka/zookeeper\nsleep 2\n\n#### Kafka service config ####\ncd /etc/systemd/system\nrm -rf kafka.service\ncat &lt;&lt;EOF &gt;&gt; kafka.service\n[Unit]\n\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/data/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\nTimeoutSec=900\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsleep 2\nchown -R kafka:kafka /data/log/kafka /data/log/zookeeper /data/kafka/zookeeper /data/kafka/kafka /opt/kafka\n</code></pre>"},{"location":"devops/falcon-logscale/kafkainstallation/","title":"Kafka Installation","text":"<p>LogScale recommend that the latest Kafka version be used with your LogScale deployment. The latest version of Kafka is available at Kafka Downloads</p> <p><pre><code>apt-get update\napt-get upgrade\n</code></pre>  Next, create a non-administrative user named, <code>kafka</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser kafka --shell=/bin/false --no-create-home --system --group\n</code></pre></p>"},{"location":"devops/falcon-logscale/kafkainstallation/#installation","title":"Installation","text":"<p>To install Kafka, you'll need to go to the /opt directory and download the latest release. You can do that like so with wget.  <pre><code>cd /opt\nwget https://www-us.apache.org/dist/kafka/x.x.x/kafka_x.x.x.x.tgz\n</code></pre> You would adjust this last line, change the Xs to the latest version number. Once it downloads, untar the file and then create the directories it needs like this:  <pre><code>tar zxf kafka_x.x.x.x.tgz\n\nmkdir /var/log/kafka\nmkdir /var/kafka/data\nchown kafka:kafka /var/log/kafka\nchown kafka:kafka /var/kafka/data\n\nln -s /opt/kafka_x.x.x.x /opt/kafka\n</code></pre></p> <p>The four lines in the middle here create the directories for Kafka's logs and data, and changes the ownership of those directories to the kafka user. The last line creates a symbolic to /opt/kafka. You would adjust that, though, replacing the Xs with the version number.</p>"},{"location":"devops/falcon-logscale/kafkainstallation/#configuration","title":"Configuration","text":"<p>Using a simple text editor, open the Kafka properties file, server.properties, located in the kafka/config sub-directory. You'll need to set a few options \u2014 the lines below are not necessarily the order in which they'll be found in the configuration file: <pre><code>broker.id=1\nlog.dirs=/var/kafka/data\ndelete.topic.enable = true\n</code></pre> The first line sets the broker.id value to match the server number <code>(myid)</code> you set when configuring ZooKeeper. The second sets the data directory. The third line should be added to the end of the configuration file. When you're finished, save the file and change the owner to the kafka user:  <pre><code>chown -R kafka:kafka /opt/kafka_x.x.x.x\n</code></pre></p> <p>You'll have to adjust this to the version you installed. Note, changing the ownership of the link <code>/opt/kafka</code> doesn't change the ownership of the files in the directory.</p> <p>Now you'll need to create a service file for starting Kafka. Use a simple text editor to create a file named, kafka.service in the <code>/etc/systemd/system/</code> sub-directory. Then add the following lines to the service file: <pre><code>[Unit]\nRequires=zookeeper.service\nAfter=zookeeper.service\n\n[Service]\nType=simple\nUser=kafka\nLimitNOFILE=800000\nEnvironment=\"LOG_DIR=/var/log/kafka\"\nEnvironment=\"GC_LOG_ENABLED=true\"\nEnvironment=\"KAFKA_HEAP_OPTS=-Xms512M -Xmx4G\"\nExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Now you're ready to start the Kafka service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start kafka\nsystemctl status kafka\nsystemctl enable kafka\n</code></pre></p>"},{"location":"devops/falcon-logscale/readme/","title":"Kafka Installation (kafka.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code></p> <p>KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Ex: KAFKA_HOST_1=\"172.296.22.10\"</p> <p>Usage of KAFKA_NODE_NUMBER</p> <p>If you are installing more than one kafka, you must specify a node number for each kafka. The value you provide indicates the rank of the kafka node.</p> <p><code>KAFKA_NODE_NUMBER=&lt;Kafka Node Number&gt;</code></p>"},{"location":"devops/falcon-logscale/readme/#3-cluster-kafka-example","title":"3 Cluster Kafka Example:","text":"<p><code>(Primary Node kafka.sh)</code>   : KAFKA_NODE_NUMBER=1</p> <p><code>(Secondary Node kafka.sh)</code> : KAFKA_NODE_NUMBER=2</p> <p><code>(Third Node kafka.sh)</code> : KAFKA_NODE_NUMBER=3</p> <p>Usage of KAFKA_DOWNLOAD_LINK / KAFKA_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"KAFKA_DOWNLOAD_LINK\"</code> variable and leave the <code>\"KAFKA_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"KAFKA_PACKAGE_PATH\"</code> variable and leave the <code>\"KAFKA_PACKAGE_URL\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>KAFKA_PACKAGE_PATH=\"\" KAFKA_DOWNLOAD_LINK=\"https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\"</p> <p>Example Usage (We will use PATH):</p> <p>KAFKA_PACKAGE_PATH=\"/tmp/packages/kafka_2.13-3.7.0.tgz\" KAFKA_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#humio-setup-humiosh","title":"Humio Setup (humio.sh)","text":"<p>Usage of KAFKA_HOST</p> <p>If you are going to install more than one kafka, you need to change the following variables</p> <p>KAFKA_HOST_1=<code>\"&lt;Node1 Kafka IP Address&gt;\"</code> KAFKA_HOST_2=<code>\"&lt;Node2 Kafka IP Address&gt;\"</code> KAFKA_HOST_3=<code>\"&lt;Node3 Kafka IP Address&gt;\"</code></p> <p>Usage of EXTERNAL_IP / INTERNAL_IP</p> <p>To access the Humio interface:</p> <p>EXTERNAL_IP=<code>\"&lt;Humio External IP Address&gt;\"</code></p> <p>INTERNAL_IP=<code>\"&lt;Humio Internal IP Address&gt;\"</code></p> <p>Usage of HUMIO_DOWNLOAD_LINK / HUMIO_PACKAGE_PATH</p> <p>If you are going to download a package from humio.com in the Kafka installation, you must fill in the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable and leave the <code>\"HUMIO_PACKAGE_PATH\"</code> variable empty. If you are going to use a package on the server, you must fill in the <code>\"HUMIO_PACKAGE_PATH\"</code> variable and leave the <code>\"HUMIO_DOWNLOAD_LINK\"</code> variable empty.</p> <p>Sample Usage (We will use URL):</p> <p>HUMIO_PACKAGE_PATH=\"\" HUMIO_DOWNLOAD_LINK=\"https://repo.humio.com/repository/maven-releases/com/humio/server/1.131.1/server-1.131.1.tar.gz\"</p> <p>Example Usage (We will use PATH):</p> <p>HUMIO_PACKAGE_PATH=\"/tmp/packages/server-1.131.1.tar.gz\" HUMIO_DOWNLOAD_LINK=\"\"</p>"},{"location":"devops/falcon-logscale/readme/#run-the-services","title":"Run The Services","text":"<p>After the installation is completed, quickly run the following commands one by one.</p> <p>For each node (node1, node2, node3)</p> <p><code>systemctl enable zookeeper</code></p> <p><code>systemctl start zookeeper</code></p> <p><code>systemctl status zookeeper</code></p> <p><code>systemctl enable kafka</code></p> <p><code>systemctl start kafka</code></p> <p><code>systemctl status kafka</code></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/","title":"Zookeeper Installation","text":"<p><pre><code>apt-get update\napt-get upgrade\n</code></pre> Next, create a non-administrative user named, <code>zookeeper</code> to run Kafka. You can do this by executing the following from the command-line:  <pre><code>adduser zookeeper --shell=/bin/false --no-create-home --system --group\n</code></pre> You should add this user to the DenyUsers section of your nodes <code>/etc/ssh/sshd_config</code> file to prevent it from being able to ssh or sftp into the node. Remember to restart the sshd daemon after making the change. Once the system has finished updating and the user has been created, you can install ZooKeeper.</p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#installation","title":"Installation","text":"<p>Navigate to opt directory and download a of ZooKeeper. The official release site is Apache Zookeeper Release <pre><code>cd /opt\nwget https://archive.apache.org/dist/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz\n</code></pre> After the file downloads, untar the ZooKeeper file and create a symbolic to <code>/opt/zookeeper</code> like so:  <pre><code>tar -zxf apache-zookeeper-x.x.x-bin.tar.gz\nln -s /opt/apache-zookeeper-x.x.x-bin /opt/zookeeper\n</code></pre> Navigate to zookeeper sub-directory and create a data directory for ZooKeeper:  <pre><code>cd /opt/zookeeper\nmkdir -p /var/zookeeper/data\n</code></pre></p>"},{"location":"devops/falcon-logscale/zookeeperinstallation/#configuration","title":"Configuration","text":"<p>Using a text editor, create the ZooKeeper configuration file in the conf sub-directory. Name the file, zoo.cfg. For example, <code>/opt/zookeeper/conf/zoo.cfg</code>. Copy the lines below into that file: <pre><code>tickTime = 2000\ndataDir = /var/zookeeper/data\nclientPort = 2181\ninitLimit = 5\nsyncLimit = 2\nmaxClientCnxns=60\nautopurge.purgeInterval=1\nadmin.enableServer=false\n4lw.commands.whitelist=*\nserver.1=127.0.0.1:2888:3888\nadmin.enableServer=false\n</code></pre> Create a myid file in the data sub-directory with just the number 1 as its contents. They you can start ZooKeeper to verify that the configuration is working:</p> <p><pre><code>bash -c 'echo 1 &gt; /var/zookeeper/data/myid'\n\n./bin/zkServer.sh start\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-x.x.x/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n</code></pre> Stop ZooKeeper and change the ownership of the zookeeper directory like so, adjusting for the version number you installed:  <pre><code>./bin/zkServer.sh stop\n\nchown -R zookeeper:zookeeper /opt/apache-zookeeper-x.x.x\nchown -R zookeeper:zookeeper /var/zookeeper/data\n</code></pre></p> <p>So that ZooKeeper will start when the server is rebooted, you'll need to create a ZooKeeper service file named zookeeper.service in the <code>/etc/systemd/system/</code> sub-directory. Use a text editor to create the file and copy the following lines into it. <pre><code>[Unit]\nDescription=ZooKeeper Daemon\nDocumentation=http://zookeeper.apache.org\nRequires=network.target\nAfter=network.target\n\n[Service]\nType=forking\nWorkingDirectory=/opt/zookeeper\nUser=zookeeper\nGroup=zookeeper\nExecStart=/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg\nExecStop=/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg\nExecReload=/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg\nTimeoutSec=30\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n</code></pre> Start the ZooKeeper service. Enter the first line below to start it. When it finishes, enter the second line to check that it's running and there are no errors reported:  <pre><code>systemctl start zookeeper\nsystemctl status zookeeper\nsystemctl enable zookeeper\n</code></pre></p>"},{"location":"devops/git/Commands/","title":"Commands","text":"Command Explanation Usage push Pushing is the process of sending local commits to a remote repository. <code>git push origin branch_name</code> pull Pulling is the process of fetching and merging remote changes into the local repository. <code>git pull origin branch_name</code> add . Adds all modified and new files to the staging area. <code>git add .</code> add  Adds a specific file to the staging area. <code>git add file_name</code> commit A commit is a snapshot of changes made to the repository. <code>git commit -m \"Commit message\"</code> init Initializes a new Git repository in the current directory. <code>git init</code> log Log displays the commit history of the repository. <code>git log</code> status Status shows the current state of the repository. <code>git status</code> branch A branch is a separate line of development within a repository. <code>git branch branch_name</code> merge Merging combines changes from different branches into a single branch. <code>git merge branch_name</code> revert Revert creates a new commit that undoes changes from a previous commit. <code>git revert commit_hash</code> reset Reset moves the current branch pointer to a specific commit, potentially discarding commits. <code>git reset commit_hash</code> rm / remove Removes a file from the repository and the working directory. <code>git rm file_name</code> mv / move Renames or moves a file or directory within the repository. <code>git mv old_file_name new_file_name</code> clone Cloning creates a local copy of a remote repository. <code>git clone repository_url</code> echo Echo prints a message or value to the terminal or a file. <code>echo \"Hello, World!\"</code> touch Touch creates an empty file or updates the timestamp of an existing file. <code>touch file_name</code> ls / list List displays the files and directories in the current directory. <code>ls</code> or <code>list</code> cat Cat displays the contents of a file. <code>cat file_name</code> diff Diff shows the differences between different versions of files. <code>git diff</code> checkout Checkout allows you to switch between branches or restore files from previous commits. <code>git checkout branch_name</code> .gitignore A .gitignore file specifies files and patterns to be ignored by Git. Create a .gitignore file and list files/patterns to ignore repository A repository is a location where Git stores all the files, history, and changes for a project. <code>git init</code> fork Forking creates a copy of a repository under your GitHub account. Click on the \"Fork\" button in the GitHub UI pull request A pull request proposes changes from a forked repository to the original repository. Create a pull request through the GitHub UI PR (Pull Request) PR is an abbreviation for pull request. Use the term \"PR\" interchangeably with \"pull request\" master Master is the default branch in Git. The initial branch created in a repository (commonly used) config Config sets configuration options for Git. <code>git config --global user.name \"Your Name\"</code> remote Remote refers to a remote repository, typically on a server. <code>git remote add origin repository_url</code> stash Stash temporarily saves local modifications for later use. <code>git stash save \"Stash message\"</code> pop Pop applies the most recent stash and removes it from the stash list. <code>git stash pop</code> reflog Reflog shows a log of all reference updates in the repository. <code>git reflog</code> Action Explanation Usage Create and Switch to New Branch Creates a new branch and switches to it. <code>git checkout -b &lt;branch&gt;</code> Merge Branch into Main Branch Merges changes from a feature branch into the main branch. <code>git checkout &lt;main_branch&gt;</code><code>git merge &lt;feature_branch&gt;</code> Rebase Branch onto Main Branch Updates the feature branch with the latest changes from the main branch. <code>git checkout &lt;feature_branch&gt;</code><code>git rebase &lt;main_branch&gt;</code> Interactive Rebase Allows interactive modification, reordering, or squashing of commits. <code>git rebase -i HEAD~&lt;number_of_commits&gt;</code> Revert a Commit Creates a new commit that undoes the changes from a specific commit. <code>git revert &lt;commit_hash&gt;</code> Undo Last Commit (Keep Changes) Moves the branch pointer back one commit, keeping changes in the staging area. <code>git reset --soft HEAD~1</code> Discard Last Commit (Lose Changes) Moves the branch pointer back one commit, discarding changes in the working directory and staging area. <code>git reset --hard HEAD~1</code>"},{"location":"devops/git/Description/","title":"Description","text":""},{"location":"devops/git/Description/#version-control-system-vcs","title":"Version Control System (VCS)","text":"<p>A Version Control System (VCS) is a software tool that helps track changes made to files and directories over time. It allows multiple people to collaborate on a project, keeping a history of changes, and providing mechanisms to manage different versions of files. VCS enables teams to work concurrently, facilitating efficient collaboration and providing features like branching, merging, and conflict resolution.</p>"},{"location":"devops/git/Description/#git","title":"Git","text":"<p>Git is a widely used distributed version control system designed for speed, flexibility, and data integrity. It is free and open-source, offering powerful features that make it popular among individuals and large organizations. Git provides a decentralized approach, allowing users to have a local copy of the entire repository, including its history, branches, and tags.</p>"},{"location":"devops/git/installation/","title":"Installation","text":""},{"location":"devops/git/installation/#downloading-git","title":"Downloading Git","text":"<p>To download Git for different operating systems, follow these instructions:</p>"},{"location":"devops/git/installation/#windows","title":"Windows","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the latest Git version for Windows.</li> <li>Run the installer and follow the prompts.</li> <li>Choose the desired installation options and complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#macos","title":"macOS","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the macOS version of Git.</li> <li>Run the installer package and follow the prompts.</li> <li>Complete the installation process.</li> </ul>"},{"location":"devops/git/installation/#linux-ubuntu","title":"Linux (Ubuntu)","text":"<ul> <li>Open the terminal.</li> <li>Install Git using the package manager:</li> <li>For Debian/Ubuntu-based systems: <code>sudo apt-get install git</code></li> <li>For Fedora: <code>sudo dnf install git</code></li> <li>For CentOS/RHEL: <code>sudo yum install git</code></li> </ul>"},{"location":"devops/git/installation/#linux-generic","title":"Linux (Generic)","text":"<ul> <li>Visit the official Git website: <code>https://git-scm.com/</code>.</li> <li>Click on the \"Downloads\" link.</li> <li>Download the source code archive for Linux.</li> <li>Extract the archive to a desired location.</li> <li>In the terminal, navigate to the extracted directory.</li> <li>Run the following commands:</li> <li><code>make prefix=/usr/local all</code></li> <li><code>sudo make prefix=/usr/local install</code></li> </ul>"},{"location":"devops/git/installation/#verification","title":"Verification","text":"<p>Once Git is installed on your system, you can verify the installation by opening a terminal and running <code>git --version</code> to display the installed version.</p>"},{"location":"devops/git/installation/#interfaces","title":"Interfaces","text":"<p>Git provides a command-line interface (CLI) and various graphical user interface (GUI) tools. You can choose the interface that suits your preference and start using Git for version control in your projects.</p>"},{"location":"devops/jenkins/","title":"Jenkins","text":""},{"location":"devops/jenkins/jenkins-installation/","title":"Jenkins Install","text":""},{"location":"devops/jenkins/jenkins-installation/#jenkins-installers-are-available-for-several-linux-distributions","title":"Jenkins installers are available for several Linux distributions.","text":"<ul> <li>Debian/Ubuntu</li> <li>Fedora</li> <li>Red Hat/Alma/Rocky</li> </ul>"},{"location":"devops/jenkins/jenkins-installation/#prerequisites","title":"Prerequisites ;","text":""},{"location":"devops/jenkins/jenkins-installation/#minimum-hardware-requirements","title":"Minimum hardware requirements:","text":"Resources Limits <code>256MB</code>  MEMORY <code>512MB</code>  CPU <code>1GB</code>      STORAGE <p>1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container)</p>"},{"location":"devops/jenkins/jenkins-installation/#recommended-hardware-configuration-for-a-small-team","title":"Recommended hardware configuration for a small team:","text":"Resources Limits <code>4GB</code>  MEMORY <code>2</code>  CPU <code>50GB</code>      STORAGE"},{"location":"devops/jenkins/jenkins-installation/#installation-of-java","title":"Installation of Java","text":"<p><code>Jenkins requires Java in order to run, yet certain distributions don\u2019t include this by default and some Java versions are incompatible with Jenkins. There are multiple Java implementations which you can use. OpenJDK is the most popular one at the moment, we will use it in this guide. Update the Debian apt repositories, install OpenJDK 11, and check the installation with the commands:</code></p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jre\njava -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment (build 11.0.12+7-post-Debian-2)\nOpenJDK 64-Bit Server VM (build 11.0.12+7-post-Debian-2, mixed mode, sharing)\n</code></pre> <p>After installing Java without any problems, we will install jenkins</p>"},{"location":"devops/jenkins/jenkins-installation/#jenkins-install_1","title":"Jenkins Install","text":"<pre><code>curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install jenkins\n</code></pre> <p>Quote</p> <p>If Jenkins fails to start because a port is in use, run -- systemctl edit jenkins -- and add the following ; <pre><code>[Service]\nEnvironment=\"JENKINS_PORT=8081\"\n</code></pre></p>"},{"location":"devops/jenkins/jenkins-installation/#start-jenkins","title":"Start Jenkins","text":"<p>You can enable the Jenkins service to start at boot with the command: <pre><code>sudo systemctl enable jenkins\n</code></pre></p> <p>You can start the Jenkins service with the command: <pre><code>sudo systemctl start jenkins\n</code></pre></p> <p>You can check the status of the Jenkins service using the command: <pre><code>sudo systemctl status jenkins\n</code></pre></p> <p>If everything has been set up correctly, you should see an output like this: <pre><code>Loaded: loaded (/lib/systemd/system/jenkins.service; enabled; vendor preset: enabled)\nActive: active (running) since Tue 2018-11-13 16:19:01 +03; 4min 57s ago\n</code></pre></p> <p>If you have a firewall installed, you must add Jenkins as an exception. You must change YOURPORT in the script below to the port you want to use. Port 8080 is the most common. <pre><code>YOURPORT=8080\nPERM=\"--permanent\"\nSERV=\"$PERM --service=jenkins\"\n\nfirewall-cmd $PERM --new-service=jenkins\nfirewall-cmd $SERV --set-short=\"Jenkins ports\"\nfirewall-cmd $SERV --set-description=\"Jenkins port exceptions\"\nfirewall-cmd $SERV --add-port=$YOURPORT/tcp\nfirewall-cmd $PERM --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n</code></pre></p> <ul> <li> After installing Jenkins, we go to the instance ip address e.g : <code>&lt; ip-address &gt;:8080</code> </li> <li> <p> On the page that opens, it asks us to enter a token. We take this token from the server with the <code>cat /var/lib/jenkins/secrets/initialAdminPassword</code> code and paste it into the input and log in.</p> </li> <li> <p> Jenkins gives you two options.</p> <ul> <li> <code>Install Suggested Plugins</code></li> <li> <code>Select Plugins To Install</code></li> </ul> </li> </ul> <p>If you don't see jenkins when you go to your server's ip address, allow port 8080</p> <p> You can login to Jenkins by choosing the one you want and creating Username and Password.</p> <p>For more installation details</p>"},{"location":"devops/jenkins/jenkins-installation/#happy-jenkins","title":"Happy Jenkins","text":""},{"location":"devops/jenkins/shared-library/","title":"Jenkins Shared Library","text":"<p>Hello everybody from Hepapi.We will talk about Jenkins Shared Library.Lets start. We are in a period where modern applications are generally divided into small components and run on a microservice architecture. Compared to a monolithic application, there are many extra pipelines in an application with microservice architecture. Therefore, it is very important to ensure modularity and reusability of the created pipelines. Thanks to Jenkins Shared Library, we can get rid of the code complexity in the pipeline and comply with the DRY (Don't Repeat Yourself) principle.</p> <p>Example: You can perform the docker build process that you perform jointly for more than one service by simply sending the docker image information to a single function for all services.</p> <pre><code>// var/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String REPOSITORY) {\n  String REGISTRY = \"ersinsari\"\n  sh \"docker build -t ${REGISTRY}/${REPOSITORY}:latest .\"\n}\n// Jenkinsfile\n@Library('mylibrary') _\npipeline {\n  stages {\n    stage('Docker Build') {\n      dockerBuild \"nodejs-helloworld\"    \n    }\n  }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#folder-structure-of-shared-library","title":"Folder Structure of Shared Library","text":"<pre><code>\u2514\u2500\u2500 jenkins-shared\n    \u251c\u2500\u2500 src\n    \u2502   \u251c\u2500\u2500 main\n    \u2502       \u2514\u2500\u2500 groovy\n    |           \u2514\u2500\u2500 *.groovy\n    \u251c\u2500\u2500 vars           \n    |   \u251c\u2500\u2500 *.groovy\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 config.properties\n        \u2514\u2500\u2500 template.xml\n</code></pre> <ul> <li> <p>The src directory is structured like a standard Java project. This means that you can use the import statement to import classes from other directories in the src directory.</p> </li> <li> <p>The vars directory is a special directory that contains global variables that are defined in the shared library. These variables can be accessed from any Jenkins job that imports the shared library.</p> </li> <li> <p>The resources directory is a regular directory that can contain any type of file. However, it is typically used to store static resources that are used by the shared library.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#global-shared-libraries","title":"Global Shared Libraries","text":"<p>There are several places where Shared Libraries can be defined, depending on the use-case</p> <ul> <li>Manage Jenkins \u00bb System \u00bb Global Pipeline Libraries as many libraries as necessary can be configured</li> </ul> <p>These libraries are considered \"trusted:\" they can run any methods in Java, Groovy, Jenkins internal APIs, Jenkins plugins, or third-party libraries. This allows you to define libraries which encapsulate individually unsafe APIs in a higher-level wrapper safe for use from any Pipeline. Beware that anyone able to push commits to this SCM repository could obtain unlimited access to Jenkins. You need the Overall/RunScripts permission to configure these libraries (normally this will be granted to Jenkins administrators).</p>"},{"location":"devops/jenkins/shared-library/#folder-level-shared-libraries","title":"Folder-level Shared Libraries","text":"<p>Any Folder created can have Shared Libraries associated with it. This mechanism allows scoping of specific libraries to all the Pipelines inside of the folder or subfolder.Folder-based libraries are not considered \"trusted:\" they run in the Groovy sandbox just like typical Pipelines.</p>"},{"location":"devops/jenkins/shared-library/#automatic-shared-libraries","title":"Automatic Shared Libraries","text":"<p>Other plugins may add ways of defining libraries on the fly. For example, the Pipeline: GitHub Groovy Libraries plugin allows a script to use an untrusted library named like github.com/someorg/somerepo without any additional configuration. In this case, the specified GitHub repository would be loaded, from the master branch, using an anonymous checkout.</p>"},{"location":"devops/jenkins/shared-library/#using-libraries","title":"Using Libraries","text":"<p>Shared Libraries marked Load implicitly allows Pipelines to immediately use classes or global variables defined by any such libraries. To access other shared libraries, the Jenkinsfile needs to use the @Library annotation, specifying the library\u2019s name:</p> <pre><code>@Library('my-shared-library') _\n\n/* Using a version specifier, such as branch, tag, etc */\n@Library('my-shared-library@1.0') _\n\n/* Accessing multiple libraries with one statement */\n@Library(['my-shared-library', 'otherlib@abc1234']) _\n</code></pre>"},{"location":"devops/jenkins/shared-library/#loading-libraries-dynamically","title":"Loading libraries dynamically","text":"<p>As of version 2.7 of the Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time during the build.</p> <p>If you are only interested in using global variables/functions (from the vars/ directory), the syntax is quite simple:</p> <pre><code>library 'my-shared-library'\n</code></pre> <p>Thereafter, any global variables from that library will be accessible to the script.</p> <p>Using classes from the src/ directory is also possible, but trickier. Whereas the @Library annotation prepares the \u201cclasspath\u201d of the script prior to compilation, by the time a library step is encountered the script has already been compiled. Therefore you cannot import or otherwise \u201cstatically\u201d refer to types from the library.</p>"},{"location":"devops/jenkins/shared-library/#demo","title":"DEMO","text":"<p>First of all we need jenkins server for this demo.There are some options to deploy jenkins-server</p> <pre><code>https://www.jenkins.io/doc/book/installing/\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-1-creating-shared-library","title":"Step-1 Creating Shared library","text":"<p>Let's first create the groovy scripts of the Jenkins shared library and push them to git. We have two simple scripts for this example. We will perform docker build and docker push operations using these scripts.</p> <pre><code>// vars/dockerBuild.groovy\n#!/usr/bin/env groovy\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n    dir(\"${WORKSPACE}\") {\n        sh \"docker build -t ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER) .\"\n    }\n}\n</code></pre> <pre><code>// vars/dockerPush.groovy\n#!/usr/bin/env groovy\n\ndef call(String APP_IMAGE_REGISTRY, String APP_IMAGE_REPOSITORY) {\n\n    dir(\"${WORKSPACE}\") {\n        sh \"echo $DOCKERHUB_CRED_PSW | docker login -u $DOCKERHUB_CRED_USR --password-stdin\"\n        sh \"docker push ${APP_IMAGE_REGISTRY}/${APP_IMAGE_REPOSITORY}:$(BUILD_NUMBER)\"\n    }\n}\n</code></pre> <pre><code>// vars/sayHello.groovy\n#!/usr/bin/env groovy\n\ndef call(String name = 'human') {\n  echo \"Hello, ${name}.\"\n}\n</code></pre> <p>Go to the Global Pipeline Library to Jenkins. After accessing the Jenkins dashboard, navigate to Manage Jenkins &gt;  System to find the Global Pipeline Libraries section and click on the add button to add a new library. Manage Jenkins ---&gt; System ---&gt; Global Pipeline Libraries ---&gt; Click Add button</p> <ul> <li>Library Name: my-shared-library</li> <li>Default version: main</li> <li>Retrivial method: Modern SCM</li> <li>Source Code Management</li> <li> <p>Project Repository: https://github.com/ersinsari13/jenkins-shared-library.git  #enter your repo name</p> </li> <li> <p>We can leave other parameters as default and save the configuration.</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-2-creating-jenkinsfile","title":"Step-2 Creating jenkinsfile","text":"<p>Let's first create github repo named docker-build-push then create the Jenkinsfile and push them to git.</p> <pre><code>@Library('my-shared-library') _\npipeline {\n    agent any\n\n    environment {\n        DOCKERHUB_CRED=credentials('docker-hub-credential')\n    }\n    stages {\n        stage('Set Environment') {\n            steps {\n                script {\n                    REGISTRY = \"ersinsari\"\n                    REPOSITORY = \"docker-build-push\"\n                }\n            }\n        }\n        stage('Hello') {\n            steps {\n                sayHello \"hepapi\"\n            }\n        }\n        stage('Build') {\n            steps {\n                dockerBuild \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n        stage('Push') {\n            steps {\n                dockerPush \"${REGISTRY}\", \"${REPOSITORY}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"devops/jenkins/shared-library/#step-3-creating-pipeline","title":"Step-3 Creating Pipeline","text":"<ul> <li>Go to Jenkins Server Dashboard</li> <li>Click New Item</li> <li>Enter \"docker-build-push\" as a name and select pipeline and click Ok</li> <li> <p>Under Pipeline Section:</p> </li> <li> <p>Definition: Pipeline script from SCM</p> </li> <li>SCM: Git</li> <li>Repository URL: https://github.com/ersinsari13/docker-build-push.git  #enter your repository name</li> <li>Branch: main</li> </ul> <p>Since the Jenkinsfile in the repo is not under any folder, it will be sufficient to just specify its name. For example, if my Jenkinsfile file was located under a folder named build; I should have written build/Jenkinsfile in the Script Path section.</p> <ul> <li>Save the pipeline configuration</li> </ul>"},{"location":"devops/jenkins/shared-library/#step-4-credential-for-dockerhub","title":"Step-4 Credential for Dockerhub","text":"<p>In order for the images we created in Pipeline to be pushed to the registry, we need to provide registry credential information to the Jenkins server.</p> <ul> <li>Go to Jenkins Dashboard</li> <li>Select Manage Jenkins</li> <li>Click Credentials</li> <li>Select Global</li> <li> <p>Click Add Credentials button:</p> </li> <li> <p>Kind: Username with password</p> </li> <li>username: enter your dockerhub registry username</li> <li>password: enter your dockerhub registry password</li> <li>id: dockerhub-registry-credentials</li> <li> <p>description: dockerhub-registry-credentials</p> </li> <li> <p>Save credentials configuration</p> </li> </ul>"},{"location":"devops/jenkins/shared-library/#step-5-build-pipeline","title":"Step-5 Build Pipeline","text":"<p>After all the steps, the pipeline we created is now ready to be tested.</p> <ul> <li>Go to Jenkins Server Dashboard</li> <li>Select Pipeline was created</li> <li>Click Build Now</li> <li>You should see the pipeline result success</li> <li>You can check your new image on the Dockerhub registry</li> </ul>"},{"location":"devops/k8s-engine/k3s-installation/","title":"K3S Setup","text":""},{"location":"devops/k8s-engine/k3s-installation/#k3s-requirements","title":"K3S Requirements :","text":"Resources Limits <code>512MB</code>  MEMORY (we recommend at least 1GB) <code>1CPU</code>  CPU (we recommend at least 2CPU) <code>20GB</code>      STORAGE"},{"location":"devops/k8s-engine/k3s-installation/#cpu-and-memory","title":"CPU and Memory","text":"<p>The following are the minimum CPU and memory requirements for nodes in a high-availability K3S server</p> Deployment Size Nodes VCPUS RAM Small Up to 10 2 4 GB Medium Up to 100 4 8 GB Large Up to 250 8 16 GB X-Large Up to 500 16 32 GB XX-Large 500+ 32 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#disks","title":"Disks","text":"<p> The cluster performance depends on database performance. To ensure optimal speed, we recommend always using SSD disks to back your K3S cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS.</p>"},{"location":"devops/k8s-engine/k3s-installation/#database","title":"Database","text":"<p>K3S supports different databases including MySQL, PostgreSQL, MariaDB, and etcd, the following is a sizing guide for the database resources you need to run large clusters:</p> Deployment Size Nodes VCPUS RAM Small Up to 10 1 2 GB Medium Up to 100 2 8 GB Large Up to 250 4 16 GB X-Large Up to 500 8 32 GB XX-Large 500+ 16 64 GB"},{"location":"devops/k8s-engine/k3s-installation/#single-master-node-k3s-install","title":"Single Master Node K3S Install","text":"<p>A single-node server installation is a fully-functional Kubernetes cluster, including all the datastore, control-plane, kubelet, and container runtime components necessary to host workload pods. It is not necessary to add additional server or agents nodes, but you may want to do so to add additional capacity or redundancy to your cluster.</p> <p><code>Run this command :</code> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#high-available-k3s-install","title":"High Available K3S Install","text":"<p>To run K3S in this mode, you must have an odd number of server nodes. We recommend starting with three nodes.</p> <p>To get started, first launch a server node with the <code>cluster-init</code> flag to enable clustering and a token that will be used as a shared secret to join additional servers to the cluster.</p> <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s.service</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 13:54:17 UTC; 37min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>After launching the first server, join the second and third servers to the cluster using the shared secret: <pre><code>curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://&lt;ip or hostname of server1&gt;:6443\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#cluster-access","title":"Cluster Access","text":"<p>The kubeconfig file stored at <code>/etc/rancher/k3s/k3s.yaml</code> is used to configure access to the Kubernetes cluster. If you have installed upstream Kubernetes command line tools such as kubectl or helm you will need to configure them with the correct kubeconfig path.This can be done by either exporting the <code>KUBECONFIG</code> environment variable or by invoking the <code>--kubeconfig</code> command line flag. Refer to the examples below for details.</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Check to see that the second and third servers are now part of the cluster: <pre><code>$ kubectl get nodes\n\nNAME        STATUS   ROLES                       AGE   VERSION\nserver1     Ready    control-plane,etcd,master   28m   vX.Y.Z\nserver2     Ready    control-plane,etcd,master   13m   vX.Y.Z\nserver3     Ready    control-plane,etcd,master   10m   vX.Y.Z\n</code></pre></p>"},{"location":"devops/k8s-engine/k3s-installation/#k3s-agent-install","title":"K3S Agent Install","text":"<p>To install additional agent nodes and add them to the cluster, run the installation script with the <code>K3S_URL</code> and <code>K3S_TOKEN</code> environment variables. Here is an example showing how to join an agent: <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -\n</code></pre></p> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status k3s-agent</code> and make sure it is <code>active</code>. <pre><code>\u25cf k3s-agent.service - Lightweight Kubernetes\n Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: enabled)\n Active: active (running) since Tue 2023-06-06 14:24:53 UTC; 7min ago\n   Docs: https://k3s.io\n</code></pre></p> <p>Setting the <code>K3S_URL</code> parameter causes the installer to configure K3S as an agent, instead of a server. The K3S agent will register with the K3S server listening at the supplied URL. if you have not set a stable token the value to use for <code>K3S_TOKEN</code> is stored at <code>/var/lib/rancher/k3s/server/node-token</code> on your server node.</p> <p>Example</p> Leverage the KUBECONFIG environment variable:Or specify the location of the kubeconfig file in the command: <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n</code></pre> <pre><code>kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces\nhelm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces\n</code></pre>"},{"location":"devops/k8s-engine/k3s-installation/#uninstalling-k3s","title":"Uninstalling K3S","text":"<p>Example</p> Uninstalling Servers:Uninstalling Agents: <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre> <pre><code>/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre> <p>Make sure all required port numbers are open</p> <p>For More Details</p>"},{"location":"devops/k8s-engine/rke2-installation-ansible/","title":"RKE2 Cluster Installation With Ansible","text":"<p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#set-up-passwordless-ssh","title":"Set up passwordless SSH","text":"<ol> <li> <p>Generate ssh key</p> <pre><code>ssh-keygen\n</code></pre> <p>Simply press enter when prompted for default name and no password, this will create <code>~/.ssh/id_rsa</code></p> </li> <li> <p>Copy ssh keys to master and worker nodes</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa root@10.40.140.4\n</code></pre> <p>Enter the ssh password when prompted and repeat for all master and worker nodes.</p> </li> <li> <p>Add the ssh key to the ssh-agent</p> <pre><code>ssh-agent bash\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>If you get an error run the following and try again</p> <pre><code>eval `ssh-agent`\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#install-ansible-and-rke2-deployment-role","title":"Install Ansible and RKE2 deployment role","text":"<ol> <li> <p>Install ansible</p> <pre><code>sudo apt update\nsudo apt install ansible -y\n</code></pre> </li> <li> <p>Install the lablabs/rke2 ansible role (https://galaxy.ansible.com/lablabs/rke2)</p> <pre><code>ansible-galaxy install lablabs.rke2\n</code></pre> <p>This will install the role under the <code>~/.ansible</code> directory</p> </li> <li> <p>Navigate to the <code>~/.ansible</code> directory</p> <pre><code>cd ~/.ansible\n</code></pre> </li> <li> <p>Create inventory file in <code>~/.ansible</code> (can be copied from local with scp)</p> <pre><code>vi inventory\n</code></pre> <p>Go into insert mode by pressing i</p> <p>Copy the following and paste with ctrl+shift+v</p> <pre><code>[masters]\nmaster-01 ansible_host=10.40.140.4 rke2_type=server\nmaster-02 ansible_host=10.40.140.5 rke2_type=server\nmaster-03 ansible_host=10.40.140.6 rke2_type=server\n\n[workers]\nworker-01 ansible_host=10.40.140.7 rke2_type=agent\nworker-02 ansible_host=10.40.140.8 rke2_type=agent\nworker-03 ansible_host=10.40.140.9 rke2_type=agent\n\n[k8s_cluster:children]\nmasters\nworkers\n</code></pre> <p>Host names (ex: master-01) can be changed, however they must be lowercase.</p> <p>Save and quit by pressing Esc, then :wq!</p> </li> <li> <p>Create playbook.yaml file in <code>~/.ansible</code> (check previous step for help with vi)</p> <pre><code>- name: Deploy RKE2\n  hosts: all\n  become: yes\n  vars:\n    rke2_ha_mode: true\n    rke2_api_ip : 10.40.140.4\n    rke2_download_kubeconf: true\n    rke2_ha_mode_keepalived: false\n    rke2_server_node_taints:\n      - 'CriticalAddonsOnly=true:NoExecute'\n  roles:\n    - role: lablabs.rke2\n</code></pre> <p><code>rke2_api_ip</code> should point to your load balancer for master nodes, this should be a TCP load balancer on port 6443. If you don't have one it can point to one of the master nodes.</p> <p>Alternatively you can use keepalived by removing the <code>rke2_ha_mode_keepalived</code> line, and pointing the <code>rke2_api_ip</code> to an unused static IP such as <code>10.40.140.100</code></p> </li> <li> <p>Confirm inventory is working</p> <pre><code>ansible all -i inventory -m ping\n</code></pre> <p>There should be no errors.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#run-the-playbook-and-confirm-the-rke2-cluster-is-up","title":"Run the playbook and confirm the RKE2 cluster is up","text":"<ol> <li> <p>Run the ansible playbook</p> <pre><code>ansible-playbook -i inventory playbook.yaml\n</code></pre> <p>If you're ssh'ing to the other machines as a non-root user, run the following instead:</p> <pre><code>ansible-playbook -i inventory playbook.yaml -K\n</code></pre> <p>Provide the password for the user you're logging in as when prompted.</p> <p>If it hangs at 'Wait for remaining nodes to be ready', check if rke2 is installed on all machines, if not the <code>rke2.sh</code> script may not be running. To fix this, edit the <code>~/.ansible/roles/lablabs.rke2/tasks/rke2.yml</code> by removing the <code>Check RKE2 version</code> task and replacing it with:</p> <pre><code>- name: Check rke2 bin exists\n  ansible.builtin.stat:\n    path: \"{{ rke2_bin_path }}\"\n  register: rke2_exists\n\n- name: Check RKE2 version\n  ansible.builtin.shell: |\n    set -o pipefail\n    {{ rke2_bin_path }} --version | grep -E \"rke2 version\" | awk '{print $3}'\n  args:\n    executable: /bin/bash\n  changed_when: false\n  register: installed_rke2_version\n  when: rke2_exists.stat.exists\n</code></pre> </li> <li> <p>Install kubectl</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Copy kubeconfig file to a better directory and export kubeconfig</p> <pre><code>cp /tmp/rke2.yaml ~/rke2.yaml\n</code></pre> <p>Now we can manage our cluster with <code>kubectl --kubeconfig ~/rke2.yaml</code>, or we can do the following to shorten our commands:</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm our cluster is running and with correct internal IP addresses</p> <pre><code>kubectl get nodes -o wide\n</code></pre> </li> <li> <p>Check the health of our pods</p> <pre><code>kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#cleanup","title":"Cleanup","text":""},{"location":"devops/k8s-engine/rke2-installation-ansible/#jump","title":"Jump","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Kubectl and Ansible</p> <pre><code>sudo snap remove kubectl\nsudo apt remove ansible\n</code></pre> </li> <li> <p>Remove remaining artifacts</p> <pre><code>rm -rf /root/.ansible\nrm /root/.ssh/id_rsa /root/.ssh/id_rsa.pub\nrm ~/rke2.yaml\nrm -rf /root/.kube\napt autoremove -y\n</code></pre> <p>If you followed the installation guide as a non-root user the directories might be different.</p> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation-ansible/#master-and-worker-nodes","title":"Master and Worker nodes","text":"<p>Repeat the following on all master and worker nodes</p> <ol> <li> <p>ssh into the node</p> <pre><code>ssh root@10.40.140.4\n</code></pre> </li> <li> <p>Run the RKE2 uninstall script</p> <pre><code>sudo /usr/local/bin/rke2-uninstall.sh\n</code></pre> </li> </ol>"},{"location":"devops/k8s-engine/rke2-installation/","title":"RKE2 Setup","text":""},{"location":"devops/k8s-engine/rke2-installation/#rke2-requirements","title":"RKE2 Requirements :","text":"Resources Limits <code>4GB</code>  MEMORY (we recommend at least 8GB) <code>2</code>  CPU (we recommend at least 4CPU) <code>60GB</code>      STORAGE <p> We turn off the firewall to avoid problems in the future. We update the packages and clean up any files left over from previous installations.</p> <p>Ubuntu:</p> <pre><code># Ubuntu instructions \n# stop the software firewall\nsystemctl disable --now ufw\n# get updates, install nfs, and apply\napt update\napt install nfs-common -y  \napt upgrade -y\n# clean up\napt autoremove -y\n</code></pre> <p>Now that we have all the nodes up to date, let's focus on <code>rancher1</code>. While this might seem controversial, <code>curl | bash</code> does work nicely. The install script will use the tarball install for Ubuntu and the RPM install for Rocky/Centos. Please be patient, the start command can take a minute. Here are the rke2 docs and install options for reference.</p>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-server-install","title":"RKE2 Server Install","text":"<pre><code>#rancher1\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=server sh -\n\nmkdir -p /etc/rancher/rke2/\ncat &lt;&lt; EOF &gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\nEOF\n\n# enable and start\nsystemctl enable --now rke2-server.service\n</code></pre> <p>Success</p> <p>Let's validate everything worked as expected. Run a <code>systemctl status rke2-server</code> and make sure it is <code>active</code>.</p> <p>Quote</p> <p>If you want to install a specific version use the following command ;</p> <pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.24 INSTALL_RKE2_TYPE=server sh -\n</code></pre> <p>Perfect!  Now we can start talking Kubernetes. We need to symlink the <code>kubectl</code> cli on <code>rancher1</code> that gets installed from RKE2.</p> <pre><code># add kubectl conf\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# check node status\nkubectl get node\n</code></pre> <p>Quote</p> <p>In addition these commands can be used for KUBECONFIG</p> <pre><code># simlink all the things - kubectl\nln -s $(find /var/lib/rancher/rke2/data/ -name kubectl) /usr/local/bin/kubectl\n\n# add kubectl conf\nexport KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n</code></pre> <p>We will also need to get the token from <code>rancher1</code>.</p> <pre><code># save this for rancher2 and rancher3\ncat /var/lib/rancher/rke2/server/node-token\n</code></pre>"},{"location":"devops/k8s-engine/rke2-installation/#rke2-agent-install","title":"RKE2 Agent Install","text":"<p>The agent install is VERY similar to the server install. Except that we need an agent config file before starting. We will start with <code>rancher2</code>. We need to install the agent and setup the configuration file.</p> <pre><code># we add INSTALL_RKE2_TYPE=agent\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=agent sh -\n\n# create config file\nmkdir -p /etc/rancher/rke2/ \n\n# change the ip to reflect your rancher1 ip\necho \"server: https://$RANCHER1_IP:9345\" &gt; /etc/rancher/rke2/config.yaml\n\n# change the Token to the one from rancher1 /var/lib/rancher/rke2/server/node-token \necho \"token: $TOKEN\" &gt;&gt; /etc/rancher/rke2/config.yaml\n\n# enable and start\nsystemctl enable --now rke2-agent.service\n</code></pre> <p>If you want to add your Node as control plane, etcd replace here with below code</p> <p>Example</p> Orjinal CodeChange Code <pre><code>systemctl enable --now rke2-agent.service\n</code></pre> <pre><code>systemctl enable --now rke2-server.service\n</code></pre> <p>Rinse and repeat. Run the same install commands on <code>rancher2</code>, <code>rancher3</code>. Next we can validate all the nodes are playing nice by running kubectl get node -o wide on rancher1. </p>"},{"location":"devops/k8s-engine/rke2-installation/#run-this-code","title":"Run this code !","text":"'To check that Kubernetes is running'<pre><code>kubectl get node\n</code></pre> <p>Important Installations Notes</p> <p>If you are having problems with installations, make sure there are no problems with instances' accessing each other <code>(For Example --Ssh connection--: Permission Denied)</code></p> <p>Check below steps if RKE2-Server or RKE2-Agent is not working </p> <ul> <li> Rancher1 instances Node Token is correct ? </li> <li> Instances IP address is Correct ? </li> <li> Instance Ports is open (9345, 6443) ?</li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/","title":"Restoring RKE2 Clusters","text":"<p>During this process, the cluster will be unavailable. </p> <ul> <li>ETCD should be restored on the first master node that was created in the cluster.</li> <li>ETCD restoration process starts by disabling all rke2 services on all nodes.</li> <li>Don't forget to remove the <code>etcd database directory</code> on secondary master nodes before starting the <code>rke2-server</code>.</li> <li>ETCD restoration process starts with limitin etcd to single node. </li> </ul>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-existing-nodes","title":"Restoring RKE2 Cluster ETCD on Existing Nodes","text":"<p>This process will restore the ETCD on the existing nodes. </p> <p>If you're adding new nodes to the cluster, you should follow the Restoring RKE2 Cluster ETCD on New Nodes section.</p> <p>1) On ALL nodes, disable the <code>rke2-server</code> service: <pre><code>systemctl disable rke2-server -now\n</code></pre></p> <p>2) On ALL nodes, kill all remaining processes: <pre><code>rke2-killall.sh\n</code></pre> 3 On FIRST NODE, restore etcd: <pre><code>rke2 server --cluster-reset \\\n    --cluster-reset-restore-path=$PATH_TO_SNAPSHOT_FILE\n</code></pre> 4) On FIRST NODE, start the rke2: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p> <p>5) On SECONDARY MASTER NODES, delete the etcd data directory: <pre><code># delete the old etcd data directory\n# make sure the directory is correct for your installation\nrm -rf /var/lib/rancher/rke2/server/db\n</code></pre> 6) On SECONDARY MASTER NODES, start the rke2 server: <pre><code>systemctl enable rke2-server -now\nsystemctl start rke2-server -now\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-ha-etcd-restore/#restoring-rke2-cluster-etcd-on-new-nodes","title":"Restoring RKE2 Cluster ETCD on New Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/","title":"Deploy RKE2 Highly Available Cluster","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prerequisites","title":"Prerequisites","text":"a b c 3 Linux Hosts RKE2 System Requirements"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#prepare-the-hosts","title":"Prepare the hosts","text":"<p>[ATTENTION]: Do this on all of the master nodes</p> <p>1) Become root</p> <pre><code>sudo su\n</code></pre> <p>2) Install the OS dependencies</p> <pre><code>apt update -y &amp;&amp; apt upgrade -y\n\nsystemctl disable --now ufw\napt install nfs-common jq libselinux1 curl apparmor-profiles -y\napt autoremove -y\n\napparmor_status # check if apparmor is running\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-nodes","title":"MASTER NODES","text":""},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-1","title":"MASTER NODE 1","text":"<p>Notes</p> <p>[ATTENTION] Run this steps on the first master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation.</p> <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> <p>2) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nkube-apiserver-arg: \"kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\"\n\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n#etcd-s3: true\n#etcd-s3-endpoint: \"s3.amazonaws.com\"\n#etcd-s3-access-key: \"AKIAAAAAAAAAAAAAAAA\"\n#etcd-s3-secret-key: \"BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\"\n#etcd-s3-region: \"eu-central-1\"\n#etcd-s3-bucket: \"---my-s3-bucket-where-i-store-etcd-backups\"\n#etcd-s3-folder: \"---myclustername-etcd-backups-folder/\"\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>3) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>4) Start the <code>rke2-server</code></p> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> <p>5) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre> <p>6) [ATTENTION] Copy <code>node-token</code> to a safe place for joining other master nodes later on</p> <pre><code>cat /var/lib/rancher/rke2/server/node-token\n# output should look like this:\n# K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#master-node-2-3","title":"MASTER NODE 2 &amp; 3","text":"<p>Prerequisites</p> Required Name Description \u2705 <code>MASTER_IP</code> Static endpoint for the first master node \u2705 <code>MASTER_NODE_TOKEN</code> Should get this from first master node installation \u2705 <code>INSTALL_RKE2_VERSION</code> The RKE2 release version that you want to upgrade to <p>Notes</p> <p>[ATTENTION] Run this steps on the second and third master node.</p> <p>Steps</p> <p>1) [ATTENTION] Set the Prerequisites. <pre><code>export MASTER_IP=\"172.31.15.113\" # TODO: change this to your master-1 IP\nexport MASTER_NODE_TOKEN=\"K10b2896e9397d199b72da45ddc91c2449b27caa4155d36cd8f5dfe679c7b0f0b25::server:ddb937957cd932c74e496059dd1e0f03\"\n</code></pre> 2) [ATTENTION] Set the Installation Parameters. Select your version of RKE2 and the type of installation. <pre><code># set the RKE2 environment variables\nexport DEBUG=1 # enable debug messages for rke2 scripts\nexport INSTALL_RKE2_VERSION=\"v1.21.7+rke2r2\" # `latest` or `stable` works as well\nexport INSTALL_RKE2_TYPE=\"server\"  # server for master nodes\n</code></pre> 3) Create RKE2 Configuration File: <code>/etc/rancher/rke2/config.yaml</code></p> <pre><code>mkdir -p /etc/rancher/rke2/ # create the directory if not exists\n\n# create the rke2 configuration file\n# IMPORTANT\ncat &lt;&lt; EOF &gt;&gt; /etc/rancher/rke2/config.yaml\nserver: https://$MASTER_IP:9345\ntoken: $MASTER_NODE_TOKEN\n\n# ---------------- OPTIONAL CONFIGURATION BELOW ----------------\n# ---- Uncomment &amp; Change the lines below to enable the configuration ----\n\n# ------ OPTION: Node Labels ------\n#node-label:\n#- other=what\n#- foo=three\n\n# ------ OPTION: ETCD Automatic Backups Cronjob ------\n# This configuration will take snapshots every 5 mins.\n# and keep the last 40 snapshots. For help: https://crontab.guru/\netcd-snapshot-schedule-cron: '*/5 * * * *'\netcd-snapshot-retention: 40\n\n\n# ------ OPTION: ETCD Automatic Backups Upload to S3 Storage ------\n# Note: Doing S3 Upload configuration only on the first master node is enough\n\n\n# ------ OPTION: Control Plane Resource Requests and Limits ------\n#https://docs.rke2.io/advanced#control-plane-component-resource-requestslimits\n#control-plane-resource-requests:\n#- kube-apiserver-cpu=500m\n#- kube-apiserver-memory=512M\n#- kube-scheduler-cpu=250m\n#- kube-scheduler-memory=512M\n#- etcd-cpu=1000m\nEOF\n\n\n# ATTENTION: check if the configuration file is valid\ncat /etc/rancher/rke2/config.yaml\n</code></pre> <p>4) Install RKE2</p> <pre><code># run the install script and save the output to a log file\n./rke2_install_script.sh\n# IMPORTANT: copy the output to a log file\n\n\n# persist path and kubeconfig configuration\necho 'export PATH=$PATH:/var/lib/rancher/rke2/bin KUBECONFIG=/etc/rancher/rke2/rke2.yaml' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>5) Start the <code>rke2-server</code> <pre><code>systemctl enable rke2-server.service\nsystemctl start rke2-server.service\n</code></pre> 6) Wait for <code>rke2-server</code> to become available</p> <pre><code># check the status of the service\nsystemctl status rke2-server.service\n\n# check the logs\njournalctl -u rke2-server -f # follow logs\n</code></pre>"},{"location":"devops/k8s-engine/rke2-ha/rke2-highly-available-installation/#adding-worker-nodes","title":"Adding Worker Nodes","text":"<p>TODO: add docs</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/","title":"System Upgrade Controller","text":"<p>rancher/system-upgrade-controller</p> <p>RKE2 Docs: Automatic Upgrades</p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Highly Available RKE2 Cluster</li> <li>Install the <code>system-upgrade-controller</code> on the cluster</li> </ul> <p>Visit for the latest installation instructions: RKE2 Docs: Install the system-upgrade-controller</p> <p>For now, the command is: (the version might change, better to follow the docs) <pre><code>kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.9.1/system-upgrade-controller.yaml\n</code></pre></p>"},{"location":"devops/k8s-engine/rke2-ha/rke2-system-upgrade-controller-installation/#upgrading-master-nodes-1-by-1","title":"Upgrading Master Nodes 1-by-1","text":"<p>1) [ATTENTION] Decide on which version of RKE2 to upgrade to. <pre><code># Kubernetes Label: remove '+' and replace with '-' or it doesnt work\n\n# TODO: SET THIS TO A RELEASE NUMBER or `latest` or `stable` works as well\nexport RKE2_UPGRADE_VERSION=\"v1.21.14+rke2r1\" \nexport RKE2_UPGRADE_VERSION_SAFE_STRING=$(echo -n $RKE2_UPGRADE_VERSION | tr '+' '-')\n# \"v1.21.14+rke2r1\" --becomes--&gt; \"v1.21.14-rke2r1\"\n</code></pre></p> <p>2) [ATTENTION] Create Master Node Upgrade Plan yaml file.</p> <p>This command will create a file named <code>rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml</code></p> <p>This plan can only be applied to master nodes that have the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p>You can also add the label <code>rke2-upgrade: disabled</code> to a master node to prevent it from being upgraded. <pre><code># create the upgrade-plan for it \ncat &lt;&lt; EOF &gt; rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_STRING.yaml\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: server-plan-for-$RKE2_UPGRADE_VERSION_SAFE_STRING\n  namespace: system-upgrade\n  labels:\n    rke2-upgrade: server\nspec:\n  concurrency: 1\n  nodeSelector:\n    matchExpressions:\n      - {key: node-role.kubernetes.io/control-plane, operator: In, values: [\"true\"]}\n      - {key: rke2-upgrade, operator: Exists}\n      - {key: rke2-upgrade, operator: NotIn, values: [\"disabled\", \"false\"]}\n      - {key: rke2-upgrade-to, operator: In, values: [\"$RKE2_UPGRADE_VERSION_SAFE_STRING\"]}\n  serviceAccountName: system-upgrade\n  cordon: true\n  drain:\n    force: true\n  upgrade:\n    image: rancher/rke2-upgrade\n  version: \"$RKE2_UPGRADE_VERSION\"\nEOF\n</code></pre></p> <p>3) Apply the <code>Plan</code> to the cluster</p> <p>This will not upgrade the nodes. That'll happen when we label the nodes correctly.</p> <p><code>Plan</code> will create <code>Job</code>s for each master node that has the labels:</p> <ul> <li><code>rke2-upgrade</code>: <code>enabled</code></li> <li><code>rke2-upgrade-to</code>: $RKE2_UPGRADE_VERSION_SAFE_STRING</li> </ul> <p><pre><code># apply the plan and check it\nkubectl apply -f rke2-master-upgrade-plan-$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL.yaml\n\n# check the plan and jobs\nkubectl -n system-upgrade get plans,jobs\n</code></pre> 4) [OPTIONAL] Disable a node from upgrading <pre><code>kubectl label nodes &lt;node-name&gt; rke2-upgrade=disabled --overwrite\n</code></pre></p> <p>5) [ATTENTION] Labeling &amp; Upgrading the master nodes 1-by-1</p> <p>This step should be done for each master node one after other.</p> <p>Labeling the master nodes will trigger an automatic upgrade.</p> <p>Do this step for all master nodes one after other.</p> <pre><code>echo \"RKE2_UPGRADE_VERSION=$RKE2_UPGRADE_VERSION\"\necho \"RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL=$RKE2_UPGRADE_VERSION_SAFE_K8S_LABEL\"\n\n# label the a master node to upgrade\nkubectl label nodes &lt;node-name&gt; rke2-upgrade=enabled --overwrite\nkubectl label nodes &lt;node-name&gt; rke2-upgrade-to=$RKE2_UPGRADE_VERSION_SAFE_STRING --overwrite\n\n# check if a job is created for the upgrade of the node\nkubectl -n system-upgrade get jobs,plans\n\n# watch as the node is upgraded\nkubectl get nodes --watch\n</code></pre>"},{"location":"devops/k8s-storage/longhorn/","title":"Longhorn Block Storage","text":"<p>Hello everybody! \ud83e\udee1</p> <p>Today I'm going to tell you about Longhorn Block Storage. First of all, what is Longhorn? What is it for? Why is it used? Let's find out... \ud83d\udcaa</p> <p>Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes.</p> <p>Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation.</p> <p>With Longhorn, you can:</p> <ul> <li>Use Longhorn volumes as persistent storage for the distributed stateful applications in your Kubernetes cluster</li> <li>Partition your block storage into Longhorn volumes so that you can use Kubernetes volumes with or without a cloud provider</li> <li>Replicate block storage across multiple nodes and data centers to increase availability</li> <li>Store backup data in external storage such as NFS or AWS S3</li> <li>Create cross-cluster disaster recovery volumes so that data from a primary Kubernetes cluster can be quickly recovered from backup in a second Kubernetes cluster</li> <li>Schedule recurring snapshots of a volume, and schedule recurring backups to NFS or S3-compatible secondary storage</li> <li>Restore volumes from backup</li> <li>Upgrade Longhorn without disrupting persistent volumes</li> </ul> <p>So what have we learned about Longhorn so far? Now let's see how we install K8s on it.</p> <p>\ud83d\udccc First of all, we need to set up the <code>helm</code> in our cluster.</p> <p>Okay, let's go on now.</p> <p>Now let's add <code>Longhorn</code> to the helm repository and update our repom. <pre><code>helm repo add longhorn https://charts.longhorn.io \nHelm repo update\n</code></pre></p> <p>If we get here smoothly, we can go on.</p> <p>Next, we have to set up the Longhorn. Let's create a new namespace and install Longhorn. \ud83d\udc47</p> <pre><code>kubectl create namespace longhorn-system helm install longhorn longhorn/longhorn --namespace longhorn-system\n</code></pre> <p>Installation finished \u2705</p> <p>How easy it was, wasn't it? It would be nice to see Longhorn as the UI at once,is it? Let's do that.</p> <pre><code>kubectl get service -n longhorn-system\n</code></pre> <p>Let's list <code>longhorn-frontend</code> with the command above. And with the command below, let's get this service <code>port-forward</code>.</p> <pre><code>kubectl port-forward -n longhorn-system service/longhorn-frontend 8080:80\n</code></pre> <p>Now visit <code>localhost:8080</code> and enjoy <code>longhorn</code>.</p> <p>We need to test the last <code>Longhorn</code> that we set up, right?</p> <pre><code>helm install my-release oci://registry-1.docker.io/bitnamicharts/mysql\n</code></pre> <p>Let's use the following command to display bitnamicharts/mysql <code>PersistentVolume, PersistentVolumeClaim</code> and the <code>Longhorn StorageClass</code> that we have installed.</p> <pre><code>kubectl get pv,pvc -n default\nkubectl get storageclass -n default\n</code></pre> <p>In fact, the <code>Longhorn</code> installation is so simple. See you on the next blog. \ud83c\udf88</p>"},{"location":"devops/k8s-storage/nfs-install/","title":"NFS Setup Requirements","text":"<ul> <li>1 Master Node</li> <li>1 Worker Node (NFS)</li> </ul> <p>First of all, we save the ip address of our NFS server in the <code>/etc/hosts</code> file on all our nodes.(<code>master - nfs</code>)</p> <pre><code>/etc/hosts\n\n172.31.18.194 master\n172.31.20.138 k8s-ankara-nfs01\n</code></pre> <p>Then we install the necessary applications for our NFS structure.(<code>nfs</code>)</p> <pre><code>sudo apt-get update\nsudo apt-get install -y nfs-kernel-server\n</code></pre> <p>Let's create a directory on our server to store files.(<code>nfs</code>) <pre><code>sudo mkdir /k8s-data &amp;&amp; sudo mkdir /k8s-data/ankara-data\nsudo chmod 1777 /k8s-data/ankara-data\ntouch /k8s-data/ankara-data/ankara-cluster.txt\n</code></pre> We need to edit the NFS server file for the directory we just created. At this stage, we will share the directory with all our nodes.(<code>nfs</code>) <pre><code>sudo nano /etc/exports\n</code></pre></p> <p>After opening, add the following values at the end.(<code>nfs</code>) <pre><code># /etc/exports: the access control list for filesystems which may be exported\n#               to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n\n/k8s-data/ankara-data *(rw,sync,no_root_squash,subtree_check)\n</code></pre></p> <p>Then run the following command to re-read exportfs and confirm the changes.(<code>nfs</code>) <pre><code>sudo exportfs -ra\n</code></pre></p> <p>Now we will do the following operations on all our Kubernetes nodes.(<code>master - nfs</code>) <pre><code>sudo apt-get -y install nfs-common\n</code></pre></p> <p>After the installation is finished, we check whether there is an access problem by running the following command on all our nodes.(<code>master</code>) <pre><code>showmount -e k8s-ankara-nfs01\n</code></pre></p> <p>There does not seem to be any problem. Now we can mount the folder we opened on our NFS server to our nodes.(<code>master</code>) <pre><code>sudo mount k8s-ankara-nfs01:/k8s-data/ankara-data /mnt\nls -l /mnt\n</code></pre></p> <p>We have made all the setup and adjustments for NFS. We can store the data of our pods without any problems.</p> <p>To make our NFS available to pods, the sample Persistent Volume, Persistent Volume Claim and Pod yaml file should be as follows. <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ankara-cluster-test-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /k8s-data/ankara-data\n    server: k8s-ankara-nfs01\n    readOnly: false\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ankara-pv-claim\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: data-generator-pod\nspec:\n  containers:\n  - name: data-generator-container\n    image: alpine\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      while true; do\n        echo \"$(date) - Data content: $(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 10)\" &gt; /data/data-$(date +%s).txt\n        sleep 1\n      done\n    volumeMounts:\n    - name: data-volume\n      mountPath: /data\n  volumes:\n  - name: data-volume\n    persistentVolumeClaim:\n      claimName: ankara-pv-claim\n</code></pre></p>"},{"location":"devops/kubernetes/keda/keda/","title":"KEDA (Kubernetes Event-driven Autoscaling)","text":"<p>What is KEDA ?</p> <p>KEDA is a lightweight, open-source Kubernetes event-driven autoscaler used by DevOps, SRE, and Ops teams to horizontally scale pods based on external events or triggers. KEDA helps to extend the capability of native Kubernetes autoscaling solutions, which rely on standard resource metrics such as CPU or memory. You can deploy KEDA into a Kubernetes cluster and manage the scaling of pods using custom resource definitions (CRDs). Built on top of Kubernetes HPA, KEDA scales pods based on information from event sources such as AWS SQS, Kafka, RabbitMQ, etc. These event sources are monitored using scalers, which activate or deactivate deployments based on the rules set for them. KEDA scalers can also feed custom metrics for a specific event source, helping DevOps teams observe metrics relevant to them</p> <p> KEDA scales down the number of pods to zero in case there are no events to process. This is harder to do using the standard HPA, and it helps ensure effective resource utilization and cost optimization, ultimately bringing down the cloud bills..</p> <p>Quote</p> <p>KEDA supports a lot of built-in scalers and external scalers. External scalers include Redis, MYSQL,Prometheus,Rabbit MQ etc. Using external events as triggers aids efficient autoscaling, especially for message-driven microservices like payment gateways or order systems. Since KEDA can be extended by developing integrations with any data source, it can easily fit into any DevOps toolchain. </p>"},{"location":"devops/kubernetes/keda/keda/#keda-components","title":"KEDA Components","text":"<p>Event Sources</p> <p>These are the external event/trigger sources by which KEDA changes the number of pods. Prometheus, RabbitMQ, and Apache Pulsar are some examples of event sources.</p> <p>Metric Adapter</p> <p>Metrics adapter takes metrics from scalers and translates or adapts them into a form that HPA/controller component can understand.</p> <p>Controller</p> <p>The controller/operator acts upon the metrics provided by the adapter and brings about the desired deployment state specified in the ScaledObject (refer below).</p> <p>ScaledObject and ScaledJob:</p> <p>ScaledObject represents the mapping between event sources and objects, and specifies the scaling rules for a Deployment, StatefulSet, Jobs or any Custom Resource in a K8s cluster. Similarly, ScaledJob is used to specify scaling rules for Kubernetes Jobs.</p> <p>Below is an example of a ScaledObject which configures KEDA autoscaling based on Prometheus metrics. Here, the deployment object \u2018keda-test\u2019 is scaled based on the trigger threshold (50) from Prometheus metrics. KEDA will scale the number of replicas between a minimum of 1 and a maximum of 10, and scale down to 0 replicas if the metric value drops below the threshold.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: demo3\nspec:\n  scaleTargetRef:\n    apiVersion: argoproj.io/v1alpha1\n    kind: Rollout\n    name: keda-test-demo3\n  triggers:\n    - type: prometheus\n      metadata:\n      serverAddress:  http://&lt;prometheus-host&gt;:9090\n      metricName: http_request_total\n      query: envoy_cluster_upstream_rq{appId=\"300\", cluster_name=\"300-0\", container=\"envoy\", namespace=\"test\", response_code=\"200\" }\n      threshold: \"50\"\n  idleReplicaCount: 0                       \n  minReplicaCount: 1\n  maxReplicaCount: 10\n</code></pre> <p>Deploying KEDA on any Kubernetes cluster is easy, as it doesn\u2019t need overwriting or duplication of existing functionalities. Once deployed and the components are ready, the event-based scaling starts with the external event source. The scaler will continuously monitor for events based on the source set in ScaledObject and pass the metrics to the metrics adapter in case of any trigger events. The metrics adapter then adapts the metrics and provides them to the controller component, which then scales up or down the deployment based on the scaling rules set in ScaledObject.</p> <p>Note that KEDA activates or deactivates a deployment by scaling the number of replicas to zero or one. It then triggers HPA to scale the number of workloads from one to n based on the cluster resources</p>"},{"location":"devops/kubernetes/keda/keda/#keda-deployment","title":"Keda Deployment","text":"<p>KEDA can be deployed in a Kubernetes cluster through Helm charts, operator hub, or YAML declarations</p> <p>Helm Chart</p> <ul> <li>Add Helm repo</li> </ul> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\n</code></pre> <ul> <li>Update Helm repo</li> </ul> <pre><code>helm repo update\n</code></pre> <ul> <li>Install keda Helm chart</li> </ul> <pre><code>helm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre> <p>Deploying with operator Hub</p> <p>On Operator Hub Marketplace locate and install KEDA operator to namespace keda Create KedaController resource named keda in namespace keda</p> <p>NOTE: Further information on Operator Hub installation method can be found in the following repository.</p> <p>https://github.com/kedacore/keda-olm-operator</p> <p>Deploying using the deployment YAML files</p> <p>Run the following command (if needed, replace the version, in this case 2.13.0, with the one you are using):</p> <pre><code># Including admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0.yaml\n# Without admission webhooks\nkubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0-core.yaml\n</code></pre>"},{"location":"devops/linux/tips/","title":"Linux Tips","text":""},{"location":"devops/linux/tooling/","title":"Linux Tooling","text":""},{"location":"devops/linux/tooling/#fzf-fuzzy-finder","title":"FZF - Fuzzy Finder ().","text":"<ul> <li>Github: https://github.com/junegunn/fzf</li> <li>Description: A command-line fuzzy finder. Use <code>&lt;C-r&gt;</code> to search through your command history, <code>&lt;C-t&gt;</code> to search through your files.</li> </ul>"},{"location":"devops/linux/tooling/#tldr","title":"tldr ()","text":"<ul> <li>Website: https://tldr.sh/</li> <li>Description: Too Long Didn't Read. Use it to learn about a command and its most useful options. <code>tldr &lt;any command&gt;</code> (e.g. <code>tldr curl</code>).   </li> </ul>"},{"location":"devops/linux/shell/ampersand-nohup/","title":"nohup, &","text":""},{"location":"devops/linux/shell/ampersand-nohup/#nohup-and","title":"nohup and &amp;","text":"<p>Ampersand (&amp;)</p> <p>The ampersand (<code>&amp;</code>) is an operator that is used to send the execution of a command to the background.</p> <p>Syntax:</p> <p><code>[command] &amp;</code></p> <p>For example, the command <code>sleep 5 &amp;</code> will start the sleep command in the background. You can then run other commands without having to wait for the sleep command to finish.</p> <p>The nohup command</p> <p>The <code>nohup</code> command is used to run a command in the background without any interruption, even when the terminal session is closed.</p> <p>Syntax:</p> <p><code>nohup [command]</code></p> <p>For example, the command <code>nohup sleep 5 &amp;</code> will start the sleep command in the background and will continue to run even if you close the terminal session.</p>"},{"location":"devops/linux/shell/ampersand-nohup/#difference-between-nohup-and","title":"Difference between <code>nohup</code> and <code>&amp;</code>","text":"<p>The <code>nohup</code> and <code>&amp;</code> commands are both used to run commands in the background. However, there are some key differences between the two commands.</p> <ul> <li><code>nohup</code> prevents the command from being interrupted by the <code>HUP</code> signal, even if the terminal session is closed. The <code>HUP</code> signal is typically sent to a process when the terminal session is closed. This can cause the process to stop running. However, the <code>nohup</code> command catches the <code>HUP</code> signal and ignores it, so that the command continues to run even after the terminal session is closed.</li> <li><code>&amp;</code> does not prevent the command from being interrupted by the <code>HUP</code> signal. If you run a command with the <code>&amp;</code> operator and then close the terminal session, the command will stop running.</li> </ul> <p>Differences between the <code>nohup</code> and <code>&amp;</code> commands:</p> <code>nohup</code> <code>&amp;</code> Prevents <code>HUP</code> signal Yes No Redirects output to file Yes No Suitable for Running commands that need to continue running even after the terminal session is closed Running commands that don't need to continue running after the terminal session is closed"},{"location":"devops/linux/shell/cat/","title":"cat","text":"<ul> <li>Description: Concatenate files and print on the standard output. Can be used to create a file from the standard input.</li> </ul> <p>This would display the contents of the file in the terminal window.</p> <p>Options</p> <p>The <code>cat</code> command provides several options that can be used to modify its behavior. Here are some of the most useful options:</p> <ul> <li><code>-n</code>: Shows line numbers in the output.</li> <li><code>-e</code>: Shows end-of-line characters as \"$\".</li> <li><code>-t</code>: Shows tab characters as \"^I\".</li> <li><code>-v</code>: Shows non-printable characters as \"^M\".</li> <li><code>-s</code>: Squeezes consecutive empty lines into a single line.</li> <li><code>-E</code>: Prints a \"$\" character at the end of each line.</li> </ul> <p>Heredoc (Here Document)</p> <p>The <code>heredoc</code> (Here Document) is a type of redirection that allows you to pass multiple lines of input to a command.</p> <p>The basic syntax for <code>heredoc</code> looks like this:</p> <pre><code>cat &lt;&lt; LimitString\n  text...\nLimitString\n</code></pre> <p>Here, LimitString is any string you choose, and text... is the text you want to pass to the command.</p> <p>Inline file creation with redirection</p> <p>Same thing above applies here and it saves you from creating a file and then editing it.</p> <pre><code>cat &lt;&lt; EOF &gt; newfile.txt\nThis is line 1.\nThis is line 2.\nEOF\n</code></pre> <p>This command will create <code>newfile.txt</code> file with the two lines of text.</p>"},{"location":"devops/linux/shell/chtsh/","title":"cht.sh Command Tool","text":"<p>In Linux it can be hard to remember some commands or options and there is a great tool for that. <code>cht.sh</code></p> <p>For example :</p> <pre><code>curl cht.sh/cat\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/cat\n cheat.sheets:cat\n# POSIX way in which to cat(1); see cat(1posix).\ncat -u [FILE_1 [FILE_2] ...]\n\n# Output a file, expanding any escape sequences (default). Using this short\n# one-liner let's you view the boot log how it was show at boot-time.\ncat /var/log/boot.log\n\n# This is an ever-popular useless use of cat.\ncat /etc/passwd | grep '^root'\n# The sane way:\ngrep '^root' /etc/passwd\n\n# If in bash(1), this is often (but not always) a useless use of cat(1).\nBuffer=`cat /etc/passwd`\n# The sane way:\nBuffer=`&lt; /etc/passwd`\n\n cheat:cat\n# To display the contents of a file:\ncat &lt;file&gt;\n\n# To display file contents with line numbers\ncat -n &lt;file&gt;\n\n# To display file contents with line numbers (blank lines excluded)\ncat -b &lt;file&gt;\n\n tldr:cat\n# cat\n# Print and concatenate files.\n# More information: &lt;https://www.gnu.org/software/coreutils/cat&gt;.\n\n# Print the contents of a file to the standard output:\ncat path/to/file\n\n# Concatenate several files into an output file:\ncat path/to/file1 path/to/file2 ... &gt; path/to/output_file\n\n# Append several files to an output file:\ncat path/to/file1 path/to/file2 ... &gt;&gt; path/to/output_file\n\n# Copy the contents of a file into an output file without buffering:\ncat -u /dev/tty12 &gt; /dev/tty13\n\n# Write `stdin` to a file:\ncat - &gt; path/to/file\n</code></pre> <p>You may say, why do you need this when you already have the <code>man</code> command in Linux? The most important feature that makes <code>cht.sh</code> different is that it explains each option in the simplest and most simple way. It does not require installation.</p> <p>Here are a few more examples :</p> <pre><code>curl cht.sh/tail\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/tail\n cheat:tail\n# To show the last 10 lines of &lt;file&gt;:\ntail &lt;file&gt;\n\n# To show the last &lt;number&gt; lines of &lt;file&gt;:\ntail -n &lt;number&gt; &lt;file&gt;\n\n# To show the last lines of &lt;file&gt; starting with &lt;number&gt;:\ntail -n +&lt;number&gt; &lt;file&gt;\n\n# To show the last &lt;number&gt; bytes of &lt;file&gt;:\ntail -c &lt;number&gt; &lt;file&gt;\n\n# To show the last 10 lines of &lt;file&gt; and to wait for &lt;file&gt; to grow:\ntail -f &lt;file&gt;\n\n tldr:tail\n# tail\n# Display the last part of a file.\n# See also: `head`.\n# More information: &lt;https://www.gnu.org/software/coreutils/tail&gt;.\n\n# Show last 'count' lines in file:\ntail --lines count path/to/file\n\n# Print a file from a specific line number:\ntail --lines +count path/to/file\n\n# Print a specific count of bytes from the end of a given file:\ntail --bytes count path/to/file\n\n# Print the last lines of a given file and keep reading file until `Ctrl + C`:\ntail --follow path/to/file\n\n# Keep reading file until `Ctrl + C`, even if the file is inaccessible:\ntail --retry --follow path/to/file\n\n# Show last 'num' lines in 'file' and refresh every 'n' seconds:\ntail --lines count --sleep-interval seconds --follow path/to/file\n</code></pre> <pre><code>curl cht.sh/touch\n</code></pre> <pre><code>vagrant@ubuntu-focal:~$ curl cht.sh/touch\n cheat:touch\n# To change a file's modification time:\ntouch -d &lt;time&gt; &lt;file&gt;\ntouch -d 12am &lt;file&gt;\ntouch -d \"yesterday 6am\" &lt;file&gt;\ntouch -d \"2 days ago 10:00\" &lt;file&gt;\ntouch -d \"tomorrow 04:00\" &lt;file&gt;\n\n# To put the timestamp of a file on another:\ntouch -r &lt;refrence-file&gt; &lt;target-file&gt;\n\n tldr:touch\n# touch\n# Create files and set access/modification times.\n# More information: &lt;https://manned.org/man/freebsd-13.1/touch&gt;.\n\n# Create specific files:\ntouch path/to/file1 path/to/file2 ...\n\n# Set the file [a]ccess or [m]odification times to the current one and don't [c]reate file if it doesn't exist:\ntouch -c -a|m path/to/file1 path/to/file2 ...\n\n# Set the file [t]ime to a specific value and don't [c]reate file if it doesn't exist:\ntouch -c -t YYYYMMDDHHMM.SS path/to/file1 path/to/file2 ...\n\n# Set the file time of a specific file to the time of anothe[r] file and don't [c]reate file if it doesn't exist:\ntouch -c -r ~/.emacs path/to/file1 path/to/file2 ...\n</code></pre>"},{"location":"devops/linux/shell/jobs-bg-fg/","title":"jobs, bg, fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs-bg-and-fg","title":"jobs, bg, and fg","text":""},{"location":"devops/linux/shell/jobs-bg-fg/#jobs","title":"jobs","text":"<p>The <code>jobs</code> command will list all jobs on the system; active, stopped, or otherwise.</p>"},{"location":"devops/linux/shell/jobs-bg-fg/#example-usage","title":"Example usage:","text":"<p>1.Create a job with using</p> <p><code>sleep 500 &amp;</code></p> <p>and stop it with <code>ctrl + z</code>. </p> <p>2.List all the jobs with the command : <code>jobs</code></p> <p>You will see that you have a single stopped job identified by the job number [1].</p> <p>Other options to know for this command include:</p> <ul> <li><code>-l</code> - list PIDs in addition to default info</li> <li><code>-n</code> - list only processes that have changed since the last notification</li> <li><code>-p</code> - list PIDs only</li> <li><code>-r</code> - show only running jobs</li> <li><code>-s</code> - show only stopped jobs</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#background","title":"Background","text":"<p>The <code>bg</code> command restarts a suspended job, and runs it in the background.</p> <p><code>bg [JOB_SPEC]</code></p> <p>Where JOB_SPEC can be one of the following:</p> <ul> <li><code>%n</code>: where <code>n</code> is the job number.</li> <li><code>%abc</code>: refers to a job started by a command beginning with <code>abc</code>.</li> <li><code>%?abc</code>: refers to a job started by a command containing <code>abc</code>.</li> <li><code>%-</code>: specifies the previous job.</li> </ul>"},{"location":"devops/linux/shell/jobs-bg-fg/#foreground","title":"Foreground","text":"<p>The <code>fg</code> command switches a job running in the background into the foreground.</p> <p><code>fg [JOB_SPEC]</code></p> <p>NOTE: If no <code>JOB_SPEC</code> is provided, <code>bg</code> and <code>fg</code> operate on the current job.</p> <p>For example, if you have two jobs running in the background, and you run the command <code>bg</code>, the job that was most recently started will be brought to the foreground.</p> <p>You can also use the <code>%</code> character to specify a job by its job number, or by a partial command name.</p>"},{"location":"devops/linux/shell/netstat/","title":"Netstat &amp; SS Command","text":"<p>You can check the listening ports and applications with netstat as follows.</p> <p>Prerequisite By default, netstat command may not be installed on your system. Hence, use the apk command on Alpine Linux, dnf command/yum command on RHEL &amp; co, apt command/apt-get command on Debian, Ubuntu &amp; co, zypper command on SUSE/OpenSUSE, pacman command on Arch Linux to install the netstat.</p> <pre><code>sudo apt update\nsudo apt install net-tools\n</code></pre> <p>Run the netstat command along with grep command to filter out port in LISTEN state:</p> <pre><code>netstat -tulpn | grep LISTEN\nnetstat -tulpn | more\n</code></pre> <p>Where netstat command options are:</p> <p><code>-t</code> : Select all TCP ports</p> <p><code>-u</code> : Select all UDP ports</p> <p><code>-l</code> : Show listening server sockets (open TCP and UDP ports in listing state)</p> <p><code>-p</code> : Display PID/Program name for sockets. In other words, this option tells who opened the TCP or UDP port. For example, on my system, Nginx opened TCP port 80/443, so I will /usr/sbin/nginx or its PID.</p> <p><code>-n</code> : Don\u2019t resolve name (avoid dns lookup, this speed up the netstat on busy Linux/Unix servers)</p> <p>The netstat command <code>deprecated</code> for some time on Linux. Therefore, you need to use the ss command as follows:</p> <pre><code>sudo ss -tulw\nsudo ss -tulwn\nsudo ss -tulwn | grep LISTEN\n</code></pre> <p><code>-t</code> : Show only TCP sockets on Linux</p> <p><code>-u</code> : Display only UDP sockets on Linux</p> <p><code>-l</code> : Show listening sockets. For example, TCP port 22 is opened by SSHD server.</p> <p><code>-p</code> : List process name that opened sockets</p> <p><code>-n</code> : Don\u2019t resolve service names i.e. don\u2019t use DNS</p>"},{"location":"devops/linux/shell/netstat/#ps-command","title":"PS Command","text":"<p>The ps command without any options displays information about processes that are bound by the controlling terminal. <pre><code>ps\n</code></pre> The command returns a similar output: <pre><code>PID TTY      TIME     CMD\n285 pts/2    00:00:00 zsh\n334 pts/2    00:00:00 ps\n</code></pre></p> <p>The default output of the ps command contains four columns that provide the following information:</p> <p><code>PID</code>: The process ID is your system\u2019s tracking number for the process. The PID is useful when you need to use a command like kill or nice, which take a PID as their input.</p> <p><code>TTY</code>: The controlling terminal associated with the process. Processes that do not originate from a controlling terminal and were initiated by the system at boot are displayed with a question mark.</p> <p><code>TIME</code>: The CPU usage of the process. Displays the amount of CPU time used by the process. This value is not the run time of the process.</p> <p><code>CMD</code>: The name of the command or executable that is running. The output only includes the name of the command or executable and does not display any options that were passed in.</p>"},{"location":"devops/linux/shell/netstat/#the-aux-shortcut","title":"The <code>aux</code> shortcut","text":"<p>Now that you understand the basics of the <code>ps</code> command, this section covers the benefits to the <code>ps</code> <code>aux</code> command. The <code>ps</code> <code>aux</code> displays the most amount of information a user usually needs to understand the current state of their system\u2019s running processes. Take a look at the following example: <pre><code>ps aux\n</code></pre> <pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0    892   572 ?        Sl   Nov28   0:00 /init\nroot       227  0.0  0.0    900    80 ?        Ss   Nov28   0:00 /init\nroot       228  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nzaphod     229  0.0  0.1 749596 31000 pts/0    Ssl+ Nov28   0:15 docker\nroot       240  0.0  0.0      0     0 ?        Z    Nov28   0:00 [init] &lt;defunct&gt;\nroot       247  0.0  0.0    900    88 ?        S    Nov28   0:00 /init\nroot       248  0.0  0.1 1758276 31408 pts/1   Ssl+ Nov28   0:10 /mnt/wsl/docker-desktop/docker-desktop-proxy\nroot       283  0.0  0.0    892    80 ?        Ss   Dec01   0:00 /init\nroot       284  0.0  0.0    892    80 ?        R    Dec01   0:00 /init\nzaphod     285  0.0  0.0  11964  5764 pts/2    Ss   Dec01   0:00 -zsh\nzaphod     343  0.0  0.0  23764  9836 pts/2    T    17:44   0:00 vi foo\nroot       349  0.0  0.0    892    80 ?        Ss   17:45   0:00 /init\nroot       350  0.0  0.0    892    80 ?        S    17:45   0:00 /init\nzaphod     351  0.0  0.0  11964  5764 pts/3    Ss+  17:45   0:00 -zsh\nzaphod     601  0.0  0.0  10612  3236 pts/2    R+   18:24   0:00 ps aux\n</code></pre></p> <p>The <code>ps aux</code> command displays more useful information than other similar options. For example, the <code>UID</code> column is replaced with a human-readable <code>username</code> column. <code>ps aux</code> also displays statistics about your Linux system, like the percent of CPU and memory that the process is using. The <code>VSZ</code> column displays amount of virtual memory being consumed by the process. <code>RSS</code> is the actual physical wired-in memory that is being used. The <code>START</code> column shows the date or time for when the process was started. This is different from the CPU time reported by the <code>TIME</code> column.</p>"},{"location":"devops/linux/shell/nmap/","title":"NMap Command","text":"<p>Nmap (an acronym of Network Mapper) is an open-source command-line utility to securely manage the network. Nmap command has an extensive list of options to deal with security auditing and network exploration.</p> <p>Prerequisites To use the Nmap utility, the Nmap must be installed on your Ubuntu 22.04. Nmap is available on the official repository of Ubuntu 22.04. Before installation, it is a better practice to update the core libraries of Ubuntu 22.04 as follows:</p> <p><pre><code>sudo apt update\nsudo apt install nmap\n</code></pre> or <pre><code>sudo apt update\nsnap install nmap\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#syntax-of-nmap-command","title":"Syntax of Nmap command","text":"<p>The syntax of the Nmap command is given below: <pre><code>nmap [options] [IP-adress or web-address]\n</code></pre> The Nmap command can be used to scan through the open ports of the host. For instance, the following command will scan the \u201cxxx.xxx.xxx\u201d for open ports..</p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-scan-specific-ports","title":"How to use the Nmap command to scan specific port(s)","text":"<p>By default, the Nmap scans through only 1000 most used ports (these are not consecutive but important). However, there are a total of 65535 ports. The Nmap command can be used to scan a specific port or all the ports.</p> <p>To scan all ports: The -p- flag of the Nmap command helps to scan through all 65535 ports: <pre><code>nmap -p- 192.168.214.138\n</code></pre></p> <p>To scan a <code>specific port</code>: One can specify the port number as well. For instance, the following command will scan for port 88 only: <pre><code>nmap -p 88 88 192.168.214.138\n</code></pre></p>"},{"location":"devops/linux/shell/nmap/#how-to-use-the-nmap-command-to-get-the-os-information","title":"How to use the Nmap command to get the OS information","text":"<p>The Nmap command can be used to get the <code>Operating System\u2019s information</code>. For instance, the following command will get the information of the OS associated with the IP address.</p> <pre><code>sudo nmap -O 192.168.214.138\n</code></pre>"},{"location":"devops/linux/shell/nslookup/","title":"NSLOOKUP","text":""},{"location":"devops/linux/shell/nslookup/#what-is-the-nslookup","title":"What is the 'nslookup'","text":""},{"location":"devops/linux/shell/nslookup/#nslookup-stands-for-name-server-lookup-is-a-useful-command-for-getting-information-from-the-dns-server-it-is-a-network-administration-tool-for-querying-the-domain-name-system-dns-to-obtain-domain-name-or-ip-address-mapping-or-any-other-specific-dns-record-it-is-also-used-to-troubleshoot-dns-related-problems","title":"Nslookup (stands for \u201cName Server Lookup\u201d) is a useful command for getting information from the DNS server. It is a network administration tool for querying the Domain Name System (DNS) to obtain domain name or IP address mapping or any other specific DNS record. It is also used to troubleshoot DNS-related problems.","text":"<p>Syntax of the -<code>nslookup</code>- command in Linux System <pre><code>nslookup [option] [hosts]\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#options-of-nslookup-command","title":"Options of nslookup command:","text":"Options Description -domain=[domain-name] <code>allows you to change the default DNS name.</code> -debug <code>enables the display of debugging information.</code> -port=[port-number] <code>Use the -port option to specify the port number for queries. By default, nslookup uses port 53 for DNS queries</code> -timeout=[seconds] <code>you can specify the time allowed for the DNS server to respond. By default, the timeout is set to a few seconds</code> -type=a <code>Lookup for a record We can also view all the available DNS records for a particular record using the -type=a option</code> -type=any <code>Lookup for any record We can also view all the available DNS records using the -type=any option.</code> -type=hinfo <code>displays hardware-related information about the host. It provides details about the operating system and hardware platform</code> -type=mx <code>Lookup for an mx record MX (Mail Exchange) maps a domain name to a list of mail exchange servers for that domain. The MX record says that all the mails sent to \u201cgoogle.com\u201d should be routed to the Mail server in that domain.</code> -type=ns <code>Lookup for an ns record NS (Name Server) record maps a domain name to a list of DNS servers authoritative for that domain. It will output the name serves which are associated with the given domain.</code> -type=ptr <code>used in reverse DNS lookups. It retrieves the Pointer (PTR) records, which map IP addresses to domain names.</code> -type=soa <code>Lookup for a soa record SOA record (start of authority), provides the authoritative information about the domain, the e-mail address of the domain admin, the domain serial number, etc\u2026</code>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-service","title":"Examples For K8S Service","text":"<p><pre><code>kubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre> <code>kubectl exec busybox -- nslookup nginx-svc</code> <pre><code>Name:   nginx-svc.default.svc.cluster.local\nAddress: 10.100.245.19\n\nnslookup: can't resolve 'kubernetes.default'\n</code></pre></p>"},{"location":"devops/linux/shell/nslookup/#examples-for-k8s-pod","title":"Examples For K8S Pod","text":"<p><pre><code>pod-ip-address.my-namespace.pod.cluster-domain.example\n</code></pre> <code>kubectl exec busybox -- nslookup 10-244-1-2.default.pod.cluster.local</code> <pre><code>172-17-0-3.default.pod.cluster.local\n</code></pre></p>"},{"location":"devops/linux/shell/scp/","title":"SCP Command","text":"<p>SCP (secure copy) is a command-line utility that allows you to securely copy files and directories between two locations.</p> <p>From your local system to a remote system. <pre><code>scp -i \"pam.pem\" /home/kullanici/dizin/local_file.txt ubuntu@18.204.206.157:/home/ubuntu/\n</code></pre> From a remote system to your local system. <pre><code>scp -i \"pam.pem\" -r ubuntu@3.86.225.192:/home/ubuntu/  .\n</code></pre></p> <p>Attention  pam.pem is the password file of Ec2 instance</p> <p><code>scp</code> provides a number of options that control every aspect of its behavior. The most widely used options are:</p> <p><code>-P</code> - Specifies the remote host ssh port.</p> <p><code>-p</code> - Preserves files modification and access times.</p> <p><code>-q</code> - Use this option if you want to suppress the progress meter and non-error messages.</p> <p><code>-C</code> - This option forces scp to compresses the data as it is sent to the destination machine.</p> <p><code>-r</code> - This option tells scp to copy directories recursively.</p>"},{"location":"devops/linux/shell/script/","title":"<code>script</code> command","text":"<pre><code>man script\n</code></pre>"},{"location":"devops/linux/shell/script/#save-your-terminal-session","title":"Save your terminal session","text":"<pre><code>script -a -t 5 sav-my-session-name.log\n\n# do stuff\n\nexit\n# run cat sav-my-session-name.log to see the output\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/","title":"ELK Stack with FileBeat","text":"<p>The ELK Stack (Elasticsearch, Logstash, and Kibana) is the world\u2019s most popular open-source log analysis platform. ELK is quickly overtaking existing proprietary solutions and becoming companies\u2019 top choice for log analysis and management solutions. There is one more component \u2014 Beats \u2014 which collects the data and sends it to Logstash. This led Elastic to rename ELK as the Elastic Stack.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is a NoSQL database. It is based on Lucene search engine, and it is built with RESTful APIS. Elasticsearch offers simple deployment, maximum reliability, and easy management. It also offers advanced queries to perform detail analysis and stores all the data centrally. It is helpful for executing a quick search of the documents. Elasticsearch also allows you to store, search and analyze big volume of data. Modern web and mobile applications have adopted it in search engine platforms.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#why-use-elasticsearch","title":"Why use Elasticsearch?","text":"<ul> <li> <p>Search: The main advantage of using Elasticsearch is it\u2019s rapid and accurate search functionality. For large datasets, relational databases takes a lot more time for search queries because of the number of joins the query has to go through.</p> </li> <li> <p>Scaling: Distributed architecture of Elasticsearch allows you to scale a lot of servers and data. We can scale the clusters to hundreds of nodes and also we can replicate data to prevent data loss in case of a node failure.</p> </li> <li> <p>Analytical engine: Elasticsearch analytical use case has become more popular than the search use case. Elasticsearch is specifically used for log analysis</p> </li> </ul>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#logstash","title":"Logstash","text":"<p>Logstash is a  open-source powerful tool for obtaining, filtering, and normalizing log files. A wide range of plugins for input, output and filtering specifications gives the user a great opportunity to easily configure Logstash to collect, process and channel logs data in many different architectures.</p> <p>Working with log files is divided into one or more pipelines. In each configured pipeline, one or more input plugins retrieve or gather data that is then placed on an internal queue. Process handling threads read queued data in small data series and process these batches via specified filter plugins in sequence.</p> <p></p> <p>After finishing data processing, threads send the data to the related output plugins which in turn are responsible for formatting and sending data to Elasticsearch or any other corresponding engine.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#filebeat","title":"FileBeat","text":"<p>Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.</p> <p>How Filebeat works: When you start Filebeat, it starts one or more inputs that look in the locations you\u2019ve specified for log data. For each log that Filebeat locates, Filebeat starts a harvester. Each harvester reads a single log for new content and sends the new log data to libbeat, which aggregates the events and sends the aggregated data to the output that you\u2019ve configured for Filebeat.</p> <p></p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#kibana","title":"Kibana","text":"<p>Kibana is a data visualization which completes the ELK stack. This tool is used for visualizing the Elasticsearch documents and helps developers to have a quick insight into it. Kibana dashboard offers various interactive diagrams, geospatial data, and graphs to visualize complex queries.</p> <p>It is used to search, view, and interact with data stored in Elasticsearch directories and helps you to perform advanced data analysis and visualize your data in a variety of tables, charts, and maps.</p>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#demo","title":"Demo","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#create-aks-cluster","title":"Create AKS Cluster","text":"<p>To create an AKS cluster, use the az aks create command. The following example creates a cluster named myAKSCluster with one node and enables a system-assigned managed identity.</p> <pre><code>az group create -l westus -n MyResourceGroup\n</code></pre> <pre><code>az aks create --resource-group myResourceGroup --name myAKSCluster --enable-managed-identity --node-count 3 -l westus\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#connect-to-the-cluster","title":"Connect to the cluster","text":"<p>Configure kubectl to connect to your Kubernetes cluster using the az aks get-credentials command. This command downloads credentials and configures the Kubernetes CLI to use them.</p> <pre><code>az aks get-credentials --resource-group myResourceGroup --name myAKSCluster --overwrite-existing\n</code></pre> <p>Verify the connection to your cluster using the kubectl get command. This command returns a list of the cluster nodes.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#install-elk-stack-in-aks-cluster-using-helm","title":"Install ELK stack in AKS cluster using Helm:","text":"<ul> <li>Create  namespace with the name of elk.</li> </ul> <p><pre><code>kubectl create ns elk\nkubectl get ns\n</code></pre> - Install ELK Stack helm repo into your local repo with helm command.</p> <p><pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> - Update your repo after installation.</p> <p><pre><code>helm repo update\n</code></pre> - Liste your repo packages.</p> <pre><code>helm repo ls\n</code></pre> <ul> <li>List your helm chart and manifest files.</li> </ul> <pre><code>helm search repo elastic\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#elk-stack-elasticsearch-filebeat-logstashkibana-installation-and-configuration-in-aks-cluster-with-helm","title":"ELK Stack (Elasticsearch, Filebeat, Logstash,Kibana) installation and configuration in AKS cluster with Helm:","text":"<ul> <li>Save your elastic/elasticsearch values and save it as elasticsearch.values file in order to make some configuration.</li> </ul> <p><pre><code>helm show values elastic/elasticsearch &gt;&gt; elasticsearch.values\n</code></pre> elasticsearch.values</p> <pre><code>---\nclusterName: \"elasticsearch\"\nnodeGroup: \"master\"\n\nroles:\n  - master\n  - data\n  - data_content\n  - data_hot\n  - data_warm\n  - data_cold\n  - ingest\n  - ml\n  - remote_cluster_client\n  - transform\n\nreplicas: 2\nminimumMasterNodes: 2\nimage: \"docker.elastic.co/elasticsearch/elasticsearch\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\nnetworkHost: \"0.0.0.0\"\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 30Gi\n\npersistence:\n  enabled: true\n  labels:\n    enabled: false\n  annotations: {}\n\nenableServiceLinks: true\n\nprotocol: https\nhttpPort: 9200\ntransportPort: 9300\n\nservice:\n  enabled: true\n  labels: {}\n  labelsHeadless: {}\n  type: ClusterIP\n  publishNotReadyAddresses: false\n  nodePort: \"\"\n  annotations: {}\n  httpPortName: http\n  transportPortName: transport\n  loadBalancerIP: \"\"\n  loadBalancerSourceRanges: []\n  externalTrafficPolicy: \"\"\n\nmaxUnavailable: 1\nsysctlVmMaxMapCount: 262144\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n\n\ntests:\n  enabled: true\n</code></pre> <pre><code>helm install elasticsearch elastic/elasticsearch -f elasticsearch.values -n elk\n</code></pre> <pre><code>helm ls -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-kibana-via-helm-into-aks-cluster","title":"Installation and configuration of Kibana via Helm into AKS Cluster:","text":"<p><pre><code>helm show values elastic/kibana &gt;&gt; kibana.values\n</code></pre> - Change the values configuration with LoadBalancer.</p> <pre><code>---\nelasticsearchHosts: \"https://elasticsearch-master:9200\"\n\nreplicas: 1\n\n\nimage: \"docker.elastic.co/kibana/kibana\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"2Gi\"\n\nprotocol: http\n\nserverHost: \"0.0.0.0\"\n\nhealthCheckPath: \"/app/kibana\"\n\nautomountToken: true\n\nhttpPort: 5601\n\nservice:\n  type: LoadBalancer\n  loadBalancerIP: \"\"\n  port: 5601\n  nodePort: \"\"\n  labels: {}\n  annotations: {}\n  loadBalancerSourceRanges: []\n  httpPortName: http\n\nreadinessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 3\n  timeoutSeconds: 5\n</code></pre> <pre><code>helm install kibana elastic/kibana -f kibana.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-logstash-via-helm-into-aks-cluster","title":"Installation and configuration of logstash via Helm into AKS Cluster:","text":"<ul> <li>Installation of logstash via Helm.</li> </ul> <pre><code>helm show values elastic/logstash &gt;&gt; logstash.values\n</code></pre> <ul> <li>logstash.values ---&gt; !!! update elasticsearch password you can access alestichsearcg password this command: </li> </ul> <pre><code>kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode\n</code></pre> <pre><code>---\nreplicas: 1\n\nlogstashConfig:\n  logstash.yml: |\n    http.host: 0.0.0.0\n    # xpack.monitoring.enabled: false\n\n\nlogstashPipeline:\n  logstash.conf: |\n    input {\n      beats {\n        port =&gt; 5044\n      }\n    }\n    output {\n      elasticsearch {\n        hosts =&gt; [ \"https://elasticsearch-master:9200\" ]\n        ssl =&gt; true\n        manage_template =&gt; false\n        ssl_certificate_verification =&gt; true\n        index =&gt; \"logstash-%{+YYYY.MM.dd}\"\n        document_type =&gt; \"%{[@metadata][type]}\"\n        cacert =&gt; \"/usr/share/logstash/certs/ca.crt\"\n        user =&gt; \"${ELASTICSEARCH_USERNAME}\"\n        password =&gt; \"${ELASTICSEARCH_PASSWORD}\"  #update password\n      }\n    }\n\n\nimage: \"docker.elastic.co/logstash/logstash\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\n\nextraEnvs:\n  - name: \"ELASTICSEARCH_USERNAME\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: username\n  - name: \"ELASTICSEARCH_PASSWORD\"\n    valueFrom:\n      secretKeyRef:\n        name: elasticsearch-master-credentials\n        key: password\n\n\nsecretMounts:\n  - name: elasticsearch-master-certs\n    secretName: elasticsearch-master-certs\n    path: /usr/share/logstash/certs/\n\n\nlogstashJavaOpts: \"-Xmx1g -Xms1g\"\n\nresources:\n  requests:\n    cpu: \"100m\"\n    memory: \"1536Mi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"1536Mi\"\n\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n\n\nhttpPort: 9600\n\nmaxUnavailable: 1\n\nlivenessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 300\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n\nreadinessProbe:\n  httpGet:\n    path: /\n    port: http\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 3\n\nservice:\n  annotations: {}\n  type: ClusterIP\n  loadBalancerIP: \"\"\n  ports:\n    - name: beats\n      port: 5044\n      protocol: TCP\n      targetPort: 5044\n    - name: http\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n</code></pre> <pre><code>helm install logstash elastic/logstash -f logstash.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#installation-and-configuration-of-filebeat-via-helm-into-aks-cluster","title":"Installation and configuration of Filebeat via Helm into AKS Cluster:","text":"<ul> <li>Installation of Filebeat via Helm.</li> </ul> <pre><code>helm show values elastic/filebeat &gt;&gt; filebeat.values\n</code></pre> <ul> <li>filebeat.values</li> </ul> <pre><code>---\ndaemonset:\n  enabled: true\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n      - type: container\n        paths:\n          - /var/log/containers/*.log\n        processors:\n        - add_kubernetes_metadata:\n            host: ${NODE_NAME}\n            matchers:\n            - logs_path:\n                logs_path: \"/var/log/containers/\"\n\n      output.logstash:\n        hosts: [\"logstash-logstash:5044\"]\n\n  maxUnavailable: 1\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n\ndeployment:\n\n  enabled: false\n  extraEnvs:\n    - name: \"ELASTICSEARCH_USERNAME\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: username\n    - name: \"ELASTICSEARCH_PASSWORD\"\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n\n  filebeatConfig:\n    filebeat.yml: |\n      filebeat.inputs:\n        - type: log\n          paths:\n            - /usr/share/filebeat/logs/filebeat\n\n      output.elasticsearch:\n        host: \"${NODE_NAME}\"\n        hosts: '[\"https://${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}\"]'\n        username: \"${ELASTICSEARCH_USERNAME}\"\n        password: \"${ELASTICSEARCH_PASSWORD}\"\n        protocol: https\n        ssl.certificate_authorities: [\"/usr/share/filebeat/certs/ca.crt\"]\n\n\n  secretMounts:\n    - name: elasticsearch-master-certs\n      secretName: elasticsearch-master-certs\n      path: /usr/share/filebeat/certs/\n\n  securityContext:\n    runAsUser: 0\n    privileged: false\n  resources:\n    requests:\n      cpu: \"100m\"\n      memory: \"100Mi\"\n    limits:\n      cpu: \"1000m\"\n      memory: \"200Mi\"\n\nreplicas: 1\n\nhostPathRoot: /var/lib\n\nimage: \"docker.elastic.co/beats/filebeat\"\nimageTag: \"8.5.1\"\nimagePullPolicy: \"IfNotPresent\"\nimagePullSecrets: []\n\nlivenessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        curl --fail 127.0.0.1:5066\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nreadinessProbe:\n  exec:\n    command:\n      - sh\n      - -c\n      - |\n        #!/usr/bin/env bash -e\n        filebeat test output\n  failureThreshold: 3\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n\nmanagedServiceAccount: true\n\nclusterRoleRules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n      - nodes\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"apps\"\n    resources:\n      - replicasets\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre> <pre><code>helm install filebeat elastic/filebeat -f filebeat.values -n elk\n</code></pre> <pre><code>kubectl get all -n elk\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#deployment-of-sample-applications-into-aks-kubernetes-environment","title":"Deployment of sample applications into AKS kubernetes environment:","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: db-pv-vol\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/home/ubuntu/pv-data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: database-persistent-volume-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: manual\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mongo\n  template:\n    metadata:\n      labels:\n        name: mongo\n        app: todoapp\n    spec:\n      containers:\n      - image: mongo:5.0\n        name: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n          - name: mongo-storage\n            mountPath: /data/db\n      volumes:\n        #- name: mongo-storage\n        #  hostPath:\n        #    path: /home/ubuntu/pv-data\n        - name: mongo-storage\n          persistentVolumeClaim:\n            claimName: database-persistent-volume-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: db-service\n  labels:\n    name: mongo\n    app: todoapp\nspec:\n  selector:\n    name: mongo\n  type: ClusterIP\n  ports:\n    - name: db\n      port: 27017\n      targetPort: 27017\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: web\n  template:\n    metadata:\n      labels:\n        name: web\n        app: todoapp\n    spec:\n      containers: \n        - image: ersinsari/todo\n          imagePullPolicy: Always\n          name: myweb\n          ports: \n            - containerPort: 3000\n          env:\n            - name: \"DBHOST\"\n              value: db-service\n          resources:\n            limits:\n              memory: 500Mi\n              cpu: 100m\n            requests:\n              memory: 250Mi\n              cpu: 80m  \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\n  labels:\n    name: web\n    app: todoapp\nspec:\n  selector:\n    name: web \n  type: LoadBalancer\n  ports:\n   - name: http\n     port: 3000\n     targetPort: 3000\n     protocol: TCP\n</code></pre> <pre><code>kubectl apply -f to_do.yaml\n</code></pre> <pre><code>kubectl get all\n</code></pre>"},{"location":"devops/logging/ELK-stack-with-FileBeat/#part-5-kibana-dashboard-configuration-and-sample-app-log-monitoring","title":"Part-5 Kibana Dashboard configuration and sample app log monitoring:","text":""},{"location":"devops/logging/ELK-stack-with-FileBeat/#dashboard-configuration-and-index-pattern-creation","title":"Dashboard configuration and index pattern creation:","text":"<ul> <li>Go to http://loadbalancer-ip:5601</li> </ul> <p>username: elastic password: $(kubectl get secrets -n elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 --decode)</p> <ul> <li> <p>Discover: create an index pattern</p> </li> <li> <p>logstash-*</p> </li> <li> <p>select @timestamp</p> </li> <li> <p>create an index pattern.</p> </li> <li> <p>filter your data using KQL syntax : kubernetes.deployment.name web-deployment</p> </li> <li>You can explore logs info</li> </ul>"},{"location":"devops/logging/Index-Lifecycle-Management/","title":"Elasticsearch Index Lifecycle Management","text":""},{"location":"devops/logging/Index-Lifecycle-Management/#indexing","title":"Indexing:","text":"<p>Indices based on the indexname defined on Logstash need to be created to be visible on Kibana. To do this, the pattern specified in the left menu is written and created by clicking on the Create Index Pattern section under Stack management. </p> <p></p>"},{"location":"devops/logging/Index-Lifecycle-Management/#create-repository","title":"Create Repository:","text":"<p>However, a repo must be created and registered to keep snapshots.</p> <p>/Stack management/Snapshot and Restore/Repositories</p> <p>Here repo elasticsearch can be on a separate server as it could be on the built-in server. Importantly, this directory should be specified in the elasticsearch configuration file (elasticSearch.yaml, values.jaml). If it will be on another server, it should be mounted.</p> <p></p> <p>For example, commands executed on the Elasticsearch server for a separate server to be used as an NFS server:</p> <pre><code>sudo apt install nfs-common\n\nsudo apt install cifs-utils\n\nsudo mount.nfs &lt;path on nfs server&gt; &lt;path on elasticserach server&gt;\n\nsudo  mount.nfs :/mnt/disk2/elasticmount /mnt/elasticmount \n\nchown -R elasticsearch:elasticsearch elasticmount \n</code></pre>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-template","title":"Index Template:","text":"<p>A template should be created to manage the created indexes and define a lifecycle policy.</p> <p>/Stack management/ Index Management/Index Templates</p> <p>Here, click create template to create a template that belongs to a particular pattern.</p> <p></p> <p>Under Index settings section:</p> <p>Add:</p> <p>{ \"index\": {\"lifecycle\": { \"name\": \"kubernetes-pod-policy\" } } }</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#index-lifecycle-policy","title":"Index Lifecycle Policy:","text":"<p>A policy is created for what to do with the indexes of the specified pattern. </p> <p>For this;</p> <p>Click /Stack management/ Index Lifecycle Policies</p> <p>Here a new policy is created with create Policy.</p> <p></p> <p>This section specifies how long it will last in which phase and what to do during that time. </p> <p>For example, the policy of given pod-logs is to remove indexes with a one-hour lifetime in the warm phase after the replica numbers are drawn to 0 (to avoid holding space), and the policy is to delete indexes that have a seven-day lifecycle in the delete phase when the snapshot policy is applied.</p>"},{"location":"devops/logging/Index-Lifecycle-Management/#snapshot-policy","title":"Snapshot Policy:","text":"<p>It can be deleted by taking a snapshot at certain intervals so that the specified indexes do not hold their place. If necessary, it can be restored from these snapshots. </p> <p>A policy is defined for taking these snapshots.</p> <p>/Stack management/Snapshot and Restore/Policies</p> <p></p> <p>For example, when creating a kubernetes-pod-daily-snapshot policy in the form;</p> <ul> <li> <p>The snapshot to be taken is created on a day-based basis, defined as , <li> <p>specified in which repository the snapshot to be taken will be held, defined by the schedule of the time of the day,</p> </li> <li> <p>specified which pattern index is to be taken,</p> </li> <li> <p>The validity period of this snapshot is specified (expiration - after which time deletion permission is given),</p> </li> <li> <p>This policy specifies the number of snapshots to hold min and max.</p> </li>"},{"location":"devops/logging/Index-Lifecycle-Management/#restore-snapshots","title":"Restore snapshots:","text":"<p>To restore snapshots taken on a specific date</p> <p>Stack management/Snapshot and Restore/snapshot</p> <p></p> <p>Here you click the snapshot of the day. The restore button will be clicked on the screen that opens. </p> <p>Here the snapshots will belong to more than a day. However, it should not be forgotten that it is taken incrementally.</p> <p>To restore a day's snapshot, untick Data streams and indices and click deselect all below. The restore is then done by clicking on the index of the desired day.</p>"},{"location":"devops/logging/elasticsearch-exporter/","title":"Elasticsearch-Exporter","text":"<p>https://github.com/prometheus-community/elasticsearch_exporter</p> <p>The Elasticsearch Exporter is a tool that may be utilized to check the performance and health of Elasticsearch. This application gathers metrics and information from Elasticsearch and makes them available to Prometheus, a widely used open-source monitoring system. The Elasticsearch Exporter enables you to monitor a range of metrics, including cluster, node, and index-level information. These metrics encompass CPU utilization, memory usage, indexing rate, search rate, and other relevant data.</p>"},{"location":"devops/logging/elasticsearch-exporter/#step-by-step-guide-to-configure-elasticsearch-exporter","title":"Step-by-step guide to configure Elasticsearch Exporter","text":""},{"location":"devops/logging/elasticsearch-exporter/#create-a-system-user-for-elasticsearch","title":"Create a system user for Elasticsearch","text":"<pre><code>sudo useradd elastic_search\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#download-elasticsearch-exporter","title":"Download Elasticsearch Exporter","text":"<pre><code>sudo wget https://github.com/prometheus-community/elasticsearch_exporter/releases/download/v1.7.0/elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#extract-the-targz-file","title":"Extract the tar.gz file","text":"<pre><code>sudo tar -xvzf elasticsearch_exporter-1.7.0.linux-amd64.tar.gz\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#move-to-the-extracted-directory","title":"Move to the extracted directory","text":"<pre><code>cd elasticsearch_exporter-1.7.0.linux-amd64/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#copy-the-exporter-binary-to-usrlocalbin","title":"Copy the exporter binary to /usr/local/bin/","text":"<pre><code>sudo cp elasticsearch_exporter /usr/local/bin/\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#change-ownership-of-the-exporter-binary","title":"Change ownership of the exporter binary","text":"<pre><code>sudo chown elastic_search:elastic_search /usr/local/bin/elasticsearch_exporter\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#create-a-systemd-service-file","title":"Create a systemd service file","text":"<p>Copy the /etc/elasticsearch/certs/http_ca.crt file to the /home/elastic_search directory and set the necessary permissions with the chown elastic_search:elasticsearch /home/elastic_search/http_ca.crt command.</p> <p><pre><code>sudo vim /etc/systemd/system/elasticsearch_exporter.service\n-----------------------------------------------------------\n[Unit]\nDescription=Prometheus ES_exporter\nAfter=local-fs.target network-online.target network.target\nWants=local-fs.target network-online.target network.target\n[Service]\nUser=elastic_search\nNice=10\nExecStart=/usr/local/bin/elasticsearch_exporter --es.uri=https://elastic:password@localhost:9200 --es.ca /home/elastic_search/http_ca.crt  --es.all --es.indices --es.timeout 20s\nExecStop=/usr/bin/killall elasticsearch_exporter\n[Install]\nWantedBy=default.target\n</code></pre> ! Note: Update your user and password for elasticsearch</p>"},{"location":"devops/logging/elasticsearch-exporter/#start-the-elasticsearch-exporter-service-and-enable-the-service-to-start-on-boot","title":"Start the Elasticsearch Exporter service and enable the service to start on boot","text":"<p><pre><code>sudo systemctl start elasticsearch_exporter.service\nsudo systemctl enable elasticsearch_exporter.service\nsudo systemctl status elasticsearch_exporter.service\n</code></pre> !Note: Elastic search exporter uses port 9114, therefore expose it within the VPC in the security group.</p>"},{"location":"devops/logging/elasticsearch-exporter/#configure-prometheusyml","title":"Configure Prometheus.yml","text":"<p>To pull data from the elasticsearch-exporter into Prometheus and Grafana, you should update the prometheus.yml file with the following code, replacing the Elasticsearch node IP addresses as needed</p> <pre><code>    additionalScrapeConfigs:\n    - job_name: 'elasticsearch-exporter'\n      static_configs:\n        - targets: ['&lt;elasticsearch-node-ip-1&gt;:9114', '&lt;elasticsearch-node-ip-2&gt;:9114', '&lt;elasticsearch-node-ip-3&gt;:9114']\n</code></pre>"},{"location":"devops/logging/elasticsearch-exporter/#add-grafana-dashboard-for-elasticsearch","title":"Add Grafana Dashboard for Elasticsearch","text":"<p>Go to Grafana WebUI Click Dashboard --&gt; New --&gt; New Dashboard --&gt; Import --&gt; Add 266 --&gt; load</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"devops/logging/elasticsearch-exporter/#add-alert","title":"Add alert","text":"<p>You can use Alertmanager or Grafana Alert section.</p> <p>elasticsearch.rules.yml</p> <pre><code>groups:\n  - name: elasticsearch\n    rules:\n      - record: elasticsearch_filesystem_data_used_percent\n        expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)\n          / elasticsearch_filesystem_data_size_bytes\n      - record: elasticsearch_filesystem_data_free_percent\n        expr: 100 - elasticsearch_filesystem_data_used_percent\n      - alert: ElasticsearchTooFewNodesRunning\n        expr: elasticsearch_cluster_health_number_of_nodes &lt; 3\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          description: There are only {{$value}} &lt; 3 Elasticsearch nodes running\n          summary: Elasticsearch running on less than 3 nodes\n      - alert: ElasticsearchHeapTooHigh\n        expr: elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n          &gt; 0.9\n        for: 15m\n        labels:\n          severity: critical\n        annotations:\n          description: The heap usage is over 90% for 15m\n          summary: Elasticsearch node {{$labels.node}} heap usage is high\n</code></pre> <p>elasticsearch.rules</p> <pre><code># calculate filesystem used and free percent\nelasticsearch_filesystem_data_used_percent = 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes\nelasticsearch_filesystem_data_free_percent = 100 - elasticsearch_filesystem_data_used_percent\n\n# alert if too few nodes are running\nALERT ElasticsearchTooFewNodesRunning\n  IF elasticsearch_cluster_health_number_of_nodes &lt; 3\n  FOR 5m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"There are only {{$value}} &lt; 3 Elasticsearch nodes running\", summary=\"Elasticsearch running on less than 3 nodes\"}\n\n# alert if heap usage is over 90%\nALERT ElasticsearchHeapTooHigh\n  IF elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} &gt; 0.9\n  FOR 15m\n  LABELS {severity=\"critical\"}\n  ANNOTATIONS {description=\"The heap usage is over 90% for 15m\", summary=\"Elasticsearch node {{$labels.node}} heap usage is high\"}\n</code></pre> <p>You can add slack,email or another connector for alert mechanism.</p>"},{"location":"devops/logging/loki/","title":"Install Loki,Promtail,Grafana","text":""},{"location":"devops/logging/loki/#what-is-loki","title":"What is Loki?","text":"<p>Loki is a log aggregation system developed by Grafana Labs, designed specifically for storing and querying logs. Unlike traditional logging solutions, Loki is optimized for a \"cost-effective\" and \"lightweight\" approach by only indexing metadata (like labels) and not the full log content, making it more efficient and affordable to operate.</p>"},{"location":"devops/logging/loki/#what-is-promtail","title":"What is Promtail","text":"<p>Promtail is an agent that collects logs from various sources and sends them to Loki for storage and querying. It\u2019s part of the Grafana Loki logging stack, designed to simplify log collection and forwarding.</p>"},{"location":"devops/logging/loki/#loki-stack-helm-chart","title":"Loki-Stack Helm Chart","text":"<p>The Loki Stack Helm chart is a pre-configured set of resources that deploys the full Loki logging stack in Kubernetes. This Helm chart simplifies the deployment and management of Loki, Promtail, and other optional components like Grafana, making it easier to set up a complete logging solution within a Kubernetes cluster</p>"},{"location":"devops/logging/loki/#what-are-we-going-to-need","title":"What are we going to need?","text":"<p>Kubernetes cluster. Grafana installation. Grafana Loki installation. Promtail agent on every node of the Kubernetes cluster.</p>"},{"location":"devops/logging/loki/#create-kubernetes-cluster","title":"Create Kubernetes Cluster","text":"<p>Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node.</p> <ul> <li>First, install Docker Desktop on your Mac. The easiest way to do so is to get the .dmg file from Docker\u2019s website.</li> </ul> <pre><code>brew install minikube kubectl\nminikube start\n</code></pre>"},{"location":"devops/logging/loki/#loki-stack-helm-chart_1","title":"Loki-Stack Helm Chart","text":"<ul> <li>Add Repo</li> </ul> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>We can check all repo </li> </ul> <p><pre><code>helm search repo loki\n</code></pre> </p> <ul> <li>We will use grafana/loki-stack helm chart so lets get values.yaml</li> </ul> <pre><code>helm show values grafana/loki-stack &gt; loki-values.yaml\n</code></pre> <ul> <li>Customize values.yaml for our scenario.</li> </ul> <pre><code>loki:\n  enabled: true\n  isDefault: true\n  url: http://{{(include \"loki.serviceName\" .)}}:{{ .Values.loki.service.port }}\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  datasource:\n    jsonData: \"{}\"\n    uid: \"\"\n\n\npromtail:\n  enabled: true\n  config:\n    logLevel: info\n    serverPort: 3101\n    clients:\n      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push\n\n\ngrafana:\n  enabled: true\n  sidecar:\n    datasources:\n      label: \"\"\n      labelValue: \"\"\n      enabled: true\n      maxLines: 1000\n  image:\n    tag: latest\n</code></pre> <ul> <li>Install loki stack include promtail,loki and grafana</li> </ul> <p><pre><code>helm upgrade --install loki -f loki-values.yaml -n logging --create-namespace grafana/loki-stack \n</code></pre> </p> <ul> <li>Port-forward grafana services and go to grafana U\u0130</li> </ul> <pre><code>kubectl port-forward service/loki-grafana 3000:80\n</code></pre> <ul> <li>You can get grafana admin-password below command</li> </ul> <pre><code>kubectl get secret loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre> <pre><code>username: admin\npassword: admin-password # add your password\n</code></pre> <ul> <li>Thanks to grafana/loki-stack heml-chart you can see Loki has been configured In Data sources  as shown below</li> </ul> <p></p> <ul> <li>We can check all pod logs grafana explore section.</li> </ul> <p> </p>"},{"location":"devops/logging/loki/#deploy-app-to-collect-logs","title":"Deploy App to Collect logs","text":"<ul> <li>Lets deploy a app and collect logs by using grafana-loki</li> </ul> <p>deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-generator\n  labels:\n    app: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n        - name: log-generator\n          image: busybox\n          command:\n            - /bin/sh\n            - -c\n            - |\n              while true; do\n                echo \"$(date) - Test log message from log-generator\"\n                sleep 5\n              done\n</code></pre> <pre><code>kubectl create ns app\nkubectl apply -f deployment.yaml -n app\n</code></pre> <ul> <li>Go to grafana ui and select explore section  in left-hand-menu and select log generator pod</li> </ul> <p> </p> <ul> <li>We can add labels for spesific pod or app by manipulating promtail-config</li> </ul> <pre><code>kubectl get secret \nkubectl get secret loki-promtail -o jsonpath=\"{.data.promtail\\.yaml}\" | base64 --decode &gt; promtail-config.yaml\n</code></pre> <pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}                \n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre> <ul> <li> <p>Add below code into promtail-config.yaml</p> </li> <li> <p>our logs format : <pre><code>2024-10-26 14:19:06.065 \n{\"log\":\"Sat Oct 26 11:19:06 UTC 2024 - Test log message from log-generator\\n\",\"stream\":\"stdout\",\"time\":\"2024-10-26T11:19:06.022721667Z\"}\n</code></pre></p> </li> <li> <p>We want to add 'time' and 'stream' section as a labels.</p> </li> </ul> <pre><code>      - match:\n          selector: '{app=\"log-generator\"}'\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:\n</code></pre> <ul> <li>new promtail-config.yaml as below.</li> </ul> <p><pre><code>server:\n  log_level: info\n  log_format: logfmt\n  http_listen_port: 3101\n\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\npositions:\n  filename: /run/promtail/positions.yaml\n\nscrape_configs:\n  - job_name: kubernetes-pods\n    pipeline_stages:\n      - cri: {}\n\n      - match:                                         ##first\n          selector: '{app=\"log-generator\"}'            ## app label\n          stages:\n            - json:\n                expressions:\n                  stream: stream\n                  time: time\n              labels:\n                code:\n                time:                                  ### last \n\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels:\n          - __meta_kubernetes_pod_controller_name\n        regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n        action: replace\n        target_label: __tmp_controller_name\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_name\n          - __meta_kubernetes_pod_label_app\n          - __tmp_controller_name\n          - __meta_kubernetes_pod_name\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: app\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n          - __meta_kubernetes_pod_label_instance\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: instance\n      - source_labels:\n          - __meta_kubernetes_pod_label_app_kubernetes_io_component\n          - __meta_kubernetes_pod_label_component\n        regex: ^;*([^;]+)(;.*)?$\n        action: replace\n        target_label: component\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - namespace\n        - app\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        regex: true/(.*)\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\nlimits_config:\ntracing:\n  enabled: false\n</code></pre> - We can see time and stream as a label in log </p> <p></p>"},{"location":"devops/logging/loki/#add-dashboard-grafana-to-logs","title":"Add Dashboard Grafana to logs","text":"<ul> <li>Go to grafana webui and select dashboard left-hand-menu and click new and import</li> <li>Paste the template ID  --&gt; 15141  and click load</li> <li>Select Loki as a data source</li> </ul>"},{"location":"devops/nexus/docker-hosted-repo/","title":"Docker Hosted Repository","text":""},{"location":"devops/nexus/docker-hosted-repo/#docker-hosted-repository","title":"Docker Hosted Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(hosted)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8082.</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul>"},{"location":"devops/nexus/docker-hosted-repo/#push-an-image-to-nexus","title":"Push an image to Nexus","text":"<p>Tag the image to repository url with HTTP connector port:</p> <pre><code>docker tag &lt;IMAGE&gt;:&lt;VERSION&gt; &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8082\" # hosted repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>If access to a repository requires the user to be authenticated, Docker will check for authentication access in the <code>.docker/config.json</code>file on your local machine. If authentication is not found, you will need to perform a <code>docker login</code> command.</p> <pre><code>docker login -u &lt;username&gt; &lt;EC2_PUBLIC_IP&gt;:8082\n</code></pre> <p>Then, push the image:</p> <pre><code>docker push &lt;EC2_PUBLIC_IP&gt;:8082/&lt;IMAGE&gt;:&lt;VERSION&gt;\n</code></pre>"},{"location":"devops/nexus/docker-proxy-repo/","title":"Docker Proxy Repository","text":""},{"location":"devops/nexus/docker-proxy-repo/#docker-proxy-repository","title":"Docker Proxy Repository","text":"<ul> <li>Go to repositories and click <code>Create repository</code></li> <li>Select <code>docker(proxy)</code></li> <li>Type the repository name.</li> <li>Select HTTP as repository connector on port 8083.</li> <li>Enable Docker V1 API support if required by the remote repository.</li> <li>Add remote storage URL being proxied (e.g. https://registry-1.docker.io, https://gcr.io)</li> <li>If your remote repository is docker hub, select docker index as \"Use Docker Hub\". Otherwise, select \"Use proxy registry (specified above)\"</li> <li>Finally, click <code>Create repository</code> at the bottom.</li> </ul> <p>Then, edit docker daemon to insecure connection via HTTP.</p> <pre><code>sudo vim /etc/docker/daemon.json\n</code></pre> <p>Add this and restart the docker service.</p> <pre><code>{\n  \"insecure-registries\": [\"&lt;EC2_PUBLIC_IP&gt;:8083\"  # proxy repo\n  ]\n}\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <p>When you specify <code>--restart always</code> in docker run command, the container will also always start on daemon startup, regardless of the current state of the container. If docker service is not running, after restart the service, start the container again:</p> <pre><code>docker ps -a # Get ID or name of container\ndocker start &lt;nexus-container&gt;\n</code></pre> <p>From docker cli, pull an image but don't pull it from docker hub or gcr, pull it through the HTTP endpoint of your docker proxy repo that you have created above like so:</p> <pre><code>docker pull &lt;EC2_PUBLIC_IP&gt;:8083/example-image\n</code></pre> <p>This will create a pull request to your Nexus OSS, which will proxy the request to remote repository you specified before. The image from remote repository will be cached in your Nexus and will be delivered to you.</p>"},{"location":"devops/nexus/nexus-installation/","title":"Nexus Installation - Docker Private Registry","text":"<p>This guide shows how to install Nexus to an EC2 instance and run as a container.</p>"},{"location":"devops/nexus/nexus-installation/#requirements","title":"Requirements","text":"<ul> <li>EC2 Instance</li> <li>Docker</li> </ul>"},{"location":"devops/nexus/nexus-installation/#ec2-configurations","title":"EC2 Configurations","text":"<ul> <li>Min. 4 GB memory</li> <li>Edit security group rules to allow port range from 8080 to 8082.</li> <li>Enable Auto-assign public IP</li> </ul>"},{"location":"devops/nexus/nexus-installation/#installation","title":"Installation","text":"<p>Since docker volumes are persistent, a volume can be created specifically for this purpose. This is the recommended approach.</p> <pre><code>docker volume create --name nexus-data\n</code></pre> <p>The next step is to mount the volume with docker run command. We will use port 8081 to connect Nexus. Other port(s) are used for repository connections. Repository ports must be unique.</p> <p>In this guide, we open port 8082 as docker hosted repository connection and port 8083 as docker proxy connection.</p> <pre><code>docker run -d -p 8081:8081 -p 8082:8082 -p 8083:8083 --restart always --name nexus -v nexus-data:/nexus-data sonatype/nexus3\n</code></pre> <p>Browse following URL:</p> <pre><code>http://&lt;EC2_INSTANCE_PUBLIC_IP&gt;:8081\n</code></pre> <p>Default username is admin. To see the password, run the command:</p> <pre><code>sudo cat /var/lib/docker/volumes/nexus-data/_data/admin.password\n</code></pre> <p> After login, it is mandatory to set a new password.</p>"},{"location":"devops/nexus/nexus-user-and-roles/","title":"User And Roles","text":""},{"location":"devops/nexus/nexus-user-and-roles/#principle-of-least-privilege","title":"Principle of least privilege","text":"<p>For security purposes, we should use roles and users to grant permissions for specific tasks.</p>"},{"location":"devops/nexus/nexus-user-and-roles/#create-role-and-user","title":"Create Role and User","text":"<ul> <li>Type : Select Nexus role</li> <li>Privileges: Add <code>nx-repository-admin-*-*-*</code> This permission will allow all actions for all artifact and repository types. <ul> <li>First and second \"*\" represent recipe and repository type (docker hosted, docker proxy, apt hosted, apt proxy etc.) </li> <li>Last one represents actions (add,browse,read,edit,delete)</li> </ul> </li> <li>Create a new user using the role just created.</li> </ul> <p> In Nexus Repository, the <code>Docker Bearer Token Realm</code> is required in order to allow anonymous pulls from Docker repositories</p> <p>To allow anonymous pull:</p> <ul> <li>Go to <code>Realms</code> in Secutiry, add <code>Docker Bearer Token Realm</code> to active category.</li> <li>Edit the repo and click <code>Allow anonymous docker pull</code></li> </ul>"},{"location":"devops/nexus/pull-to-kubernetes/","title":"Pull Images to Kubernetes","text":""},{"location":"devops/nexus/pull-to-kubernetes/#copy-nexus-credentials-into-kubernetes","title":"Copy Nexus Credentials into Kubernetes","text":"<p>As we mentioned before, the login process creates or updates a config.json file that holds an authorization token.</p> <p>View the config.json file:</p> <pre><code>cat ~/.docker/config.json\n</code></pre> <p>The output contains a section similar to this:</p> <p><pre><code>{\n    \"auths\": {\n        \"&lt;EC2_PUBLIC_IP&gt;:8082\": {\n            \"auth\": \"c3R...zE2\"\n        }\n    }\n}\n</code></pre> A Kubernetes cluster uses the secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p> <p>If you already ran docker login, you can copy that credential into Kubernetes:</p> <pre><code>kubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=~/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then, add the secret to default service account.</p> <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"secret-name\"}]}'\n</code></pre> <p>Here is a manifest for an example Pod that needs access to your Docker credentials:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-private-pod\nspec:\n  containers:\n    - name: private\n      image: yourusername/privateimage:version\n  imagePullSecrets:\n    - name: secret-name\n</code></pre>"},{"location":"devops/nexus/registry-configuration/","title":"RKE2 Registry Configuration","text":"<p>Upon startup, RKE2 will check to see if a <code>registries.yaml</code> file exists at <code>/etc/rancher/rke2/</code> and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.</p> <p>Configuration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the <code>registries.yaml</code> file and give different examples of using private registry configuration in RKE2.</p>"},{"location":"devops/nexus/registry-configuration/#configuration-file","title":"Configuration File","text":"<p>The file consists of two main sections:</p> <ul> <li>mirrors</li> <li>configs</li> </ul> <p>Mirrors is a directive that defines the names and endpoints of the private registries. Private registries can be used as a local mirror for the default docker.io registry, or for images where the registry is explicitly specified.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\n</code></pre> <p>When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one.</p> <p>The configs section defines the TLS and credential configuration for each mirror. For each mirror you can define <code>auth</code> and/or <code>tls</code>. The credentials consist of either username/password or authentication token.</p>"},{"location":"devops/nexus/registry-configuration/#with-tls","title":"With TLS","text":"<p>Below are examples showing how you may configure <code>/etc/rancher/rke2/registries.yaml</code> on each node when using TLS.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"https://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: username # this is the registry username\n      password: password # this is the registry password\n    tls:\n      cert_file:            # path to the cert file used to authenticate to the registry\n      key_file:             # path to the key file for the certificate used to authenticate to the registry\n      ca_file:              # path to the ca file used to verify the registry's certificate\n      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate\n</code></pre> <p> If using a registry using plaintext HTTP without TLS, you need to specify <code>http://</code> as the endpoint URI scheme.</p> <pre><code>mirrors:\n  &lt;EC2_PUBLIC_IP&gt;:8083:\n    endpoint:\n      - \"http://&lt;EC2_PUBLIC_IP&gt;:8083\"\nconfigs:\n  \"&lt;EC2_PUBLIC_IP&gt;:8083\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n</code></pre>"},{"location":"devops/postgres/backup-restore/","title":"Backup","text":"<p>https://www.postgresql.org/docs/current/app-pgdump.html</p>"},{"location":"devops/postgres/backup-restore/#opsec","title":"OPSEC","text":"<p>About the versions</p> <p>Make sure to use the same version on all postgres tooling. Do this OPSEC every time!</p> <ul> <li>Check the postgres config, including the version number on the last line:   <pre><code>pg_config\n</code></pre></li> <li>Check the <code>pg_dump</code> version:   <pre><code>pg_dump --version\npg_dumpall --version\n</code></pre></li> <li>Check the <code>psql</code> version:   <pre><code>psql --version\n</code></pre></li> <li>Check the <code>pg_restore</code> version:   <pre><code>pg_restore --version\n</code></pre></li> </ul> <p>Make sure all the versions match. Otherwise you might run into issues when restoring the backup.</p>"},{"location":"devops/postgres/backup-restore/#choosing-the-right-backup-tool","title":"Choosing the right backup tool","text":"tool right time to use <code>pg_basebackup</code> [Physical Copy] Typically used for disaster recovery scenarios or when you need a complete copy of the database cluster. It allows for easy and efficient restoration of the entire database cluster to a specific point in time. <code>pg_dumpall</code> [All DBs in Server] Routine backups, database migration, or replicating the database structure and data to another server. <code>pg_dump</code> [Single DB in Server] Commonly used for routine backups of individual databases, database migration, and selective restoration of specific databases or objects."},{"location":"devops/postgres/backup-restore/#pg_dump","title":"pg_dump","text":"<ul> <li><code>pg_dump</code> only dumps a single database.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_dumpall","title":"pg_dumpall","text":"<ul> <li> <p>Use <code>pg_dumpall</code> to back up an entire cluster, or to back up global objects that are common to all databases in a cluster (such as roles and tablespaces).</p> </li> <li> <p>Used for logical backups, creating a script that contains SQL commands to recreate the database objects and data.</p> </li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_basebackup","title":"pg_basebackup","text":"<ul> <li>Primarily used for creating physical backups of the entire PostgreSQL database cluster, including all databases, tablespaces, and configuration files.</li> </ul>"},{"location":"devops/postgres/backup-restore/#pg_restore","title":"pg_restore","text":"<p>TODO: add pg_restore docs</p>"},{"location":"devops/postgres/configuration/","title":"Postgres Configuration","text":""},{"location":"devops/postgres/configuration/#helpers","title":"Helpers","text":"<ul> <li>postgresqlco.nf/ Documentation for explanations of all the configuration options.<ul> <li>You can upload your <code>postgresql.conf</code> file and it will give you recommendations on how to improve it.</li> </ul> </li> <li>pgtune.leopard.in.ua/#/ for generating a <code>postgresql.conf</code> file based on your hardware and database usage.</li> </ul>"},{"location":"devops/postgres/configuration/#notes","title":"Notes","text":"<ul> <li> <p>Always set the <code>PGDATA</code> environment variable to the path of the data directory. As the default directory used by the postgres image could change in the future.</p> </li> <li> <p>Always set the <code>POSTGRES_PASSWORD</code> environment variable. If not set, a random password will be generated and printed in the logs. </p> </li> <li>Always manage the password externally, for example by using a Kubernetes Secret.</li> </ul> <p>Status of postgres process</p> <ul> <li>Check the status     <pre><code>sudo systemctl restart postgresql.service\n</code></pre></li> <li>[If Applicable] Enable the service to start on boot      <pre><code>sudo systemctl enable postgresql.service\n</code></pre></li> </ul>"},{"location":"devops/postgres/configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li> <p>Located at: <code>/etc/postgresql/&lt;version&gt;/main/*</code></p> </li> <li> <p>After changing configuration, you must apply it with      <pre><code>sudo systemctl restart postgresql.service\n</code></pre></p> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_identconf-os-user-identification","title":"pg_ident.conf (OS User Identification)","text":"<ul> <li>This file is used for Operation System User -&gt; Database User mapping in PostgreSQL. </li> <li>It allows you to define mappings between a local operating system user and a PostgreSQL database user. </li> </ul>"},{"location":"devops/postgres/configuration/#postgresqlconf-configuration","title":"postgresql.conf (Configuration)","text":"<ul> <li>This is the primary configuration file for PostgreSQL. </li> <li>It contains a wide range of settings that control the behavior of the database server. </li> <li>These settings include parameters such as: <ul> <li>database connection settings, </li> <li>memory allocation, </li> <li>logging options, </li> <li>security configurations,</li> <li>and performance-related settings.</li> </ul> </li> </ul>"},{"location":"devops/postgres/configuration/#pg_hbaconf-host-based-authentication","title":"pg_hba.conf (Host Based Authentication)","text":"<ul> <li>This file controls client authentication in PostgreSQL. </li> <li>It specifies the rules for allowing or denying client connections based on various authentication methods such as: <ul> <li>password authentication, </li> <li>certificate-based authentication, </li> <li>or IP address-based authentication. </li> </ul> </li> <li>It is responsible for defining who can connect to the database and how they are authenticated.</li> </ul>"},{"location":"devops/postgres/configuration/#pg_statconf-statistics","title":"pg_stat.conf (Statistics)","text":"<ul> <li>This file is related to the statistics collection in PostgreSQL. </li> <li>It defines which statistics are collected and how they are stored. </li> <li>By configuring this file, you can control the collection of various statistics such as: <ul> <li>the number of tuples read or written, </li> <li>index usage, </li> <li>query execution time, and more.  These statistics help in monitoring and performance tuning of the database.</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/","title":"PoC","text":""},{"location":"devops/postgres/poc-backup-restore/#setup","title":"Setup","text":"<ul> <li>create an Ubuntu VM</li> <li>install postgres</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#backup","title":"Backup","text":"<ul> <li>Assumes you have a shell access running postgres instance.<ul> <li>If you're trying to do this over a network, you'll need to figure out other options for the commands run (e.g. host, port, etc.)</li> </ul> </li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#steps","title":"Steps","text":""},{"location":"devops/postgres/poc-backup-restore/#1-go-to-a-directory-where-the-postgres-user-has-write-access","title":"1. Go to a directory where the <code>postgres</code> user has write access","text":"<pre><code>cd /tmp\n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#2-create-a-backup-file","title":"2. Create a Backup file","text":"<p>!!! Note If you're not certain on which tool to use, try to use the <code>pg_dumpall</code>.</p> <p>2.A. Using <code>pg_dump</code></p> <p>Only dumps a single database named <code>postgres</code>. </p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dump -U postgres -d postgres -f db_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dump -Fc -U postgres postgres &gt; db_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul> <p>2.B. Using <code>pg_dumpall</code></p> <ul> <li>Option 1: Write to SQL file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).sql\n</code></pre></li> <li>Option 2: Write to .dump file     <pre><code>sudo -u postgres -- pg_dumpall --clean -f db_full_backup-$(date +%d-%m-%Y_%H-%M).dump\n</code></pre></li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#upload-the-backup-file-to-a-remote-storage","title":"Upload the backup file to a remote storage","text":"<ul> <li>Create a script that will upload the backup file to a remote storage (e.g. AWS S3, Azure Blob Storage, etc.)</li> </ul>"},{"location":"devops/postgres/poc-backup-restore/#restore","title":"Restore","text":""},{"location":"devops/postgres/poc-backup-restore/#restore-a-single-db","title":"Restore a Single DB","text":"<p>We are now working on the host machine that'll be backed up to.</p> <p>[IA] Delete the existing DB from the server <pre><code>sudo -u postgres -- dropdb existing-db-name\n</code></pre></p> <p>[IA] Create a new DB to restore to</p> <p>Create a new DB named <code>new-db-name</code> from the <code>template0</code> template. <pre><code>sudo -u postgres -- createdb -T template0 new-db-name\n</code></pre></p> <p>Restore a single DB from .sql file</p> <p>SQL files are restored with <code>psql</code>.</p> <pre><code>sudo -u postgres -- psql -U postgres -f db_backup.sql new-db-name \n</code></pre>"},{"location":"devops/postgres/poc-backup-restore/#restore-a-entire-db-server","title":"Restore a Entire DB Server","text":""},{"location":"devops/postgres/poc-backup-restore/#check-the-restoration","title":"Check the Restoration","text":"<p>Enter the psql shell to verify the restore was successful. <pre><code>sudo -u postgres -- psql -U postgres\n</code></pre> Run psql commands: <pre><code>-- list databases \n\\l\n-- connect to the new db\n\\c new-db-name\n-- list tables\n\\dt\n-- list rows in a table\nSELECT * FROM users;\n-- list users\n\\du\n-- list groups\n\\dg\n</code></pre></p>"},{"location":"devops/postgres/psql/","title":"psql CLI","text":""},{"location":"devops/postgres/psql/#installation","title":"Installation","text":"<p>Make sure to use the appropriate version of the client for the server</p> <p><code>bash     sudo apt install postgresql-client-&lt;version-number&gt;</code></p> <p>Go to official Download Page</p>"},{"location":"devops/postgres/psql/#running-commands","title":"Running commands","text":"<p>Accessing with the OS user</p> <p>Run your commands as the <code>postgres</code> user. This is the default user created by the postgres image.</p> <pre><code># sudo -u postgres:: will run the command as the postgres user\nsudo -u postgres psql &lt;database-name&gt;\n</code></pre> <p>The <code>postgres</code> user is the only user that can connect to the database without a password, create other users and databases.</p> <p>Using the credentials</p> <pre><code>psql --host &lt;your-servers-dns-or-ip&gt; \\\n    --username postgres \\\n    --password \\\n    --dbname template1\n</code></pre>"},{"location":"devops/postgres/psql/#psql-commands","title":"psql commands","text":"Command Description <code>\\conninfo</code> Details about the current db connection <code>\\l</code> List all databases <code>\\c &lt;db-name&gt;</code> Connect/Select a database <code>\\dt+</code> Show tables in the current database <code>\\dg+</code> Show users in the current database"},{"location":"devops/rancher/rancher-installation/","title":"Rancher Installation","text":"<p>This is a follow up to \"RKE2 Ansible Installation\" and assumes you're working on an RKE2 cluster similar to the one set up in that document.</p> <p>Example commands and configs are for 3 masters, 3 workers and an additional jump node all running Ubuntu.</p> <p>Example topology:  </p> Name IP Master-01 10.40.140.4 Master-02 10.40.140.5 Master-03 10.40.140.6 Worker-01 10.40.140.7 Worker-02 10.40.140.8 Worker-03 10.40.140.9 Jump 10.40.140.10 <ol> <li> <p>ssh into the jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> <p>Enter the ssh password when prompted</p> </li> <li> <p>Install kubectl if it's not already installed</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Confirm that our nodes and pods are correct and health</p> <pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#install-rancher-on-the-rke2-cluster","title":"Install Rancher on the RKE2 cluster","text":"<ol> <li> <p>Install Helm</p> <pre><code>sudo snap install helm --classic\n</code></pre> </li> <li> <p>Add Helm chart repository (used latest here, can be latest, stable or alpha)</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> </li> <li> <p>Create a namespace for Rancher</p> <pre><code>kubectl create namespace cattle-system\n</code></pre> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-rancher-generated-tls-cert","title":"Using Rancher-Generated TLS Cert","text":"<ol> <li> <p>Install cert-manager (needed if using Rancher-generated TLS cert or Let\u2019s Encrypt)</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.11.0\n</code></pre> </li> <li> <p>Verify that cert-manager is deployed correctly</p> <pre><code>kubectl get pods --namespace cert-manager\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --set hostname=10.40.140.8.nip.io \\\n  --set bootstrapPassword=admin \\\n  --set global.cattle.psp.enabled=false\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> <li> <p>Wait for Rancher to be rolled out</p> <pre><code>watch kubectl -n cattle-system get pods\n</code></pre> </li> <li> <p>In a web browser navigate to the DNS name that points to your load balancer (ex: <code>10.40.140.8:nip.io</code>), you should see the login page</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#using-your-own-certs","title":"Using your own Certs","text":""},{"location":"devops/rancher/rancher-installation/#formatting-certs","title":"Formatting Certs","text":"<p>It can cause complications while editing files on a machine running some form of Windows and uploading them to a Linux server. Windows-based text editors put special characters at the end of lines to denote a line return or newline. There is a simple way to correct this problem.</p> <ul> <li> <p>Install <code>dos2unix</code> and execute the command to convert line endings from DOS to Unix</p> <pre><code>sudo apt install -y dos2unix\n\ndos2unix /path/to/file/&lt;file-name&gt;\n</code></pre> </li> <li> <p>Windows servers use .pfx files which contain the public and private key. However, this can also be converted to .pem files to be used on Linux server</p> <pre><code>openssl pkcs12 -in cert.pfx -nocerts -out tls.key -nodes\nopenssl pkcs12 -in cert.pfx -nokeys -out tls.crt\n</code></pre> </li> </ul>"},{"location":"devops/rancher/rancher-installation/#validating-certs","title":"Validating Certs","text":"<p>Before you set up your certificates, it's a good idea to test them to ensure that they are correct and will work together.</p> <ol> <li> <p>Check to see if the private key and main certificate are in PEM format. <code>openssl</code> must be installed</p> <pre><code>sudo apt install openssl -y\n\nopenssl rsa -inform PEM -in /path/to/key/tls.key\nopenssl x509 -inform PEM -in /path/to/cert/tls.crt\n</code></pre> </li> <li> <p>Verify that the private key and main certificate match</p> <pre><code>openssl x509 -noout -modulus -in tls.crt | openssl md5\nopenssl rsa -noout -modulus -in tls.key | openssl md5\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Verify that the public keys contained in the private key file and the main certificate are the same</p> <pre><code>openssl x509 -in tls.crt -noout -pubkey\nopenssl rsa -in tls.key -pubout\n\n## The output of these two commands should be the same.\n</code></pre> </li> <li> <p>Check the validty of certificate chain</p> <pre><code>openssl verify -CAfile cacerts.pem tls.crt\n\n# Response must be OK.\n</code></pre> </li> <li> <p>Check if <code>Subject Alternative Names</code> contains <code>Common Name</code></p> </li> </ol> <p>Subject Alternative Name must contains the same value of the CN. If it does not, the certificate is not valid because the industry moves away from CN</p> <pre><code>openssl x509 -noout -subject -in tls.crt\n# subject= /CN=&lt;example.domain.com&gt;\nopenssl x509 -noout -in tls.crt -text | grep DNS\n# DNS:&lt;example.domain.com&gt;\n</code></pre>"},{"location":"devops/rancher/rancher-installation/#create-secrets-and-install","title":"Create Secrets and Install","text":"<ol> <li> <p>Create tls-ca secret with your private CA's root certificate</p> <pre><code>kubectl -n cattle-system create secret generic tls-ca \\\n    --from-file=cacerts.pem=./cacerts.pem\n</code></pre> </li> <li> <p>Create cert and key secrets</p> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n    --cert=tls.crt \\\n    --key=tls.key\n</code></pre> </li> <li> <p>Install Rancher with Helm</p> <pre><code>helm install rancher rancher-latest/rancher \\\n    --namespace cattle-system \\\n    --set hostname=10.40.140.8.nip.io \\\n    --set bootstrapPassword=admin \\\n    --set global.cattle.psp.enabled=false \\\n    --set ingress.tls.source=secret \\\n    --set privateCA=true\n</code></pre> <ul> <li> <p>hostname should be the DNS name you pointed at your load balancer for the worker nodes, .nip.io can be added to the ip if there\u2019s no DNS name</p> </li> <li> <p><code>global.cattle.psp.enabled</code> is set to <code>false</code> due to the rancher helm chart requiring the deprecated <code>podsecuritypolicy</code></p> </li> </ul> <p>Save the <code>--set</code> options used here, you will need to use the same options when you upgrade Rancher to new versions with Helm.</p> </li> </ol>"},{"location":"devops/rancher/rancher-installation/#cleanup","title":"Cleanup","text":"<ol> <li> <p>ssh into the Jump machine</p> <pre><code>ssh root@10.40.140.10\n</code></pre> </li> <li> <p>Make sure we're using the correct kubeconfig</p> <pre><code>export KUBECONFIG=~/rke2.yaml\n</code></pre> </li> <li> <p>Remove Rancher using helm</p> <pre><code>helm uninstall rancher -n cattle-system\n</code></pre> </li> <li> <p>Remove Helm repositories</p> <pre><code>helm repo remove jetstack\nhelm repo remove rancher-latest\n</code></pre> <p>Change <code>rancher-latest</code> with the version you used while installing (ex: <code>stable</code>, <code>alpha</code>)</p> </li> <li> <p>Remove Helm</p> <pre><code>sudo snap remove helm\n</code></pre> </li> </ol>"},{"location":"devops/sealed-secrets/sealed-secrets/","title":"Understanding Sealed Secrets: Solving the Challenge of Securely Managing Kubernetes Secrets","text":"<p>Let's start by defining Sealed Secrets and addressing the problem it aims to solve.</p> <p>When working with Kubernetes manifests, we often store them in Git version control systems alongside our application code. However, a challenge arises when dealing with Kubernetes Secret manifests, as they contain data that needs to be hidden but is stored in base64 format, which can be easily decrypted.</p> <p>This is where Sealed Secrets comes to the rescue. It allows us to encrypt our Secrets into SealedSecrets, making them safe to store, even in public repositories. The decryption of SealedSecrets is only possible by the controller running in the target cluster, ensuring that not even the original author can obtain the original Secret from the SealedSecret.</p> <p>To achieve this, two components are required: kubeseal, which we install locally, and the controller running in Kubernetes. With kubeseal, we encrypt our Secret manifests before pushing them to the Git repository. The Kubernetes controller is responsible for decrypting these encrypted Secret objects.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#sealed-secret-example","title":"Sealed Secret Example","text":"<p>These encrypted secrets are encoded in a SealedSecret resource, which you can see as a recipe for creating a secret. Here is how it looks:</p> <p><pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\nspec:\n  encryptedData:\n    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n</code></pre> Once unsealed, this produces a Secret equivalent to the following: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> Having explored how a Secret is securely stored in a Git repository, let's proceed to the usage.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#installation","title":"Installation","text":"<p>Firstly, let's set up the Sealed Secrets controller in Kubernetes. I'll perform this task using Helm.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#helm-chart","title":"Helm Chart","text":"<p>The Sealed Secrets helm chart is now officially supported and hosted in this GitHub repo.</p> <p><pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\n</code></pre> <pre><code>helm install sealed-secrets -n kube-system sealed-secrets/sealed-secrets\n</code></pre></p> <p>When the controller in the cluster up-and-running, it will generate a key. We will perform the encryption process with kubeseal, which we will install locally. You can find the kubeseal installation below for Mac and linux.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#kubeseal","title":"Kubeseal","text":""},{"location":"devops/sealed-secrets/sealed-secrets/#for-mac-homebrew","title":"For Mac (Homebrew)","text":"<p>The <code>kubeseal</code> client is also available on homebrew:</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"devops/sealed-secrets/sealed-secrets/#for-linux","title":"For Linux","text":"<p>The <code>kubeseal</code> client can be installed on Linux, using the below commands:</p> <p><pre><code>KUBESEAL_VERSION='' # Set this to, for example, KUBESEAL_VERSION='0.23.0'\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION:?}/kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION:?}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> If you have <code>curl</code> and <code>jq</code> installed on your machine, you can get the version dynamically this way. This can be useful for environments used in automation and such.</p> <p><pre><code># Fetch the latest sealed-secrets version using GitHub API\nKUBESEAL_VERSION=$(curl -s https://api.github.com/repos/bitnami-labs/sealed-secrets/tags | jq -r '.[0].name' | cut -c 2-)\n\n# Check if the version was fetched successfully\nif [ -z \"$KUBESEAL_VERSION\" ]; then\n    echo \"Failed to fetch the latest KUBESEAL_VERSION\"\n    exit 1\nfi\n\nwget \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION}/kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> With the tools in place, both the cluster-side controller/operator and the client-side utility kubeseal are ready.</p>"},{"location":"devops/sealed-secrets/sealed-secrets/#usage","title":"Usage","text":"<p>Assuming we have a Kubernetes Secret manifest file named <code>mysecret.yaml</code> as follows: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: mynamespace\ndata:\n  foo: YmFy  # &lt;- base64 encoded \"bar\"\n</code></pre> We can encrypt the Kubernetes Secret file using the following command: <pre><code>kubeseal --controller-name sealed-secrets -o yaml -n kube-system &lt; mysecrets.yaml &gt; encrypted-mysecret.yaml\n</code></pre> Subsequently, we can apply the <code>encrypted-mysecret.yaml</code> file: <pre><code>kubectl apply -f encrypted-mysecret.yaml\n</code></pre> To summarize the process:</p> <ul> <li>The Sealed Secrets controller, installed with Helm, generated public and private keys.</li> <li>We encrypted the decrypted mysecret.yaml file locally using the kubeseal command.</li> <li>When deploying the encrypted-mysecret.yaml file into Kubernetes, the controller decrypted it with the private key, converting it into a Kubernetes Secret.</li> </ul> <p>Now, with peace of mind, we can store our Secret manifests in Git repositories, especially if you are using GitOps, where you can automate your work by planning the file directory containing your encrypted manifests as an ArgoCD application.</p>"},{"location":"devops/sonarqube/advanced-installation/","title":"How to Set Up SonarQube with PostgreSQL, Nginx and LDAP Using Docker Compose: A Comprehensive Guide","text":"<p>SonarQube is a top-tier source code quality management application that provides comprehensive code analysis and support for 17 programming languages. It is the preferred solution for static code analysis and code coverage, and it is extensively used by both developers and organizations.</p> <p>This article will provide you with a step-by-step guide to establishing SonarQube using Docker Compose, which is integrated with a PostgreSQL database and a Nginx proxy to redirect traffic to port 80 on your domain.</p> <p>Finally, we will configure SonarQube authentication and authorization to an LDAP server by configuring the appropriate values in <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code>.</p>"},{"location":"devops/sonarqube/advanced-installation/#prerequisites","title":"Prerequisites","text":"<p>Before we start, ensure you have the following:</p> <ul> <li>An instance with a minimum of 2 vCPUs and 4 GB RAM.</li> <li>Docker and Docker Compose are installed on your machine.</li> <li>Open port 80</li> </ul> <p>We may move further with the system settings now.</p>"},{"location":"devops/sonarqube/advanced-installation/#system-configuration","title":"System Configuration","text":"<p>SonarQube has to make some system adjustments because it uses Elasticsearch to store its indices in an MMap FS directory. You must ensure that:</p> <ul> <li>The process is allowed to have a maximum of 524288 memory map areas, as specified by the <code>vm.max_map_count</code> parameter.</li> <li>The value of the maximum number of open file descriptors <code>fs.file-max</code> is set to a minimum of 131072.</li> <li>The SonarQube user has a minimum capacity to open 131072 file descriptors.</li> <li>The SonarQube user has the capability to initiate a minimum of 8192 threads.</li> </ul> <p>Use the steps provided below according to your operating system:</p>"},{"location":"devops/sonarqube/advanced-installation/#for-red-hat-centos-or-amazon-linux","title":"For Red Hat, CentOS, or Amazon Linux","text":"<pre><code>sudo yum update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#for-ubuntu-or-debian","title":"For Ubuntu or Debian","text":"<pre><code>sudo apt update -y\nsysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#installation","title":"Installation","text":"<p>In order to establish our services, we require a <code>docker-compose.yml</code> file.</p> <pre><code>version: \"3\"\n\nservices:\n  sonarqube:\n    image: sonarqube:latest\n    container_name: sonarqube\n    restart: unless-stopped\n    depends_on:\n      - db\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar\n      SONAR_JDBC_USERNAME: sonar\n      SONAR_JDBC_PASSWORD: sonar\n    volumes:\n      - sonarqube_data:/opt/sonarqube/data\n      - sonarqube_extensions:/opt/sonarqube/extensions\n      - sonarqube_logs:/opt/sonarqube/logs\n    ports:\n      - \"9000:9000\"\n\n  db:\n    image: postgres:15\n    container_name: postgresql\n    environment:\n      POSTGRES_USER: sonar\n      POSTGRES_PASSWORD: sonar\n      POSTGRES_DB: sonar\n    volumes:\n      - postgresql:/var/lib/postgresql\n      - postgresql_data:/var/lib/postgresql/data\n\n  nginx:\n    image: nginx:latest\n    container_name: nginx\n    restart: unless-stopped\n    depends_on:\n      - sonarqube\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n    ports:\n      - \"80:80\"\n\nvolumes:\n  sonarqube_data:\n  sonarqube_extensions:\n  sonarqube_logs:\n  postgresql:\n  postgresql_data:\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#what-we-have-here","title":"What We Have Here?","text":"<p>Three services (SonarQube, PostgreSQL, and Nginx) are included in this compose file.</p> <ul> <li>SonarQube Service uses the latest SonarQube image, connects to the PostgreSQL database, and exposes port 9000 for the web interface. It includes volumes for persistent data, extensions, and logs.</li> <li>PostgreSQL Service sets up a PostgreSQL database with environment variables for user credentials and includes volumes for data persistence.</li> <li>Nginx Service acts as a reverse proxy using the latest Nginx image, mapping port 80 to the host. It relies on a custom Nginx configuration file.</li> </ul> <p>In order to redirect the traffic from <code>localhost:9000</code> to your domain on port 80, it is necessary to create a <code>nginx.conf</code> file. Make sure that this file is located in the same directory as the docker-compose.yml file.</p> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n\n    location / {\n        proxy_pass http://sonarqube:9000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#running-the-setup","title":"Running the Setup","text":"<p>Run the following command to start the setup:</p> <p><pre><code>sudo docker-compose up -d\n</code></pre> Docker Compose orchestrates and executes your complete application. To access SonarQube, use the domain indicated in your <code>nginx.conf</code> file, which is <code>yourdomain.com</code></p>"},{"location":"devops/sonarqube/advanced-installation/#ldap-configuration","title":"LDAP Configuration","text":"<p>Integrating LDAP (Lightweight Directory Access Protocol) with SonarQube is an essential process for firms seeking to centrally manage user authentication and authorization. LDAP integration enables the utilization of an already established LDAP directory, such as Active Directory, for the purpose of managing users. This streamlines administration by ensuring that there is just one authoritative source for user data. </p> <p>The main setup for LDAP integration in SonarQube is performed via the <code>sonar.properties</code> file. This file is usually located in  <code>&lt;SONARQUBE_HOME&gt;/conf/sonar.properties</code></p> <pre><code># LDAP Configuration\n# Enable the LDAP feature\nsonar.security.realm=LDAP\n\n# General Configuration\nldap.url=ldap://ldap_server_ip:389\nldap.bindDn=CN=your_sonar_user,OU=Service Accounts,DC=your_domain,DC=top_level_domain\nldap.bindPassword=password_here\n\n# User Configuration\nldap.user.baseDn=DC=your_domain,DC=top_level_domain\nldap.user.request=(&amp;(objectClass=user)(sAMAccountName={login}))\nldap.user.realNameAttribute=cn\nldap.user.emailAttribute=mail\n\n# Group Configuration\nldap.server1.group.baseDn=DC=your_domain,DC=top_level_domain\nldap.server1.group.request=(&amp;(objectClass=group)(memberUid={uid}))\n</code></pre> <p>To implement the new configuration, restart the SonarQube server after saving the changes to the sonar.properties file.</p> <pre><code>sudo systemctl restart sonarqube\n</code></pre>"},{"location":"devops/sonarqube/advanced-installation/#accessing-sonarqube","title":"Accessing SonarQube","text":"<p>Verify the LDAP configuration by attempting to log in with an LDAP user account after SonarQube has restarted. Ensure that the user attributes, such as email and name, are accurately populated from the LDAP directory.</p> <p> </p>"},{"location":"devops/sre/k8sgpt/","title":"k8sgpt","text":"<p>As Kubernetes continues to grow in popularity, managing and monitoring your clusters can become quite complex. That's where k8sgpt comes in. k8sgpt is a tool that uses advanced AI to make Kubernetes management easier. It helps you with tasks like monitoring your clusters and solving problems by understanding and responding to natural language commands. In this blog, I\u2019ll look at what k8sgpt does, how it can help you, and why it might be a great addition to your Kubernetes setup.</p> <p>The project already supports multiple installation options and different AI backends such as openAI,localAI,azureAI etc. In this blog, I will show you how to install and get started with K8sGPT, both the CLI tools and the Operator as well as how K8sGPT supports additional integrations.</p>"},{"location":"devops/sre/k8sgpt/#installation","title":"Installation","text":"<p>You can choose from several installation methods based on your operating system and personal preferences. The installation section of the documentation provides a list of these options.</p> <pre><code>https://docs.k8sgpt.ai/getting-started/installation/\n</code></pre> <p>The installation of K8sGPT requires that Homebrew is installed on your Mac.</p> <p>Run the following commands:</p> <pre><code>brew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n</code></pre> <ul> <li>To verify that K8sGPT is install correctly, you can check the version installed:</li> </ul> <pre><code>k8sgpt version\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-cli","title":"K8sGPT CLI","text":"<p>The K8sGPT CLI is a tool that lets you manage your Kubernetes clusters using simple, natural language commands. It makes it easier to control and troubleshoot your clusters by letting you type commands.</p> <ul> <li>To view all the commands provided by K8sGPT:</li> </ul> <pre><code>k8sgpt --help\n</code></pre> <p></p> <p>You can see an overview of the different commands also below link:</p> <pre><code>https://docs.k8sgpt.ai/reference/cli/\n</code></pre>"},{"location":"devops/sre/k8sgpt/#authorise-an-ai-backend","title":"Authorise an AI Backend","text":"<p>K8sGPT supports multiple AI providers, allowing you to choose from a variety of options to fit your needs. For example, you can use providers like OpenAI, localAI and AzureAI. By default, OpenAI is the primary provider, but keep in mind that analyzing data with OpenAI requires a minimum balance of $5. To avoid this cost and still utilize powerful AI capabilities, I opted to use LocalAI for my setup. This choice allowed me to seamlessly integrate AI without incurring additional expenses.</p> <p>Ollama is an easy-to-use tool that helps you quickly start using open source large language models (LLMs). You can download Ollama from https://ollama.com/ and install it on your computer. Once installed, launch Ollama and use the command provided to download a LLM. Note that the mistral model requires ~ 4GB of disk space.</p> <pre><code>brew update\nbrew install ollama\nbrew services start ollama\nollama pull mistral\n</code></pre>"},{"location":"devops/sre/k8sgpt/#configure-k8sgpt-to-use-localai-using-ollama-backend","title":"Configure k8sgpt to use LocalAI using Ollama backend","text":"<pre><code>k8sgpt auth add --backend localai --model mistral --baseurl http://localhost:11434/v1\n</code></pre> <p>You can list your backends with the following command:</p> <pre><code>k8sgpt auth list\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#install-a-malicious-deployment","title":"Install a malicious Deployment","text":"<p>Next, we will install a malicious Deployment into our Kubernetes cluster.This manifest is deliberately malicious as it includes an incorrect command (slep instead of sleep), causing the pod to continuously fail and enter a CrashLoopBackOff state, which makes it an ideal example for analyzing and debugging issues with K8sGPT. Here is the YAML:</p> <p>deployment yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\n  labels:\n    app: example-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: example-container\n        image: busybox\n        command: [\"/bin/sh\", \"-c\", \"slep 3600\"]  # not slep\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vulnerable-service\n  labels:\n    app: vulnerable-app\nspec:\n  selector:\n    app: vulnerable-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> <ul> <li>Next, we will install the Deployment</li> </ul> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> <p>Now you will see the pods throwing errors:</p> <p></p>"},{"location":"devops/sre/k8sgpt/#analyze-your-cluster-by-using-k8s","title":"Analyze your cluster by using k8s","text":"<p>Run the command below to get GenAI-generated insights about problems in your cluster.</p> <pre><code>k8sgpt analyze --explain --backend localai --namespace default --filter Pod\nk8sgpt analyze --explain --backend localai --namespace default --filter Service\n</code></pre> <p></p> <p>The analysis provided by K8sGPT identifies a CrashLoopBackOff issue with the pod, indicating that the container within the pod encountered an error. The suggested steps to troubleshoot this include checking container logs, verifying the Docker image, ensuring correct configuration, scaling the deployment, and reviewing Kubernetes events for further insights.</p>"},{"location":"devops/sre/k8sgpt/#integration","title":"Integration","text":"<p>Integrating K8sGPT with tools like Trivy, Grafana, or Prometheus can enhance its capabilities for Kubernetes security, monitoring, and analysis.Trivy is a vulnerability scanner for container images and file systems. Integrating K8sGPT with Trivy can help analyze vulnerabilities in your Kubernetes clusters.</p> <p>You can list all available integration with the following command:</p> <pre><code>k8sgpt integration list\n</code></pre> <p>Next, we want to activate the Trivy integration:</p> <p><pre><code>k8sgpt integration activate trivy\n</code></pre> This will install the Trivy Operator inside of your cluster</p> <ul> <li>Once the integration is activated, we can use the Vulnerability Reports created by Trivy as part of our K8sGPT analysis through k8sgpt filters:</li> </ul> <p></p> <ul> <li>The filters in K8sGPT are mapped to specific analyzers in the code, which focus on extracting and examining only the most relevant information, such as the most critical vulnerabilities.</li> </ul> <p>To use the VulnerabilityReport filter </p> <pre><code>k8sgpt analyse --filter=VulnerabilityReport\n</code></pre> <p></p>"},{"location":"devops/sre/k8sgpt/#k8sgpt-operator","title":"K8sGPT Operator","text":"<p>The CLI tool lets cluster admins run on-the-spot scans on their infrastructure and workloads. Meanwhile, the K8sGPT operator works continuously in your cluster, running all the time. It is built specifically for Kubernetes, using Custom Resources to create reports that are stored in your cluster as YAML files.</p> <ul> <li>To install the Operator, follow the documentation link (https://docs.k8sgpt.ai/getting-started/in-cluster-operator/) or the commands provided below:</li> </ul> <p>You will  need to install the kube-prometheus-stack Helm Chart to use Grafana and Prometheus. You can do this with the following commands:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prom prometheus-community/kube-prometheus-stack -n monitoring --create-namespace\n</code></pre> <p>In this setup, we instruct K8sGPT to install a ServiceMonitor, which sends scan report metrics to Prometheus and sets up a Dashboard for K8sGPT.</p> <p>k8sgpt-values.yaml <pre><code>serviceAccount:\n  create: true\n  name: \"k8sgpt\"\n  # -- Annotations for the managed k8sgpt workload service account\n  annotations: {}\nserviceMonitor:\n  enabled: true\n  additionalLabels:\n    release: prom\n  namespace: monitoring\ngrafanaDashboard:\n  enabled: true\n  # The namespace where Grafana expects to find the dashboard\n  # namespace: \"\"\n  folder:\n    annotation: grafana_folder\n    name: ai\n  label:\n    key: grafana_dashboard\n    value: \"1\"\n  # create GrafanaDashboard custom resource referencing to the configMap.\n  # according to https://grafana-operator.github.io/grafana-operator/docs/examples/dashboard_from_configmap/readme/\n  grafanaOperator:\n    enabled: false\n    matchLabels:\n      dashboards: \"grafana\"\ncontrollerManager:\n  kubeRbacProxy:\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: gcr.io/kubebuilder/kube-rbac-proxy\n      tag: v0.16.0\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 5m\n        memory: 64Mi\n  manager:\n    sinkWebhookTimeout: 30s\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    image:\n      repository: ghcr.io/k8sgpt-ai/k8sgpt-operator\n      tag: v0.1.7  # x-release-please-version\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 10m\n        memory: 64Mi\n  replicas: 1\n  ## Node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  #\n  nodeSelector: {}\nkubernetesClusterDomain: cluster.local\nmetricsService:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: https\n  type: ClusterIP\n</code></pre></p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace -f k8sgpt-values.yaml\n</code></pre> <ul> <li>Then, we need to configure the K8sGPT Operator to know which version of K8sGPT to use and which AI backend:</li> </ul> <p>k8sgpt-backend.yaml <pre><code>apiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-ollama\nspec:\n  ai:\n    enabled: true\n    model: mistral\n    backend: localai\n    baseUrl: http://host.docker.internal:11434/v1\n  noCache: false\n  filters: [\"Pod\", \"Service\"]\n  repository: ghcr.io/k8sgpt-ai/k8sgpt\n  version: v0.3.17\n  # sink:\n  #   type: slack\n  #   webhook: &lt;url&gt;\n</code></pre> - We need to apply this file to our K8sGPT cluster namespace</p> <pre><code>kubectl apply -n k8sgpt-operator-system -f k8sgpt-backend.yaml\n</code></pre> <ul> <li>That\u2019s all with the operator setup. The operator will watch for problems in the cluster and generate analysis results that you can view using the command below</li> </ul> <pre><code>kubectl get results -n k8sgpt-operator-system -o json | jq .\n</code></pre> <ul> <li>During the installation of K8sGPT, we set serviceMonitor to true, which ensures that a ServiceMonitor is created to send metrics from K8sGPT to Prometheus. Once this is configured correctly, you should be able to see a new target for K8sGPT under the Prometheus targets section, specifically within the Kubernetes targets. This integration allows Prometheus to continuously monitor K8sGPT's performance and activity within the cluster, providing valuable insights through metrics.</li> </ul> <pre><code>kubectl port-forward service/prom-kube-prometheus-stack-prometheus -n monitoring 9090:9090\n</code></pre> <ul> <li>Open localhost:9090 and navigate to Status --&gt; Targets and then you should see the serviceMonitor with thes result:</li> </ul> <p></p> <ul> <li>Lastly Go to Grafana Dashboard.</li> </ul> <pre><code>kubectl port-forward service/prom-grafana -n monitoring 3000:3000\n</code></pre> <ul> <li>Open localhost:3000 and then navigate to Dashboards&gt;K8sGPT Overview and then you will see the Dashboard with your results:</li> </ul> <p></p> <p>This blog  explained the main features of K8sGPT. First, we looked at the K8sGPT CLI, then how it works better with integrations, and finally installed the operator for continuous analysis inside the cluster with grafana dashboard.</p>"},{"location":"devops/sumo/sumo-linux-collector/","title":"Install a Sumo Logic Collector on Linux","text":""},{"location":"devops/sumo/sumo-linux-collector/#download-a-sumo-logic-collector-from-a-static-url","title":"Download a Sumo Logic Collector from a Static URL","text":"<p>Invoke a web request utility such as wget. For Linux 64-bit host, you can wget the Collector from the command line:</p> <pre><code>wget \"https://collectors.sumologic.com/rest/download/linux/64\" -O SumoCollector.sh &amp;&amp; chmod +x SumoCollector.sh\n</code></pre> <p>For other hosts choose the related one above:</p> Platform Download URL Linux 64 https://collectors.au.sumologic.com/rest/download/linux/64 Linux Debian https://collectors.au.sumologic.com/rest/download/deb/64 Linux RPM https://collectors.au.sumologic.com/rest/download/rpm/64 Tarball https://collectors.au.sumologic.com/rest/download/tar Windows 32 https://collectors.au.sumologic.com/rest/download/windows Windows 64 https://collectors.au.sumologic.com/rest/download/win64 <p>Important Note: The latest release of the Sumo Collector targets the Java 8 runtime. Java 6 and Java 7 are no longer supported as the Collector runtime, and Solaris is no longer supported. When you upgrade Collectors, JRE 8 or later is required. The Sumo Collector with a bundled JRE now ships with JRE 8.</p>"},{"location":"devops/sumo/sumo-linux-collector/#system-requirements","title":"System Requirements","text":"<p>The Sumo Logic Collector has the following system requirements:</p> <ul> <li>Operating System: Linux, major distributions 64-bit, or any generic Unix capable of running Java 1.8</li> <li>CPU: Single core</li> <li>RAM: 512MB</li> <li>Disk Space: 8GB</li> <li>TLS: Package installers require TLS 1.2 or higher</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#install-using-the-command-line-installer","title":"Install using the command line installer","text":"<ol> <li>Add execution permissions to the downloaded Collector file (.sh):</li> </ol> <p><pre><code>chmod +x SumoCollector.sh\n</code></pre> 2. Run the script with the parameters that you want to configure </p>"},{"location":"devops/sumo/sumo-linux-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<p><pre><code>sudo ./SumoCollector.sh -q -Vsumo.token_and_url=&lt;installationToken&gt; -Vsources=&lt;absolute_filepath&gt;\n</code></pre> By default, the Collector will be installed in either <code>/opt/SumoCollector</code> or <code>/usr/local/SumoCollector</code>.</p>"},{"location":"devops/sumo/sumo-linux-collector/#other-parameters-for-the-command-line-installer","title":"Other parameters for the command line installer","text":"<p>The command line installer also supports a number of other parameters, including:</p> <ul> <li>-dir [directory] : Sets a different installation directory than the default.</li> <li>-Vsumo.accessid=[accessId] : The access ID is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.accesskey=[accessKey] : The access key is part of the authentication credentials for your Sumo Logic account.</li> <li>-Vsumo.token_and_url=[token] : The token can be either an Installation Token or Setup Wizard Token.</li> </ul> <p>The command line installer can also use all of the parameters available in the user.properties file. To use parameters from user.properties just add a <code>-V</code> to the beginning of the parameter without a space character.</p> <p>The following parameters have a different format in the command line installer:</p> user.properties cli name <code>-Vcollector.name</code> url <code>-Vcollector.url</code> proxyHost <code>-Vproxy.host</code> proxyPort <code>-Vproxy.port</code> proxyUser <code>-Vproxy.user</code> proxyPassword <code>-Vproxy.password</code>"},{"location":"devops/sumo/sumo-linux-collector/#other-userproperties-parameters","title":"Other user.properties parameters","text":"<p>The user.properties file can also be used to configure the following parameters:</p> <ul> <li>-Vdescription : Description of the collector</li> <li>-VhostName : Name of the host machine that the collector is installed</li> <li>-Vsources : The contents of the file or files are read upon Collector registration only, it is not synchronized with the Collector's configuration on an on-going basis.</li> <li>-VsyncSources : The Source definitions will be continuously monitored and synchronized with the Collector's configuration.</li> </ul>"},{"location":"devops/sumo/sumo-linux-collector/#start-or-stop-a-collector-using-scripts","title":"Start or Stop a Collector using Scripts","text":"<p>To start, stop, check the status of the Collector or restart it, run one of the following commands from the Collector installation directory: <pre><code>sudo ./collector start\n</code></pre> <pre><code>sudo ./collector stop\n</code></pre> <pre><code>sudo ./collector status\n</code></pre> <pre><code>sudo ./collector restart\n</code></pre></p>"},{"location":"devops/sumo/sumo-linux-collector/#uninstall-using-the-command-line","title":"Uninstall using the command line","text":"<ol> <li> <p>In a terminal prompt, change the directory to the collector installation directory:</p> </li> <li> <p>Run the uninstall binary with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts:</p> <pre><code>sudo ./uninstall -q\n</code></pre> </li> </ol>"},{"location":"devops/sumo/sumo-local-file-management/","title":"Local Configuration File Management","text":""},{"location":"devops/sumo/sumo-local-file-management/#local-configuration-file-management","title":"Local Configuration File Management","text":"<p>With local configuration file management, you can configure Sources for an Installed Collector in one or more UTF-8 encoded JSON files.</p> <p>IMPORTANT NOTE: After you switch over to local configuration file management, you can no longer manage Sources through the Sumo web application or the Collector Management API.</p> <p>Local configuration file management is available on Collector version v19.108 and later.</p> <p>Benefits of local configuration file management</p> <ul> <li>You don't need to log in to the Sumo web app or use API calls. Instead, you edit the JSON configuration file(s), and they are read almost immediately by the Collector.</li> <li>If you have a large scale deployment, it can be impractical to add or edit Sources one at a time. Using local configuration management allows you to manage Sources more easily.</li> <li>You can use deployment tools so that established policies for deployments are not interrupted.</li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#options-for-specifying-sources-in-local-configuration-files","title":"Options for specifying Sources in local configuration file(s)","text":"<p>There are two ways to implement local configuration file management:</p> <ol> <li>Specify all Sources in a single UTF-8 encoded JSON file.</li> <li>Use multiple UTF-8 encoded JSON files to specify your Sources, and put all of those files in a single folder.</li> </ol> <p>Note: Each JSON file must have a <code>.json</code> extension.</p>"},{"location":"devops/sumo/sumo-local-file-management/#define-multiple-sources-in-a-json-file","title":"Define multiple Sources in a JSON file","text":"<p>When you define multiple Sources in a JSON file, you can define each Source in a <code>sources</code> JSON array. For example:</p> <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"LocalFile\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    },\n    {\n      \"sourceType\": \"RemoteFile\",\n      \"name\": \"Example2\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre> <p>To define a single source in a JSON file, you just have one source definition under the <code>sources</code> array. <pre><code>{\n  \"api.version\": \"v1\",\n  \"sources\": [\n    {\n      \"sourceType\": \"DockerLogs\",\n      \"name\": \"Example1\",\n      \"pathExpression\": \"/path/to/log\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"devops/sumo/sumo-local-file-management/#configure-the-location-of-json-file-or-folder","title":"Configure the location of JSON file or folder","text":"<p>When using local file configuration management, you specify the location of the JSON file or the folder that contains multiple JSON files in the Collector's <code>config/user.properties</code> file. You need to use the <code>syncSources</code> parameter to point to your configuration file or folder.</p> <ul> <li> <p>To point to a JSON file that defines Sources for a Collector:</p> <p><code>syncSources=/path/to/sources.json</code></p> </li> <li> <p>To point to a folder that contains JSON files that define Sources for a Collector: <code>syncSources=/path/to/sources-folder</code></p> </li> <li> <p>On Windows (note the escaped backslashes): <code>syncSources=C:\\path\\to\\sources-folder\\</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#type-of-sources","title":"Type of Sources","text":"<p>In our case, we are using the <code>\"LocalFile\"</code> type value for Local File Type.</p> <p>Note: You should add the parameter <code>\"sourceType\":\"LocalFile\"</code> in the source file.</p> <p>JSON Parameters for Installed Sources</p> Parameter Description sourceType The type of the data that the collector will collect. description Type a description of the Source. category Type a category of the source. cutoffTimestamp If you have a file that contains logs with timestamps spanning an entire week and set the cutoffTimestamp to two days ago, all of the logs from the entire week will be ingested since the file itself was modified more recent than the cutoffTimestamp pathExpression A valid path expression (full path) of the file to collect denylist Comma-separated list of valid path expressions from which logs will not be collected. encoding Defines the encoding form. Default is \"UTF-8\"; options include \"UTF-16\"; \"UTF-16BE\"; \"UTF-16LE\"."},{"location":"devops/sumo/sumo-local-file-management/#important-tip","title":"Important Tip:","text":"<p>While giving the <code>pathExpression</code> use a single asterisk wildcard <code>[*]</code> for file or folder names. For example:</p> <p><code>pathExpression: \"/var/foo/*.log\"</code></p> <p>Use two asterisks <code>[**]</code> to recurse within directories and subdirectories. For example:</p> <p><code>pathExpression: \"var/**/*.log\"</code></p>"},{"location":"devops/sumo/sumo-local-file-management/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>You can edit the JSON configuration file at any time to edit Source attributes or add new Sources. When you delete Sources from the file, they are deleted from the Collector.</p> <p>To make the changes take effect, you need to restart the Collector.</p> <p>To restart the Collector, use these commands:</p> <ul> <li> <p>Linux: <code>sudo ./collector restart</code></p> </li> <li> <p>Windows: <code>net restart sumo-collector</code></p> </li> </ul>"},{"location":"devops/sumo/sumo-local-file-management/#full-example-of-local-source-file","title":"Full example of Local Source file :","text":"<pre><code>{\n   \"api.version\":\"v1\",\n   \"sources\":[\n      {\n         \"name\":\"Hepapi-Test-Logs\",\n         \"category\":\"DockerLogs\",\n         \"automaticDateParsing\":true,\n         \"multilineProcessingEnabled\":false,\n         \"useAutolineMatching\":false,\n         \"forceTimeZone\":false,\n         \"timeZone\":\"UTC\",\n         \"cutoffTimestamp\":0,\n         \"encoding\":\"UTF-8\",\n         \"pathExpression\":\"/var/lib/docker/containers/*/*-json.log\",\n         \"sourceType\":\"LocalFile\"\n      } \n   ]\n}\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/","title":"Install a Sumo Logic Collector on Windows","text":""},{"location":"devops/sumo/sumo-windows-collector/#system-requirements-for-windows","title":"System Requirements for Windows","text":"<ul> <li>Windows 7, 32 or 64 bit</li> <li>Windows 8, 32 or 64 bit</li> <li>Windows 8.1, 32 or 64 bit</li> <li>Windows 10, 32 or 64 bit</li> <li>Windows 11, 32 or 64 bit</li> <li>Windows Server 2012</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> <li>Single core, 512MB RAM</li> <li>8GB disk space</li> <li>Package installers require TLS 1.2 or higher.</li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#download-a-collector-from-a-static-url","title":"Download a Collector from a Static URL","text":"<ol> <li>Open a terminal window or command prompt.</li> <li>If you're using PowerShell on a 64-bit Windows host, you can use Invoke-WebRequest:</li> </ol>"},{"location":"devops/sumo/sumo-windows-collector/#configure-usage-of-tls","title":"Configure usage of TLS","text":"<pre><code>[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Tls,Tls11,Tls12'\n</code></pre>"},{"location":"devops/sumo/sumo-windows-collector/#download-the-installer","title":"Download the installer","text":"<pre><code>Invoke-WebRequest 'https://collectors.sumologic.com/rest/download/win64' -outfile '&lt;download_path&gt;\\SumoCollector.exe'\n</code></pre> <p>Replace the  with the location where you want to download the Collector. For example, <code>C:\\user\\example\\path\\to\\SumoCollector.exe</code>"},{"location":"devops/sumo/sumo-windows-collector/#install-the-sumo-logic-collector-using-the-command-line-installer","title":"Install the Sumo Logic Collector using the command line installer","text":"<p>From the command prompt, run the downloaded EXE file with the parameters that you want to configure. For example:</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-an-installation-token","title":"Using an Installation Token","text":"<pre><code>./SumoCollector.exe -console -q \"-Vsumo.token_and_url=&lt;installationToken&gt;\" \"-Vsources=&lt;filepath&gt;\"\n</code></pre> <p>Reminder: You can pass other <code>user.properties</code> parameters as well inside <code>\"\"</code>.</p> <p>When you see the <code>Finishing installation...</code> message, you can close the command prompt window. The installation is complete.</p> Parameter Description <code>-console</code> Only has an effect when used with <code>-q</code>. Causes the installer to send progress messages to the console. On Windows, for this option to take effect, you must run the installer with start /wait. For example: <code>start /wait installer.exe -q -console</code> <code>-q</code> Causes the installer to run silently, which means you won't be prompted to supply installation parameters. For any installation parameter that you do not define at the command line, Sumo will use a default value. No output is sent to the console during installation, unless you also use the <code>-console</code> parameter."},{"location":"devops/sumo/sumo-windows-collector/#configuring-sources-for-collectors","title":"Configuring Sources for Collectors","text":"<p>After installing Collectors, you can configure Sources directly in Sumo Logic or by providing the Source settings in a JSON file.</p>"},{"location":"devops/sumo/sumo-windows-collector/#using-a-json-file","title":"Using a JSON file","text":"<p>If you're using a UTF-8 encoded JSON file, you must provide the file before starting the Collector. The JSON file needs to be UTF-8 encoded.</p> <p>Important Note:</p> <p>In Windows Host by using double backslashes, the JSON parser will interpret each backslash as a literal character rather than an escape character. For example:</p> <ul> <li>Backslashes are NOT treated correctly:  <code>\"-Vsources=&lt;C:\\some\\path\\to\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\path\\to\\source.json&gt;\"</code></li> <li>Backslashes ARE  treated correctly:    <code>\"-Vsources=&lt;C:\\\\some\\\\path\\\\to\\\\SumoCollector.exe&gt;\"</code> or <code>\"pathExpression\":\"&lt;C:\\\\path\\\\to\\\\source.json&gt;\"</code></li> </ul>"},{"location":"devops/sumo/sumo-windows-collector/#uninstalling-from-the-command-line","title":"Uninstalling from the command line","text":"<p>From the command prompt, run the <code>uninstall.exe</code> file with the <code>-q</code> option. The <code>-q</code> option executes the command without presenting additional prompts.</p> <pre><code>./uninstall.exe -q -console\n</code></pre>"},{"location":"devops/vagrant/vagrant-quickstart/","title":"Vagrant","text":""},{"location":"devops/vagrant/vagrant-quickstart/#what-is-vagrant","title":"What is Vagrant?","text":"<p>CLI tool for managing the life-cycle of VMs</p> <ul> <li>Reproducible local dev environments  </li> <li>Vagrantfile, akin to Dockerfile for VMs</li> </ul>"},{"location":"devops/vagrant/vagrant-quickstart/#installation","title":"Installation","text":"<p>See the official Vagrant downloads page Ubuntu/Debian: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install vagrant\n</code></pre></p> <p>You also need VirtualBox, VMware or Hyper-V on your machine.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#important-commands","title":"Important Commands","text":"<p>Initialize a new Vagrant environment by creating a Vagrantfile: <code>vagrant init</code> Starts and provisions the Vagrant environment: <code>vagrant up</code> Connects to machine via ssh: <code>vagrant ssh</code> Outputs status of the machine: <code>vagrant status</code> Suspends the machine: <code>vagrant suspend</code> Stops the machine: <code>vagrant halt</code> Stops and deletes all traces of the machine: <code>vagrant destroy</code> </p>"},{"location":"devops/vagrant/vagrant-quickstart/#boxes","title":"Boxes","text":"<p>Vagrant base images are called boxes. See the official Vagrant boxes page</p>"},{"location":"devops/vagrant/vagrant-quickstart/#synced-folders","title":"Synced Folders","text":"<p>By default, Vagrant will share your project directory (the directory with the Vagrantfile) to <code>/vagrant</code>.</p>"},{"location":"devops/vagrant/vagrant-quickstart/#vagrantfile","title":"Vagrantfile","text":"<p>Example Vagrantfiles:</p> Vagrantfile for Jenkins   This Vagrantfile installs Jenkins, AWS CLI, unzip and zip tools.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/focal64\"\n\n  # Port forwarding\n  config.vm.network \"forwarded_port\", guest: 8080, host: 8080\n\n  config.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n    # Update repositories\n    sudo apt-get update\n\n    # Install CA certificates (optional but recommended)\n    sudo apt-get install -y ca-certificates\n\n    # Install Java (a requirement for Jenkins)\n    sudo apt-get install -y openjdk-11-jdk\n\n    # Add Jenkins repository\n    wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n    sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'\n\n    sudo apt-get update\n\n    # Install Jenkins\n    sudo apt-get install -y jenkins\n\n    # Start Jenkins\n    sudo systemctl start jenkins\n\n    # Install necessary utilities\n    sudo apt-get install -y unzip\n    sudo apt-get install -y zip\n\n    # Install AWS CLI version 2\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    unzip awscliv2.zip\n    sudo ./aws/install\n  SHELL\nend\n</code></pre> Vagrantfile using Ansible for provisioning   This Vagrantfile uses an Ansible playbook for provisioning.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.box = \"ubuntu/bionic64\"\n  config.vm.network :forwarded_port, guest: 80, host: 8080\n  config.vm.network :forwarded_port, guest: 443, host: 8081\n  config.vm.network :forwarded_port, guest: 8080, host: 8082\n  config.vm.provision \"ansible\" do |ansible|\n    ansible.playbook = \"main.yml\"\n  end\n\nend\n</code></pre> Vagrantfile with advanced networking   This Vagrantfile brings up 2 VMs, assigns them static hostnames and IPs, allows root login and password authentication, and installs Ansible on one of the VMs.  <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n  # Define VMs\n  (1..2).each do |i|\n    config.vm.define \"vm#{i}\" do |vmconfig|\n\n      # Use CentOS 8\n      vmconfig.vm.box = \"generic/centos8\"\n\n      # Set hostname\n      vmconfig.vm.hostname = \"vm#{i}\"\n\n      # Set private network\n      vmconfig.vm.network \"private_network\", ip: \"192.168.56.1#{i}\"\n\n      # Sync project directory to /vagrant\n      vmconfig.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\"\n\n      # Enable provisioning with a shell script\n      vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n        echo 'vagrant:vagrant' | chpasswd\n        sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config\n        sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config\n        systemctl restart sshd\n      SHELL\n\n      if i == 1\n        vmconfig.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n          sudo yum update -y\n          sudo yum install -y epel-release\n          sudo yum install -y python3-pip gcc openssl-devel libffi-devel python3-devel\n          sudo pip3 install --upgrade pip\n          sudo pip3 install setuptools_rust\n          pip3 install ansible\n        SHELL\n      end\n    end\n  end\n\n  # Enable ssh agent forwarding\n  config.ssh.forward_agent = true\n\nend\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/","title":"Markdown Syntax","text":""},{"location":"misc/how-to-contribute/about-markdown/#links","title":"Links","text":"<pre><code>[Link Text](https://www.example.com)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#images","title":"Images","text":"<p>Put a <code>!</code> in front of the link syntax.</p> <pre><code>![Alt Text](https://www.example.com/image.png)\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#syntax-highlighting","title":"Syntax Highlighting","text":"<p>Insert the language name after the first set of backticks.</p> <pre><code># ```python\nimport os\nos.system(\"echo 'Hello World'\")\n</code></pre> <pre><code># ```bash\nexport HELLO=\"world\"\ncat some-file | grep \"hello\"\n</code></pre> <pre><code># ```yaml\nsome: key\nanother: key\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#tables","title":"Tables","text":"<pre><code>| name | value |\n| ---- | ----- |\n| a    | b     |\n</code></pre> <p>You can align headers to the left, center, or right by adding colons to the header syntax.</p> <pre><code>| --name-- | --value-- | description |\n| :------- | :-------: | ----------: |\n| a        |     b     |           c |\n</code></pre>"},{"location":"misc/how-to-contribute/about-markdown/#headers","title":"Headers","text":"<pre><code># H1 Header (biggest)\n\n## H2 Header\n\n### H3 Header\n\n#### H4 Header\n\n##### H5 Header\n\n###### H6 Header (smallest)\n</code></pre>"},{"location":"misc/how-to-contribute/about-mkdocs/","title":"creating new mkdocs pages","text":""},{"location":"misc/how-to-contribute/about-mkdocs/#how-to-add-a-new-page","title":"How to add a new Page","text":"<ol> <li>Create a new <code>.md</code> file under in the <code>docs/</code> folder</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section</li> <li>Do not put <code>docs/</code> prefix on the filepath</li> <li>Commit and push your changes to the <code>main</code> branch</li> <li>GitHub Action will automatically build and deploy the changes to the website.</li> </ol>"},{"location":"misc/how-to-contribute/about-mkdocs/#about-this-website","title":"About this website","text":"<p>Stack</p> Name Description mkDocs Docs static site generator Material for MkDocs Material theme for mkDocs"},{"location":"misc/how-to-contribute/about-mkdocs/#steps-to-run-it-locally","title":"Steps to run it locally","text":"<ol> <li>Make sure <code>python3</code> is installed</li> <li>Install python requirements    <pre><code>pip install mkdocstrings[python]\npip install mkdocs-material\n</code></pre></li> <li>Clone the repository &amp; <code>cd</code> into it</li> <li>Run the local server    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"misc/how-to-contribute/mkdocs-features/","title":"MkDocs and Material Theme Features","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#to-do-lists","title":"TO-DO Lists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> Aenean pretium efficitur erat</li> </ul>"},{"location":"misc/how-to-contribute/mkdocs-features/#admonitions-or-notes","title":"Admonitions or Notes","text":"<ul> <li>Supported Icon Types: note, abstract, info, tip, success, question, warning, failure, danger, error, example, quote</li> </ul> <p>Warning Note Header</p> <p>You can write notes like this to provide additional information or warnings.</p> <p>We've done it!</p> <p>Works for code blocks as well <pre><code>sudo apt install postgresql-client-&lt;version-number&gt;\n</code></pre></p> Collapsible Note (collapsed) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> Collapsible Note (expanded) <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p> <p>Tabbed Note</p> UbuntuRHEL/CentOS/AL2 <pre><code>apt install vim\n</code></pre> <pre><code>yum install vim\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#code-block-with-title","title":"Code block with title","text":"here is a codeblock title<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#buttons","title":"Buttons","text":"<p>Button Primary Button Emoji Button </p>"},{"location":"misc/how-to-contribute/mkdocs-features/#mermaid-diagrams","title":"Mermaid Diagrams","text":""},{"location":"misc/how-to-contribute/mkdocs-features/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#flow-chart","title":"Flow Chart","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"misc/how-to-contribute/mkdocs-features/#state-diagram","title":"State Diagram","text":"<pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre>"},{"location":"qa/","title":"Welcome to QA","text":"<p>Lorem ipsum.......</p>"},{"location":"qa/#compendium","title":"Compendium","text":"<p>placeholder-subject</p> <ul> <li>placeholder</li> </ul>"},{"location":"qa/placeholder-subject/placeholder/","title":"placeholder","text":""}]}